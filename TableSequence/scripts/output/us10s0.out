/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_0', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:13, 13.85s/it]Extractor Predicting: 2it [00:15,  6.75s/it]Extractor Predicting: 3it [00:16,  3.94s/it]Extractor Predicting: 4it [00:16,  2.63s/it]Extractor Predicting: 5it [00:17,  1.92s/it]Extractor Predicting: 6it [00:18,  1.48s/it]Extractor Predicting: 7it [00:18,  1.22s/it]Extractor Predicting: 8it [00:19,  1.04s/it]Extractor Predicting: 9it [00:20,  1.10it/s]Extractor Predicting: 10it [00:20,  1.21it/s]Extractor Predicting: 11it [00:21,  1.30it/s]Extractor Predicting: 12it [00:22,  1.37it/s]Extractor Predicting: 13it [00:22,  1.41it/s]Extractor Predicting: 14it [00:23,  1.44it/s]Extractor Predicting: 15it [00:24,  1.46it/s]Extractor Predicting: 16it [00:24,  1.49it/s]Extractor Predicting: 17it [00:25,  1.48it/s]Extractor Predicting: 18it [00:26,  1.47it/s]Extractor Predicting: 19it [00:26,  1.54it/s]Extractor Predicting: 20it [00:27,  1.55it/s]Extractor Predicting: 21it [00:27,  1.56it/s]Extractor Predicting: 22it [00:28,  1.55it/s]Extractor Predicting: 23it [00:29,  1.56it/s]Extractor Predicting: 24it [00:29,  1.58it/s]Extractor Predicting: 25it [00:30,  1.58it/s]Extractor Predicting: 26it [00:31,  1.55it/s]Extractor Predicting: 27it [00:31,  1.57it/s]Extractor Predicting: 28it [00:32,  1.56it/s]Extractor Predicting: 29it [00:32,  1.58it/s]Extractor Predicting: 30it [00:33,  1.52it/s]Extractor Predicting: 31it [00:34,  1.53it/s]Extractor Predicting: 32it [00:34,  1.55it/s]Extractor Predicting: 33it [00:35,  1.56it/s]Extractor Predicting: 34it [00:36,  1.54it/s]Extractor Predicting: 35it [00:36,  1.54it/s]Extractor Predicting: 36it [00:37,  1.55it/s]Extractor Predicting: 37it [00:38,  1.53it/s]Extractor Predicting: 38it [00:38,  1.53it/s]Extractor Predicting: 39it [00:39,  1.54it/s]Extractor Predicting: 40it [00:40,  1.55it/s]Extractor Predicting: 41it [00:40,  1.55it/s]Extractor Predicting: 42it [00:41,  1.52it/s]Extractor Predicting: 43it [00:42,  1.55it/s]Extractor Predicting: 44it [00:42,  1.45it/s]Extractor Predicting: 45it [00:43,  1.46it/s]Extractor Predicting: 46it [00:44,  1.48it/s]Extractor Predicting: 47it [00:44,  1.50it/s]Extractor Predicting: 48it [00:45,  1.52it/s]Extractor Predicting: 49it [00:46,  1.53it/s]Extractor Predicting: 50it [00:46,  1.56it/s]Extractor Predicting: 51it [00:47,  1.53it/s]Extractor Predicting: 52it [00:48,  1.52it/s]Extractor Predicting: 53it [00:48,  1.53it/s]Extractor Predicting: 54it [00:49,  1.53it/s]Extractor Predicting: 55it [00:50,  1.54it/s]Extractor Predicting: 56it [00:50,  1.55it/s]Extractor Predicting: 57it [00:51,  1.54it/s]Extractor Predicting: 58it [00:51,  1.57it/s]Extractor Predicting: 59it [00:52,  1.54it/s]Extractor Predicting: 60it [00:53,  1.52it/s]Extractor Predicting: 61it [00:53,  1.50it/s]Extractor Predicting: 62it [00:54,  1.50it/s]Extractor Predicting: 63it [00:55,  1.52it/s]Extractor Predicting: 64it [00:55,  1.48it/s]Extractor Predicting: 65it [00:56,  1.49it/s]Extractor Predicting: 66it [00:57,  1.47it/s]Extractor Predicting: 67it [00:58,  1.47it/s]Extractor Predicting: 68it [00:58,  1.46it/s]Extractor Predicting: 69it [00:59,  1.46it/s]Extractor Predicting: 70it [01:00,  1.45it/s]Extractor Predicting: 71it [01:00,  1.44it/s]Extractor Predicting: 72it [01:01,  1.43it/s]Extractor Predicting: 73it [01:02,  1.45it/s]Extractor Predicting: 74it [01:02,  1.41it/s]Extractor Predicting: 75it [01:03,  1.46it/s]Extractor Predicting: 76it [01:04,  1.47it/s]Extractor Predicting: 77it [01:04,  1.45it/s]Extractor Predicting: 78it [01:05,  1.48it/s]Extractor Predicting: 79it [01:06,  1.45it/s]Extractor Predicting: 80it [01:07,  1.44it/s]Extractor Predicting: 81it [01:07,  1.44it/s]Extractor Predicting: 82it [01:08,  1.46it/s]Extractor Predicting: 83it [01:09,  1.48it/s]Extractor Predicting: 84it [01:09,  1.50it/s]Extractor Predicting: 85it [01:10,  1.49it/s]Extractor Predicting: 86it [01:11,  1.45it/s]Extractor Predicting: 87it [01:11,  1.47it/s]Extractor Predicting: 88it [01:12,  1.51it/s]Extractor Predicting: 89it [01:12,  1.56it/s]Extractor Predicting: 90it [01:13,  1.55it/s]Extractor Predicting: 91it [01:14,  1.61it/s]Extractor Predicting: 92it [01:14,  1.66it/s]Extractor Predicting: 93it [01:15,  1.69it/s]Extractor Predicting: 94it [01:15,  1.71it/s]Extractor Predicting: 95it [01:16,  1.66it/s]Extractor Predicting: 96it [01:17,  1.69it/s]Extractor Predicting: 97it [01:17,  1.65it/s]Extractor Predicting: 98it [01:18,  1.64it/s]Extractor Predicting: 99it [01:18,  1.70it/s]Extractor Predicting: 100it [01:19,  1.70it/s]Extractor Predicting: 101it [01:20,  1.67it/s]Extractor Predicting: 102it [01:20,  1.61it/s]Extractor Predicting: 103it [01:21,  1.62it/s]Extractor Predicting: 104it [01:22,  1.58it/s]Extractor Predicting: 105it [01:22,  1.59it/s]Extractor Predicting: 106it [01:23,  1.60it/s]Extractor Predicting: 107it [01:23,  1.57it/s]Extractor Predicting: 108it [01:24,  1.61it/s]Extractor Predicting: 109it [01:25,  1.63it/s]Extractor Predicting: 110it [01:25,  1.64it/s]Extractor Predicting: 111it [01:26,  1.66it/s]Extractor Predicting: 112it [01:26,  1.67it/s]Extractor Predicting: 113it [01:27,  1.69it/s]Extractor Predicting: 114it [01:28,  1.65it/s]Extractor Predicting: 115it [01:28,  1.67it/s]Extractor Predicting: 116it [01:29,  1.63it/s]Extractor Predicting: 117it [01:30,  1.58it/s]Extractor Predicting: 118it [01:30,  1.56it/s]Extractor Predicting: 119it [01:31,  1.54it/s]Extractor Predicting: 120it [01:32,  1.52it/s]Extractor Predicting: 121it [01:32,  1.52it/s]Extractor Predicting: 122it [01:33,  1.53it/s]Extractor Predicting: 123it [01:34,  1.49it/s]Extractor Predicting: 124it [01:34,  1.38it/s]Extractor Predicting: 125it [01:35,  1.41it/s]Extractor Predicting: 126it [01:36,  1.41it/s]Extractor Predicting: 127it [01:36,  1.44it/s]Extractor Predicting: 128it [01:37,  1.45it/s]Extractor Predicting: 129it [01:38,  1.48it/s]Extractor Predicting: 130it [01:38,  1.51it/s]Extractor Predicting: 131it [01:39,  1.48it/s]Extractor Predicting: 132it [01:40,  1.48it/s]Extractor Predicting: 133it [01:40,  1.47it/s]Extractor Predicting: 134it [01:41,  1.48it/s]Extractor Predicting: 135it [01:42,  1.50it/s]Extractor Predicting: 136it [01:42,  1.49it/s]Extractor Predicting: 137it [01:43,  1.49it/s]Extractor Predicting: 138it [01:44,  1.51it/s]Extractor Predicting: 139it [01:44,  1.51it/s]Extractor Predicting: 140it [01:45,  1.54it/s]Extractor Predicting: 141it [01:46,  1.50it/s]Extractor Predicting: 142it [01:46,  1.50it/s]Extractor Predicting: 143it [01:47,  1.51it/s]Extractor Predicting: 144it [01:48,  1.56it/s]Extractor Predicting: 144it [01:48,  1.33it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5611510791366906,
  "recall": 0.13402061855670103,
  "score": 0.21636615811373094,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.62it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.50it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:17,  1.53it/s]Extractor Predicting: 29it [00:18,  1.51it/s]Extractor Predicting: 30it [00:19,  1.48it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:20,  1.49it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:21,  1.53it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:23,  1.57it/s]Extractor Predicting: 37it [00:23,  1.62it/s]Extractor Predicting: 38it [00:24,  1.61it/s]Extractor Predicting: 39it [00:25,  1.60it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:26,  1.60it/s]Extractor Predicting: 42it [00:26,  1.58it/s]Extractor Predicting: 43it [00:27,  1.58it/s]Extractor Predicting: 44it [00:28,  1.59it/s]Extractor Predicting: 45it [00:28,  1.57it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:30,  1.59it/s]Extractor Predicting: 48it [00:30,  1.59it/s]Extractor Predicting: 49it [00:31,  1.58it/s]Extractor Predicting: 50it [00:32,  1.56it/s]Extractor Predicting: 51it [00:32,  1.57it/s]Extractor Predicting: 52it [00:33,  1.57it/s]Extractor Predicting: 53it [00:33,  1.57it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:35,  1.55it/s]Extractor Predicting: 56it [00:35,  1.56it/s]Extractor Predicting: 57it [00:36,  1.58it/s]Extractor Predicting: 58it [00:37,  1.61it/s]Extractor Predicting: 59it [00:37,  1.65it/s]Extractor Predicting: 60it [00:38,  1.63it/s]Extractor Predicting: 61it [00:38,  1.60it/s]Extractor Predicting: 62it [00:39,  1.60it/s]Extractor Predicting: 63it [00:40,  1.61it/s]Extractor Predicting: 64it [00:40,  1.57it/s]Extractor Predicting: 65it [00:41,  1.58it/s]Extractor Predicting: 66it [00:42,  1.57it/s]Extractor Predicting: 67it [00:42,  1.58it/s]Extractor Predicting: 68it [00:43,  1.64it/s]Extractor Predicting: 69it [00:43,  1.62it/s]Extractor Predicting: 70it [00:44,  1.62it/s]Extractor Predicting: 71it [00:45,  1.60it/s]Extractor Predicting: 72it [00:45,  1.60it/s]Extractor Predicting: 73it [00:46,  1.57it/s]Extractor Predicting: 74it [00:47,  1.57it/s]Extractor Predicting: 75it [00:47,  1.56it/s]Extractor Predicting: 76it [00:48,  1.58it/s]Extractor Predicting: 77it [00:49,  1.54it/s]Extractor Predicting: 78it [00:49,  1.56it/s]Extractor Predicting: 79it [00:50,  1.56it/s]Extractor Predicting: 80it [00:50,  1.57it/s]Extractor Predicting: 81it [00:51,  1.56it/s]Extractor Predicting: 82it [00:52,  1.54it/s]Extractor Predicting: 83it [00:52,  1.55it/s]Extractor Predicting: 84it [00:53,  1.56it/s]Extractor Predicting: 85it [00:54,  1.59it/s]Extractor Predicting: 86it [00:54,  1.57it/s]Extractor Predicting: 87it [00:55,  1.56it/s]Extractor Predicting: 88it [00:56,  1.55it/s]Extractor Predicting: 89it [00:56,  1.58it/s]Extractor Predicting: 90it [00:57,  1.57it/s]Extractor Predicting: 91it [00:58,  1.57it/s]Extractor Predicting: 92it [00:58,  1.53it/s]Extractor Predicting: 93it [00:59,  1.57it/s]Extractor Predicting: 94it [00:59,  1.54it/s]Extractor Predicting: 95it [01:00,  1.54it/s]Extractor Predicting: 96it [01:01,  1.56it/s]Extractor Predicting: 97it [01:01,  1.57it/s]Extractor Predicting: 98it [01:02,  1.57it/s]Extractor Predicting: 99it [01:03,  1.44it/s]Extractor Predicting: 100it [01:03,  1.50it/s]Extractor Predicting: 101it [01:04,  1.54it/s]Extractor Predicting: 102it [01:05,  1.56it/s]Extractor Predicting: 103it [01:05,  1.53it/s]Extractor Predicting: 104it [01:06,  1.53it/s]Extractor Predicting: 105it [01:07,  1.54it/s]Extractor Predicting: 106it [01:07,  1.54it/s]Extractor Predicting: 107it [01:08,  1.54it/s]Extractor Predicting: 108it [01:09,  1.56it/s]Extractor Predicting: 109it [01:09,  1.55it/s]Extractor Predicting: 110it [01:10,  1.56it/s]Extractor Predicting: 111it [01:11,  1.54it/s]Extractor Predicting: 112it [01:11,  1.53it/s]Extractor Predicting: 113it [01:12,  1.52it/s]Extractor Predicting: 114it [01:13,  1.51it/s]Extractor Predicting: 115it [01:13,  1.53it/s]Extractor Predicting: 116it [01:14,  1.57it/s]Extractor Predicting: 117it [01:14,  1.59it/s]Extractor Predicting: 118it [01:15,  1.58it/s]Extractor Predicting: 119it [01:16,  1.58it/s]Extractor Predicting: 120it [01:16,  1.58it/s]Extractor Predicting: 121it [01:17,  1.59it/s]Extractor Predicting: 122it [01:18,  1.61it/s]Extractor Predicting: 123it [01:18,  1.63it/s]Extractor Predicting: 124it [01:19,  1.63it/s]Extractor Predicting: 125it [01:19,  1.63it/s]Extractor Predicting: 126it [01:20,  1.62it/s]Extractor Predicting: 127it [01:21,  1.64it/s]Extractor Predicting: 128it [01:21,  1.62it/s]Extractor Predicting: 129it [01:22,  1.57it/s]Extractor Predicting: 130it [01:23,  1.56it/s]Extractor Predicting: 131it [01:23,  1.60it/s]Extractor Predicting: 132it [01:24,  1.59it/s]Extractor Predicting: 133it [01:24,  1.61it/s]Extractor Predicting: 134it [01:25,  1.62it/s]Extractor Predicting: 135it [01:26,  1.63it/s]Extractor Predicting: 136it [01:26,  1.62it/s]Extractor Predicting: 137it [01:27,  1.62it/s]Extractor Predicting: 138it [01:27,  1.59it/s]Extractor Predicting: 139it [01:28,  1.61it/s]Extractor Predicting: 140it [01:29,  1.66it/s]Extractor Predicting: 141it [01:29,  1.66it/s]Extractor Predicting: 142it [01:30,  1.63it/s]Extractor Predicting: 143it [01:30,  1.61it/s]Extractor Predicting: 144it [01:31,  1.61it/s]Extractor Predicting: 145it [01:32,  1.63it/s]Extractor Predicting: 146it [01:32,  1.65it/s]Extractor Predicting: 147it [01:33,  1.65it/s]Extractor Predicting: 148it [01:34,  1.60it/s]Extractor Predicting: 149it [01:34,  1.62it/s]Extractor Predicting: 150it [01:35,  1.61it/s]Extractor Predicting: 151it [01:35,  1.60it/s]Extractor Predicting: 152it [01:36,  1.61it/s]Extractor Predicting: 153it [01:37,  1.58it/s]Extractor Predicting: 154it [01:37,  1.61it/s]Extractor Predicting: 155it [01:38,  1.62it/s]Extractor Predicting: 156it [01:38,  1.65it/s]Extractor Predicting: 157it [01:39,  1.65it/s]Extractor Predicting: 158it [01:40,  1.71it/s]Extractor Predicting: 159it [01:40,  1.67it/s]Extractor Predicting: 160it [01:41,  1.63it/s]Extractor Predicting: 161it [01:42,  1.60it/s]Extractor Predicting: 162it [01:42,  1.58it/s]Extractor Predicting: 163it [01:43,  1.61it/s]Extractor Predicting: 164it [01:43,  1.61it/s]Extractor Predicting: 165it [01:44,  1.63it/s]Extractor Predicting: 166it [01:45,  1.65it/s]Extractor Predicting: 167it [01:45,  1.64it/s]Extractor Predicting: 168it [01:46,  1.59it/s]Extractor Predicting: 169it [01:47,  1.60it/s]Extractor Predicting: 170it [01:47,  1.59it/s]Extractor Predicting: 171it [01:48,  1.59it/s]Extractor Predicting: 172it [01:48,  1.60it/s]Extractor Predicting: 173it [01:49,  1.58it/s]Extractor Predicting: 174it [01:50,  1.58it/s]Extractor Predicting: 175it [01:50,  1.55it/s]Extractor Predicting: 176it [01:51,  1.52it/s]Extractor Predicting: 177it [01:52,  1.57it/s]Extractor Predicting: 178it [01:52,  1.56it/s]Extractor Predicting: 179it [01:53,  1.59it/s]Extractor Predicting: 180it [01:53,  1.61it/s]Extractor Predicting: 181it [01:54,  1.58it/s]Extractor Predicting: 182it [01:55,  1.58it/s]Extractor Predicting: 183it [01:55,  1.58it/s]Extractor Predicting: 184it [01:56,  1.56it/s]Extractor Predicting: 185it [01:57,  1.55it/s]Extractor Predicting: 186it [01:57,  1.55it/s]Extractor Predicting: 187it [01:58,  1.56it/s]Extractor Predicting: 188it [01:59,  1.53it/s]Extractor Predicting: 189it [01:59,  1.53it/s]Extractor Predicting: 190it [02:00,  1.41it/s]Extractor Predicting: 191it [02:01,  1.47it/s]Extractor Predicting: 192it [02:01,  1.54it/s]Extractor Predicting: 193it [02:02,  1.50it/s]Extractor Predicting: 194it [02:03,  1.50it/s]Extractor Predicting: 195it [02:03,  1.55it/s]Extractor Predicting: 196it [02:04,  1.56it/s]Extractor Predicting: 197it [02:05,  1.58it/s]Extractor Predicting: 198it [02:05,  1.56it/s]Extractor Predicting: 199it [02:06,  1.59it/s]Extractor Predicting: 200it [02:06,  1.64it/s]Extractor Predicting: 201it [02:07,  1.66it/s]Extractor Predicting: 202it [02:08,  1.66it/s]Extractor Predicting: 203it [02:08,  1.63it/s]Extractor Predicting: 204it [02:09,  1.62it/s]Extractor Predicting: 205it [02:09,  1.62it/s]Extractor Predicting: 206it [02:10,  1.61it/s]Extractor Predicting: 207it [02:11,  1.61it/s]Extractor Predicting: 208it [02:11,  1.58it/s]Extractor Predicting: 209it [02:12,  1.59it/s]Extractor Predicting: 210it [02:13,  1.59it/s]Extractor Predicting: 211it [02:13,  1.58it/s]Extractor Predicting: 212it [02:14,  1.60it/s]Extractor Predicting: 213it [02:15,  1.59it/s]Extractor Predicting: 214it [02:15,  1.57it/s]Extractor Predicting: 215it [02:16,  1.56it/s]Extractor Predicting: 216it [02:16,  1.59it/s]Extractor Predicting: 217it [02:17,  1.60it/s]Extractor Predicting: 218it [02:18,  1.59it/s]Extractor Predicting: 219it [02:18,  1.57it/s]Extractor Predicting: 220it [02:19,  1.58it/s]Extractor Predicting: 221it [02:20,  1.59it/s]Extractor Predicting: 222it [02:20,  1.58it/s]Extractor Predicting: 223it [02:21,  1.61it/s]Extractor Predicting: 224it [02:21,  1.60it/s]Extractor Predicting: 225it [02:22,  1.58it/s]Extractor Predicting: 226it [02:23,  1.57it/s]Extractor Predicting: 227it [02:23,  1.58it/s]Extractor Predicting: 228it [02:24,  1.59it/s]Extractor Predicting: 229it [02:25,  1.60it/s]Extractor Predicting: 230it [02:25,  1.61it/s]Extractor Predicting: 231it [02:26,  1.59it/s]Extractor Predicting: 232it [02:26,  1.59it/s]Extractor Predicting: 233it [02:27,  1.59it/s]Extractor Predicting: 234it [02:28,  1.61it/s]Extractor Predicting: 235it [02:28,  1.61it/s]Extractor Predicting: 236it [02:29,  1.62it/s]Extractor Predicting: 237it [02:30,  1.60it/s]Extractor Predicting: 238it [02:30,  1.60it/s]Extractor Predicting: 239it [02:31,  1.58it/s]Extractor Predicting: 240it [02:31,  1.61it/s]Extractor Predicting: 241it [02:32,  1.61it/s]Extractor Predicting: 242it [02:33,  1.64it/s]Extractor Predicting: 243it [02:33,  1.63it/s]Extractor Predicting: 244it [02:34,  1.62it/s]Extractor Predicting: 245it [02:35,  1.61it/s]Extractor Predicting: 246it [02:35,  1.60it/s]Extractor Predicting: 247it [02:36,  1.62it/s]Extractor Predicting: 248it [02:36,  1.62it/s]Extractor Predicting: 249it [02:37,  1.62it/s]Extractor Predicting: 250it [02:38,  1.64it/s]Extractor Predicting: 251it [02:38,  1.60it/s]Extractor Predicting: 252it [02:39,  1.59it/s]Extractor Predicting: 253it [02:39,  1.64it/s]Extractor Predicting: 254it [02:40,  1.62it/s]Extractor Predicting: 255it [02:41,  1.60it/s]Extractor Predicting: 256it [02:41,  1.59it/s]Extractor Predicting: 257it [02:42,  1.61it/s]Extractor Predicting: 258it [02:43,  1.63it/s]Extractor Predicting: 259it [02:43,  1.65it/s]Extractor Predicting: 260it [02:44,  1.65it/s]Extractor Predicting: 261it [02:44,  1.62it/s]Extractor Predicting: 262it [02:45,  1.63it/s]Extractor Predicting: 263it [02:46,  1.63it/s]Extractor Predicting: 264it [02:46,  1.64it/s]Extractor Predicting: 265it [02:47,  1.64it/s]Extractor Predicting: 266it [02:47,  1.68it/s]Extractor Predicting: 267it [02:48,  1.63it/s]Extractor Predicting: 268it [02:49,  1.63it/s]Extractor Predicting: 269it [02:49,  1.64it/s]Extractor Predicting: 270it [02:50,  1.64it/s]Extractor Predicting: 271it [02:50,  1.67it/s]Extractor Predicting: 272it [02:51,  1.70it/s]Extractor Predicting: 273it [02:52,  1.66it/s]Extractor Predicting: 274it [02:52,  1.66it/s]Extractor Predicting: 275it [02:53,  1.64it/s]Extractor Predicting: 276it [02:54,  1.64it/s]Extractor Predicting: 277it [02:54,  1.63it/s]Extractor Predicting: 278it [02:55,  1.63it/s]Extractor Predicting: 279it [02:55,  1.61it/s]Extractor Predicting: 280it [02:56,  1.62it/s]Extractor Predicting: 281it [02:57,  1.61it/s]Extractor Predicting: 281it [02:57,  1.59it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5160949868073879,
  "recall": 0.1450393000148302,
  "score": 0.22644130585783745,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.24it/s]Extractor Predicting: 2it [00:01,  1.35it/s]Extractor Predicting: 3it [00:02,  1.43it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.52it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.18181818181818182,
  "recall": 0.01556420233463035,
  "score": 0.02867383512544803,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:45, 16.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:34<03:43, 17.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:50<03:19, 16.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:04<02:52, 15.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:21<02:41, 16.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:38<02:29, 16.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:56<02:15, 16.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:13<01:59, 17.01s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:30<01:41, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:43<01:19, 15.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:59<01:03, 15.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:16<00:48, 16.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:33<00:33, 16.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:49<00:16, 16.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:10<00:00, 17.67s/it]Generating: 100%|██████████| 15/15 [04:10<00:00, 16.70s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : composer . Context : Later in the year , the band formed with drummer Joe Pascual who had a career in jazz , writing many popular pieces for jazz concert pianists including " Pascual , " " Le Chateau de France " , and " Jean Chateau " . Head Entity : Jean Chationau de France , Tail Entity : Joe Pascual .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : participant in .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : platform . Context : The game is a spin on " The Legend of Zelda " and is a remake of the " Zelda " series . Head Entity : The Legend of Zelda , Tail Entity : Nintendo .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : taxon rank . Context : The Second Prince of the Crown became a monarch , although his son , Prince of York , inherited many of his posts during Parliament . Head Entity : King , Tail Entity : Crown .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.8301630434782609, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8315217391304348, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.842391304347826, 'errors': {'', '(\'Best Picture\', \'nominated for\', \'\', \'" How I Met Your Mother " is nominated for Best Picture and has been nominated for several Academy Awards .\')', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8958333333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 480, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 557, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : position held .', 'success_rate': 0.7981770833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 521, 'raw': 672}
{'target': 600, 'success': 548, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 601, 'raw': 768}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.7825520833333334, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : religion .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 12898
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12998, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:01,  1.15s/it]Extractor Estimating: 2it [00:04,  2.49s/it]Extractor Estimating: 3it [00:05,  1.65s/it]Extractor Estimating: 4it [00:05,  1.28s/it]Extractor Estimating: 5it [00:06,  1.06s/it]Extractor Estimating: 6it [00:07,  1.07it/s]Extractor Estimating: 7it [00:07,  1.20it/s]Extractor Estimating: 8it [00:08,  1.27it/s]Extractor Estimating: 9it [00:09,  1.35it/s]Extractor Estimating: 10it [00:09,  1.45it/s]Extractor Estimating: 11it [00:10,  1.45it/s]Extractor Estimating: 12it [00:11,  1.48it/s]Extractor Estimating: 13it [00:11,  1.49it/s]Extractor Estimating: 14it [00:12,  1.58it/s]Extractor Estimating: 15it [00:12,  1.58it/s]Extractor Estimating: 16it [00:13,  1.60it/s]Extractor Estimating: 17it [00:14,  1.60it/s]Extractor Estimating: 18it [00:14,  1.57it/s]Extractor Estimating: 19it [00:15,  1.59it/s]Extractor Estimating: 20it [00:16,  1.57it/s]Extractor Estimating: 21it [00:17,  1.20it/s]Extractor Estimating: 22it [00:18,  1.26it/s]Extractor Estimating: 23it [00:18,  1.35it/s]Extractor Estimating: 24it [00:19,  1.41it/s]Extractor Estimating: 25it [00:20,  1.43it/s]Extractor Estimating: 26it [00:20,  1.43it/s]Extractor Estimating: 27it [00:21,  1.40it/s]Extractor Estimating: 28it [00:22,  1.42it/s]Extractor Estimating: 29it [00:22,  1.41it/s]Extractor Estimating: 30it [00:23,  1.39it/s]Extractor Estimating: 31it [00:24,  1.44it/s]Extractor Estimating: 32it [00:24,  1.45it/s]Extractor Estimating: 33it [00:25,  1.46it/s]Extractor Estimating: 34it [00:26,  1.15it/s]Extractor Estimating: 35it [00:27,  1.24it/s]Extractor Estimating: 36it [00:28,  1.27it/s]Extractor Estimating: 37it [00:29,  1.31it/s]Extractor Estimating: 38it [00:29,  1.38it/s]Extractor Estimating: 39it [00:30,  1.35it/s]Extractor Estimating: 40it [00:31,  1.38it/s]Extractor Estimating: 41it [00:31,  1.39it/s]Extractor Estimating: 42it [00:32,  1.41it/s]Extractor Estimating: 43it [00:33,  1.40it/s]Extractor Estimating: 44it [00:33,  1.41it/s]Extractor Estimating: 45it [00:34,  1.39it/s]Extractor Estimating: 46it [00:35,  1.43it/s]Extractor Estimating: 47it [00:36,  1.48it/s]Extractor Estimating: 48it [00:36,  1.49it/s]Extractor Estimating: 49it [00:37,  1.56it/s]Extractor Estimating: 50it [00:37,  1.57it/s]Extractor Estimating: 51it [00:38,  1.57it/s]Extractor Estimating: 52it [00:39,  1.59it/s]Extractor Estimating: 53it [00:39,  1.54it/s]Extractor Estimating: 54it [00:40,  1.54it/s]Extractor Estimating: 55it [00:41,  1.59it/s]Extractor Estimating: 56it [00:41,  1.59it/s]Extractor Estimating: 57it [00:42,  1.65it/s]Extractor Estimating: 58it [00:42,  1.62it/s]Extractor Estimating: 59it [00:43,  1.62it/s]Extractor Estimating: 60it [00:44,  1.61it/s]Extractor Estimating: 61it [00:44,  1.61it/s]Extractor Estimating: 62it [00:45,  1.51it/s]Extractor Estimating: 63it [00:46,  1.52it/s]Extractor Estimating: 64it [00:46,  1.53it/s]Extractor Estimating: 65it [00:47,  1.53it/s]Extractor Estimating: 66it [00:48,  1.55it/s]Extractor Estimating: 67it [00:48,  1.52it/s]Extractor Estimating: 68it [00:49,  1.59it/s]Extractor Estimating: 69it [00:49,  1.61it/s]Extractor Estimating: 70it [00:50,  1.61it/s]Extractor Estimating: 71it [00:51,  1.60it/s]Extractor Estimating: 72it [00:51,  1.62it/s]Extractor Estimating: 73it [00:52,  1.61it/s]Extractor Estimating: 74it [00:53,  1.60it/s]Extractor Estimating: 75it [00:53,  1.61it/s]Extractor Estimating: 76it [00:54,  1.67it/s]Extractor Estimating: 77it [00:54,  1.59it/s]Extractor Estimating: 78it [00:55,  1.65it/s]Extractor Estimating: 79it [00:56,  1.66it/s]Extractor Estimating: 80it [00:56,  1.67it/s]Extractor Estimating: 81it [00:57,  1.65it/s]Extractor Estimating: 82it [00:57,  1.70it/s]Extractor Estimating: 83it [00:58,  1.68it/s]Extractor Estimating: 84it [00:58,  1.69it/s]Extractor Estimating: 85it [00:59,  1.72it/s]Extractor Estimating: 86it [01:00,  1.73it/s]Extractor Estimating: 87it [01:00,  1.65it/s]Extractor Estimating: 88it [01:01,  1.67it/s]Extractor Estimating: 89it [01:01,  1.76it/s]Extractor Estimating: 90it [01:02,  1.75it/s]Extractor Estimating: 91it [01:03,  1.73it/s]Extractor Estimating: 92it [01:03,  1.74it/s]Extractor Estimating: 93it [01:04,  1.72it/s]Extractor Estimating: 94it [01:04,  1.74it/s]Extractor Estimating: 95it [01:05,  1.73it/s]Extractor Estimating: 96it [01:05,  1.70it/s]Extractor Estimating: 97it [01:06,  1.65it/s]Extractor Estimating: 98it [01:07,  1.58it/s]Extractor Estimating: 99it [01:07,  1.58it/s]Extractor Estimating: 100it [01:08,  1.63it/s]Extractor Estimating: 101it [01:09,  1.62it/s]Extractor Estimating: 102it [01:09,  1.66it/s]Extractor Estimating: 103it [01:10,  1.65it/s]Extractor Estimating: 104it [01:10,  1.63it/s]Extractor Estimating: 105it [01:11,  1.72it/s]Extractor Estimating: 106it [01:12,  1.73it/s]Extractor Estimating: 107it [01:12,  1.71it/s]Extractor Estimating: 108it [01:13,  1.69it/s]Extractor Estimating: 109it [01:13,  1.64it/s]Extractor Estimating: 110it [01:14,  1.60it/s]Extractor Estimating: 111it [01:15,  1.64it/s]Extractor Estimating: 112it [01:15,  1.68it/s]Extractor Estimating: 113it [01:16,  1.67it/s]Extractor Estimating: 114it [01:16,  1.67it/s]Extractor Estimating: 115it [01:17,  1.65it/s]Extractor Estimating: 116it [01:18,  1.65it/s]Extractor Estimating: 117it [01:18,  1.67it/s]Extractor Estimating: 118it [01:19,  1.70it/s]Extractor Estimating: 119it [01:19,  1.67it/s]Extractor Estimating: 120it [01:20,  1.70it/s]Extractor Estimating: 121it [01:21,  1.67it/s]Extractor Estimating: 122it [01:21,  1.71it/s]Extractor Estimating: 123it [01:22,  1.71it/s]Extractor Estimating: 124it [01:22,  1.74it/s]Extractor Estimating: 125it [01:23,  1.80it/s]Extractor Estimating: 126it [01:23,  1.80it/s]Extractor Estimating: 127it [01:24,  1.71it/s]Extractor Estimating: 128it [01:25,  1.73it/s]Extractor Estimating: 129it [01:25,  1.65it/s]Extractor Estimating: 130it [01:26,  1.69it/s]Extractor Estimating: 131it [01:26,  1.72it/s]Extractor Estimating: 132it [01:27,  1.76it/s]Extractor Estimating: 133it [01:27,  1.81it/s]Extractor Estimating: 134it [01:28,  1.79it/s]Extractor Estimating: 135it [01:29,  1.78it/s]Extractor Estimating: 136it [01:29,  1.81it/s]Extractor Estimating: 137it [01:30,  1.85it/s]Extractor Estimating: 138it [01:30,  1.89it/s]Extractor Estimating: 139it [01:31,  1.81it/s]Extractor Estimating: 140it [01:31,  1.76it/s]Extractor Estimating: 141it [01:32,  1.75it/s]Extractor Estimating: 142it [01:32,  1.81it/s]Extractor Estimating: 143it [01:33,  1.75it/s]Extractor Estimating: 144it [01:34,  1.79it/s]Extractor Estimating: 145it [01:34,  1.83it/s]Extractor Estimating: 146it [01:35,  1.84it/s]Extractor Estimating: 147it [01:35,  1.80it/s]Extractor Estimating: 148it [01:36,  1.81it/s]Extractor Estimating: 149it [01:36,  1.81it/s]Extractor Estimating: 150it [01:37,  1.78it/s]Extractor Estimating: 151it [01:38,  1.52it/s]Extractor Estimating: 152it [01:38,  1.48it/s]Extractor Estimating: 153it [01:39,  1.47it/s]Extractor Estimating: 154it [01:40,  1.49it/s]Extractor Estimating: 155it [01:40,  1.49it/s]Extractor Estimating: 156it [01:41,  1.54it/s]Extractor Estimating: 157it [01:42,  1.50it/s]Extractor Estimating: 158it [01:42,  1.50it/s]Extractor Estimating: 159it [01:43,  1.50it/s]Extractor Estimating: 160it [01:44,  1.53it/s]Extractor Estimating: 161it [01:44,  1.49it/s]Extractor Estimating: 162it [01:45,  1.45it/s]Extractor Estimating: 163it [01:46,  1.47it/s]Extractor Estimating: 164it [01:47,  1.45it/s]Extractor Estimating: 165it [01:47,  1.51it/s]Extractor Estimating: 166it [01:48,  1.53it/s]Extractor Estimating: 167it [01:48,  1.51it/s]Extractor Estimating: 168it [01:49,  1.47it/s]Extractor Estimating: 169it [01:50,  1.48it/s]Extractor Estimating: 170it [01:50,  1.48it/s]Extractor Estimating: 171it [01:51,  1.52it/s]Extractor Estimating: 172it [01:52,  1.51it/s]Extractor Estimating: 173it [01:52,  1.53it/s]Extractor Estimating: 174it [01:53,  1.59it/s]Extractor Estimating: 175it [01:54,  1.53it/s]Extractor Estimating: 176it [01:54,  1.56it/s]Extractor Estimating: 177it [01:55,  1.57it/s]Extractor Estimating: 178it [01:56,  1.58it/s]Extractor Estimating: 179it [01:56,  1.63it/s]Extractor Estimating: 180it [01:57,  1.64it/s]Extractor Estimating: 181it [01:57,  1.68it/s]Extractor Estimating: 182it [01:58,  1.71it/s]Extractor Estimating: 183it [01:58,  1.72it/s]Extractor Estimating: 184it [01:59,  1.72it/s]Extractor Estimating: 185it [02:01,  1.04it/s]Extractor Estimating: 186it [02:01,  1.15it/s]Extractor Estimating: 187it [02:02,  1.28it/s]Extractor Estimating: 188it [02:03,  1.33it/s]Extractor Estimating: 189it [02:03,  1.42it/s]Extractor Estimating: 190it [02:04,  1.48it/s]Extractor Estimating: 191it [02:05,  1.49it/s]Extractor Estimating: 192it [02:05,  1.53it/s]Extractor Estimating: 193it [02:06,  1.58it/s]Extractor Estimating: 194it [02:07,  1.53it/s]Extractor Estimating: 195it [02:07,  1.53it/s]Extractor Estimating: 196it [02:08,  1.59it/s]Extractor Estimating: 197it [02:08,  1.60it/s]Extractor Estimating: 198it [02:09,  1.64it/s]Extractor Estimating: 199it [02:10,  1.64it/s]Extractor Estimating: 200it [02:10,  1.64it/s]Extractor Estimating: 201it [02:11,  1.57it/s]Extractor Estimating: 202it [02:12,  1.51it/s]Extractor Estimating: 203it [02:12,  1.53it/s]Extractor Estimating: 204it [02:13,  1.50it/s]Extractor Estimating: 205it [02:14,  1.48it/s]Extractor Estimating: 206it [02:14,  1.45it/s]Extractor Estimating: 207it [02:15,  1.47it/s]Extractor Estimating: 208it [02:16,  1.48it/s]Extractor Estimating: 209it [02:16,  1.50it/s]Extractor Estimating: 210it [02:17,  1.49it/s]Extractor Estimating: 211it [02:18,  1.49it/s]Extractor Estimating: 212it [02:18,  1.46it/s]Extractor Estimating: 213it [02:19,  1.45it/s]Extractor Estimating: 214it [02:20,  1.41it/s]Extractor Estimating: 215it [02:21,  1.42it/s]Extractor Estimating: 216it [02:21,  1.48it/s]Extractor Estimating: 217it [02:22,  1.50it/s]Extractor Estimating: 218it [02:22,  1.53it/s]Extractor Estimating: 219it [02:23,  1.44it/s]Extractor Estimating: 220it [02:24,  1.45it/s]Extractor Estimating: 221it [02:25,  1.47it/s]Extractor Estimating: 222it [02:25,  1.42it/s]Extractor Estimating: 223it [02:26,  1.41it/s]Extractor Estimating: 224it [02:27,  1.48it/s]Extractor Estimating: 225it [02:27,  1.43it/s]Extractor Estimating: 226it [02:28,  1.54it/s]Extractor Estimating: 227it [02:28,  1.60it/s]Extractor Estimating: 228it [02:29,  1.65it/s]Extractor Estimating: 229it [02:30,  1.70it/s]Extractor Estimating: 230it [02:30,  1.65it/s]Extractor Estimating: 231it [02:31,  1.69it/s]Extractor Estimating: 232it [02:31,  1.69it/s]Extractor Estimating: 233it [02:32,  1.68it/s]Extractor Estimating: 234it [02:33,  1.68it/s]Extractor Estimating: 235it [02:33,  1.75it/s]Extractor Estimating: 236it [02:34,  1.73it/s]Extractor Estimating: 237it [02:34,  1.80it/s]Extractor Estimating: 238it [02:35,  1.81it/s]Extractor Estimating: 239it [02:35,  1.75it/s]Extractor Estimating: 240it [02:36,  1.77it/s]Extractor Estimating: 241it [02:36,  1.74it/s]Extractor Estimating: 242it [02:37,  1.77it/s]Extractor Estimating: 243it [02:38,  1.77it/s]Extractor Estimating: 244it [02:38,  1.76it/s]Extractor Estimating: 245it [02:39,  1.70it/s]Extractor Estimating: 246it [02:39,  1.70it/s]Extractor Estimating: 247it [02:40,  1.78it/s]Extractor Estimating: 248it [02:40,  1.75it/s]Extractor Estimating: 249it [02:41,  1.85it/s]Extractor Estimating: 250it [02:42,  1.71it/s]Extractor Estimating: 251it [02:42,  1.67it/s]Extractor Estimating: 252it [02:43,  1.63it/s]Extractor Estimating: 253it [02:44,  1.62it/s]Extractor Estimating: 254it [02:44,  1.62it/s]Extractor Estimating: 255it [02:45,  1.58it/s]Extractor Estimating: 256it [02:45,  1.55it/s]Extractor Estimating: 257it [02:46,  1.59it/s]Extractor Estimating: 258it [02:47,  1.60it/s]Extractor Estimating: 259it [02:47,  1.58it/s]Extractor Estimating: 260it [02:48,  1.63it/s]Extractor Estimating: 261it [02:49,  1.60it/s]Extractor Estimating: 262it [02:49,  1.50it/s]Extractor Estimating: 263it [02:50,  1.55it/s]Extractor Estimating: 264it [02:51,  1.48it/s]Extractor Estimating: 265it [02:51,  1.47it/s]Extractor Estimating: 266it [02:52,  1.50it/s]Extractor Estimating: 267it [02:53,  1.55it/s]Extractor Estimating: 268it [02:53,  1.58it/s]Extractor Estimating: 269it [02:54,  1.54it/s]Extractor Estimating: 270it [02:54,  1.60it/s]Extractor Estimating: 271it [02:55,  1.61it/s]Extractor Estimating: 272it [02:56,  1.59it/s]Extractor Estimating: 273it [02:56,  1.57it/s]Extractor Estimating: 274it [02:57,  1.55it/s]Extractor Estimating: 275it [02:58,  1.55it/s]Extractor Estimating: 276it [02:58,  1.56it/s]Extractor Estimating: 277it [02:59,  1.62it/s]Extractor Estimating: 278it [02:59,  1.63it/s]Extractor Estimating: 279it [03:00,  1.63it/s]Extractor Estimating: 280it [03:01,  1.56it/s]Extractor Estimating: 281it [03:01,  1.54it/s]Extractor Estimating: 282it [03:02,  1.59it/s]Extractor Estimating: 283it [03:03,  1.62it/s]Extractor Estimating: 284it [03:03,  1.61it/s]Extractor Estimating: 285it [03:04,  1.60it/s]Extractor Estimating: 286it [03:05,  1.56it/s]Extractor Estimating: 287it [03:05,  1.57it/s]Extractor Estimating: 288it [03:06,  1.61it/s]Extractor Estimating: 289it [03:06,  1.68it/s]Extractor Estimating: 290it [03:07,  1.69it/s]Extractor Estimating: 291it [03:07,  1.70it/s]Extractor Estimating: 292it [03:08,  1.62it/s]Extractor Estimating: 293it [03:09,  1.60it/s]Extractor Estimating: 294it [03:09,  1.57it/s]Extractor Estimating: 295it [03:10,  1.58it/s]Extractor Estimating: 296it [03:11,  1.55it/s]Extractor Estimating: 297it [03:11,  1.56it/s]Extractor Estimating: 298it [03:12,  1.60it/s]Extractor Estimating: 299it [03:13,  1.64it/s]Extractor Estimating: 300it [03:13,  1.66it/s]Extractor Estimating: 301it [03:14,  1.64it/s]Extractor Estimating: 302it [03:14,  1.61it/s]Extractor Estimating: 303it [03:15,  1.59it/s]Extractor Estimating: 304it [03:16,  1.59it/s]Extractor Estimating: 305it [03:16,  1.61it/s]Extractor Estimating: 306it [03:17,  1.63it/s]Extractor Estimating: 307it [03:17,  1.67it/s]Extractor Estimating: 308it [03:18,  1.71it/s]Extractor Estimating: 309it [03:19,  1.56it/s]Extractor Estimating: 310it [03:19,  1.57it/s]Extractor Estimating: 311it [03:20,  1.58it/s]Extractor Estimating: 312it [03:21,  1.60it/s]Extractor Estimating: 313it [03:21,  1.62it/s]Extractor Estimating: 314it [03:22,  1.59it/s]Extractor Estimating: 315it [03:23,  1.60it/s]Extractor Estimating: 316it [03:23,  1.55it/s]Extractor Estimating: 317it [03:24,  1.54it/s]Extractor Estimating: 318it [03:25,  1.54it/s]Extractor Estimating: 319it [03:25,  1.57it/s]Extractor Estimating: 320it [03:26,  1.52it/s]Extractor Estimating: 321it [03:26,  1.52it/s]Extractor Estimating: 322it [03:27,  1.58it/s]Extractor Estimating: 323it [03:28,  1.59it/s]Extractor Estimating: 324it [03:28,  1.59it/s]Extractor Estimating: 325it [03:29,  1.57it/s]Extractor Estimating: 326it [03:30,  1.59it/s]Extractor Estimating: 327it [03:30,  1.62it/s]Extractor Estimating: 328it [03:31,  1.67it/s]Extractor Estimating: 329it [03:31,  1.55it/s]Extractor Estimating: 330it [03:32,  1.57it/s]Extractor Estimating: 331it [03:33,  1.60it/s]Extractor Estimating: 332it [03:33,  1.67it/s]Extractor Estimating: 333it [03:34,  1.73it/s]Extractor Estimating: 334it [03:34,  1.72it/s]Extractor Estimating: 335it [03:35,  1.76it/s]Extractor Estimating: 336it [03:35,  1.77it/s]Extractor Estimating: 337it [03:36,  1.80it/s]Extractor Estimating: 338it [03:37,  1.78it/s]Extractor Estimating: 339it [03:37,  1.78it/s]Extractor Estimating: 340it [03:38,  1.84it/s]Extractor Estimating: 341it [03:38,  1.80it/s]Extractor Estimating: 342it [03:39,  1.85it/s]Extractor Estimating: 343it [03:39,  1.89it/s]Extractor Estimating: 344it [03:40,  1.82it/s]Extractor Estimating: 345it [03:40,  1.83it/s]Extractor Estimating: 346it [03:41,  1.80it/s]Extractor Estimating: 347it [03:42,  1.72it/s]Extractor Estimating: 348it [03:42,  1.71it/s]Extractor Estimating: 349it [03:43,  1.75it/s]Extractor Estimating: 350it [03:43,  1.70it/s]Extractor Estimating: 351it [03:44,  1.53it/s]Extractor Estimating: 352it [03:45,  1.56it/s]Extractor Estimating: 353it [03:45,  1.57it/s]Extractor Estimating: 354it [03:46,  1.64it/s]Extractor Estimating: 355it [03:47,  1.61it/s]Extractor Estimating: 356it [03:47,  1.55it/s]Extractor Estimating: 357it [03:48,  1.53it/s]Extractor Estimating: 358it [03:49,  1.51it/s]Extractor Estimating: 359it [03:49,  1.55it/s]Extractor Estimating: 360it [03:50,  1.56it/s]Extractor Estimating: 361it [03:50,  1.57it/s]Extractor Estimating: 362it [03:51,  1.61it/s]Extractor Estimating: 363it [03:52,  1.56it/s]Extractor Estimating: 364it [03:52,  1.54it/s]Extractor Estimating: 365it [03:53,  1.49it/s]Extractor Estimating: 366it [03:54,  1.56it/s]Extractor Estimating: 367it [03:54,  1.55it/s]Extractor Estimating: 368it [03:55,  1.55it/s]Extractor Estimating: 369it [03:56,  1.51it/s]Extractor Estimating: 370it [03:56,  1.54it/s]Extractor Estimating: 371it [03:57,  1.59it/s]Extractor Estimating: 372it [03:58,  1.57it/s]Extractor Estimating: 373it [03:58,  1.56it/s]Extractor Estimating: 374it [03:59,  1.56it/s]Extractor Estimating: 375it [04:00,  1.55it/s]Extractor Estimating: 375it [04:00,  1.56it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7482 mean pseudo reward: 0.9296434645037885
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 24909
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25009, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25009, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.326, loss:1058.8019
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.054, loss:1004.1507
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.083, loss:1028.4774
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.066, loss:992.0227
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.066, loss:963.5767
>> valid entity prec:0.5908, rec:0.5761, f1:0.5834
>> valid relation prec:0.4394, rec:0.1350, f1:0.2065
>> valid relation with NER prec:0.4394, rec:0.1350, f1:0.2065
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.453, loss:1011.8216
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.043, loss:928.4138
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.060, loss:941.1041
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.067, loss:952.4722
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.072, loss:925.3779
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5604, rec:0.5730, f1:0.5666
>> valid relation prec:0.3937, rec:0.1496, f1:0.2168
>> valid relation with NER prec:0.3937, rec:0.1496, f1:0.2168
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 164, avg_time 2.446, loss:888.2823
g_step 1200, step 264, avg_time 1.056, loss:929.4307
g_step 1300, step 52, avg_time 1.065, loss:891.0888
g_step 1400, step 152, avg_time 1.068, loss:859.1999
g_step 1500, step 252, avg_time 1.056, loss:848.8301
>> valid entity prec:0.5473, rec:0.6005, f1:0.5727
>> valid relation prec:0.2996, rec:0.1272, f1:0.1786
>> valid relation with NER prec:0.2996, rec:0.1272, f1:0.1786
g_step 1600, step 40, avg_time 2.451, loss:821.0111
g_step 1700, step 140, avg_time 1.064, loss:797.5402
g_step 1800, step 240, avg_time 1.053, loss:802.3319
g_step 1900, step 28, avg_time 1.063, loss:819.7490
g_step 2000, step 128, avg_time 1.052, loss:755.0937
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5811, rec:0.6051, f1:0.5928
>> valid relation prec:0.3715, rec:0.1350, f1:0.1980
>> valid relation with NER prec:0.3715, rec:0.1350, f1:0.1980
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 228, avg_time 2.453, loss:773.5814
g_step 2200, step 16, avg_time 1.068, loss:746.6727
g_step 2300, step 116, avg_time 1.068, loss:691.0318
g_step 2400, step 216, avg_time 1.063, loss:745.5105
g_step 2500, step 4, avg_time 1.058, loss:739.8298
>> valid entity prec:0.5964, rec:0.5777, f1:0.5869
>> valid relation prec:0.3136, rec:0.1201, f1:0.1736
>> valid relation with NER prec:0.3136, rec:0.1201, f1:0.1736
g_step 2600, step 104, avg_time 2.446, loss:663.5799
g_step 2700, step 204, avg_time 1.062, loss:700.4504
g_step 2800, step 304, avg_time 1.065, loss:703.4243
g_step 2900, step 92, avg_time 1.072, loss:653.4014
g_step 3000, step 192, avg_time 1.058, loss:667.0463
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6166, rec:0.5482, f1:0.5804
>> valid relation prec:0.3369, rec:0.1341, f1:0.1918
>> valid relation with NER prec:0.3369, rec:0.1341, f1:0.1918
g_step 3100, step 292, avg_time 2.446, loss:668.3247
g_step 3200, step 80, avg_time 1.053, loss:614.7129
g_step 3300, step 180, avg_time 1.074, loss:604.2952
g_step 3400, step 280, avg_time 1.060, loss:662.6640
g_step 3500, step 68, avg_time 1.059, loss:591.9275
>> valid entity prec:0.5605, rec:0.6028, f1:0.5809
>> valid relation prec:0.3307, rec:0.1585, f1:0.2143
>> valid relation with NER prec:0.3307, rec:0.1585, f1:0.2143
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 168, avg_time 2.455, loss:578.9114
g_step 3700, step 268, avg_time 1.065, loss:606.0569
g_step 3800, step 56, avg_time 1.058, loss:600.7656
g_step 3900, step 156, avg_time 1.059, loss:570.0171
g_step 4000, step 256, avg_time 1.063, loss:588.9129
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5918, rec:0.5555, f1:0.5731
>> valid relation prec:0.3066, rec:0.1281, f1:0.1807
>> valid relation with NER prec:0.3066, rec:0.1281, f1:0.1807
g_step 4100, step 44, avg_time 2.446, loss:560.7883
g_step 4200, step 144, avg_time 1.058, loss:542.7753
g_step 4300, step 244, avg_time 1.067, loss:559.7636
g_step 4400, step 32, avg_time 1.053, loss:563.8990
g_step 4500, step 132, avg_time 1.066, loss:514.9889
>> valid entity prec:0.5804, rec:0.5931, f1:0.5867
>> valid relation prec:0.2883, rec:0.1327, f1:0.1817
>> valid relation with NER prec:0.2883, rec:0.1327, f1:0.1817
g_step 4600, step 232, avg_time 2.450, loss:534.7086
g_step 4700, step 20, avg_time 1.058, loss:524.3994
g_step 4800, step 120, avg_time 1.068, loss:499.4252
g_step 4900, step 220, avg_time 1.062, loss:509.9050
g_step 5000, step 8, avg_time 1.058, loss:514.4686
learning rate was adjusted to 0.0008
>> valid entity prec:0.6004, rec:0.5796, f1:0.5898
>> valid relation prec:0.3090, rec:0.1295, f1:0.1825
>> valid relation with NER prec:0.3090, rec:0.1295, f1:0.1825
g_step 5100, step 108, avg_time 2.450, loss:458.1668
g_step 5200, step 208, avg_time 1.059, loss:498.8003
g_step 5300, step 308, avg_time 1.069, loss:500.2322
g_step 5400, step 96, avg_time 1.064, loss:459.3355
g_step 5500, step 196, avg_time 1.054, loss:470.2959
>> valid entity prec:0.5861, rec:0.5315, f1:0.5575
>> valid relation prec:0.2900, rec:0.1284, f1:0.1780
>> valid relation with NER prec:0.2900, rec:0.1284, f1:0.1780
g_step 5600, step 296, avg_time 2.441, loss:479.9001
g_step 5700, step 84, avg_time 1.069, loss:443.1213
g_step 5800, step 184, avg_time 1.063, loss:448.6036
g_step 5900, step 284, avg_time 1.060, loss:466.1402
g_step 6000, step 72, avg_time 1.060, loss:428.3891
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5811, rec:0.5416, f1:0.5606
>> valid relation prec:0.2707, rec:0.1238, f1:0.1699
>> valid relation with NER prec:0.2707, rec:0.1238, f1:0.1699
g_step 6100, step 172, avg_time 2.451, loss:427.6767
g_step 6200, step 272, avg_time 1.054, loss:453.0764
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:20:21 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:20:21 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-20-21_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:20:22 - WARNING - datasets.builder -   Using custom data configuration default-9df12f57fcdf2eaf
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-9df12f57fcdf2eaf/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:20:22,514 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:20:22,515 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:20:22,515 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:20:22,516 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:20:22,524 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:20:22,528 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:20:22,528 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:20:22,528 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:20:22,528 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:20:22,528 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:20:22,528 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:20:22,658 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:20:25,750 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:20:25,750 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-9df12f57fcdf2eaf/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 13:20:25 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x149ed3e600e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.97ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.00ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.59ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.93ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.13ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.29ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.38ba/s]100%|██████████| 8/8 [00:01<00:00,  5.23ba/s]100%|██████████| 8/8 [00:01<00:00,  4.29ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.93ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.23ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.34ba/s]100%|██████████| 4/4 [00:00<00:00,  5.40ba/s]100%|██████████| 4/4 [00:00<00:00,  4.89ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.17ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.61ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.90ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.06ba/s]100%|██████████| 8/8 [00:00<00:00, 10.56ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.27ba/s] 50%|█████     | 2/4 [00:00<00:00,  9.17ba/s]100%|██████████| 4/4 [00:00<00:00, 11.49ba/s]100%|██████████| 4/4 [00:00<00:00, 10.82ba/s]
[INFO|trainer.py:414] 2023-08-28 13:20:29,961 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:20:29,980 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:20:29,980 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 13:20:29,980 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:20:29,980 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:20:29,980 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:20:29,980 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:20:29,980 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:58,  3.28it/s]  0%|          | 2/585 [00:00<02:52,  3.39it/s]  1%|          | 3/585 [00:00<02:49,  3.43it/s]  1%|          | 4/585 [00:01<02:48,  3.44it/s]  1%|          | 5/585 [00:01<02:48,  3.45it/s]  1%|          | 6/585 [00:01<02:47,  3.46it/s]  1%|          | 7/585 [00:02<02:47,  3.46it/s]  1%|▏         | 8/585 [00:02<02:47,  3.45it/s]  2%|▏         | 9/585 [00:02<02:46,  3.46it/s]  2%|▏         | 10/585 [00:02<02:46,  3.46it/s]  2%|▏         | 11/585 [00:03<02:45,  3.46it/s]  2%|▏         | 12/585 [00:03<02:45,  3.46it/s]  2%|▏         | 13/585 [00:03<02:45,  3.46it/s]  2%|▏         | 14/585 [00:04<02:45,  3.46it/s]  3%|▎         | 15/585 [00:04<02:44,  3.46it/s]  3%|▎         | 16/585 [00:04<02:44,  3.46it/s]  3%|▎         | 17/585 [00:04<02:44,  3.46it/s]  3%|▎         | 18/585 [00:05<02:43,  3.46it/s]  3%|▎         | 19/585 [00:05<02:43,  3.46it/s]  3%|▎         | 20/585 [00:05<02:42,  3.47it/s]  4%|▎         | 21/585 [00:06<02:42,  3.46it/s]  4%|▍         | 22/585 [00:06<02:42,  3.47it/s]  4%|▍         | 23/585 [00:06<02:42,  3.46it/s]  4%|▍         | 24/585 [00:06<02:41,  3.47it/s]  4%|▍         | 25/585 [00:07<02:41,  3.47it/s]  4%|▍         | 26/585 [00:07<02:41,  3.45it/s]  5%|▍         | 27/585 [00:07<02:41,  3.45it/s]  5%|▍         | 28/585 [00:08<02:41,  3.46it/s]  5%|▍         | 29/585 [00:08<02:40,  3.46it/s]  5%|▌         | 30/585 [00:08<02:40,  3.46it/s]  5%|▌         | 31/585 [00:08<02:39,  3.47it/s]  5%|▌         | 32/585 [00:09<02:39,  3.46it/s]  6%|▌         | 33/585 [00:09<02:39,  3.47it/s]  6%|▌         | 34/585 [00:09<02:38,  3.47it/s]  6%|▌         | 35/585 [00:10<02:38,  3.46it/s]  6%|▌         | 36/585 [00:10<02:38,  3.46it/s]  6%|▋         | 37/585 [00:10<02:38,  3.46it/s]  6%|▋         | 38/585 [00:10<02:37,  3.47it/s]  7%|▋         | 39/585 [00:11<02:37,  3.46it/s]  7%|▋         | 40/585 [00:11<02:37,  3.46it/s]  7%|▋         | 41/585 [00:11<02:37,  3.46it/s]  7%|▋         | 42/585 [00:12<02:36,  3.46it/s]  7%|▋         | 43/585 [00:12<02:36,  3.46it/s]  8%|▊         | 44/585 [00:12<02:36,  3.45it/s]  8%|▊         | 45/585 [00:13<02:36,  3.45it/s]  8%|▊         | 46/585 [00:13<02:35,  3.46it/s]  8%|▊         | 47/585 [00:13<02:35,  3.46it/s]  8%|▊         | 48/585 [00:13<02:35,  3.46it/s]  8%|▊         | 49/585 [00:14<02:34,  3.46it/s]  9%|▊         | 50/585 [00:14<02:34,  3.46it/s]  9%|▊         | 51/585 [00:14<02:34,  3.46it/s]  9%|▉         | 52/585 [00:15<02:33,  3.46it/s]  9%|▉         | 53/585 [00:15<02:33,  3.46it/s]  9%|▉         | 54/585 [00:15<02:33,  3.46it/s]  9%|▉         | 55/585 [00:15<02:33,  3.46it/s] 10%|▉         | 56/585 [00:16<02:32,  3.47it/s] 10%|▉         | 57/585 [00:16<02:32,  3.46it/s] 10%|▉         | 58/585 [00:16<02:32,  3.47it/s] 10%|█         | 59/585 [00:17<02:31,  3.46it/s] 10%|█         | 60/585 [00:17<02:31,  3.47it/s] 10%|█         | 61/585 [00:17<02:31,  3.45it/s] 11%|█         | 62/585 [00:17<02:31,  3.45it/s] 11%|█         | 63/585 [00:18<02:31,  3.46it/s] 11%|█         | 64/585 [00:18<02:30,  3.46it/s] 11%|█         | 65/585 [00:18<02:30,  3.46it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.46it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 69/585 [00:19<02:29,  3.45it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.45it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.45it/s] 13%|█▎        | 76/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.45it/s] 14%|█▎        | 79/585 [00:22<02:26,  3.45it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 81/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.45it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.44it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.44it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.44it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.44it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.45it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.45it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.45it/s] 15%|█▌        | 90/585 [00:26<02:26,  3.38it/s] 16%|█▌        | 91/585 [00:26<02:25,  3.40it/s] 16%|█▌        | 92/585 [00:26<02:24,  3.42it/s] 16%|█▌        | 93/585 [00:26<02:23,  3.43it/s] 16%|█▌        | 94/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.43it/s] 16%|█▋        | 96/585 [00:27<02:22,  3.44it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.44it/s] 17%|█▋        | 98/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 99/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 100/585 [00:28<02:20,  3.45it/s] 17%|█▋        | 101/585 [00:29<02:20,  3.45it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.45it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.45it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.45it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.45it/s] 18%|█▊        | 108/585 [00:31<02:18,  3.45it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.45it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.45it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.45it/s] 19%|█▉        | 114/585 [00:33<02:17,  3.44it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.44it/s] 20%|█▉        | 116/585 [00:33<02:17,  3.42it/s] 20%|██        | 117/585 [00:33<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 13:21:03,925 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:21:03,925 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 13:21:03,925 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.79it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.91it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.06it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.26it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.62it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.14it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.01it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.03it/s][A
 11%|█         | 48/437 [00:01<00:08, 47.01it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.05it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.01it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.01it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.10it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.85it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.82it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.59it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.73it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.82it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.95it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.97it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.00it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.96it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.89it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.79it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.65it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.64it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.69it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.79it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.86it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.94it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.91it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.99it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.84it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.80it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.61it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.72it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.77it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.82it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.89it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.94it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.88it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.84it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.77it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.63it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.61it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.65it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.78it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.96it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.95it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.87it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.74it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.64it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.66it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.68it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.77it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.67it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.76it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.88it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.94it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.98it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.88it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.70it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.58it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.65it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.76it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.82it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.86it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.85it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.88it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.83it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.71it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.61it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.64it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.57it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.73it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.80it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.81it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.90it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.89it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.68it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.57it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.59it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.63it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.74it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.72it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.77it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:43<02:16,  3.43it/s]
100%|██████████| 437/437 [00:09<00:00, 46.77it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:21:13,284 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 13:21:13,303 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:21:15,527 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:21:15,544 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:21:15,553 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:51<41:33,  5.34s/it] 20%|██        | 119/585 [00:51<29:43,  3.83s/it] 21%|██        | 120/585 [00:51<21:26,  2.77s/it] 21%|██        | 121/585 [00:51<15:38,  2.02s/it] 21%|██        | 122/585 [00:52<11:36,  1.50s/it] 21%|██        | 123/585 [00:52<08:46,  1.14s/it] 21%|██        | 124/585 [00:52<06:47,  1.13it/s] 21%|██▏       | 125/585 [00:53<05:24,  1.42it/s] 22%|██▏       | 126/585 [00:53<04:26,  1.72it/s] 22%|██▏       | 127/585 [00:53<03:45,  2.03it/s] 22%|██▏       | 128/585 [00:53<03:17,  2.32it/s] 22%|██▏       | 129/585 [00:54<02:57,  2.57it/s] 22%|██▏       | 130/585 [00:54<02:44,  2.77it/s] 22%|██▏       | 131/585 [00:54<02:34,  2.95it/s] 23%|██▎       | 132/585 [00:55<02:26,  3.08it/s] 23%|██▎       | 133/585 [00:55<02:21,  3.19it/s] 23%|██▎       | 134/585 [00:55<02:18,  3.27it/s] 23%|██▎       | 135/585 [00:55<02:15,  3.32it/s] 23%|██▎       | 136/585 [00:56<02:13,  3.36it/s] 23%|██▎       | 137/585 [00:56<02:11,  3.39it/s] 24%|██▎       | 138/585 [00:56<02:10,  3.42it/s] 24%|██▍       | 139/585 [00:57<02:10,  3.42it/s] 24%|██▍       | 140/585 [00:57<02:09,  3.44it/s] 24%|██▍       | 141/585 [00:57<02:09,  3.42it/s] 24%|██▍       | 142/585 [00:57<02:08,  3.44it/s] 24%|██▍       | 143/585 [00:58<02:08,  3.44it/s] 25%|██▍       | 144/585 [00:58<02:07,  3.45it/s] 25%|██▍       | 145/585 [00:58<02:07,  3.45it/s] 25%|██▍       | 146/585 [00:59<02:06,  3.46it/s] 25%|██▌       | 147/585 [00:59<02:06,  3.46it/s] 25%|██▌       | 148/585 [00:59<02:06,  3.46it/s] 25%|██▌       | 149/585 [00:59<02:05,  3.46it/s] 26%|██▌       | 150/585 [01:00<02:05,  3.46it/s] 26%|██▌       | 151/585 [01:00<02:05,  3.46it/s] 26%|██▌       | 152/585 [01:00<02:05,  3.45it/s] 26%|██▌       | 153/585 [01:01<02:04,  3.46it/s] 26%|██▋       | 154/585 [01:01<02:04,  3.46it/s] 26%|██▋       | 155/585 [01:01<02:04,  3.46it/s] 27%|██▋       | 156/585 [01:02<02:04,  3.46it/s] 27%|██▋       | 157/585 [01:02<02:03,  3.47it/s] 27%|██▋       | 158/585 [01:02<02:03,  3.47it/s] 27%|██▋       | 159/585 [01:02<02:02,  3.47it/s] 27%|██▋       | 160/585 [01:03<02:02,  3.46it/s] 28%|██▊       | 161/585 [01:03<02:02,  3.46it/s] 28%|██▊       | 162/585 [01:03<02:02,  3.46it/s] 28%|██▊       | 163/585 [01:04<02:02,  3.46it/s] 28%|██▊       | 164/585 [01:04<02:01,  3.46it/s] 28%|██▊       | 165/585 [01:04<02:01,  3.45it/s] 28%|██▊       | 166/585 [01:04<02:01,  3.46it/s] 29%|██▊       | 167/585 [01:05<02:00,  3.46it/s] 29%|██▊       | 168/585 [01:05<02:00,  3.46it/s] 29%|██▉       | 169/585 [01:05<02:00,  3.46it/s] 29%|██▉       | 170/585 [01:06<01:59,  3.46it/s] 29%|██▉       | 171/585 [01:06<01:59,  3.45it/s] 29%|██▉       | 172/585 [01:06<01:59,  3.45it/s] 30%|██▉       | 173/585 [01:06<01:59,  3.45it/s] 30%|██▉       | 174/585 [01:07<01:59,  3.45it/s] 30%|██▉       | 175/585 [01:07<01:58,  3.45it/s] 30%|███       | 176/585 [01:07<01:58,  3.45it/s] 30%|███       | 177/585 [01:08<01:58,  3.45it/s] 30%|███       | 178/585 [01:08<01:57,  3.45it/s] 31%|███       | 179/585 [01:08<01:57,  3.45it/s] 31%|███       | 180/585 [01:08<01:57,  3.45it/s] 31%|███       | 181/585 [01:09<01:56,  3.46it/s] 31%|███       | 182/585 [01:09<01:56,  3.45it/s] 31%|███▏      | 183/585 [01:09<01:56,  3.46it/s] 31%|███▏      | 184/585 [01:10<01:56,  3.46it/s] 32%|███▏      | 185/585 [01:10<01:56,  3.44it/s] 32%|███▏      | 186/585 [01:10<01:55,  3.45it/s] 32%|███▏      | 187/585 [01:10<01:55,  3.45it/s] 32%|███▏      | 188/585 [01:11<01:55,  3.45it/s] 32%|███▏      | 189/585 [01:11<01:54,  3.45it/s] 32%|███▏      | 190/585 [01:11<01:54,  3.45it/s] 33%|███▎      | 191/585 [01:12<01:54,  3.45it/s] 33%|███▎      | 192/585 [01:12<01:53,  3.45it/s] 33%|███▎      | 193/585 [01:12<01:53,  3.45it/s] 33%|███▎      | 194/585 [01:13<01:53,  3.45it/s] 33%|███▎      | 195/585 [01:13<01:52,  3.46it/s] 34%|███▎      | 196/585 [01:13<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:13<01:52,  3.46it/s] 34%|███▍      | 198/585 [01:14<01:52,  3.44it/s] 34%|███▍      | 199/585 [01:14<01:51,  3.45it/s] 34%|███▍      | 200/585 [01:14<01:51,  3.45it/s] 34%|███▍      | 201/585 [01:15<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:15<01:50,  3.45it/s] 35%|███▍      | 203/585 [01:15<01:50,  3.45it/s] 35%|███▍      | 204/585 [01:15<01:50,  3.45it/s] 35%|███▌      | 205/585 [01:16<01:50,  3.45it/s] 35%|███▌      | 206/585 [01:16<01:49,  3.45it/s] 35%|███▌      | 207/585 [01:16<01:49,  3.45it/s] 36%|███▌      | 208/585 [01:17<01:49,  3.45it/s] 36%|███▌      | 209/585 [01:17<01:49,  3.44it/s] 36%|███▌      | 210/585 [01:17<01:48,  3.45it/s] 36%|███▌      | 211/585 [01:17<01:48,  3.45it/s] 36%|███▌      | 212/585 [01:18<01:48,  3.45it/s] 36%|███▋      | 213/585 [01:18<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:18<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:19<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:19<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:19<01:46,  3.45it/s] 37%|███▋      | 218/585 [01:19<01:46,  3.46it/s] 37%|███▋      | 219/585 [01:20<01:46,  3.45it/s] 38%|███▊      | 220/585 [01:20<01:45,  3.45it/s] 38%|███▊      | 221/585 [01:20<01:45,  3.45it/s] 38%|███▊      | 222/585 [01:21<01:45,  3.45it/s] 38%|███▊      | 223/585 [01:21<01:44,  3.45it/s] 38%|███▊      | 224/585 [01:21<01:44,  3.45it/s] 38%|███▊      | 225/585 [01:22<01:44,  3.45it/s] 39%|███▊      | 226/585 [01:22<01:43,  3.46it/s] 39%|███▉      | 227/585 [01:22<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:22<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:23<01:43,  3.45it/s] 39%|███▉      | 230/585 [01:23<01:43,  3.45it/s] 39%|███▉      | 231/585 [01:23<01:42,  3.44it/s] 40%|███▉      | 232/585 [01:24<01:42,  3.44it/s] 40%|███▉      | 233/585 [01:24<01:41,  3.45it/s] 40%|████      | 234/585 [01:24<01:41,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 13:21:54,639 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:21:54,639 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 13:21:54,639 >>   Batch size = 8
{'eval_loss': 1.0318280458450317, 'eval_runtime': 9.3444, 'eval_samples_per_second': 373.699, 'eval_steps_per_second': 46.766, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.57it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.75it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.90it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.26it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.76it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.48it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.37it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.02it/s][A
 11%|█         | 48/437 [00:01<00:08, 47.06it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.09it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.11it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.95it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 45.07it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 45.74it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.15it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.49it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.54it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.59it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.74it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.79it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.78it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.73it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.74it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.92it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.95it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.95it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.06it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.05it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.94it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.81it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.76it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.80it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.96it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.92it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.92it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.04it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.79it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.02it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.00it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.76it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.69it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.91it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.03it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.96it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.03it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.04it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.93it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.90it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.84it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.71it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.83it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.98it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.02it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.97it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.01it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.99it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.98it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.92it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.85it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.84it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.84it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.94it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.97it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.00it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.96it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.85it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.76it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.78it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.83it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.97it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.97it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.98it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.01it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.00it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.00it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.89it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.84it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.79it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.91it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.92it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.95it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.94it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.98it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.95it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.90it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.79it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:33<01:41,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 46.79it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:22:04,004 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 13:22:04,021 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:22:06,588 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:22:06,612 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:22:06,626 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:42<31:45,  5.44s/it] 40%|████      | 236/585 [01:42<22:40,  3.90s/it] 41%|████      | 237/585 [01:42<16:19,  2.81s/it] 41%|████      | 238/585 [01:42<11:53,  2.06s/it] 41%|████      | 239/585 [01:43<08:48,  1.53s/it] 41%|████      | 240/585 [01:43<06:38,  1.15s/it] 41%|████      | 241/585 [01:43<05:07,  1.12it/s] 41%|████▏     | 242/585 [01:44<04:04,  1.40it/s] 42%|████▏     | 243/585 [01:44<03:20,  1.71it/s] 42%|████▏     | 244/585 [01:44<02:49,  2.02it/s] 42%|████▏     | 245/585 [01:44<02:27,  2.30it/s] 42%|████▏     | 246/585 [01:45<02:12,  2.56it/s] 42%|████▏     | 247/585 [01:45<02:02,  2.77it/s] 42%|████▏     | 248/585 [01:45<01:54,  2.95it/s] 43%|████▎     | 249/585 [01:46<01:48,  3.09it/s] 43%|████▎     | 250/585 [01:46<01:44,  3.19it/s] 43%|████▎     | 251/585 [01:46<01:42,  3.27it/s] 43%|████▎     | 252/585 [01:46<01:40,  3.33it/s] 43%|████▎     | 253/585 [01:47<01:38,  3.37it/s] 43%|████▎     | 254/585 [01:47<01:37,  3.39it/s] 44%|████▎     | 255/585 [01:47<01:36,  3.41it/s] 44%|████▍     | 256/585 [01:48<01:35,  3.43it/s] 44%|████▍     | 257/585 [01:48<01:35,  3.44it/s] 44%|████▍     | 258/585 [01:48<01:35,  3.43it/s] 44%|████▍     | 259/585 [01:49<01:34,  3.44it/s] 44%|████▍     | 260/585 [01:49<01:34,  3.45it/s] 45%|████▍     | 261/585 [01:49<01:33,  3.45it/s] 45%|████▍     | 262/585 [01:49<01:33,  3.45it/s] 45%|████▍     | 263/585 [01:50<01:33,  3.46it/s] 45%|████▌     | 264/585 [01:50<01:32,  3.46it/s] 45%|████▌     | 265/585 [01:50<01:32,  3.46it/s] 45%|████▌     | 266/585 [01:51<01:32,  3.46it/s] 46%|████▌     | 267/585 [01:51<01:31,  3.46it/s] 46%|████▌     | 268/585 [01:51<01:31,  3.46it/s] 46%|████▌     | 269/585 [01:51<01:31,  3.45it/s] 46%|████▌     | 270/585 [01:52<01:31,  3.46it/s] 46%|████▋     | 271/585 [01:52<01:30,  3.46it/s] 46%|████▋     | 272/585 [01:52<01:30,  3.47it/s] 47%|████▋     | 273/585 [01:53<01:30,  3.46it/s] 47%|████▋     | 274/585 [01:53<01:29,  3.46it/s] 47%|████▋     | 275/585 [01:53<01:29,  3.46it/s] 47%|████▋     | 276/585 [01:53<01:29,  3.47it/s] 47%|████▋     | 277/585 [01:54<01:28,  3.46it/s] 48%|████▊     | 278/585 [01:54<01:28,  3.46it/s] 48%|████▊     | 279/585 [01:54<01:28,  3.46it/s] 48%|████▊     | 280/585 [01:55<01:28,  3.45it/s] 48%|████▊     | 281/585 [01:55<01:27,  3.45it/s] 48%|████▊     | 282/585 [01:55<01:27,  3.46it/s] 48%|████▊     | 283/585 [01:55<01:27,  3.46it/s] 49%|████▊     | 284/585 [01:56<01:27,  3.46it/s] 49%|████▊     | 285/585 [01:56<01:26,  3.46it/s] 49%|████▉     | 286/585 [01:56<01:26,  3.46it/s] 49%|████▉     | 287/585 [01:57<01:26,  3.46it/s] 49%|████▉     | 288/585 [01:57<01:25,  3.46it/s] 49%|████▉     | 289/585 [01:57<01:25,  3.46it/s] 50%|████▉     | 290/585 [01:57<01:25,  3.46it/s] 50%|████▉     | 291/585 [01:58<01:25,  3.45it/s] 50%|████▉     | 292/585 [01:58<01:24,  3.45it/s] 50%|█████     | 293/585 [01:58<01:24,  3.46it/s] 50%|█████     | 294/585 [01:59<01:24,  3.46it/s] 50%|█████     | 295/585 [01:59<01:23,  3.46it/s] 51%|█████     | 296/585 [01:59<01:23,  3.46it/s] 51%|█████     | 297/585 [01:59<01:23,  3.46it/s] 51%|█████     | 298/585 [02:00<01:22,  3.47it/s] 51%|█████     | 299/585 [02:00<01:22,  3.46it/s] 51%|█████▏    | 300/585 [02:00<01:22,  3.47it/s] 51%|█████▏    | 301/585 [02:01<01:22,  3.46it/s] 52%|█████▏    | 302/585 [02:01<01:21,  3.45it/s] 52%|█████▏    | 303/585 [02:01<01:21,  3.45it/s] 52%|█████▏    | 304/585 [02:02<01:21,  3.46it/s] 52%|█████▏    | 305/585 [02:02<01:21,  3.46it/s] 52%|█████▏    | 306/585 [02:02<01:20,  3.46it/s] 52%|█████▏    | 307/585 [02:02<01:20,  3.46it/s] 53%|█████▎    | 308/585 [02:03<01:20,  3.46it/s] 53%|█████▎    | 309/585 [02:03<01:19,  3.46it/s] 53%|█████▎    | 310/585 [02:03<01:19,  3.46it/s] 53%|█████▎    | 311/585 [02:04<01:19,  3.46it/s] 53%|█████▎    | 312/585 [02:04<01:18,  3.46it/s] 54%|█████▎    | 313/585 [02:04<01:18,  3.46it/s] 54%|█████▎    | 314/585 [02:04<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:05<01:18,  3.45it/s] 54%|█████▍    | 316/585 [02:05<01:17,  3.45it/s] 54%|█████▍    | 317/585 [02:05<01:17,  3.46it/s] 54%|█████▍    | 318/585 [02:06<01:17,  3.46it/s] 55%|█████▍    | 319/585 [02:06<01:16,  3.46it/s] 55%|█████▍    | 320/585 [02:06<01:16,  3.46it/s] 55%|█████▍    | 321/585 [02:06<01:16,  3.46it/s] 55%|█████▌    | 322/585 [02:07<01:16,  3.46it/s] 55%|█████▌    | 323/585 [02:07<01:15,  3.46it/s] 55%|█████▌    | 324/585 [02:07<01:15,  3.46it/s] 56%|█████▌    | 325/585 [02:08<01:15,  3.46it/s] 56%|█████▌    | 326/585 [02:08<01:15,  3.44it/s] 56%|█████▌    | 327/585 [02:08<01:14,  3.45it/s] 56%|█████▌    | 328/585 [02:08<01:14,  3.45it/s] 56%|█████▌    | 329/585 [02:09<01:14,  3.45it/s] 56%|█████▋    | 330/585 [02:09<01:13,  3.45it/s] 57%|█████▋    | 331/585 [02:09<01:13,  3.46it/s] 57%|█████▋    | 332/585 [02:10<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:10<01:12,  3.45it/s] 57%|█████▋    | 334/585 [02:10<01:12,  3.45it/s] 57%|█████▋    | 335/585 [02:10<01:12,  3.45it/s] 57%|█████▋    | 336/585 [02:11<01:12,  3.45it/s] 58%|█████▊    | 337/585 [02:11<01:12,  3.42it/s] 58%|█████▊    | 338/585 [02:11<01:12,  3.43it/s] 58%|█████▊    | 339/585 [02:12<01:11,  3.44it/s] 58%|█████▊    | 340/585 [02:12<01:11,  3.44it/s] 58%|█████▊    | 341/585 [02:12<01:10,  3.44it/s] 58%|█████▊    | 342/585 [02:13<01:10,  3.44it/s] 59%|█████▊    | 343/585 [02:13<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:13<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:13<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:14<01:09,  3.45it/s] 59%|█████▉    | 347/585 [02:14<01:09,  3.45it/s] 59%|█████▉    | 348/585 [02:14<01:08,  3.44it/s] 60%|█████▉    | 349/585 [02:15<01:08,  3.44it/s] 60%|█████▉    | 350/585 [02:15<01:08,  3.45it/s] 60%|██████    | 351/585 [02:15<01:07,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 13:22:45,667 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:22:45,668 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 13:22:45,668 >>   Batch size = 8
{'eval_loss': 1.0423589944839478, 'eval_runtime': 9.3333, 'eval_samples_per_second': 374.144, 'eval_steps_per_second': 46.822, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.80it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.08it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.09it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.29it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.73it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.53it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.36it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.93it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.76it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.91it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.03it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.97it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.95it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.95it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.92it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.93it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.86it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.77it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.95it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.93it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.78it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.87it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.82it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.79it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.91it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.88it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.84it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.99it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.94it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.92it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.83it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.86it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.78it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.80it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.81it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.74it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.83it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.87it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.82it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.89it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.79it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.82it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.79it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.72it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.69it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.65it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.71it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.81it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.93it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.86it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.75it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.82it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.82it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.82it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.86it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.82it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.79it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.87it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.88it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.92it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.84it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.76it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.81it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.81it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.81it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.89it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.92it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.84it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.88it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.95it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.87it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.83it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.89it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.81it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.88it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.87it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.85it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.90it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.91it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.89it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.85it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.89it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.78it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.92it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.87it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.80it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:25<01:07,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 46.80it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:22:55,035 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 13:22:55,052 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:22:57,594 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:22:57,607 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:22:57,614 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:33<21:05,  5.43s/it] 60%|██████    | 353/585 [02:33<15:02,  3.89s/it] 61%|██████    | 354/585 [02:33<10:49,  2.81s/it] 61%|██████    | 355/585 [02:33<07:52,  2.05s/it] 61%|██████    | 356/585 [02:34<05:49,  1.52s/it] 61%|██████    | 357/585 [02:34<04:23,  1.15s/it] 61%|██████    | 358/585 [02:34<03:22,  1.12it/s] 61%|██████▏   | 359/585 [02:35<02:41,  1.40it/s] 62%|██████▏   | 360/585 [02:35<02:11,  1.71it/s] 62%|██████▏   | 361/585 [02:35<01:51,  2.02it/s] 62%|██████▏   | 362/585 [02:35<01:36,  2.30it/s] 62%|██████▏   | 363/585 [02:36<01:26,  2.56it/s] 62%|██████▏   | 364/585 [02:36<01:19,  2.78it/s] 62%|██████▏   | 365/585 [02:36<01:14,  2.95it/s] 63%|██████▎   | 366/585 [02:37<01:10,  3.09it/s] 63%|██████▎   | 367/585 [02:37<01:08,  3.19it/s] 63%|██████▎   | 368/585 [02:37<01:06,  3.27it/s] 63%|██████▎   | 369/585 [02:37<01:04,  3.33it/s] 63%|██████▎   | 370/585 [02:38<01:03,  3.37it/s] 63%|██████▎   | 371/585 [02:38<01:03,  3.40it/s] 64%|██████▎   | 372/585 [02:38<01:02,  3.42it/s] 64%|██████▍   | 373/585 [02:39<01:01,  3.42it/s] 64%|██████▍   | 374/585 [02:39<01:01,  3.43it/s] 64%|██████▍   | 375/585 [02:39<01:00,  3.44it/s] 64%|██████▍   | 376/585 [02:40<01:00,  3.45it/s] 64%|██████▍   | 377/585 [02:40<01:00,  3.45it/s] 65%|██████▍   | 378/585 [02:40<00:59,  3.46it/s] 65%|██████▍   | 379/585 [02:40<00:59,  3.46it/s] 65%|██████▍   | 380/585 [02:41<00:59,  3.46it/s] 65%|██████▌   | 381/585 [02:41<00:58,  3.46it/s] 65%|██████▌   | 382/585 [02:41<00:58,  3.46it/s] 65%|██████▌   | 383/585 [02:42<00:58,  3.46it/s] 66%|██████▌   | 384/585 [02:42<00:58,  3.46it/s] 66%|██████▌   | 385/585 [02:42<00:57,  3.46it/s] 66%|██████▌   | 386/585 [02:42<00:57,  3.46it/s] 66%|██████▌   | 387/585 [02:43<00:57,  3.46it/s] 66%|██████▋   | 388/585 [02:43<00:56,  3.46it/s] 66%|██████▋   | 389/585 [02:43<00:56,  3.46it/s] 67%|██████▋   | 390/585 [02:44<00:56,  3.46it/s] 67%|██████▋   | 391/585 [02:44<00:56,  3.46it/s] 67%|██████▋   | 392/585 [02:44<00:55,  3.46it/s] 67%|██████▋   | 393/585 [02:44<00:55,  3.46it/s] 67%|██████▋   | 394/585 [02:45<00:55,  3.46it/s] 68%|██████▊   | 395/585 [02:45<00:55,  3.45it/s] 68%|██████▊   | 396/585 [02:45<00:54,  3.46it/s] 68%|██████▊   | 397/585 [02:46<00:54,  3.46it/s] 68%|██████▊   | 398/585 [02:46<00:54,  3.46it/s] 68%|██████▊   | 399/585 [02:46<00:53,  3.46it/s] 68%|██████▊   | 400/585 [02:46<00:53,  3.46it/s] 69%|██████▊   | 401/585 [02:47<00:53,  3.46it/s] 69%|██████▊   | 402/585 [02:47<00:52,  3.46it/s] 69%|██████▉   | 403/585 [02:47<00:52,  3.46it/s] 69%|██████▉   | 404/585 [02:48<00:52,  3.46it/s] 69%|██████▉   | 405/585 [02:48<00:52,  3.46it/s] 69%|██████▉   | 406/585 [02:48<00:51,  3.45it/s] 70%|██████▉   | 407/585 [02:48<00:51,  3.45it/s] 70%|██████▉   | 408/585 [02:49<00:51,  3.45it/s] 70%|██████▉   | 409/585 [02:49<00:50,  3.46it/s] 70%|███████   | 410/585 [02:49<00:50,  3.46it/s] 70%|███████   | 411/585 [02:50<00:50,  3.46it/s] 70%|███████   | 412/585 [02:50<00:50,  3.46it/s] 71%|███████   | 413/585 [02:50<00:49,  3.46it/s] 71%|███████   | 414/585 [02:50<00:49,  3.46it/s] 71%|███████   | 415/585 [02:51<00:49,  3.46it/s] 71%|███████   | 416/585 [02:51<00:48,  3.46it/s] 71%|███████▏  | 417/585 [02:51<00:48,  3.44it/s] 71%|███████▏  | 418/585 [02:52<00:48,  3.45it/s] 72%|███████▏  | 419/585 [02:52<00:48,  3.45it/s] 72%|███████▏  | 420/585 [02:52<00:47,  3.45it/s] 72%|███████▏  | 421/585 [02:53<00:47,  3.45it/s] 72%|███████▏  | 422/585 [02:53<00:47,  3.45it/s] 72%|███████▏  | 423/585 [02:53<00:46,  3.46it/s] 72%|███████▏  | 424/585 [02:53<00:46,  3.45it/s] 73%|███████▎  | 425/585 [02:54<00:46,  3.46it/s] 73%|███████▎  | 426/585 [02:54<00:46,  3.46it/s] 73%|███████▎  | 427/585 [02:54<00:45,  3.46it/s] 73%|███████▎  | 428/585 [02:55<00:45,  3.44it/s] 73%|███████▎  | 429/585 [02:55<00:45,  3.45it/s] 74%|███████▎  | 430/585 [02:55<00:44,  3.45it/s] 74%|███████▎  | 431/585 [02:55<00:44,  3.45it/s] 74%|███████▍  | 432/585 [02:56<00:44,  3.45it/s] 74%|███████▍  | 433/585 [02:56<00:44,  3.45it/s] 74%|███████▍  | 434/585 [02:56<00:43,  3.45it/s] 74%|███████▍  | 435/585 [02:57<00:43,  3.45it/s] 75%|███████▍  | 436/585 [02:57<00:43,  3.46it/s] 75%|███████▍  | 437/585 [02:57<00:42,  3.45it/s] 75%|███████▍  | 438/585 [02:57<00:42,  3.45it/s] 75%|███████▌  | 439/585 [02:58<00:42,  3.45it/s] 75%|███████▌  | 440/585 [02:58<00:42,  3.45it/s] 75%|███████▌  | 441/585 [02:58<00:41,  3.45it/s] 76%|███████▌  | 442/585 [02:59<00:41,  3.45it/s] 76%|███████▌  | 443/585 [02:59<00:41,  3.45it/s] 76%|███████▌  | 444/585 [02:59<00:40,  3.45it/s] 76%|███████▌  | 445/585 [02:59<00:40,  3.45it/s] 76%|███████▌  | 446/585 [03:00<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:00<00:39,  3.46it/s] 77%|███████▋  | 448/585 [03:00<00:39,  3.46it/s] 77%|███████▋  | 449/585 [03:01<00:39,  3.45it/s] 77%|███████▋  | 450/585 [03:01<00:39,  3.45it/s] 77%|███████▋  | 451/585 [03:01<00:38,  3.45it/s] 77%|███████▋  | 452/585 [03:02<00:38,  3.45it/s] 77%|███████▋  | 453/585 [03:02<00:38,  3.45it/s] 78%|███████▊  | 454/585 [03:02<00:37,  3.46it/s] 78%|███████▊  | 455/585 [03:02<00:37,  3.45it/s] 78%|███████▊  | 456/585 [03:03<00:37,  3.45it/s] 78%|███████▊  | 457/585 [03:03<00:37,  3.46it/s] 78%|███████▊  | 458/585 [03:03<00:36,  3.46it/s] 78%|███████▊  | 459/585 [03:04<00:36,  3.46it/s] 79%|███████▊  | 460/585 [03:04<00:36,  3.46it/s] 79%|███████▉  | 461/585 [03:04<00:35,  3.46it/s] 79%|███████▉  | 462/585 [03:04<00:35,  3.46it/s] 79%|███████▉  | 463/585 [03:05<00:35,  3.46it/s] 79%|███████▉  | 464/585 [03:05<00:34,  3.46it/s] 79%|███████▉  | 465/585 [03:05<00:34,  3.46it/s] 80%|███████▉  | 466/585 [03:06<00:34,  3.46it/s] 80%|███████▉  | 467/585 [03:06<00:34,  3.46it/s] 80%|████████  | 468/585 [03:06<00:33,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 13:23:36,659 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:23:36,659 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 13:23:36,659 >>   Batch size = 8
{'eval_loss': 1.0586169958114624, 'eval_runtime': 9.334, 'eval_samples_per_second': 374.116, 'eval_steps_per_second': 46.818, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.17it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.84it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.97it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.29it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.83it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.51it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.33it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.95it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.90it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.06it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.97it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.97it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.08it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.05it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.03it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.93it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.76it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.80it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.88it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.94it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.03it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.85it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.97it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.02it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.98it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.87it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.81it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.86it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.86it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.99it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.00it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.93it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.98it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.86it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.86it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.90it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.87it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.92it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.00it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.08it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.88it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.03it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.04it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.98it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.87it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.77it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.85it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.89it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.01it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.86it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.83it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.90it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.87it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.84it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.81it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.70it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.78it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.91it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.97it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.95it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.94it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.00it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.94it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.92it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.85it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.82it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.91it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.98it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.00it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.01it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.91it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.88it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.92it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.90it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.92it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.88it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.85it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.95it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.00it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.98it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.88it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.95it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.90it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.88it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.81it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.85it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:15<00:33,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 46.85it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:23:46,004 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 13:23:46,022 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:23:48,384 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:23:48,408 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:23:48,421 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:23<10:11,  5.27s/it] 80%|████████  | 470/585 [03:23<07:14,  3.78s/it] 81%|████████  | 471/585 [03:24<05:11,  2.73s/it] 81%|████████  | 472/585 [03:24<03:45,  2.00s/it] 81%|████████  | 473/585 [03:24<02:46,  1.49s/it] 81%|████████  | 474/585 [03:24<02:05,  1.13s/it] 81%|████████  | 475/585 [03:25<01:36,  1.14it/s] 81%|████████▏ | 476/585 [03:25<01:16,  1.43it/s] 82%|████████▏ | 477/585 [03:25<01:02,  1.74it/s] 82%|████████▏ | 478/585 [03:26<00:52,  2.04it/s] 82%|████████▏ | 479/585 [03:26<00:45,  2.33it/s] 82%|████████▏ | 480/585 [03:26<00:40,  2.58it/s] 82%|████████▏ | 481/585 [03:27<00:37,  2.79it/s] 82%|████████▏ | 482/585 [03:27<00:34,  2.96it/s] 83%|████████▎ | 483/585 [03:27<00:32,  3.10it/s] 83%|████████▎ | 484/585 [03:27<00:32,  3.15it/s] 83%|████████▎ | 485/585 [03:28<00:30,  3.23it/s] 83%|████████▎ | 486/585 [03:28<00:30,  3.30it/s] 83%|████████▎ | 487/585 [03:28<00:29,  3.35it/s] 83%|████████▎ | 488/585 [03:29<00:28,  3.38it/s] 84%|████████▎ | 489/585 [03:29<00:28,  3.40it/s] 84%|████████▍ | 490/585 [03:29<00:27,  3.42it/s] 84%|████████▍ | 491/585 [03:29<00:27,  3.43it/s] 84%|████████▍ | 492/585 [03:30<00:27,  3.43it/s] 84%|████████▍ | 493/585 [03:30<00:26,  3.44it/s] 84%|████████▍ | 494/585 [03:30<00:26,  3.44it/s] 85%|████████▍ | 495/585 [03:31<00:26,  3.45it/s] 85%|████████▍ | 496/585 [03:31<00:25,  3.46it/s] 85%|████████▍ | 497/585 [03:31<00:25,  3.46it/s] 85%|████████▌ | 498/585 [03:31<00:25,  3.46it/s] 85%|████████▌ | 499/585 [03:32<00:24,  3.46it/s] 85%|████████▌ | 500/585 [03:32<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [03:32<00:24,  3.46it/s] 86%|████████▌ | 501/585 [03:32<00:24,  3.46it/s] 86%|████████▌ | 502/585 [03:33<00:23,  3.46it/s] 86%|████████▌ | 503/585 [03:33<00:23,  3.45it/s] 86%|████████▌ | 504/585 [03:33<00:23,  3.44it/s] 86%|████████▋ | 505/585 [03:33<00:23,  3.45it/s] 86%|████████▋ | 506/585 [03:34<00:22,  3.45it/s] 87%|████████▋ | 507/585 [03:34<00:22,  3.46it/s] 87%|████████▋ | 508/585 [03:34<00:22,  3.46it/s] 87%|████████▋ | 509/585 [03:35<00:21,  3.46it/s] 87%|████████▋ | 510/585 [03:35<00:21,  3.46it/s] 87%|████████▋ | 511/585 [03:35<00:21,  3.45it/s] 88%|████████▊ | 512/585 [03:35<00:21,  3.46it/s] 88%|████████▊ | 513/585 [03:36<00:20,  3.46it/s] 88%|████████▊ | 514/585 [03:36<00:20,  3.44it/s] 88%|████████▊ | 515/585 [03:36<00:20,  3.45it/s] 88%|████████▊ | 516/585 [03:37<00:19,  3.45it/s] 88%|████████▊ | 517/585 [03:37<00:19,  3.46it/s] 89%|████████▊ | 518/585 [03:37<00:19,  3.46it/s] 89%|████████▊ | 519/585 [03:38<00:19,  3.46it/s] 89%|████████▉ | 520/585 [03:38<00:18,  3.46it/s] 89%|████████▉ | 521/585 [03:38<00:18,  3.46it/s] 89%|████████▉ | 522/585 [03:38<00:18,  3.46it/s] 89%|████████▉ | 523/585 [03:39<00:17,  3.46it/s] 90%|████████▉ | 524/585 [03:39<00:17,  3.46it/s] 90%|████████▉ | 525/585 [03:39<00:17,  3.44it/s] 90%|████████▉ | 526/585 [03:40<00:17,  3.44it/s] 90%|█████████ | 527/585 [03:40<00:16,  3.45it/s] 90%|█████████ | 528/585 [03:40<00:16,  3.45it/s] 90%|█████████ | 529/585 [03:40<00:16,  3.45it/s] 91%|█████████ | 530/585 [03:41<00:15,  3.45it/s] 91%|█████████ | 531/585 [03:41<00:15,  3.45it/s] 91%|█████████ | 532/585 [03:41<00:15,  3.45it/s] 91%|█████████ | 533/585 [03:42<00:15,  3.45it/s] 91%|█████████▏| 534/585 [03:42<00:14,  3.45it/s] 91%|█████████▏| 535/585 [03:42<00:14,  3.45it/s] 92%|█████████▏| 536/585 [03:42<00:14,  3.44it/s] 92%|█████████▏| 537/585 [03:43<00:13,  3.44it/s] 92%|█████████▏| 538/585 [03:43<00:13,  3.45it/s] 92%|█████████▏| 539/585 [03:43<00:13,  3.45it/s] 92%|█████████▏| 540/585 [03:44<00:13,  3.45it/s] 92%|█████████▏| 541/585 [03:44<00:12,  3.45it/s] 93%|█████████▎| 542/585 [03:44<00:12,  3.45it/s] 93%|█████████▎| 543/585 [03:44<00:12,  3.45it/s] 93%|█████████▎| 544/585 [03:45<00:11,  3.45it/s] 93%|█████████▎| 545/585 [03:45<00:11,  3.45it/s] 93%|█████████▎| 546/585 [03:45<00:11,  3.46it/s] 94%|█████████▎| 547/585 [03:46<00:10,  3.46it/s] 94%|█████████▎| 548/585 [03:46<00:10,  3.45it/s] 94%|█████████▍| 549/585 [03:46<00:10,  3.46it/s] 94%|█████████▍| 550/585 [03:46<00:10,  3.45it/s] 94%|█████████▍| 551/585 [03:47<00:09,  3.45it/s] 94%|█████████▍| 552/585 [03:47<00:09,  3.41it/s] 95%|█████████▍| 553/585 [03:47<00:09,  3.42it/s] 95%|█████████▍| 554/585 [03:48<00:09,  3.43it/s] 95%|█████████▍| 555/585 [03:48<00:08,  3.44it/s] 95%|█████████▌| 556/585 [03:48<00:08,  3.44it/s] 95%|█████████▌| 557/585 [03:49<00:08,  3.45it/s] 95%|█████████▌| 558/585 [03:49<00:07,  3.45it/s] 96%|█████████▌| 559/585 [03:49<00:07,  3.45it/s] 96%|█████████▌| 560/585 [03:49<00:07,  3.45it/s] 96%|█████████▌| 561/585 [03:50<00:06,  3.45it/s] 96%|█████████▌| 562/585 [03:50<00:06,  3.45it/s] 96%|█████████▌| 563/585 [03:50<00:06,  3.45it/s] 96%|█████████▋| 564/585 [03:51<00:06,  3.45it/s] 97%|█████████▋| 565/585 [03:51<00:05,  3.45it/s] 97%|█████████▋| 566/585 [03:51<00:05,  3.45it/s] 97%|█████████▋| 567/585 [03:51<00:05,  3.45it/s] 97%|█████████▋| 568/585 [03:52<00:04,  3.45it/s] 97%|█████████▋| 569/585 [03:52<00:04,  3.45it/s] 97%|█████████▋| 570/585 [03:52<00:04,  3.44it/s] 98%|█████████▊| 571/585 [03:53<00:04,  3.44it/s] 98%|█████████▊| 572/585 [03:53<00:03,  3.44it/s] 98%|█████████▊| 573/585 [03:53<00:03,  3.45it/s] 98%|█████████▊| 574/585 [03:53<00:03,  3.45it/s] 98%|█████████▊| 575/585 [03:54<00:02,  3.45it/s] 98%|█████████▊| 576/585 [03:54<00:02,  3.45it/s] 99%|█████████▊| 577/585 [03:54<00:02,  3.45it/s] 99%|█████████▉| 578/585 [03:55<00:02,  3.45it/s] 99%|█████████▉| 579/585 [03:55<00:01,  3.45it/s] 99%|█████████▉| 580/585 [03:55<00:01,  3.45it/s] 99%|█████████▉| 581/585 [03:55<00:01,  3.45it/s] 99%|█████████▉| 582/585 [03:56<00:00,  3.45it/s]100%|█████████▉| 583/585 [03:56<00:00,  3.46it/s]100%|█████████▉| 584/585 [03:56<00:00,  3.46it/s]100%|██████████| 585/585 [03:57<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 13:24:27,135 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:24:27,136 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 13:24:27,136 >>   Batch size = 8
{'eval_loss': 1.068346381187439, 'eval_runtime': 9.323, 'eval_samples_per_second': 374.558, 'eval_steps_per_second': 46.873, 'epoch': 4.0}
{'loss': 0.7186, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.81it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.86it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.13it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.26it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.87it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.66it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.40it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.09it/s][A
 11%|█         | 48/437 [00:00<00:08, 46.91it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.83it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.84it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.98it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.08it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.12it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.02it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.98it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.00it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.91it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.95it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.92it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.94it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.00it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.94it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.94it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.00it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.99it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.90it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.91it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.82it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.88it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.96it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.06it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.93it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.98it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.90it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.89it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.89it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.83it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.82it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.84it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.01it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.96it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.05it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.93it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.86it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.75it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.80it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.73it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.87it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.99it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.94it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.91it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.93it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.98it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.99it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.91it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.86it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.74it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.94it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.91it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.95it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.90it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.89it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.88it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.97it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.89it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.93it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.96it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.93it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.76it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.87it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.88it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.95it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.94it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.90it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.80it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.90it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.93it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.98it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.94it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.93it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.80it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.86it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.84it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.96it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.95it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:06<00:00,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 46.95it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:24:36,466 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 13:24:36,484 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:24:38,912 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:24:38,930 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:24:38,940 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:24:44,009 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:24:44,012 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117 (score: 1.0318280458450317).
                                                 100%|██████████| 585/585 [04:15<00:00,  3.45it/s]100%|██████████| 585/585 [04:15<00:00,  2.29it/s]
[INFO|trainer.py:1894] 2023-08-28 13:24:45,716 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 13:24:45,732 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:24:48,129 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:24:48,145 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:24:48,155 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:24:48,365 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:48,365 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:48,365 >>   train_loss               =     0.7136
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:48,365 >>   train_runtime            = 0:04:15.72
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:48,365 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:48,365 >>   train_samples_per_second =    146.641
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:48,365 >>   train_steps_per_second   =      2.288
{'eval_loss': 1.0722236633300781, 'eval_runtime': 9.3084, 'eval_samples_per_second': 375.146, 'eval_steps_per_second': 46.947, 'epoch': 5.0}
{'train_runtime': 255.7273, 'train_samples_per_second': 146.641, 'train_steps_per_second': 2.288, 'train_loss': 0.7135989327716012, 'epoch': 5.0}
08/28/2023 13:24:48 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:24:48,400 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:24:48,400 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 13:24:48,400 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 59.14it/s]  3%|▎         | 12/437 [00:00<00:08, 51.86it/s]  4%|▍         | 18/437 [00:00<00:08, 49.75it/s]  5%|▌         | 24/437 [00:00<00:08, 48.98it/s]  7%|▋         | 29/437 [00:00<00:08, 48.64it/s]  8%|▊         | 34/437 [00:00<00:08, 48.28it/s]  9%|▉         | 39/437 [00:00<00:08, 48.16it/s] 10%|█         | 44/437 [00:00<00:08, 47.69it/s] 11%|█         | 49/437 [00:01<00:08, 47.18it/s] 12%|█▏        | 54/437 [00:01<00:08, 47.17it/s] 14%|█▎        | 59/437 [00:01<00:07, 47.30it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.36it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.37it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.40it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.46it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.41it/s] 20%|██        | 89/437 [00:01<00:07, 47.22it/s] 22%|██▏       | 94/437 [00:01<00:07, 46.96it/s] 23%|██▎       | 99/437 [00:02<00:07, 46.92it/s] 24%|██▍       | 104/437 [00:02<00:07, 46.91it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.11it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.28it/s] 27%|██▋       | 119/437 [00:02<00:06, 47.32it/s] 28%|██▊       | 124/437 [00:02<00:06, 47.35it/s] 30%|██▉       | 129/437 [00:02<00:06, 47.44it/s] 31%|███       | 134/437 [00:02<00:06, 47.30it/s] 32%|███▏      | 139/437 [00:02<00:06, 47.07it/s] 33%|███▎      | 144/437 [00:03<00:06, 46.89it/s] 34%|███▍      | 149/437 [00:03<00:06, 47.00it/s] 35%|███▌      | 154/437 [00:03<00:06, 47.04it/s] 36%|███▋      | 159/437 [00:03<00:05, 47.23it/s] 38%|███▊      | 164/437 [00:03<00:05, 47.18it/s] 39%|███▊      | 169/437 [00:03<00:05, 47.25it/s] 40%|███▉      | 174/437 [00:03<00:05, 47.27it/s] 41%|████      | 179/437 [00:03<00:05, 47.33it/s] 42%|████▏     | 184/437 [00:03<00:05, 47.12it/s] 43%|████▎     | 189/437 [00:03<00:05, 46.98it/s] 44%|████▍     | 194/437 [00:04<00:05, 47.07it/s] 46%|████▌     | 199/437 [00:04<00:05, 47.06it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.11it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.20it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.24it/s] 50%|█████     | 219/437 [00:04<00:04, 47.19it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.04it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.05it/s] 54%|█████▎    | 234/437 [00:04<00:04, 46.94it/s] 55%|█████▍    | 239/437 [00:05<00:04, 47.01it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.06it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.07it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.04it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.09it/s] 60%|██████    | 264/437 [00:05<00:03, 47.24it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.13it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.04it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.01it/s] 65%|██████▍   | 284/437 [00:05<00:03, 46.79it/s] 66%|██████▌   | 289/437 [00:06<00:03, 46.88it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.00it/s] 68%|██████▊   | 299/437 [00:06<00:02, 46.91it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.05it/s] 71%|███████   | 309/437 [00:06<00:02, 47.08it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.10it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.11it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.21it/s] 75%|███████▌  | 329/437 [00:06<00:02, 46.96it/s] 76%|███████▋  | 334/437 [00:07<00:02, 47.03it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.20it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.10it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.01it/s] 81%|████████  | 354/437 [00:07<00:01, 47.14it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.07it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.16it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.23it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.18it/s] 87%|████████▋ | 379/437 [00:08<00:01, 46.99it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.13it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.06it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.11it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.07it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.20it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.07it/s] 95%|█████████▍| 414/437 [00:08<00:00, 47.08it/s] 96%|█████████▌| 419/437 [00:08<00:00, 47.13it/s] 97%|█████████▋| 424/437 [00:08<00:00, 46.99it/s] 98%|█████████▊| 429/437 [00:09<00:00, 46.94it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.12it/s]100%|██████████| 437/437 [00:09<00:00, 47.26it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:24:57,669 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:57,669 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:57,670 >>   eval_loss               =     1.0318
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:57,670 >>   eval_runtime            = 0:00:09.26
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:57,670 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:57,670 >>   eval_samples_per_second =    376.727
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:57,670 >>   eval_steps_per_second   =     47.145
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:24:57,670 >>   perplexity              =     2.8062
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:02,664 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:02,670 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:02,670 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:02,670 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:02,670 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:25:03,374 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:25:03,375 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:25:03,941 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:25:04,981 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:25:04,981 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:07,825 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:07,827 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:07,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:07,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:25:07,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:25:08,442 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:25:08,447 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:25:09,012 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:25:09,168 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:25:09,168 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.48it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.45it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.45it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.50it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.51it/s]Extractor Predicting: 24it [00:16,  1.52it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:18,  1.50it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:20,  1.46it/s]Extractor Predicting: 31it [00:20,  1.47it/s]Extractor Predicting: 32it [00:21,  1.49it/s]Extractor Predicting: 33it [00:22,  1.50it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:23,  1.49it/s]Extractor Predicting: 36it [00:24,  1.49it/s]Extractor Predicting: 37it [00:24,  1.48it/s]Extractor Predicting: 38it [00:25,  1.48it/s]Extractor Predicting: 39it [00:26,  1.49it/s]Extractor Predicting: 40it [00:26,  1.50it/s]Extractor Predicting: 41it [00:27,  1.50it/s]Extractor Predicting: 42it [00:28,  1.48it/s]Extractor Predicting: 43it [00:28,  1.49it/s]Extractor Predicting: 44it [00:29,  1.49it/s]Extractor Predicting: 45it [00:30,  1.47it/s]Extractor Predicting: 46it [00:30,  1.46it/s]Extractor Predicting: 47it [00:34,  1.39s/it]Extractor Predicting: 48it [00:34,  1.23s/it]Extractor Predicting: 49it [00:35,  1.07s/it]Extractor Predicting: 50it [00:36,  1.06it/s]Extractor Predicting: 51it [00:36,  1.14it/s]Extractor Predicting: 52it [00:37,  1.21it/s]Extractor Predicting: 53it [00:38,  1.27it/s]Extractor Predicting: 54it [00:39,  1.32it/s]Extractor Predicting: 55it [00:39,  1.37it/s]Extractor Predicting: 56it [00:40,  1.40it/s]Extractor Predicting: 57it [00:41,  1.42it/s]Extractor Predicting: 58it [00:41,  1.45it/s]Extractor Predicting: 59it [00:42,  1.44it/s]Extractor Predicting: 60it [00:43,  1.43it/s]Extractor Predicting: 61it [00:43,  1.42it/s]Extractor Predicting: 62it [00:44,  1.43it/s]Extractor Predicting: 63it [00:45,  1.44it/s]Extractor Predicting: 64it [00:45,  1.41it/s]Extractor Predicting: 65it [00:46,  1.42it/s]Extractor Predicting: 66it [00:47,  1.40it/s]Extractor Predicting: 67it [00:48,  1.40it/s]Extractor Predicting: 68it [00:48,  1.39it/s]Extractor Predicting: 69it [00:49,  1.39it/s]Extractor Predicting: 70it [00:50,  1.39it/s]Extractor Predicting: 71it [00:51,  1.39it/s]Extractor Predicting: 72it [00:51,  1.37it/s]Extractor Predicting: 73it [00:52,  1.39it/s]Extractor Predicting: 74it [00:53,  1.36it/s]Extractor Predicting: 75it [00:53,  1.40it/s]Extractor Predicting: 76it [00:54,  1.40it/s]Extractor Predicting: 77it [00:55,  1.38it/s]Extractor Predicting: 78it [00:56,  1.40it/s]Extractor Predicting: 79it [00:56,  1.38it/s]Extractor Predicting: 80it [00:57,  1.36it/s]Extractor Predicting: 81it [00:58,  1.36it/s]Extractor Predicting: 82it [00:58,  1.39it/s]Extractor Predicting: 83it [00:59,  1.40it/s]Extractor Predicting: 84it [01:00,  1.42it/s]Extractor Predicting: 85it [01:01,  1.42it/s]Extractor Predicting: 86it [01:01,  1.39it/s]Extractor Predicting: 87it [01:02,  1.40it/s]Extractor Predicting: 88it [01:03,  1.44it/s]Extractor Predicting: 89it [01:03,  1.50it/s]Extractor Predicting: 90it [01:04,  1.49it/s]Extractor Predicting: 91it [01:05,  1.54it/s]Extractor Predicting: 92it [01:05,  1.59it/s]Extractor Predicting: 93it [01:06,  1.62it/s]Extractor Predicting: 94it [01:06,  1.65it/s]Extractor Predicting: 95it [01:07,  1.59it/s]Extractor Predicting: 96it [01:08,  1.62it/s]Extractor Predicting: 97it [01:08,  1.57it/s]Extractor Predicting: 98it [01:09,  1.57it/s]Extractor Predicting: 99it [01:09,  1.63it/s]Extractor Predicting: 100it [01:10,  1.63it/s]Extractor Predicting: 101it [01:11,  1.60it/s]Extractor Predicting: 102it [01:11,  1.55it/s]Extractor Predicting: 103it [01:12,  1.55it/s]Extractor Predicting: 104it [01:13,  1.51it/s]Extractor Predicting: 105it [01:13,  1.52it/s]Extractor Predicting: 106it [01:14,  1.54it/s]Extractor Predicting: 107it [01:15,  1.50it/s]Extractor Predicting: 108it [01:15,  1.54it/s]Extractor Predicting: 109it [01:16,  1.56it/s]Extractor Predicting: 110it [01:17,  1.57it/s]Extractor Predicting: 111it [01:17,  1.59it/s]Extractor Predicting: 112it [01:18,  1.59it/s]Extractor Predicting: 113it [01:18,  1.62it/s]Extractor Predicting: 114it [01:19,  1.57it/s]Extractor Predicting: 115it [01:20,  1.60it/s]Extractor Predicting: 116it [01:20,  1.56it/s]Extractor Predicting: 117it [01:21,  1.52it/s]Extractor Predicting: 118it [01:22,  1.52it/s]Extractor Predicting: 119it [01:22,  1.49it/s]Extractor Predicting: 120it [01:23,  1.46it/s]Extractor Predicting: 121it [01:24,  1.46it/s]Extractor Predicting: 122it [01:25,  1.46it/s]Extractor Predicting: 123it [01:25,  1.45it/s]Extractor Predicting: 124it [01:26,  1.42it/s]Extractor Predicting: 125it [01:27,  1.42it/s]Extractor Predicting: 126it [01:27,  1.41it/s]Extractor Predicting: 127it [01:28,  1.42it/s]Extractor Predicting: 128it [01:29,  1.31it/s]Extractor Predicting: 129it [01:30,  1.35it/s]Extractor Predicting: 130it [01:30,  1.40it/s]Extractor Predicting: 131it [01:31,  1.39it/s]Extractor Predicting: 132it [01:32,  1.39it/s]Extractor Predicting: 133it [01:32,  1.40it/s]Extractor Predicting: 134it [01:33,  1.41it/s]Extractor Predicting: 135it [01:34,  1.43it/s]Extractor Predicting: 136it [01:35,  1.42it/s]Extractor Predicting: 137it [01:35,  1.42it/s]Extractor Predicting: 138it [01:36,  1.45it/s]Extractor Predicting: 139it [01:37,  1.45it/s]Extractor Predicting: 140it [01:37,  1.47it/s]Extractor Predicting: 141it [01:38,  1.43it/s]Extractor Predicting: 142it [01:39,  1.43it/s]Extractor Predicting: 143it [01:39,  1.45it/s]Extractor Predicting: 144it [01:40,  1.48it/s]Extractor Predicting: 144it [01:40,  1.43it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:57,986 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:57,995 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:57,995 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:57,995 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:57,995 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:26:58,602 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:26:58,603 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:26:59,175 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:27:00,226 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:27:00,226 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:03,317 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:03,319 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:03,320 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:03,320 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:27:03,320 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:27:03,974 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:27:03,975 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:27:04,545 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:27:04,708 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:27:04,708 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4800637958532695,
  "recall": 0.17239404352806414,
  "score": 0.2536873156342183,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.51it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.53it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:12,  1.50it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.50it/s]Extractor Predicting: 22it [00:14,  1.45it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:16,  1.50it/s]Extractor Predicting: 25it [00:16,  1.41it/s]Extractor Predicting: 26it [00:17,  1.42it/s]Extractor Predicting: 27it [00:18,  1.42it/s]Extractor Predicting: 28it [00:18,  1.43it/s]Extractor Predicting: 29it [00:19,  1.43it/s]Extractor Predicting: 30it [00:20,  1.41it/s]Extractor Predicting: 31it [00:21,  1.42it/s]Extractor Predicting: 32it [00:21,  1.42it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:23,  1.47it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:24,  1.50it/s]Extractor Predicting: 37it [00:25,  1.53it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:26,  1.53it/s]Extractor Predicting: 40it [00:27,  1.53it/s]Extractor Predicting: 41it [00:27,  1.53it/s]Extractor Predicting: 42it [00:28,  1.52it/s]Extractor Predicting: 43it [00:28,  1.52it/s]Extractor Predicting: 44it [00:29,  1.52it/s]Extractor Predicting: 45it [00:30,  1.50it/s]Extractor Predicting: 46it [00:30,  1.52it/s]Extractor Predicting: 47it [00:31,  1.52it/s]Extractor Predicting: 48it [00:32,  1.52it/s]Extractor Predicting: 49it [00:32,  1.51it/s]Extractor Predicting: 50it [00:33,  1.50it/s]Extractor Predicting: 51it [00:34,  1.51it/s]Extractor Predicting: 52it [00:34,  1.51it/s]Extractor Predicting: 53it [00:35,  1.51it/s]Extractor Predicting: 54it [00:36,  1.51it/s]Extractor Predicting: 55it [00:36,  1.49it/s]Extractor Predicting: 56it [00:37,  1.50it/s]Extractor Predicting: 57it [00:38,  1.53it/s]Extractor Predicting: 58it [00:38,  1.56it/s]Extractor Predicting: 59it [00:39,  1.59it/s]Extractor Predicting: 60it [00:40,  1.57it/s]Extractor Predicting: 61it [00:40,  1.55it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:42,  1.55it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:43,  1.51it/s]Extractor Predicting: 66it [00:44,  1.50it/s]Extractor Predicting: 67it [00:44,  1.51it/s]Extractor Predicting: 68it [00:45,  1.57it/s]Extractor Predicting: 69it [00:46,  1.54it/s]Extractor Predicting: 70it [00:46,  1.54it/s]Extractor Predicting: 71it [00:47,  1.53it/s]Extractor Predicting: 72it [00:48,  1.53it/s]Extractor Predicting: 73it [00:48,  1.49it/s]Extractor Predicting: 74it [00:49,  1.49it/s]Extractor Predicting: 75it [00:50,  1.50it/s]Extractor Predicting: 76it [00:50,  1.51it/s]Extractor Predicting: 77it [00:51,  1.49it/s]Extractor Predicting: 78it [00:52,  1.50it/s]Extractor Predicting: 79it [00:52,  1.50it/s]Extractor Predicting: 80it [00:53,  1.50it/s]Extractor Predicting: 81it [00:54,  1.49it/s]Extractor Predicting: 82it [00:54,  1.48it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:56,  1.50it/s]Extractor Predicting: 85it [00:56,  1.52it/s]Extractor Predicting: 86it [00:57,  1.50it/s]Extractor Predicting: 87it [00:58,  1.50it/s]Extractor Predicting: 88it [00:58,  1.48it/s]Extractor Predicting: 89it [00:59,  1.51it/s]Extractor Predicting: 90it [01:00,  1.50it/s]Extractor Predicting: 91it [01:00,  1.50it/s]Extractor Predicting: 92it [01:01,  1.47it/s]Extractor Predicting: 93it [01:02,  1.50it/s]Extractor Predicting: 94it [01:02,  1.47it/s]Extractor Predicting: 95it [01:03,  1.47it/s]Extractor Predicting: 96it [01:04,  1.49it/s]Extractor Predicting: 97it [01:04,  1.50it/s]Extractor Predicting: 98it [01:05,  1.49it/s]Extractor Predicting: 99it [01:06,  1.51it/s]Extractor Predicting: 100it [01:06,  1.52it/s]Extractor Predicting: 101it [01:07,  1.53it/s]Extractor Predicting: 102it [01:08,  1.53it/s]Extractor Predicting: 103it [01:08,  1.48it/s]Extractor Predicting: 104it [01:09,  1.46it/s]Extractor Predicting: 105it [01:10,  1.47it/s]Extractor Predicting: 106it [01:10,  1.47it/s]Extractor Predicting: 107it [01:11,  1.33it/s]Extractor Predicting: 108it [01:12,  1.38it/s]Extractor Predicting: 109it [01:13,  1.38it/s]Extractor Predicting: 110it [01:13,  1.42it/s]Extractor Predicting: 111it [01:14,  1.42it/s]Extractor Predicting: 112it [01:15,  1.42it/s]Extractor Predicting: 113it [01:15,  1.42it/s]Extractor Predicting: 114it [01:16,  1.42it/s]Extractor Predicting: 115it [01:17,  1.44it/s]Extractor Predicting: 116it [01:17,  1.49it/s]Extractor Predicting: 117it [01:18,  1.50it/s]Extractor Predicting: 118it [01:19,  1.50it/s]Extractor Predicting: 119it [01:19,  1.50it/s]Extractor Predicting: 120it [01:20,  1.49it/s]Extractor Predicting: 121it [01:21,  1.51it/s]Extractor Predicting: 122it [01:21,  1.53it/s]Extractor Predicting: 123it [01:22,  1.54it/s]Extractor Predicting: 124it [01:23,  1.55it/s]Extractor Predicting: 125it [01:23,  1.54it/s]Extractor Predicting: 126it [01:24,  1.54it/s]Extractor Predicting: 127it [01:25,  1.55it/s]Extractor Predicting: 128it [01:25,  1.53it/s]Extractor Predicting: 129it [01:26,  1.50it/s]Extractor Predicting: 130it [01:27,  1.49it/s]Extractor Predicting: 131it [01:27,  1.52it/s]Extractor Predicting: 132it [01:28,  1.52it/s]Extractor Predicting: 133it [01:29,  1.53it/s]Extractor Predicting: 134it [01:29,  1.54it/s]Extractor Predicting: 135it [01:30,  1.54it/s]Extractor Predicting: 136it [01:30,  1.54it/s]Extractor Predicting: 137it [01:31,  1.54it/s]Extractor Predicting: 138it [01:32,  1.51it/s]Extractor Predicting: 139it [01:32,  1.52it/s]Extractor Predicting: 140it [01:33,  1.57it/s]Extractor Predicting: 141it [01:34,  1.57it/s]Extractor Predicting: 142it [01:34,  1.55it/s]Extractor Predicting: 143it [01:35,  1.53it/s]Extractor Predicting: 144it [01:36,  1.53it/s]Extractor Predicting: 145it [01:36,  1.54it/s]Extractor Predicting: 146it [01:37,  1.56it/s]Extractor Predicting: 147it [01:38,  1.57it/s]Extractor Predicting: 148it [01:38,  1.52it/s]Extractor Predicting: 149it [01:39,  1.54it/s]Extractor Predicting: 150it [01:40,  1.53it/s]Extractor Predicting: 151it [01:40,  1.53it/s]Extractor Predicting: 152it [01:41,  1.53it/s]Extractor Predicting: 153it [01:42,  1.51it/s]Extractor Predicting: 154it [01:42,  1.53it/s]Extractor Predicting: 155it [01:43,  1.54it/s]Extractor Predicting: 156it [01:43,  1.57it/s]Extractor Predicting: 157it [01:44,  1.57it/s]Extractor Predicting: 158it [01:45,  1.62it/s]Extractor Predicting: 159it [01:45,  1.58it/s]Extractor Predicting: 160it [01:46,  1.55it/s]Extractor Predicting: 161it [01:47,  1.53it/s]Extractor Predicting: 162it [01:47,  1.51it/s]Extractor Predicting: 163it [01:48,  1.54it/s]Extractor Predicting: 164it [01:49,  1.54it/s]Extractor Predicting: 165it [01:49,  1.56it/s]Extractor Predicting: 166it [01:50,  1.58it/s]Extractor Predicting: 167it [01:50,  1.57it/s]Extractor Predicting: 168it [01:51,  1.52it/s]Extractor Predicting: 169it [01:52,  1.52it/s]Extractor Predicting: 170it [01:53,  1.51it/s]Extractor Predicting: 171it [01:53,  1.52it/s]Extractor Predicting: 172it [01:54,  1.52it/s]Extractor Predicting: 173it [01:54,  1.51it/s]Extractor Predicting: 174it [01:55,  1.51it/s]Extractor Predicting: 175it [01:56,  1.48it/s]Extractor Predicting: 176it [01:57,  1.45it/s]Extractor Predicting: 177it [01:57,  1.50it/s]Extractor Predicting: 178it [01:58,  1.49it/s]Extractor Predicting: 179it [01:59,  1.51it/s]Extractor Predicting: 180it [01:59,  1.53it/s]Extractor Predicting: 181it [02:00,  1.50it/s]Extractor Predicting: 182it [02:01,  1.51it/s]Extractor Predicting: 183it [02:01,  1.50it/s]Extractor Predicting: 184it [02:02,  1.48it/s]Extractor Predicting: 185it [02:03,  1.47it/s]Extractor Predicting: 186it [02:03,  1.47it/s]Extractor Predicting: 187it [02:04,  1.48it/s]Extractor Predicting: 188it [02:05,  1.46it/s]Extractor Predicting: 189it [02:05,  1.46it/s]Extractor Predicting: 190it [02:06,  1.45it/s]Extractor Predicting: 191it [02:07,  1.48it/s]Extractor Predicting: 192it [02:07,  1.52it/s]Extractor Predicting: 193it [02:08,  1.46it/s]Extractor Predicting: 194it [02:09,  1.45it/s]Extractor Predicting: 195it [02:09,  1.46it/s]Extractor Predicting: 196it [02:10,  1.47it/s]Extractor Predicting: 197it [02:11,  1.49it/s]Extractor Predicting: 198it [02:11,  1.47it/s]Extractor Predicting: 199it [02:12,  1.50it/s]Extractor Predicting: 200it [02:13,  1.41it/s]Extractor Predicting: 201it [02:13,  1.47it/s]Extractor Predicting: 202it [02:14,  1.49it/s]Extractor Predicting: 203it [02:15,  1.49it/s]Extractor Predicting: 204it [02:15,  1.49it/s]Extractor Predicting: 205it [02:16,  1.51it/s]Extractor Predicting: 206it [02:17,  1.50it/s]Extractor Predicting: 207it [02:17,  1.51it/s]Extractor Predicting: 208it [02:18,  1.49it/s]Extractor Predicting: 209it [02:19,  1.50it/s]Extractor Predicting: 210it [02:19,  1.50it/s]Extractor Predicting: 211it [02:20,  1.50it/s]Extractor Predicting: 212it [02:21,  1.51it/s]Extractor Predicting: 213it [02:21,  1.51it/s]Extractor Predicting: 214it [02:22,  1.49it/s]Extractor Predicting: 215it [02:23,  1.48it/s]Extractor Predicting: 216it [02:23,  1.51it/s]Extractor Predicting: 217it [02:24,  1.53it/s]Extractor Predicting: 218it [02:25,  1.51it/s]Extractor Predicting: 219it [02:25,  1.49it/s]Extractor Predicting: 220it [02:26,  1.50it/s]Extractor Predicting: 221it [02:27,  1.51it/s]Extractor Predicting: 222it [02:27,  1.50it/s]Extractor Predicting: 223it [02:28,  1.53it/s]Extractor Predicting: 224it [02:29,  1.53it/s]Extractor Predicting: 225it [02:29,  1.51it/s]Extractor Predicting: 226it [02:30,  1.49it/s]Extractor Predicting: 227it [02:31,  1.50it/s]Extractor Predicting: 228it [02:31,  1.50it/s]Extractor Predicting: 229it [02:32,  1.52it/s]Extractor Predicting: 230it [02:33,  1.53it/s]Extractor Predicting: 231it [02:33,  1.52it/s]Extractor Predicting: 232it [02:34,  1.51it/s]Extractor Predicting: 233it [02:35,  1.51it/s]Extractor Predicting: 234it [02:35,  1.52it/s]Extractor Predicting: 235it [02:36,  1.53it/s]Extractor Predicting: 236it [02:37,  1.53it/s]Extractor Predicting: 237it [02:37,  1.52it/s]Extractor Predicting: 238it [02:38,  1.51it/s]Extractor Predicting: 239it [02:39,  1.50it/s]Extractor Predicting: 240it [02:39,  1.53it/s]Extractor Predicting: 241it [02:40,  1.53it/s]Extractor Predicting: 242it [02:41,  1.55it/s]Extractor Predicting: 243it [02:41,  1.55it/s]Extractor Predicting: 244it [02:42,  1.54it/s]Extractor Predicting: 245it [02:43,  1.53it/s]Extractor Predicting: 246it [02:43,  1.52it/s]Extractor Predicting: 247it [02:44,  1.54it/s]Extractor Predicting: 248it [02:44,  1.53it/s]Extractor Predicting: 249it [02:45,  1.54it/s]Extractor Predicting: 250it [02:46,  1.56it/s]Extractor Predicting: 251it [02:46,  1.52it/s]Extractor Predicting: 252it [02:47,  1.50it/s]Extractor Predicting: 253it [02:48,  1.56it/s]Extractor Predicting: 254it [02:48,  1.54it/s]Extractor Predicting: 255it [02:49,  1.52it/s]Extractor Predicting: 256it [02:50,  1.51it/s]Extractor Predicting: 257it [02:50,  1.52it/s]Extractor Predicting: 258it [02:51,  1.55it/s]Extractor Predicting: 259it [02:52,  1.56it/s]Extractor Predicting: 260it [02:52,  1.56it/s]Extractor Predicting: 261it [02:53,  1.53it/s]Extractor Predicting: 262it [02:54,  1.54it/s]Extractor Predicting: 263it [02:54,  1.55it/s]Extractor Predicting: 264it [02:55,  1.56it/s]Extractor Predicting: 265it [02:55,  1.57it/s]Extractor Predicting: 266it [02:56,  1.60it/s]Extractor Predicting: 267it [02:57,  1.55it/s]Extractor Predicting: 268it [02:57,  1.55it/s]Extractor Predicting: 269it [02:58,  1.56it/s]Extractor Predicting: 270it [02:59,  1.56it/s]Extractor Predicting: 271it [02:59,  1.59it/s]Extractor Predicting: 272it [03:00,  1.61it/s]Extractor Predicting: 273it [03:01,  1.43it/s]Extractor Predicting: 274it [03:01,  1.46it/s]Extractor Predicting: 275it [03:02,  1.49it/s]Extractor Predicting: 276it [03:03,  1.51it/s]Extractor Predicting: 277it [03:03,  1.52it/s]Extractor Predicting: 278it [03:04,  1.53it/s]Extractor Predicting: 279it [03:05,  1.51it/s]Extractor Predicting: 280it [03:05,  1.53it/s]Extractor Predicting: 281it [03:06,  1.52it/s]Extractor Predicting: 281it [03:06,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:18,845 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:18,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:18,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:18,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:18,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:30:19,459 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:30:19,460 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:30:20,049 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:30:21,105 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:30:21,105 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:23,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:23,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:23,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:23,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:30:23,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:30:24,601 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:30:24,602 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:30:25,172 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:30:25,323 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:30:25,323 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.42529614061902943,
  "recall": 0.16506006228681597,
  "score": 0.23782051282051284,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.27it/s]Extractor Predicting: 2it [00:01,  1.32it/s]Extractor Predicting: 3it [00:02,  1.39it/s]Extractor Predicting: 4it [00:02,  1.40it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:03,  1.84it/s]Extractor Predicting: 6it [00:03,  1.57it/s]
[INFO|configuration_utils.py:515] 2023-08-28 13:30:29,531 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:30:29,532 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:30:29,536 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:30:29,537 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 13:30:29,540 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:30:32,607 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 13:30:32,610 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 13:30:32,625 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:30:32,625 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:30:32,633 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:30:32,635 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:30:32,635 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:30:32,635 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:30:32,635 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:30:32,636 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:30:32,636 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.09836065573770492,
  "recall": 0.023346303501945526,
  "score": 0.03773584905660377,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 13:30:32,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:33,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:34,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:35,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:36,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:36,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:37,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:38,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:39,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:39,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:40,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:41,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:42,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:43,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:43,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:44,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:45,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:46,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:47,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:47,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:43, 15.95s/it][WARNING|generation_utils.py:914] 2023-08-28 13:30:48,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:49,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:50,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:51,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:52,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:53,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:53,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:54,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:55,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:56,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:56,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:57,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:58,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:59,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:00,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:00,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:01,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:02,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:03,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:04,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:05,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:05,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:43, 17.16s/it][WARNING|generation_utils.py:914] 2023-08-28 13:31:06,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:07,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:08,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:09,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:09,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:10,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:11,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:12,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:12,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:13,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:14,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:14,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:15,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:16,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:17,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:18,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:18,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:19,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:20,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:21,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:21,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:22,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:50<03:21, 16.81s/it][WARNING|generation_utils.py:914] 2023-08-28 13:31:23,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:23,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:24,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:25,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:25,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:26,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:27,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:28,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:28,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:29,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:30,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:30,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:31,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:32,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:32,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:33,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:34,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:35,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:35,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:36,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:37,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:37,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:05<02:57, 16.18s/it][WARNING|generation_utils.py:914] 2023-08-28 13:31:38,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:39,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:39,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:40,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:41,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:42,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:43,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:44,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:44,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:45,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:46,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:47,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:48,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:48,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:49,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:50,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:51,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:52,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:52,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:53,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:54,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:55,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:22<02:46, 16.63s/it][WARNING|generation_utils.py:914] 2023-08-28 13:31:55,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:56,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:57,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:57,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:58,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:59,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:00,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:00,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:01,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:02,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:03,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:03,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:04,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:05,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:06,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:06,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:07,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:08,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:08,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:09,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:10,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:10,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:11,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:39<02:29, 16.59s/it][WARNING|generation_utils.py:914] 2023-08-28 13:32:12,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:13,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:14,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:14,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:15,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:16,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:17,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:17,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:18,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:19,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:20,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:20,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:21,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:22,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:23,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:23,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:24,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:25,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:26,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:27,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:27,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:28,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:56<02:14, 16.79s/it][WARNING|generation_utils.py:914] 2023-08-28 13:32:29,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:30,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:31,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:31,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:32,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:33,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:34,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:34,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:35,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:36,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:36,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:37,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:38,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:39,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:39,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:40,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:41,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:42,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:42,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:43,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:44,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:45,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:45,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:13<01:57, 16.79s/it][WARNING|generation_utils.py:914] 2023-08-28 13:32:46,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:47,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:47,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:48,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:49,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:50,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:51,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:51,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:52,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:53,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:54,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:55,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:55,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:56,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:57,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:58,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:59,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:32:59,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:00,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:01,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:02,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:03,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:31<01:42, 17.07s/it][WARNING|generation_utils.py:914] 2023-08-28 13:33:04,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:04,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:05,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:06,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:06,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:07,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:08,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:08,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:09,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:10,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:10,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:11,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:12,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:13,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:13,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:14,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:15,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:15,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:16,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:17,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:17,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:45<01:21, 16.27s/it][WARNING|generation_utils.py:914] 2023-08-28 13:33:18,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:19,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:19,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:20,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:21,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:22,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:22,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:23,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:24,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:25,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:25,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:26,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:27,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:28,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:29,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:29,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:30,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:31,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:32,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:32,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:33,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:01<01:04, 16.13s/it][WARNING|generation_utils.py:914] 2023-08-28 13:33:34,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:35,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:35,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:36,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:37,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:38,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:39,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:39,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:40,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:41,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:42,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:42,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:43,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:44,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:45,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:45,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:46,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:47,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:48,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:48,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:49,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:17<00:48, 16.08s/it][WARNING|generation_utils.py:914] 2023-08-28 13:33:50,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:51,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:51,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:52,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:53,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:53,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:54,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:55,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:56,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:56,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:57,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:58,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:58,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:33:59,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:00,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:01,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:02,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:02,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:03,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:04,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:04,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:05,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:06,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:07,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:35<00:33, 16.63s/it][WARNING|generation_utils.py:914] 2023-08-28 13:34:08,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:08,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:09,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:10,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:10,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:11,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:12,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:12,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:13,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:14,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:14,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:15,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:16,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:16,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:17,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:18,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:19,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:19,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:20,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:21,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:22,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:22,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:23,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:24,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:25,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:53<00:17, 17.04s/it][WARNING|generation_utils.py:914] 2023-08-28 13:34:26,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:26,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:27,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:28,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:29,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:30,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:31,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:31,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:32,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:33,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:34,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:35,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:35,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:36,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:37,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:38,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:39,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:39,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:40,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:41,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:42,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:43,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:34:44,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:11<00:00, 17.51s/it]Generating: 100%|██████████| 15/15 [04:11<00:00, 16.80s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:51,234 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:51,240 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:51,240 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:51,240 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:51,240 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:34:51,849 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:34:51,850 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:34:52,415 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:34:53,477 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:34:53,477 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:56,323 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:56,332 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:56,332 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:56,332 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:34:56,332 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:34:56,961 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:34:56,962 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:34:57,233 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:34:57,392 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:34:57,392 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8892045454545454, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : participant in .', 'success_rate': 0.8579545454545454, 'errors': {''}}
['Relation : platform . Context : The game is a spin on " The Legend of Zelda " and is a remake of the " Zelda " series . Head Entity : The Legend of Zelda , Tail Entity : Nintendo .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8206521739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : location .', 'success_rate': 0.8678977272727273, 'errors': {'', '(\'Central Hotel\', \'location\', \'\', "The city is known for the building of the Central Hotel , a structure located on the eastern edge of the city \'s outskirts , and designed by local architect Charles Lassalle .")'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8464673913043478, 'errors': {'', '(\'Andrea Horwath\', \'member of political party\', \'\', "On 29 May 2010 , the party \'s leader , Andrea Horwath , announced that it would not vote for her in the European Parliament .")'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 607, 'raw': 704}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8622159090909091, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9241071428571429, 'errors': {''}}
['Relation : owned by . Context : Later in the year , the property was bought by a real estate dealer who had a stake in the firm , who then bought it in 1987 for a reported $ 12 million . Head Entity : Real estate dealer , Tail Entity : Sotheby Regency .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8928571428571429, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : position held .', 'success_rate': 0.8125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 412, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 584, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.765, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married the fourth Baronet of the East , Lord Marjorie of Warwick , the brother of the Protestant Reformation , and the sister of the Roman Catholic Pope . Head Entity : Lord Marjorie of Warwick , Tail Entity : Christian Reformation .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : religion .', 'success_rate': 0.8478260869565217, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 11657
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11757, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.52it/s]Extractor Estimating: 2it [00:01,  1.39it/s]Extractor Estimating: 3it [00:02,  1.44it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.42it/s]Extractor Estimating: 6it [00:04,  1.47it/s]Extractor Estimating: 7it [00:04,  1.48it/s]Extractor Estimating: 8it [00:05,  1.51it/s]Extractor Estimating: 9it [00:06,  1.47it/s]Extractor Estimating: 10it [00:06,  1.48it/s]Extractor Estimating: 11it [00:07,  1.52it/s]Extractor Estimating: 12it [00:08,  1.47it/s]Extractor Estimating: 13it [00:08,  1.53it/s]Extractor Estimating: 14it [00:09,  1.50it/s]Extractor Estimating: 15it [00:10,  1.50it/s]Extractor Estimating: 16it [00:10,  1.52it/s]Extractor Estimating: 17it [00:11,  1.53it/s]Extractor Estimating: 18it [00:12,  1.54it/s]Extractor Estimating: 19it [00:12,  1.53it/s]Extractor Estimating: 20it [00:13,  1.48it/s]Extractor Estimating: 21it [00:14,  1.50it/s]Extractor Estimating: 22it [00:14,  1.42it/s]Extractor Estimating: 23it [00:15,  1.42it/s]Extractor Estimating: 24it [00:16,  1.50it/s]Extractor Estimating: 25it [00:16,  1.51it/s]Extractor Estimating: 26it [00:17,  1.48it/s]Extractor Estimating: 27it [00:18,  1.50it/s]Extractor Estimating: 28it [00:18,  1.53it/s]Extractor Estimating: 29it [00:19,  1.55it/s]Extractor Estimating: 30it [00:20,  1.44it/s]Extractor Estimating: 31it [00:20,  1.44it/s]Extractor Estimating: 32it [00:21,  1.49it/s]Extractor Estimating: 33it [00:22,  1.52it/s]Extractor Estimating: 34it [00:22,  1.49it/s]Extractor Estimating: 35it [00:23,  1.51it/s]Extractor Estimating: 36it [00:24,  1.50it/s]Extractor Estimating: 37it [00:24,  1.47it/s]Extractor Estimating: 38it [00:25,  1.44it/s]Extractor Estimating: 39it [00:26,  1.45it/s]Extractor Estimating: 40it [00:26,  1.49it/s]Extractor Estimating: 41it [00:27,  1.49it/s]Extractor Estimating: 42it [00:28,  1.53it/s]Extractor Estimating: 43it [00:28,  1.48it/s]Extractor Estimating: 44it [00:29,  1.40it/s]Extractor Estimating: 45it [00:30,  1.36it/s]Extractor Estimating: 46it [00:31,  1.37it/s]Extractor Estimating: 47it [00:31,  1.39it/s]Extractor Estimating: 48it [00:32,  1.42it/s]Extractor Estimating: 49it [00:33,  1.42it/s]Extractor Estimating: 50it [00:34,  1.41it/s]Extractor Estimating: 51it [00:34,  1.43it/s]Extractor Estimating: 52it [00:35,  1.45it/s]Extractor Estimating: 53it [00:36,  1.48it/s]Extractor Estimating: 54it [00:36,  1.50it/s]Extractor Estimating: 55it [00:37,  1.57it/s]Extractor Estimating: 56it [00:37,  1.62it/s]Extractor Estimating: 57it [00:38,  1.64it/s]Extractor Estimating: 58it [00:38,  1.64it/s]Extractor Estimating: 59it [00:39,  1.59it/s]Extractor Estimating: 60it [00:40,  1.68it/s]Extractor Estimating: 61it [00:40,  1.68it/s]Extractor Estimating: 62it [00:41,  1.70it/s]Extractor Estimating: 63it [00:42,  1.59it/s]Extractor Estimating: 64it [00:42,  1.61it/s]Extractor Estimating: 65it [00:43,  1.56it/s]Extractor Estimating: 66it [00:44,  1.56it/s]Extractor Estimating: 67it [00:44,  1.59it/s]Extractor Estimating: 68it [00:45,  1.60it/s]Extractor Estimating: 69it [00:45,  1.64it/s]Extractor Estimating: 70it [00:46,  1.65it/s]Extractor Estimating: 71it [00:46,  1.65it/s]Extractor Estimating: 72it [00:47,  1.66it/s]Extractor Estimating: 73it [00:48,  1.62it/s]Extractor Estimating: 74it [00:48,  1.59it/s]Extractor Estimating: 75it [00:49,  1.60it/s]Extractor Estimating: 76it [00:50,  1.60it/s]Extractor Estimating: 77it [00:50,  1.59it/s]Extractor Estimating: 78it [00:51,  1.57it/s]Extractor Estimating: 79it [00:52,  1.60it/s]Extractor Estimating: 80it [00:52,  1.58it/s]Extractor Estimating: 81it [00:53,  1.55it/s]Extractor Estimating: 82it [00:54,  1.53it/s]Extractor Estimating: 83it [00:54,  1.55it/s]Extractor Estimating: 84it [00:55,  1.59it/s]Extractor Estimating: 85it [00:55,  1.59it/s]Extractor Estimating: 86it [00:56,  1.59it/s]Extractor Estimating: 87it [00:57,  1.58it/s]Extractor Estimating: 88it [00:57,  1.57it/s]Extractor Estimating: 89it [00:58,  1.55it/s]Extractor Estimating: 90it [00:59,  1.58it/s]Extractor Estimating: 91it [00:59,  1.54it/s]Extractor Estimating: 92it [01:00,  1.56it/s]Extractor Estimating: 93it [01:00,  1.60it/s]Extractor Estimating: 94it [01:01,  1.53it/s]Extractor Estimating: 95it [01:02,  1.58it/s]Extractor Estimating: 96it [01:02,  1.57it/s]Extractor Estimating: 97it [01:03,  1.58it/s]Extractor Estimating: 98it [01:04,  1.60it/s]Extractor Estimating: 99it [01:04,  1.54it/s]Extractor Estimating: 100it [01:05,  1.57it/s]Extractor Estimating: 101it [01:06,  1.61it/s]Extractor Estimating: 102it [01:06,  1.64it/s]Extractor Estimating: 103it [01:07,  1.67it/s]Extractor Estimating: 104it [01:07,  1.65it/s]Extractor Estimating: 105it [01:08,  1.71it/s]Extractor Estimating: 106it [01:08,  1.72it/s]Extractor Estimating: 107it [01:09,  1.68it/s]Extractor Estimating: 108it [01:10,  1.64it/s]Extractor Estimating: 109it [01:10,  1.66it/s]Extractor Estimating: 110it [01:11,  1.67it/s]Extractor Estimating: 111it [01:11,  1.69it/s]Extractor Estimating: 112it [01:12,  1.67it/s]Extractor Estimating: 113it [01:13,  1.53it/s]Extractor Estimating: 114it [01:13,  1.59it/s]Extractor Estimating: 115it [01:14,  1.58it/s]Extractor Estimating: 116it [01:15,  1.66it/s]Extractor Estimating: 117it [01:15,  1.57it/s]Extractor Estimating: 118it [01:16,  1.60it/s]Extractor Estimating: 119it [01:17,  1.54it/s]Extractor Estimating: 120it [01:17,  1.58it/s]Extractor Estimating: 121it [01:18,  1.63it/s]Extractor Estimating: 122it [01:18,  1.71it/s]Extractor Estimating: 123it [01:19,  1.65it/s]Extractor Estimating: 124it [01:19,  1.72it/s]Extractor Estimating: 125it [01:20,  1.70it/s]Extractor Estimating: 126it [01:21,  1.71it/s]Extractor Estimating: 127it [01:21,  1.71it/s]Extractor Estimating: 128it [01:22,  1.68it/s]Extractor Estimating: 129it [01:22,  1.69it/s]Extractor Estimating: 130it [01:23,  1.71it/s]Extractor Estimating: 131it [01:24,  1.68it/s]Extractor Estimating: 132it [01:24,  1.68it/s]Extractor Estimating: 133it [01:25,  1.66it/s]Extractor Estimating: 134it [01:25,  1.65it/s]Extractor Estimating: 135it [01:26,  1.65it/s]Extractor Estimating: 136it [01:27,  1.72it/s]Extractor Estimating: 137it [01:27,  1.76it/s]Extractor Estimating: 138it [01:28,  1.71it/s]Extractor Estimating: 139it [01:28,  1.73it/s]Extractor Estimating: 140it [01:29,  1.74it/s]Extractor Estimating: 141it [01:29,  1.71it/s]Extractor Estimating: 142it [01:30,  1.73it/s]Extractor Estimating: 143it [01:31,  1.76it/s]Extractor Estimating: 144it [01:31,  1.78it/s]Extractor Estimating: 145it [01:32,  1.66it/s]Extractor Estimating: 146it [01:32,  1.71it/s]Extractor Estimating: 147it [01:33,  1.71it/s]Extractor Estimating: 148it [01:34,  1.73it/s]Extractor Estimating: 149it [01:34,  1.76it/s]Extractor Estimating: 150it [01:35,  1.67it/s]Extractor Estimating: 151it [01:35,  1.57it/s]Extractor Estimating: 152it [01:36,  1.52it/s]Extractor Estimating: 153it [01:37,  1.50it/s]Extractor Estimating: 154it [01:38,  1.50it/s]Extractor Estimating: 155it [01:38,  1.48it/s]Extractor Estimating: 156it [01:39,  1.53it/s]Extractor Estimating: 157it [01:40,  1.47it/s]Extractor Estimating: 158it [01:40,  1.42it/s]Extractor Estimating: 159it [01:41,  1.43it/s]Extractor Estimating: 160it [01:42,  1.42it/s]Extractor Estimating: 161it [01:42,  1.47it/s]Extractor Estimating: 162it [01:43,  1.48it/s]Extractor Estimating: 163it [01:44,  1.47it/s]Extractor Estimating: 164it [01:45,  1.39it/s]Extractor Estimating: 165it [01:45,  1.40it/s]Extractor Estimating: 166it [01:46,  1.43it/s]Extractor Estimating: 167it [01:46,  1.49it/s]Extractor Estimating: 168it [01:47,  1.46it/s]Extractor Estimating: 169it [01:48,  1.39it/s]Extractor Estimating: 170it [01:49,  1.38it/s]Extractor Estimating: 171it [01:49,  1.44it/s]Extractor Estimating: 172it [01:50,  1.47it/s]Extractor Estimating: 173it [01:51,  1.50it/s]Extractor Estimating: 174it [01:51,  1.49it/s]Extractor Estimating: 175it [01:52,  1.46it/s]Extractor Estimating: 176it [01:53,  1.50it/s]Extractor Estimating: 177it [01:53,  1.54it/s]Extractor Estimating: 178it [01:54,  1.49it/s]Extractor Estimating: 179it [01:55,  1.54it/s]Extractor Estimating: 180it [01:55,  1.50it/s]Extractor Estimating: 181it [01:56,  1.51it/s]Extractor Estimating: 182it [01:57,  1.55it/s]Extractor Estimating: 183it [01:57,  1.58it/s]Extractor Estimating: 184it [01:58,  1.64it/s]Extractor Estimating: 185it [01:58,  1.64it/s]Extractor Estimating: 186it [01:59,  1.64it/s]Extractor Estimating: 187it [02:00,  1.64it/s]Extractor Estimating: 188it [02:00,  1.64it/s]Extractor Estimating: 189it [02:01,  1.47it/s]Extractor Estimating: 190it [02:02,  1.50it/s]Extractor Estimating: 191it [02:02,  1.53it/s]Extractor Estimating: 192it [02:03,  1.58it/s]Extractor Estimating: 193it [02:04,  1.53it/s]Extractor Estimating: 194it [02:04,  1.53it/s]Extractor Estimating: 195it [02:05,  1.54it/s]Extractor Estimating: 196it [02:05,  1.54it/s]Extractor Estimating: 197it [02:06,  1.59it/s]Extractor Estimating: 198it [02:07,  1.57it/s]Extractor Estimating: 199it [02:07,  1.61it/s]Extractor Estimating: 200it [02:08,  1.59it/s]Extractor Estimating: 201it [02:09,  1.50it/s]Extractor Estimating: 202it [02:09,  1.48it/s]Extractor Estimating: 203it [02:10,  1.46it/s]Extractor Estimating: 204it [02:11,  1.44it/s]Extractor Estimating: 205it [02:12,  1.38it/s]Extractor Estimating: 206it [02:12,  1.39it/s]Extractor Estimating: 207it [02:13,  1.43it/s]Extractor Estimating: 208it [02:14,  1.40it/s]Extractor Estimating: 209it [02:15,  1.38it/s]Extractor Estimating: 210it [02:15,  1.41it/s]Extractor Estimating: 211it [02:16,  1.39it/s]Extractor Estimating: 212it [02:17,  1.37it/s]Extractor Estimating: 213it [02:17,  1.38it/s]Extractor Estimating: 214it [02:18,  1.36it/s]Extractor Estimating: 215it [02:19,  1.32it/s]Extractor Estimating: 216it [02:20,  1.32it/s]Extractor Estimating: 217it [02:20,  1.35it/s]Extractor Estimating: 218it [02:21,  1.44it/s]Extractor Estimating: 219it [02:22,  1.42it/s]Extractor Estimating: 220it [02:22,  1.44it/s]Extractor Estimating: 221it [02:23,  1.43it/s]Extractor Estimating: 222it [02:24,  1.42it/s]Extractor Estimating: 223it [02:24,  1.45it/s]Extractor Estimating: 224it [02:25,  1.40it/s]Extractor Estimating: 225it [02:26,  1.41it/s]Extractor Estimating: 226it [02:27,  1.48it/s]Extractor Estimating: 227it [02:27,  1.63it/s]Extractor Estimating: 228it [02:28,  1.71it/s]Extractor Estimating: 229it [02:28,  1.70it/s]Extractor Estimating: 230it [02:29,  1.77it/s]Extractor Estimating: 231it [02:29,  1.73it/s]Extractor Estimating: 232it [02:30,  1.75it/s]Extractor Estimating: 233it [02:30,  1.79it/s]Extractor Estimating: 234it [02:31,  1.82it/s]Extractor Estimating: 235it [02:31,  1.87it/s]Extractor Estimating: 236it [02:32,  1.82it/s]Extractor Estimating: 237it [02:33,  1.76it/s]Extractor Estimating: 238it [02:33,  1.81it/s]Extractor Estimating: 239it [02:34,  1.65it/s]Extractor Estimating: 240it [02:35,  1.57it/s]Extractor Estimating: 241it [02:35,  1.65it/s]Extractor Estimating: 242it [02:36,  1.72it/s]Extractor Estimating: 243it [02:36,  1.67it/s]Extractor Estimating: 244it [02:37,  1.71it/s]Extractor Estimating: 245it [02:37,  1.69it/s]Extractor Estimating: 246it [02:38,  1.75it/s]Extractor Estimating: 247it [02:38,  1.72it/s]Extractor Estimating: 248it [02:39,  1.74it/s]Extractor Estimating: 249it [02:40,  1.74it/s]Extractor Estimating: 250it [02:40,  1.64it/s]Extractor Estimating: 251it [02:41,  1.61it/s]Extractor Estimating: 252it [02:42,  1.60it/s]Extractor Estimating: 253it [02:42,  1.60it/s]Extractor Estimating: 254it [02:43,  1.58it/s]Extractor Estimating: 255it [02:44,  1.59it/s]Extractor Estimating: 256it [02:44,  1.59it/s]Extractor Estimating: 257it [02:45,  1.61it/s]Extractor Estimating: 258it [02:45,  1.56it/s]Extractor Estimating: 259it [02:46,  1.53it/s]Extractor Estimating: 260it [02:47,  1.51it/s]Extractor Estimating: 261it [02:47,  1.50it/s]Extractor Estimating: 262it [02:48,  1.46it/s]Extractor Estimating: 263it [02:49,  1.45it/s]Extractor Estimating: 264it [02:50,  1.46it/s]Extractor Estimating: 265it [02:50,  1.48it/s]Extractor Estimating: 266it [02:51,  1.46it/s]Extractor Estimating: 267it [02:52,  1.49it/s]Extractor Estimating: 268it [02:52,  1.54it/s]Extractor Estimating: 269it [02:53,  1.56it/s]Extractor Estimating: 270it [02:54,  1.44it/s]Extractor Estimating: 271it [02:54,  1.48it/s]Extractor Estimating: 272it [02:55,  1.55it/s]Extractor Estimating: 273it [02:55,  1.52it/s]Extractor Estimating: 274it [02:56,  1.51it/s]Extractor Estimating: 275it [02:57,  1.49it/s]Extractor Estimating: 276it [02:57,  1.55it/s]Extractor Estimating: 277it [02:58,  1.57it/s]Extractor Estimating: 278it [02:59,  1.53it/s]Extractor Estimating: 279it [02:59,  1.49it/s]Extractor Estimating: 280it [03:00,  1.52it/s]Extractor Estimating: 281it [03:01,  1.48it/s]Extractor Estimating: 282it [03:01,  1.53it/s]Extractor Estimating: 283it [03:02,  1.58it/s]Extractor Estimating: 284it [03:03,  1.58it/s]Extractor Estimating: 285it [03:03,  1.62it/s]Extractor Estimating: 286it [03:04,  1.65it/s]Extractor Estimating: 287it [03:04,  1.62it/s]Extractor Estimating: 288it [03:05,  1.60it/s]Extractor Estimating: 289it [03:06,  1.55it/s]Extractor Estimating: 290it [03:06,  1.55it/s]Extractor Estimating: 291it [03:07,  1.54it/s]Extractor Estimating: 292it [03:08,  1.58it/s]Extractor Estimating: 293it [03:08,  1.60it/s]Extractor Estimating: 294it [03:09,  1.58it/s]Extractor Estimating: 295it [03:09,  1.62it/s]Extractor Estimating: 296it [03:10,  1.60it/s]Extractor Estimating: 297it [03:11,  1.57it/s]Extractor Estimating: 298it [03:11,  1.60it/s]Extractor Estimating: 299it [03:12,  1.66it/s]Extractor Estimating: 300it [03:13,  1.65it/s]Extractor Estimating: 301it [03:13,  1.68it/s]Extractor Estimating: 302it [03:14,  1.62it/s]Extractor Estimating: 303it [03:14,  1.63it/s]Extractor Estimating: 304it [03:15,  1.61it/s]Extractor Estimating: 305it [03:16,  1.65it/s]Extractor Estimating: 306it [03:16,  1.60it/s]Extractor Estimating: 307it [03:17,  1.57it/s]Extractor Estimating: 308it [03:18,  1.60it/s]Extractor Estimating: 309it [03:18,  1.62it/s]Extractor Estimating: 310it [03:19,  1.57it/s]Extractor Estimating: 311it [03:19,  1.61it/s]Extractor Estimating: 312it [03:20,  1.64it/s]Extractor Estimating: 313it [03:21,  1.69it/s]Extractor Estimating: 314it [03:21,  1.60it/s]Extractor Estimating: 315it [03:22,  1.62it/s]Extractor Estimating: 316it [03:23,  1.55it/s]Extractor Estimating: 317it [03:23,  1.57it/s]Extractor Estimating: 318it [03:24,  1.57it/s]Extractor Estimating: 319it [03:24,  1.61it/s]Extractor Estimating: 320it [03:25,  1.62it/s]Extractor Estimating: 321it [03:26,  1.61it/s]Extractor Estimating: 322it [03:26,  1.63it/s]Extractor Estimating: 323it [03:27,  1.62it/s]Extractor Estimating: 324it [03:28,  1.56it/s]Extractor Estimating: 325it [03:28,  1.55it/s]Extractor Estimating: 326it [03:29,  1.61it/s]Extractor Estimating: 327it [03:29,  1.67it/s]Extractor Estimating: 328it [03:30,  1.66it/s]Extractor Estimating: 329it [03:30,  1.70it/s]Extractor Estimating: 330it [03:31,  1.72it/s]Extractor Estimating: 331it [03:32,  1.74it/s]Extractor Estimating: 332it [03:32,  1.74it/s]Extractor Estimating: 333it [03:33,  1.69it/s]Extractor Estimating: 334it [03:33,  1.69it/s]Extractor Estimating: 335it [03:34,  1.71it/s]Extractor Estimating: 336it [03:35,  1.72it/s]Extractor Estimating: 337it [03:35,  1.71it/s]Extractor Estimating: 338it [03:36,  1.73it/s]Extractor Estimating: 339it [03:36,  1.73it/s]Extractor Estimating: 340it [03:37,  1.67it/s]Extractor Estimating: 341it [03:38,  1.67it/s]Extractor Estimating: 342it [03:38,  1.71it/s]Extractor Estimating: 343it [03:39,  1.72it/s]Extractor Estimating: 344it [03:39,  1.72it/s]Extractor Estimating: 345it [03:40,  1.66it/s]Extractor Estimating: 346it [03:40,  1.68it/s]Extractor Estimating: 347it [03:41,  1.69it/s]Extractor Estimating: 348it [03:42,  1.72it/s]Extractor Estimating: 349it [03:42,  1.71it/s]Extractor Estimating: 350it [03:43,  1.67it/s]Extractor Estimating: 351it [03:43,  1.65it/s]Extractor Estimating: 352it [03:44,  1.57it/s]Extractor Estimating: 353it [03:45,  1.57it/s]Extractor Estimating: 354it [03:46,  1.48it/s]Extractor Estimating: 355it [03:46,  1.49it/s]Extractor Estimating: 356it [03:47,  1.53it/s]Extractor Estimating: 357it [03:47,  1.55it/s]Extractor Estimating: 358it [03:48,  1.55it/s]Extractor Estimating: 359it [03:49,  1.56it/s]Extractor Estimating: 360it [03:50,  1.41it/s]Extractor Estimating: 361it [03:50,  1.53it/s]Extractor Estimating: 362it [03:51,  1.47it/s]Extractor Estimating: 363it [03:52,  1.49it/s]Extractor Estimating: 364it [03:52,  1.56it/s]Extractor Estimating: 365it [03:53,  1.61it/s]Extractor Estimating: 366it [03:53,  1.56it/s]Extractor Estimating: 367it [03:54,  1.49it/s]Extractor Estimating: 368it [03:55,  1.48it/s]Extractor Estimating: 369it [03:56,  1.44it/s]Extractor Estimating: 370it [03:56,  1.44it/s]Extractor Estimating: 371it [03:57,  1.48it/s]Extractor Estimating: 372it [03:58,  1.49it/s]Extractor Estimating: 373it [03:58,  1.51it/s]Extractor Estimating: 374it [03:59,  1.53it/s]Extractor Estimating: 375it [04:00,  1.43it/s]Extractor Estimating: 375it [04:00,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:08,419 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:08,424 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:08,424 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:08,424 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:08,424 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:39:08,737 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:39:08,738 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:39:09,426 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:39:10,491 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:39:10,491 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:11,804 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:11,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:11,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:11,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:39:11,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:39:12,135 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:39:12,136 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:39:12,402 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:39:12,557 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:39:12,557 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 16:02:10,044 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 16:02:10,094 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7493 mean pseudo reward: 0.9614402134736598
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 22472
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22572, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22572, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.133, loss:686.8242
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.118, loss:650.7553
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.098, loss:637.0763
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.089, loss:612.6322
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.116, loss:584.2607
>> valid entity prec:0.6206, rec:0.5506, f1:0.5835
>> valid relation prec:0.3406, rec:0.1613, f1:0.2189
>> valid relation with NER prec:0.3406, rec:0.1613, f1:0.2189
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.485, loss:625.5318
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.095, loss:583.4386
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.109, loss:589.8791
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.105, loss:645.9701
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.107, loss:588.8870
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5863, rec:0.5532, f1:0.5693
>> valid relation prec:0.3233, rec:0.1450, f1:0.2002
>> valid relation with NER prec:0.3233, rec:0.1450, f1:0.2002
g_step 1100, step 161, avg_time 2.469, loss:588.7749
g_step 1200, step 261, avg_time 1.107, loss:631.4158
g_step 1300, step 48, avg_time 1.103, loss:585.6135
g_step 1400, step 148, avg_time 1.116, loss:591.6322
g_step 1500, step 248, avg_time 1.102, loss:595.3814
>> valid entity prec:0.6021, rec:0.5773, f1:0.5894
>> valid relation prec:0.3778, rec:0.1476, f1:0.2122
>> valid relation with NER prec:0.3778, rec:0.1476, f1:0.2122
new max entity f1 on valid!
g_step 1600, step 35, avg_time 2.489, loss:597.0866
g_step 1700, step 135, avg_time 1.106, loss:548.0138
g_step 1800, step 235, avg_time 1.109, loss:571.0892
g_step 1900, step 22, avg_time 1.100, loss:551.1036
g_step 2000, step 122, avg_time 1.117, loss:516.8347
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5947, rec:0.5969, f1:0.5958
>> valid relation prec:0.3043, rec:0.1530, f1:0.2036
>> valid relation with NER prec:0.3043, rec:0.1530, f1:0.2036
new max entity f1 on valid!
g_step 2100, step 222, avg_time 2.488, loss:530.7854
g_step 2200, step 9, avg_time 1.096, loss:525.3860
g_step 2300, step 109, avg_time 1.115, loss:501.0871
g_step 2400, step 209, avg_time 1.092, loss:505.6673
g_step 2500, step 309, avg_time 1.112, loss:519.7425
>> valid entity prec:0.5886, rec:0.5855, f1:0.5871
>> valid relation prec:0.3075, rec:0.1430, f1:0.1952
>> valid relation with NER prec:0.3075, rec:0.1430, f1:0.1952
g_step 2600, step 96, avg_time 2.486, loss:468.5741
g_step 2700, step 196, avg_time 1.103, loss:487.6781
g_step 2800, step 296, avg_time 1.112, loss:518.9468
g_step 2900, step 83, avg_time 1.090, loss:439.3865
g_step 3000, step 183, avg_time 1.119, loss:468.2252
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5944, rec:0.5836, f1:0.5890
>> valid relation prec:0.2902, rec:0.1407, f1:0.1895
>> valid relation with NER prec:0.2902, rec:0.1407, f1:0.1895
g_step 3100, step 283, avg_time 2.488, loss:510.2040
g_step 3200, step 70, avg_time 1.117, loss:423.0049
g_step 3300, step 170, avg_time 1.092, loss:443.9505
g_step 3400, step 270, avg_time 1.111, loss:461.9549
g_step 3500, step 57, avg_time 1.105, loss:436.2960
>> valid entity prec:0.5743, rec:0.5959, f1:0.5849
>> valid relation prec:0.2973, rec:0.1473, f1:0.1970
>> valid relation with NER prec:0.2973, rec:0.1473, f1:0.1970
g_step 3600, step 157, avg_time 2.490, loss:411.6423
g_step 3700, step 257, avg_time 1.086, loss:434.5029
g_step 3800, step 44, avg_time 1.111, loss:434.5497
g_step 3900, step 144, avg_time 1.104, loss:407.8217
g_step 4000, step 244, avg_time 1.105, loss:435.6540
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6034, rec:0.5370, f1:0.5682
>> valid relation prec:0.2956, rec:0.1281, f1:0.1787
>> valid relation with NER prec:0.2956, rec:0.1281, f1:0.1787
g_step 4100, step 31, avg_time 2.481, loss:411.3906
g_step 4200, step 131, avg_time 1.094, loss:387.7794
g_step 4300, step 231, avg_time 1.100, loss:392.1162
g_step 4400, step 18, avg_time 1.119, loss:405.0074
g_step 4500, step 118, avg_time 1.109, loss:371.9380
>> valid entity prec:0.6065, rec:0.5795, f1:0.5926
>> valid relation prec:0.3098, rec:0.1444, f1:0.1970
>> valid relation with NER prec:0.3098, rec:0.1444, f1:0.1970
g_step 4600, step 218, avg_time 2.485, loss:391.0457
g_step 4700, step 5, avg_time 1.098, loss:385.0542
g_step 4800, step 105, avg_time 1.100, loss:343.5047
g_step 4900, step 205, avg_time 1.116, loss:382.2184
g_step 5000, step 305, avg_time 1.095, loss:389.8016
learning rate was adjusted to 0.0008
>> valid entity prec:0.5763, rec:0.5679, f1:0.5721
>> valid relation prec:0.3013, rec:0.1312, f1:0.1828
>> valid relation with NER prec:0.3013, rec:0.1312, f1:0.1828
g_step 5100, step 92, avg_time 2.485, loss:345.0845
g_step 5200, step 192, avg_time 1.110, loss:357.0875
g_step 5300, step 292, avg_time 1.111, loss:343.4438
g_step 5400, step 79, avg_time 1.107, loss:325.3255
g_step 5500, step 179, avg_time 1.094, loss:343.3258
>> valid entity prec:0.6118, rec:0.5449, f1:0.5764
>> valid relation prec:0.2862, rec:0.1370, f1:0.1853
>> valid relation with NER prec:0.2862, rec:0.1370, f1:0.1853
g_step 5600, step 279, avg_time 2.481, loss:355.5868
g_step 5700, step 66, avg_time 1.109, loss:332.5863
g_step 5800, step 166, avg_time 1.097, loss:323.5906
g_step 5900, step 266, avg_time 1.105, loss:347.3035
g_step 6000, step 53, avg_time 1.090, loss:312.4192
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5820, rec:0.5782, f1:0.5801
>> valid relation prec:0.2867, rec:0.1367, f1:0.1851
>> valid relation with NER prec:0.2867, rec:0.1367, f1:0.1851
g_step 6100, step 153, avg_time 2.497, loss:321.0295
g_step 6200, step 253, avg_time 1.110, loss:318.9293
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 16:02:10 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 16:02:10 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_16-02-10_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 16:02:11 - WARNING - datasets.builder -   Using custom data configuration default-ea61f99a9a380296
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ea61f99a9a380296/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 16:02:11,418 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:02:11,420 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:02:11,420 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:02:11,421 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:02:11,431 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:02:11,436 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:02:11,436 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:02:11,436 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:02:11,436 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:02:11,436 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:02:11,436 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 16:02:11,567 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:02:14,703 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 16:02:14,707 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ea61f99a9a380296/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.13ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.92ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.26ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.42ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.54ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.60ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.62ba/s]100%|██████████| 8/8 [00:01<00:00,  5.48ba/s]100%|██████████| 8/8 [00:01<00:00,  4.71ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.86ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.21ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.34ba/s]100%|██████████| 4/4 [00:00<00:00,  5.44ba/s]100%|██████████| 4/4 [00:00<00:00,  4.90ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.14ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.27ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.68ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.90ba/s]100%|██████████| 8/8 [00:00<00:00, 10.30ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.39ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.35ba/s]100%|██████████| 4/4 [00:00<00:00, 10.54ba/s]
[INFO|trainer.py:414] 2023-08-28 16:02:18,890 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 16:02:18,904 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 16:02:18,904 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 16:02:18,904 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 16:02:18,904 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 16:02:18,904 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 16:02:18,904 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 16:02:18,904 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:53,  3.36it/s]  0%|          | 2/585 [00:00<02:50,  3.42it/s]  1%|          | 3/585 [00:00<02:49,  3.44it/s]  1%|          | 4/585 [00:01<02:48,  3.46it/s]  1%|          | 5/585 [00:01<02:47,  3.46it/s]  1%|          | 6/585 [00:01<02:47,  3.46it/s]  1%|          | 7/585 [00:02<02:47,  3.46it/s]  1%|▏         | 8/585 [00:02<02:47,  3.45it/s]  2%|▏         | 9/585 [00:02<02:46,  3.46it/s]  2%|▏         | 10/585 [00:02<02:46,  3.46it/s]  2%|▏         | 11/585 [00:03<02:45,  3.46it/s]  2%|▏         | 12/585 [00:03<02:45,  3.46it/s]  2%|▏         | 13/585 [00:03<02:45,  3.46it/s]  2%|▏         | 14/585 [00:04<02:44,  3.46it/s]  3%|▎         | 15/585 [00:04<02:44,  3.47it/s]  3%|▎         | 16/585 [00:04<02:44,  3.47it/s]  3%|▎         | 17/585 [00:04<02:43,  3.47it/s]  3%|▎         | 18/585 [00:05<02:43,  3.47it/s]  3%|▎         | 19/585 [00:05<02:43,  3.46it/s]  3%|▎         | 20/585 [00:05<02:43,  3.46it/s]  4%|▎         | 21/585 [00:06<02:42,  3.46it/s]  4%|▍         | 22/585 [00:06<02:42,  3.46it/s]  4%|▍         | 23/585 [00:06<02:42,  3.46it/s]  4%|▍         | 24/585 [00:06<02:42,  3.46it/s]  4%|▍         | 25/585 [00:07<02:41,  3.46it/s]  4%|▍         | 26/585 [00:07<02:41,  3.46it/s]  5%|▍         | 27/585 [00:07<02:41,  3.46it/s]  5%|▍         | 28/585 [00:08<02:40,  3.46it/s]  5%|▍         | 29/585 [00:08<02:40,  3.46it/s]  5%|▌         | 30/585 [00:08<02:40,  3.45it/s]  5%|▌         | 31/585 [00:08<02:40,  3.46it/s]  5%|▌         | 32/585 [00:09<02:40,  3.46it/s]  6%|▌         | 33/585 [00:09<02:39,  3.46it/s]  6%|▌         | 34/585 [00:09<02:39,  3.46it/s]  6%|▌         | 35/585 [00:10<02:39,  3.46it/s]  6%|▌         | 36/585 [00:10<02:38,  3.46it/s]  6%|▋         | 37/585 [00:10<02:38,  3.46it/s]  6%|▋         | 38/585 [00:10<02:38,  3.46it/s]  7%|▋         | 39/585 [00:11<02:37,  3.46it/s]  7%|▋         | 40/585 [00:11<02:37,  3.46it/s]  7%|▋         | 41/585 [00:11<02:38,  3.44it/s]  7%|▋         | 42/585 [00:12<02:37,  3.45it/s]  7%|▋         | 43/585 [00:12<02:37,  3.45it/s]  8%|▊         | 44/585 [00:12<02:36,  3.45it/s]  8%|▊         | 45/585 [00:13<02:36,  3.46it/s]  8%|▊         | 46/585 [00:13<02:35,  3.46it/s]  8%|▊         | 47/585 [00:13<02:35,  3.46it/s]  8%|▊         | 48/585 [00:13<02:35,  3.46it/s]  8%|▊         | 49/585 [00:14<02:35,  3.46it/s]  9%|▊         | 50/585 [00:14<02:34,  3.46it/s]  9%|▊         | 51/585 [00:14<02:34,  3.46it/s]  9%|▉         | 52/585 [00:15<02:34,  3.45it/s]  9%|▉         | 53/585 [00:15<02:34,  3.45it/s]  9%|▉         | 54/585 [00:15<02:33,  3.45it/s]  9%|▉         | 55/585 [00:15<02:33,  3.45it/s] 10%|▉         | 56/585 [00:16<02:33,  3.46it/s] 10%|▉         | 57/585 [00:16<02:32,  3.45it/s] 10%|▉         | 58/585 [00:16<02:32,  3.46it/s] 10%|█         | 59/585 [00:17<02:32,  3.46it/s] 10%|█         | 60/585 [00:17<02:31,  3.46it/s] 10%|█         | 61/585 [00:17<02:31,  3.46it/s] 11%|█         | 62/585 [00:17<02:31,  3.46it/s] 11%|█         | 63/585 [00:18<02:30,  3.46it/s] 11%|█         | 64/585 [00:18<02:30,  3.46it/s] 11%|█         | 65/585 [00:18<02:30,  3.46it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.46it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.45it/s] 12%|█▏        | 69/585 [00:19<02:29,  3.45it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 71/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 72/585 [00:20<02:28,  3.45it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.45it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.45it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.45it/s] 13%|█▎        | 76/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.46it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.45it/s] 14%|█▎        | 79/585 [00:22<02:26,  3.45it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.45it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.45it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.45it/s] 14%|█▍        | 83/585 [00:24<02:25,  3.46it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.44it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.45it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.45it/s] 15%|█▌        | 90/585 [00:26<02:23,  3.45it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.45it/s] 16%|█▌        | 93/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.46it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.45it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.46it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 101/585 [00:29<02:20,  3.45it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.45it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.45it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.43it/s] 18%|█▊        | 106/585 [00:30<02:19,  3.43it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.44it/s] 18%|█▊        | 108/585 [00:31<02:18,  3.44it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.45it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.45it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.45it/s] 19%|█▉        | 114/585 [00:33<02:16,  3.45it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.45it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.45it/s] 20%|██        | 117/585 [00:33<02:15,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 16:02:52,818 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:02:52,819 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 16:02:52,819 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.50it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.61it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.81it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.16it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.53it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.23it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.28it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.91it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.93it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.86it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.85it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.79it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.80it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.83it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.85it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.86it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.77it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.83it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.84it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.89it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.76it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.73it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.83it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.79it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.87it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.79it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.79it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.84it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.79it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.83it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.69it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.74it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.67it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.75it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.73it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.81it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.81it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.81it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.86it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.84it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.76it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.78it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.73it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.75it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.83it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.80it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.82it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.85it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.74it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.84it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.78it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.78it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.73it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.79it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.73it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.62it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.58it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.58it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.61it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.68it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.70it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.66it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.68it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.81it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.84it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.70it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.77it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.74it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.66it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.76it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.81it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.69it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.83it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.81it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.82it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.69it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.66it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.70it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.66it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.64it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.79it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.63it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.79it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.76it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.75it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.77it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.77it/s][A 20%|██        | 117/585 [00:43<02:15,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:03:02,186 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 16:03:02,200 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:03:04,920 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:03:04,942 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:03:04,957 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:51<43:31,  5.59s/it] 20%|██        | 119/585 [00:52<31:05,  4.00s/it] 21%|██        | 120/585 [00:52<22:23,  2.89s/it] 21%|██        | 121/585 [00:52<16:18,  2.11s/it] 21%|██        | 122/585 [00:53<12:04,  1.56s/it] 21%|██        | 123/585 [00:53<09:05,  1.18s/it] 21%|██        | 124/585 [00:53<07:01,  1.09it/s] 21%|██▏       | 125/585 [00:53<05:34,  1.38it/s] 22%|██▏       | 126/585 [00:54<04:33,  1.68it/s] 22%|██▏       | 127/585 [00:54<03:50,  1.99it/s] 22%|██▏       | 128/585 [00:54<03:20,  2.28it/s] 22%|██▏       | 129/585 [00:55<02:59,  2.54it/s] 22%|██▏       | 130/585 [00:55<02:45,  2.75it/s] 22%|██▏       | 131/585 [00:55<02:34,  2.93it/s] 23%|██▎       | 132/585 [00:55<02:27,  3.07it/s] 23%|██▎       | 133/585 [00:56<02:22,  3.18it/s] 23%|██▎       | 134/585 [00:56<02:18,  3.25it/s] 23%|██▎       | 135/585 [00:56<02:15,  3.31it/s] 23%|██▎       | 136/585 [00:57<02:13,  3.35it/s] 23%|██▎       | 137/585 [00:57<02:12,  3.38it/s] 24%|██▎       | 138/585 [00:57<02:11,  3.41it/s] 24%|██▍       | 139/585 [00:57<02:10,  3.42it/s] 24%|██▍       | 140/585 [00:58<02:09,  3.43it/s] 24%|██▍       | 141/585 [00:58<02:09,  3.43it/s] 24%|██▍       | 142/585 [00:58<02:08,  3.44it/s] 24%|██▍       | 143/585 [00:59<02:08,  3.44it/s] 25%|██▍       | 144/585 [00:59<02:07,  3.45it/s] 25%|██▍       | 145/585 [00:59<02:07,  3.45it/s] 25%|██▍       | 146/585 [00:59<02:07,  3.45it/s] 25%|██▌       | 147/585 [01:00<02:06,  3.46it/s] 25%|██▌       | 148/585 [01:00<02:06,  3.46it/s] 25%|██▌       | 149/585 [01:00<02:06,  3.46it/s] 26%|██▌       | 150/585 [01:01<02:05,  3.46it/s] 26%|██▌       | 151/585 [01:01<02:05,  3.46it/s] 26%|██▌       | 152/585 [01:01<02:05,  3.44it/s] 26%|██▌       | 153/585 [01:01<02:05,  3.44it/s] 26%|██▋       | 154/585 [01:02<02:05,  3.45it/s] 26%|██▋       | 155/585 [01:02<02:04,  3.45it/s] 27%|██▋       | 156/585 [01:02<02:04,  3.45it/s] 27%|██▋       | 157/585 [01:03<02:03,  3.46it/s] 27%|██▋       | 158/585 [01:03<02:03,  3.46it/s] 27%|██▋       | 159/585 [01:03<02:03,  3.46it/s] 27%|██▋       | 160/585 [01:04<02:02,  3.46it/s] 28%|██▊       | 161/585 [01:04<02:02,  3.46it/s] 28%|██▊       | 162/585 [01:04<02:02,  3.46it/s] 28%|██▊       | 163/585 [01:04<02:02,  3.45it/s] 28%|██▊       | 164/585 [01:05<02:02,  3.45it/s] 28%|██▊       | 165/585 [01:05<02:01,  3.45it/s] 28%|██▊       | 166/585 [01:05<02:01,  3.45it/s] 29%|██▊       | 167/585 [01:06<02:00,  3.46it/s] 29%|██▊       | 168/585 [01:06<02:00,  3.46it/s] 29%|██▉       | 169/585 [01:06<02:00,  3.45it/s] 29%|██▉       | 170/585 [01:06<02:00,  3.46it/s] 29%|██▉       | 171/585 [01:07<01:59,  3.46it/s] 29%|██▉       | 172/585 [01:07<01:59,  3.46it/s] 30%|██▉       | 173/585 [01:07<01:59,  3.46it/s] 30%|██▉       | 174/585 [01:08<01:59,  3.45it/s] 30%|██▉       | 175/585 [01:08<01:58,  3.45it/s] 30%|███       | 176/585 [01:08<01:58,  3.45it/s] 30%|███       | 177/585 [01:08<01:58,  3.45it/s] 30%|███       | 178/585 [01:09<01:57,  3.45it/s] 31%|███       | 179/585 [01:09<01:57,  3.46it/s] 31%|███       | 180/585 [01:09<01:57,  3.46it/s] 31%|███       | 181/585 [01:10<01:56,  3.46it/s] 31%|███       | 182/585 [01:10<01:56,  3.46it/s] 31%|███▏      | 183/585 [01:10<01:56,  3.46it/s] 31%|███▏      | 184/585 [01:10<01:56,  3.46it/s] 32%|███▏      | 185/585 [01:11<01:56,  3.44it/s] 32%|███▏      | 186/585 [01:11<01:55,  3.45it/s] 32%|███▏      | 187/585 [01:11<01:55,  3.45it/s] 32%|███▏      | 188/585 [01:12<01:55,  3.45it/s] 32%|███▏      | 189/585 [01:12<01:54,  3.45it/s] 32%|███▏      | 190/585 [01:12<01:54,  3.45it/s] 33%|███▎      | 191/585 [01:12<01:54,  3.46it/s] 33%|███▎      | 192/585 [01:13<01:53,  3.45it/s] 33%|███▎      | 193/585 [01:13<01:53,  3.46it/s] 33%|███▎      | 194/585 [01:13<01:53,  3.46it/s] 33%|███▎      | 195/585 [01:14<01:52,  3.46it/s] 34%|███▎      | 196/585 [01:14<01:52,  3.45it/s] 34%|███▎      | 197/585 [01:14<01:52,  3.45it/s] 34%|███▍      | 198/585 [01:15<01:51,  3.46it/s] 34%|███▍      | 199/585 [01:15<01:51,  3.45it/s] 34%|███▍      | 200/585 [01:15<01:51,  3.45it/s] 34%|███▍      | 201/585 [01:15<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:16<01:50,  3.46it/s] 35%|███▍      | 203/585 [01:16<01:51,  3.42it/s] 35%|███▍      | 204/585 [01:16<01:51,  3.43it/s] 35%|███▌      | 205/585 [01:17<01:50,  3.44it/s] 35%|███▌      | 206/585 [01:17<01:50,  3.44it/s] 35%|███▌      | 207/585 [01:17<01:49,  3.44it/s] 36%|███▌      | 208/585 [01:17<01:49,  3.45it/s] 36%|███▌      | 209/585 [01:18<01:49,  3.45it/s] 36%|███▌      | 210/585 [01:18<01:48,  3.45it/s] 36%|███▌      | 211/585 [01:18<01:48,  3.45it/s] 36%|███▌      | 212/585 [01:19<01:48,  3.45it/s] 36%|███▋      | 213/585 [01:19<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:19<01:47,  3.44it/s] 37%|███▋      | 215/585 [01:19<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:20<01:47,  3.45it/s] 37%|███▋      | 217/585 [01:20<01:46,  3.45it/s] 37%|███▋      | 218/585 [01:20<01:46,  3.45it/s] 37%|███▋      | 219/585 [01:21<01:45,  3.45it/s] 38%|███▊      | 220/585 [01:21<01:45,  3.45it/s] 38%|███▊      | 221/585 [01:21<01:45,  3.45it/s] 38%|███▊      | 222/585 [01:21<01:45,  3.45it/s] 38%|███▊      | 223/585 [01:22<01:44,  3.45it/s] 38%|███▊      | 224/585 [01:22<01:44,  3.45it/s] 38%|███▊      | 225/585 [01:22<01:44,  3.44it/s] 39%|███▊      | 226/585 [01:23<01:44,  3.45it/s] 39%|███▉      | 227/585 [01:23<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:23<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:23<01:43,  3.45it/s] 39%|███▉      | 230/585 [01:24<01:42,  3.45it/s] 39%|███▉      | 231/585 [01:24<01:42,  3.45it/s] 40%|███▉      | 232/585 [01:24<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:25<01:42,  3.45it/s] 40%|████      | 234/585 [01:25<01:41,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 16:03:44,397 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:03:44,397 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 16:03:44,397 >>   Batch size = 8
{'eval_loss': 1.0573742389678955, 'eval_runtime': 9.3517, 'eval_samples_per_second': 373.409, 'eval_steps_per_second': 46.73, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.02it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.80it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.07it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.23it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.84it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.57it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.33it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.94it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.84it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.01it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.88it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.89it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.97it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.00it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.00it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.97it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.69it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.73it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.79it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.83it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.75it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.89it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.89it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.87it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.84it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.84it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.74it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.90it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.90it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.74it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.86it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.92it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.87it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.89it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.70it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.68it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.76it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.77it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.83it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.90it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.80it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.88it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.92it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.89it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.79it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.70it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.67it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.73it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.78it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.87it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.73it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.81it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.84it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.82it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.67it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.67it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.73it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.83it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.84it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.82it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.76it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.73it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.75it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.76it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.67it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.78it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.77it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.69it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.78it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.78it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.73it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.68it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.65it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.74it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.49it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.69it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.78it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.85it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.80it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.78it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.82it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.78it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.75it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.69it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.78it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.76it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.76it/s][A 40%|████      | 234/585 [01:34<01:41,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:03:53,756 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 16:03:53,779 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:03:56,908 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:03:56,932 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:03:56,942 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:43<32:51,  5.63s/it] 40%|████      | 236/585 [01:43<23:26,  4.03s/it] 41%|████      | 237/585 [01:44<16:51,  2.91s/it] 41%|████      | 238/585 [01:44<12:16,  2.12s/it] 41%|████      | 239/585 [01:44<09:03,  1.57s/it] 41%|████      | 240/585 [01:44<06:49,  1.19s/it] 41%|████      | 241/585 [01:45<05:15,  1.09it/s] 41%|████▏     | 242/585 [01:45<04:10,  1.37it/s] 42%|████▏     | 243/585 [01:45<03:24,  1.68it/s] 42%|████▏     | 244/585 [01:46<02:52,  1.98it/s] 42%|████▏     | 245/585 [01:46<02:29,  2.27it/s] 42%|████▏     | 246/585 [01:46<02:13,  2.53it/s] 42%|████▏     | 247/585 [01:47<02:03,  2.74it/s] 42%|████▏     | 248/585 [01:47<01:55,  2.93it/s] 43%|████▎     | 249/585 [01:47<01:49,  3.07it/s] 43%|████▎     | 250/585 [01:47<01:45,  3.18it/s] 43%|████▎     | 251/585 [01:48<01:42,  3.26it/s] 43%|████▎     | 252/585 [01:48<01:40,  3.31it/s] 43%|████▎     | 253/585 [01:48<01:38,  3.36it/s] 43%|████▎     | 254/585 [01:49<01:37,  3.39it/s] 44%|████▎     | 255/585 [01:49<01:36,  3.41it/s] 44%|████▍     | 256/585 [01:49<01:36,  3.42it/s] 44%|████▍     | 257/585 [01:49<01:35,  3.43it/s] 44%|████▍     | 258/585 [01:50<01:35,  3.42it/s] 44%|████▍     | 259/585 [01:50<01:34,  3.44it/s] 44%|████▍     | 260/585 [01:50<01:34,  3.44it/s] 45%|████▍     | 261/585 [01:51<01:33,  3.45it/s] 45%|████▍     | 262/585 [01:51<01:33,  3.45it/s] 45%|████▍     | 263/585 [01:51<01:36,  3.35it/s] 45%|████▌     | 264/585 [01:51<01:35,  3.37it/s] 45%|████▌     | 265/585 [01:52<01:34,  3.40it/s] 45%|████▌     | 266/585 [01:52<01:33,  3.42it/s] 46%|████▌     | 267/585 [01:52<01:32,  3.43it/s] 46%|████▌     | 268/585 [01:53<01:32,  3.44it/s] 46%|████▌     | 269/585 [01:53<01:32,  3.43it/s] 46%|████▌     | 270/585 [01:53<01:31,  3.44it/s] 46%|████▋     | 271/585 [01:53<01:31,  3.44it/s] 46%|████▋     | 272/585 [01:54<01:30,  3.45it/s] 47%|████▋     | 273/585 [01:54<01:30,  3.45it/s] 47%|████▋     | 274/585 [01:54<01:30,  3.45it/s] 47%|████▋     | 275/585 [01:55<01:29,  3.45it/s] 47%|████▋     | 276/585 [01:55<01:29,  3.46it/s] 47%|████▋     | 277/585 [01:55<01:29,  3.46it/s] 48%|████▊     | 278/585 [01:56<01:28,  3.46it/s] 48%|████▊     | 279/585 [01:56<01:28,  3.46it/s] 48%|████▊     | 280/585 [01:56<01:28,  3.45it/s] 48%|████▊     | 281/585 [01:56<01:28,  3.45it/s] 48%|████▊     | 282/585 [01:57<01:27,  3.45it/s] 48%|████▊     | 283/585 [01:57<01:27,  3.45it/s] 49%|████▊     | 284/585 [01:57<01:27,  3.45it/s] 49%|████▊     | 285/585 [01:58<01:26,  3.46it/s] 49%|████▉     | 286/585 [01:58<01:26,  3.46it/s] 49%|████▉     | 287/585 [01:58<01:26,  3.46it/s] 49%|████▉     | 288/585 [01:58<01:25,  3.46it/s] 49%|████▉     | 289/585 [01:59<01:25,  3.46it/s] 50%|████▉     | 290/585 [01:59<01:25,  3.46it/s] 50%|████▉     | 291/585 [01:59<01:25,  3.45it/s] 50%|████▉     | 292/585 [02:00<01:24,  3.45it/s] 50%|█████     | 293/585 [02:00<01:24,  3.45it/s] 50%|█████     | 294/585 [02:00<01:24,  3.45it/s] 50%|█████     | 295/585 [02:00<01:23,  3.46it/s] 51%|█████     | 296/585 [02:01<01:23,  3.46it/s] 51%|█████     | 297/585 [02:01<01:23,  3.46it/s] 51%|█████     | 298/585 [02:01<01:22,  3.46it/s] 51%|█████     | 299/585 [02:02<01:22,  3.46it/s] 51%|█████▏    | 300/585 [02:02<01:22,  3.46it/s] 51%|█████▏    | 301/585 [02:02<01:22,  3.46it/s] 52%|█████▏    | 302/585 [02:02<01:22,  3.43it/s] 52%|█████▏    | 303/585 [02:03<01:22,  3.44it/s] 52%|█████▏    | 304/585 [02:03<01:21,  3.44it/s] 52%|█████▏    | 305/585 [02:03<01:21,  3.45it/s] 52%|█████▏    | 306/585 [02:04<01:20,  3.45it/s] 52%|█████▏    | 307/585 [02:04<01:20,  3.45it/s] 53%|█████▎    | 308/585 [02:04<01:20,  3.45it/s] 53%|█████▎    | 309/585 [02:05<01:19,  3.45it/s] 53%|█████▎    | 310/585 [02:05<01:19,  3.45it/s] 53%|█████▎    | 311/585 [02:05<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:05<01:19,  3.46it/s] 54%|█████▎    | 313/585 [02:06<01:18,  3.46it/s] 54%|█████▎    | 314/585 [02:06<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:06<01:18,  3.46it/s] 54%|█████▍    | 316/585 [02:07<01:17,  3.46it/s] 54%|█████▍    | 317/585 [02:07<01:17,  3.46it/s] 54%|█████▍    | 318/585 [02:07<01:17,  3.45it/s] 55%|█████▍    | 319/585 [02:07<01:17,  3.45it/s] 55%|█████▍    | 320/585 [02:08<01:16,  3.45it/s] 55%|█████▍    | 321/585 [02:08<01:16,  3.45it/s] 55%|█████▌    | 322/585 [02:08<01:16,  3.45it/s] 55%|█████▌    | 323/585 [02:09<01:15,  3.45it/s] 55%|█████▌    | 324/585 [02:09<01:15,  3.45it/s] 56%|█████▌    | 325/585 [02:09<01:15,  3.45it/s] 56%|█████▌    | 326/585 [02:09<01:15,  3.45it/s] 56%|█████▌    | 327/585 [02:10<01:14,  3.45it/s] 56%|█████▌    | 328/585 [02:10<01:14,  3.46it/s] 56%|█████▌    | 329/585 [02:10<01:14,  3.44it/s] 56%|█████▋    | 330/585 [02:11<01:14,  3.44it/s] 57%|█████▋    | 331/585 [02:11<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:11<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:11<01:12,  3.45it/s] 57%|█████▋    | 334/585 [02:12<01:12,  3.45it/s] 57%|█████▋    | 335/585 [02:12<01:12,  3.45it/s] 57%|█████▋    | 336/585 [02:12<01:12,  3.46it/s] 58%|█████▊    | 337/585 [02:13<01:11,  3.45it/s] 58%|█████▊    | 338/585 [02:13<01:11,  3.46it/s] 58%|█████▊    | 339/585 [02:13<01:11,  3.45it/s] 58%|█████▊    | 340/585 [02:13<01:11,  3.45it/s] 58%|█████▊    | 341/585 [02:14<01:10,  3.45it/s] 58%|█████▊    | 342/585 [02:14<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:14<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:15<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:15<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:15<01:09,  3.46it/s] 59%|█████▉    | 347/585 [02:16<01:08,  3.46it/s] 59%|█████▉    | 348/585 [02:16<01:08,  3.46it/s] 60%|█████▉    | 349/585 [02:16<01:08,  3.46it/s] 60%|█████▉    | 350/585 [02:16<01:07,  3.46it/s] 60%|██████    | 351/585 [02:17<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 16:04:36,120 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:04:36,120 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 16:04:36,120 >>   Batch size = 8
{'eval_loss': 1.065983533859253, 'eval_runtime': 9.343, 'eval_samples_per_second': 373.757, 'eval_steps_per_second': 46.773, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.17it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.92it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.11it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.45it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.95it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.58it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.38it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.05it/s][A
 11%|█         | 48/437 [00:00<00:08, 46.98it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.85it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.98it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.10it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.06it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.96it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.96it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.76it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.63it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.71it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.71it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.87it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.95it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.94it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.01it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.95it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.84it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.80it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.73it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.72it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.74it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.88it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.93it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.03it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.01it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.88it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.83it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.71it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.68it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.78it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.71it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.76it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.92it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.96it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.87it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.88it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.75it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.76it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.83it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.76it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.72it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.75it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.83it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.81it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.81it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.77it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.74it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.82it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.78it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.80it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.79it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.70it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.85it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.90it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.70it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.72it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.77it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.70it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.80it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.85it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.69it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.68it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.79it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.87it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.85it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.85it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.83it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.82it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.82it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.83it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.79it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.83it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.83it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.85it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.82it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.78it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.73it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.71it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.71it/s][A 60%|██████    | 351/585 [02:26<01:07,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:04:45,487 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 16:04:45,510 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:04:48,096 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:04:48,115 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:04:48,128 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:34<21:08,  5.45s/it] 60%|██████    | 353/585 [02:34<15:04,  3.90s/it] 61%|██████    | 354/585 [02:35<10:50,  2.82s/it] 61%|██████    | 355/585 [02:35<07:53,  2.06s/it] 61%|██████    | 356/585 [02:35<05:49,  1.53s/it] 61%|██████    | 357/585 [02:36<04:23,  1.16s/it] 61%|██████    | 358/585 [02:36<03:23,  1.12it/s] 61%|██████▏   | 359/585 [02:36<02:41,  1.40it/s] 62%|██████▏   | 360/585 [02:36<02:11,  1.70it/s] 62%|██████▏   | 361/585 [02:37<01:51,  2.01it/s] 62%|██████▏   | 362/585 [02:37<01:36,  2.30it/s] 62%|██████▏   | 363/585 [02:37<01:26,  2.56it/s] 62%|██████▏   | 364/585 [02:38<01:19,  2.77it/s] 62%|██████▏   | 365/585 [02:38<01:14,  2.95it/s] 63%|██████▎   | 366/585 [02:38<01:11,  3.08it/s] 63%|██████▎   | 367/585 [02:38<01:08,  3.19it/s] 63%|██████▎   | 368/585 [02:39<01:06,  3.27it/s] 63%|██████▎   | 369/585 [02:39<01:05,  3.32it/s] 63%|██████▎   | 370/585 [02:39<01:04,  3.36it/s] 63%|██████▎   | 371/585 [02:40<01:03,  3.39it/s] 64%|██████▎   | 372/585 [02:40<01:02,  3.41it/s] 64%|██████▍   | 373/585 [02:40<01:01,  3.43it/s] 64%|██████▍   | 374/585 [02:41<01:01,  3.44it/s] 64%|██████▍   | 375/585 [02:41<01:01,  3.43it/s] 64%|██████▍   | 376/585 [02:41<01:00,  3.44it/s] 64%|██████▍   | 377/585 [02:41<01:00,  3.45it/s] 65%|██████▍   | 378/585 [02:42<00:59,  3.45it/s] 65%|██████▍   | 379/585 [02:42<00:59,  3.45it/s] 65%|██████▍   | 380/585 [02:42<00:59,  3.45it/s] 65%|██████▌   | 381/585 [02:43<00:59,  3.46it/s] 65%|██████▌   | 382/585 [02:43<00:58,  3.46it/s] 65%|██████▌   | 383/585 [02:43<00:58,  3.46it/s] 66%|██████▌   | 384/585 [02:43<00:58,  3.46it/s] 66%|██████▌   | 385/585 [02:44<00:57,  3.46it/s] 66%|██████▌   | 386/585 [02:44<00:57,  3.45it/s] 66%|██████▌   | 387/585 [02:44<00:57,  3.45it/s] 66%|██████▋   | 388/585 [02:45<00:57,  3.45it/s] 66%|██████▋   | 389/585 [02:45<00:56,  3.46it/s] 67%|██████▋   | 390/585 [02:45<00:56,  3.46it/s] 67%|██████▋   | 391/585 [02:45<00:56,  3.46it/s] 67%|██████▋   | 392/585 [02:46<00:55,  3.46it/s] 67%|██████▋   | 393/585 [02:46<00:55,  3.46it/s] 67%|██████▋   | 394/585 [02:46<00:55,  3.46it/s] 68%|██████▊   | 395/585 [02:47<00:54,  3.46it/s] 68%|██████▊   | 396/585 [02:47<00:54,  3.46it/s] 68%|██████▊   | 397/585 [02:47<00:54,  3.45it/s] 68%|██████▊   | 398/585 [02:47<00:54,  3.45it/s] 68%|██████▊   | 399/585 [02:48<00:53,  3.45it/s] 68%|██████▊   | 400/585 [02:48<00:53,  3.45it/s] 69%|██████▊   | 401/585 [02:48<00:53,  3.45it/s] 69%|██████▊   | 402/585 [02:49<00:52,  3.46it/s] 69%|██████▉   | 403/585 [02:49<00:52,  3.46it/s] 69%|██████▉   | 404/585 [02:49<00:52,  3.46it/s] 69%|██████▉   | 405/585 [02:49<00:52,  3.46it/s] 69%|██████▉   | 406/585 [02:50<00:51,  3.46it/s] 70%|██████▉   | 407/585 [02:50<00:51,  3.46it/s] 70%|██████▉   | 408/585 [02:50<00:51,  3.45it/s] 70%|██████▉   | 409/585 [02:51<00:51,  3.45it/s] 70%|███████   | 410/585 [02:51<00:50,  3.45it/s] 70%|███████   | 411/585 [02:51<00:52,  3.32it/s] 70%|███████   | 412/585 [02:52<00:51,  3.35it/s] 71%|███████   | 413/585 [02:52<00:50,  3.38it/s] 71%|███████   | 414/585 [02:52<00:50,  3.41it/s] 71%|███████   | 415/585 [02:52<00:49,  3.43it/s] 71%|███████   | 416/585 [02:53<00:49,  3.43it/s] 71%|███████▏  | 417/585 [02:53<00:48,  3.44it/s] 71%|███████▏  | 418/585 [02:53<00:48,  3.45it/s] 72%|███████▏  | 419/585 [02:54<00:48,  3.44it/s] 72%|███████▏  | 420/585 [02:54<00:47,  3.44it/s] 72%|███████▏  | 421/585 [02:54<00:47,  3.45it/s] 72%|███████▏  | 422/585 [02:54<00:47,  3.45it/s] 72%|███████▏  | 423/585 [02:55<00:46,  3.45it/s] 72%|███████▏  | 424/585 [02:55<00:46,  3.45it/s] 73%|███████▎  | 425/585 [02:55<00:46,  3.45it/s] 73%|███████▎  | 426/585 [02:56<00:46,  3.46it/s] 73%|███████▎  | 427/585 [02:56<00:45,  3.46it/s] 73%|███████▎  | 428/585 [02:56<00:45,  3.46it/s] 73%|███████▎  | 429/585 [02:56<00:45,  3.46it/s] 74%|███████▎  | 430/585 [02:57<00:44,  3.46it/s] 74%|███████▎  | 431/585 [02:57<00:44,  3.46it/s] 74%|███████▍  | 432/585 [02:57<00:44,  3.46it/s] 74%|███████▍  | 433/585 [02:58<00:43,  3.46it/s] 74%|███████▍  | 434/585 [02:58<00:43,  3.46it/s] 74%|███████▍  | 435/585 [02:58<00:43,  3.46it/s] 75%|███████▍  | 436/585 [02:58<00:43,  3.44it/s] 75%|███████▍  | 437/585 [02:59<00:42,  3.45it/s] 75%|███████▍  | 438/585 [02:59<00:42,  3.45it/s] 75%|███████▌  | 439/585 [02:59<00:42,  3.45it/s] 75%|███████▌  | 440/585 [03:00<00:41,  3.45it/s] 75%|███████▌  | 441/585 [03:00<00:41,  3.45it/s] 76%|███████▌  | 442/585 [03:00<00:41,  3.46it/s] 76%|███████▌  | 443/585 [03:01<00:41,  3.45it/s] 76%|███████▌  | 444/585 [03:01<00:40,  3.46it/s] 76%|███████▌  | 445/585 [03:01<00:40,  3.46it/s] 76%|███████▌  | 446/585 [03:01<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:02<00:40,  3.43it/s] 77%|███████▋  | 448/585 [03:02<00:39,  3.44it/s] 77%|███████▋  | 449/585 [03:02<00:39,  3.44it/s] 77%|███████▋  | 450/585 [03:03<00:39,  3.45it/s] 77%|███████▋  | 451/585 [03:03<00:38,  3.45it/s] 77%|███████▋  | 452/585 [03:03<00:38,  3.45it/s] 77%|███████▋  | 453/585 [03:03<00:38,  3.45it/s] 78%|███████▊  | 454/585 [03:04<00:37,  3.45it/s] 78%|███████▊  | 455/585 [03:04<00:37,  3.46it/s] 78%|███████▊  | 456/585 [03:04<00:37,  3.46it/s] 78%|███████▊  | 457/585 [03:05<00:37,  3.46it/s] 78%|███████▊  | 458/585 [03:05<00:36,  3.45it/s] 78%|███████▊  | 459/585 [03:05<00:36,  3.45it/s] 79%|███████▊  | 460/585 [03:05<00:36,  3.45it/s] 79%|███████▉  | 461/585 [03:06<00:35,  3.45it/s] 79%|███████▉  | 462/585 [03:06<00:35,  3.45it/s] 79%|███████▉  | 463/585 [03:06<00:35,  3.45it/s] 79%|███████▉  | 464/585 [03:07<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:07<00:34,  3.46it/s] 80%|███████▉  | 466/585 [03:07<00:34,  3.46it/s] 80%|███████▉  | 467/585 [03:07<00:34,  3.46it/s] 80%|████████  | 468/585 [03:08<00:33,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 16:05:27,217 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:05:27,217 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 16:05:27,217 >>   Batch size = 8
{'eval_loss': 1.0856022834777832, 'eval_runtime': 9.3377, 'eval_samples_per_second': 373.969, 'eval_steps_per_second': 46.8, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.63it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.92it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.01it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.40it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.79it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.59it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.39it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.96it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.90it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.86it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.89it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.09it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.19it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.11it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.94it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.90it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.70it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.75it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.77it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.79it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.87it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.01it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.04it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.08it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.94it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.72it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.77it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.73it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.78it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.92it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.03it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.94it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.00it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.02it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.69it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.78it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.75it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.69it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.87it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.99it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.00it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.04it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.02it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.93it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.79it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.74it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.71it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.84it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.98it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.91it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.97it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.97it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.81it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.81it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.70it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.70it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.64it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.78it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.84it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.66it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.91it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.83it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.86it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.81it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.71it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.62it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.69it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.81it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.95it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.98it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.95it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.92it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.90it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.90it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.84it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.73it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.71it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.87it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.85it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.90it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.88it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.82it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.85it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.89it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.75it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.75it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.75it/s][A 80%|████████  | 468/585 [03:17<00:33,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:05:36,563 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 16:05:36,591 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:05:38,876 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:05:38,891 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:05:38,899 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:25<10:20,  5.35s/it] 80%|████████  | 470/585 [03:25<07:21,  3.84s/it] 81%|████████  | 471/585 [03:26<05:15,  2.77s/it] 81%|████████  | 472/585 [03:26<03:49,  2.03s/it] 81%|████████  | 473/585 [03:26<02:48,  1.51s/it] 81%|████████  | 474/585 [03:26<02:06,  1.14s/it] 81%|████████  | 475/585 [03:27<01:37,  1.13it/s] 81%|████████▏ | 476/585 [03:27<01:16,  1.42it/s] 82%|████████▏ | 477/585 [03:27<01:02,  1.72it/s] 82%|████████▏ | 478/585 [03:28<00:52,  2.03it/s] 82%|████████▏ | 479/585 [03:28<00:45,  2.32it/s] 82%|████████▏ | 480/585 [03:28<00:40,  2.57it/s] 82%|████████▏ | 481/585 [03:28<00:37,  2.77it/s] 82%|████████▏ | 482/585 [03:29<00:34,  2.95it/s] 83%|████████▎ | 483/585 [03:29<00:33,  3.09it/s] 83%|████████▎ | 484/585 [03:29<00:31,  3.19it/s] 83%|████████▎ | 485/585 [03:30<00:30,  3.27it/s] 83%|████████▎ | 486/585 [03:30<00:29,  3.32it/s] 83%|████████▎ | 487/585 [03:30<00:29,  3.36it/s] 83%|████████▎ | 488/585 [03:30<00:28,  3.39it/s] 84%|████████▎ | 489/585 [03:31<00:28,  3.41it/s] 84%|████████▍ | 490/585 [03:31<00:27,  3.42it/s] 84%|████████▍ | 491/585 [03:31<00:27,  3.43it/s] 84%|████████▍ | 492/585 [03:32<00:27,  3.43it/s] 84%|████████▍ | 493/585 [03:32<00:26,  3.44it/s] 84%|████████▍ | 494/585 [03:32<00:26,  3.44it/s] 85%|████████▍ | 495/585 [03:32<00:26,  3.45it/s] 85%|████████▍ | 496/585 [03:33<00:25,  3.45it/s] 85%|████████▍ | 497/585 [03:33<00:25,  3.45it/s] 85%|████████▌ | 498/585 [03:33<00:25,  3.45it/s] 85%|████████▌ | 499/585 [03:34<00:24,  3.46it/s] 85%|████████▌ | 500/585 [03:34<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [03:34<00:24,  3.46it/s] 86%|████████▌ | 501/585 [03:34<00:24,  3.46it/s] 86%|████████▌ | 502/585 [03:34<00:24,  3.46it/s] 86%|████████▌ | 503/585 [03:35<00:23,  3.45it/s] 86%|████████▌ | 504/585 [03:35<00:23,  3.45it/s] 86%|████████▋ | 505/585 [03:35<00:23,  3.46it/s] 86%|████████▋ | 506/585 [03:36<00:22,  3.46it/s] 87%|████████▋ | 507/585 [03:36<00:22,  3.46it/s] 87%|████████▋ | 508/585 [03:36<00:22,  3.46it/s] 87%|████████▋ | 509/585 [03:37<00:21,  3.46it/s] 87%|████████▋ | 510/585 [03:37<00:21,  3.46it/s] 87%|████████▋ | 511/585 [03:37<00:21,  3.46it/s] 88%|████████▊ | 512/585 [03:37<00:21,  3.46it/s] 88%|████████▊ | 513/585 [03:38<00:20,  3.46it/s] 88%|████████▊ | 514/585 [03:38<00:20,  3.44it/s] 88%|████████▊ | 515/585 [03:38<00:20,  3.45it/s] 88%|████████▊ | 516/585 [03:39<00:19,  3.45it/s] 88%|████████▊ | 517/585 [03:39<00:19,  3.45it/s] 89%|████████▊ | 518/585 [03:39<00:19,  3.45it/s] 89%|████████▊ | 519/585 [03:39<00:19,  3.45it/s] 89%|████████▉ | 520/585 [03:40<00:18,  3.46it/s] 89%|████████▉ | 521/585 [03:40<00:18,  3.46it/s] 89%|████████▉ | 522/585 [03:40<00:18,  3.46it/s] 89%|████████▉ | 523/585 [03:41<00:17,  3.46it/s] 90%|████████▉ | 524/585 [03:41<00:17,  3.45it/s] 90%|████████▉ | 525/585 [03:41<00:17,  3.45it/s] 90%|████████▉ | 526/585 [03:41<00:17,  3.45it/s] 90%|█████████ | 527/585 [03:42<00:16,  3.45it/s] 90%|█████████ | 528/585 [03:42<00:16,  3.45it/s] 90%|█████████ | 529/585 [03:42<00:16,  3.45it/s] 91%|█████████ | 530/585 [03:43<00:15,  3.45it/s] 91%|█████████ | 531/585 [03:43<00:15,  3.45it/s] 91%|█████████ | 532/585 [03:43<00:15,  3.45it/s] 91%|█████████ | 533/585 [03:43<00:15,  3.45it/s] 91%|█████████▏| 534/585 [03:44<00:14,  3.45it/s] 91%|█████████▏| 535/585 [03:44<00:14,  3.46it/s] 92%|█████████▏| 536/585 [03:44<00:14,  3.44it/s] 92%|█████████▏| 537/585 [03:45<00:13,  3.44it/s] 92%|█████████▏| 538/585 [03:45<00:13,  3.45it/s] 92%|█████████▏| 539/585 [03:45<00:13,  3.45it/s] 92%|█████████▏| 540/585 [03:45<00:13,  3.45it/s] 92%|█████████▏| 541/585 [03:46<00:12,  3.45it/s] 93%|█████████▎| 542/585 [03:46<00:12,  3.45it/s] 93%|█████████▎| 543/585 [03:46<00:12,  3.45it/s] 93%|█████████▎| 544/585 [03:47<00:11,  3.46it/s] 93%|█████████▎| 545/585 [03:47<00:11,  3.46it/s] 93%|█████████▎| 546/585 [03:47<00:11,  3.46it/s] 94%|█████████▎| 547/585 [03:48<00:10,  3.46it/s] 94%|█████████▎| 548/585 [03:48<00:10,  3.45it/s] 94%|█████████▍| 549/585 [03:48<00:10,  3.45it/s] 94%|█████████▍| 550/585 [03:48<00:10,  3.45it/s] 94%|█████████▍| 551/585 [03:49<00:09,  3.45it/s] 94%|█████████▍| 552/585 [03:49<00:09,  3.45it/s] 95%|█████████▍| 553/585 [03:49<00:09,  3.45it/s] 95%|█████████▍| 554/585 [03:50<00:09,  3.44it/s] 95%|█████████▍| 555/585 [03:50<00:08,  3.45it/s] 95%|█████████▌| 556/585 [03:50<00:08,  3.45it/s] 95%|█████████▌| 557/585 [03:50<00:08,  3.45it/s] 95%|█████████▌| 558/585 [03:51<00:07,  3.45it/s] 96%|█████████▌| 559/585 [03:51<00:07,  3.45it/s] 96%|█████████▌| 560/585 [03:51<00:07,  3.27it/s] 96%|█████████▌| 561/585 [03:52<00:07,  3.31it/s] 96%|█████████▌| 562/585 [03:52<00:06,  3.36it/s] 96%|█████████▌| 563/585 [03:52<00:06,  3.39it/s] 96%|█████████▋| 564/585 [03:52<00:06,  3.41it/s] 97%|█████████▋| 565/585 [03:53<00:05,  3.42it/s] 97%|█████████▋| 566/585 [03:53<00:05,  3.43it/s] 97%|█████████▋| 567/585 [03:53<00:05,  3.44it/s] 97%|█████████▋| 568/585 [03:54<00:04,  3.44it/s] 97%|█████████▋| 569/585 [03:54<00:04,  3.45it/s] 97%|█████████▋| 570/585 [03:54<00:04,  3.45it/s] 98%|█████████▊| 571/585 [03:55<00:04,  3.45it/s] 98%|█████████▊| 572/585 [03:55<00:03,  3.41it/s] 98%|█████████▊| 573/585 [03:55<00:03,  3.43it/s] 98%|█████████▊| 574/585 [03:55<00:03,  3.44it/s] 98%|█████████▊| 575/585 [03:56<00:02,  3.44it/s] 98%|█████████▊| 576/585 [03:56<00:02,  3.45it/s] 99%|█████████▊| 577/585 [03:56<00:02,  3.45it/s] 99%|█████████▉| 578/585 [03:57<00:02,  3.45it/s] 99%|█████████▉| 579/585 [03:57<00:01,  3.45it/s] 99%|█████████▉| 580/585 [03:57<00:01,  3.45it/s] 99%|█████████▉| 581/585 [03:57<00:01,  3.45it/s] 99%|█████████▉| 582/585 [03:58<00:00,  3.45it/s]100%|█████████▉| 583/585 [03:58<00:00,  3.44it/s]100%|█████████▉| 584/585 [03:58<00:00,  3.45it/s]100%|██████████| 585/585 [03:59<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 16:06:17,992 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:06:17,992 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 16:06:17,992 >>   Batch size = 8
{'eval_loss': 1.0932612419128418, 'eval_runtime': 9.3308, 'eval_samples_per_second': 374.243, 'eval_steps_per_second': 46.834, 'epoch': 4.0}
{'loss': 0.6285, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.58it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.58it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.87it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.29it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.93it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.50it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.30it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.03it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.88it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.94it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.90it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.68it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.82it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.81it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.90it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.93it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.74it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.78it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.85it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.78it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.74it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.84it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.83it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.87it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.89it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.81it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.76it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.75it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.82it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.71it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.73it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.73it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.75it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.84it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.77it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.73it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.79it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.81it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.78it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.75it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.81it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.82it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.84it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.81it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.80it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.81it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.84it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.84it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.64it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.74it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.82it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.84it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.79it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.81it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.68it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.79it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.80it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.81it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.86it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.87it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.81it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.84it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.72it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.64it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.74it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.72it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.74it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.84it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.75it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.76it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.84it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.74it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.73it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.59it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.69it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.74it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.70it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.78it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.69it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.72it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.79it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.77it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.71it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.68it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.71it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.65it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.65it/s][A100%|██████████| 585/585 [04:08<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:06:27,338 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 16:06:27,365 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:06:29,608 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:06:29,626 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:06:29,636 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 16:06:34,408 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 16:06:34,411 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117 (score: 1.0573742389678955).
                                                 100%|██████████| 585/585 [04:17<00:00,  3.45it/s]100%|██████████| 585/585 [04:17<00:00,  2.27it/s]
[INFO|trainer.py:1894] 2023-08-28 16:06:36,229 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 16:06:36,254 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:06:38,732 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:06:38,759 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:06:38,780 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:06:38,953 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:38,953 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:38,953 >>   train_loss               =     0.6249
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:38,953 >>   train_runtime            = 0:04:17.32
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:38,953 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:38,953 >>   train_samples_per_second =    145.732
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:38,954 >>   train_steps_per_second   =      2.273
{'eval_loss': 1.1003129482269287, 'eval_runtime': 9.3346, 'eval_samples_per_second': 374.094, 'eval_steps_per_second': 46.815, 'epoch': 5.0}
{'train_runtime': 257.3211, 'train_samples_per_second': 145.732, 'train_steps_per_second': 2.273, 'train_loss': 0.6248561565692609, 'epoch': 5.0}
08/28/2023 16:06:38 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 16:06:38,995 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:06:38,995 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 16:06:38,995 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 59.17it/s]  3%|▎         | 12/437 [00:00<00:08, 51.77it/s]  4%|▍         | 18/437 [00:00<00:08, 49.87it/s]  5%|▌         | 24/437 [00:00<00:08, 49.00it/s]  7%|▋         | 29/437 [00:00<00:08, 48.55it/s]  8%|▊         | 34/437 [00:00<00:08, 48.31it/s]  9%|▉         | 39/437 [00:00<00:08, 47.98it/s] 10%|█         | 44/437 [00:00<00:08, 47.69it/s] 11%|█         | 49/437 [00:01<00:08, 47.23it/s] 12%|█▏        | 54/437 [00:01<00:08, 47.11it/s] 14%|█▎        | 59/437 [00:01<00:08, 47.24it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.36it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.41it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.40it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.39it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.48it/s] 20%|██        | 89/437 [00:01<00:07, 47.18it/s] 22%|██▏       | 94/437 [00:01<00:07, 47.04it/s] 23%|██▎       | 99/437 [00:02<00:07, 46.87it/s] 24%|██▍       | 104/437 [00:02<00:07, 46.93it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.08it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.26it/s] 27%|██▋       | 119/437 [00:02<00:06, 46.44it/s] 28%|██▊       | 124/437 [00:02<00:06, 46.79it/s] 30%|██▉       | 129/437 [00:02<00:06, 47.02it/s] 31%|███       | 134/437 [00:02<00:06, 46.92it/s] 32%|███▏      | 139/437 [00:02<00:06, 46.83it/s] 33%|███▎      | 144/437 [00:03<00:06, 46.89it/s] 34%|███▍      | 149/437 [00:03<00:06, 46.87it/s] 35%|███▌      | 154/437 [00:03<00:06, 47.02it/s] 36%|███▋      | 159/437 [00:03<00:05, 47.11it/s] 38%|███▊      | 164/437 [00:03<00:05, 47.11it/s] 39%|███▊      | 169/437 [00:03<00:05, 47.21it/s] 40%|███▉      | 174/437 [00:03<00:05, 47.16it/s] 41%|████      | 179/437 [00:03<00:05, 47.24it/s] 42%|████▏     | 184/437 [00:03<00:05, 47.11it/s] 43%|████▎     | 189/437 [00:03<00:05, 47.07it/s] 44%|████▍     | 194/437 [00:04<00:05, 46.99it/s] 46%|████▌     | 199/437 [00:04<00:05, 47.00it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.15it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.22it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.21it/s] 50%|█████     | 219/437 [00:04<00:04, 47.23it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.29it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.19it/s] 54%|█████▎    | 234/437 [00:04<00:04, 47.10it/s] 55%|█████▍    | 239/437 [00:05<00:04, 47.08it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.07it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.07it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.17it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.19it/s] 60%|██████    | 264/437 [00:05<00:03, 47.07it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.18it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.24it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.13it/s] 65%|██████▍   | 284/437 [00:05<00:03, 47.13it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.16it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.09it/s] 68%|██████▊   | 299/437 [00:06<00:02, 47.09it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.19it/s] 71%|███████   | 309/437 [00:06<00:02, 47.07it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.13it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.04it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.06it/s] 75%|███████▌  | 329/437 [00:06<00:02, 47.02it/s] 76%|███████▋  | 334/437 [00:07<00:02, 47.09it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.04it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.02it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.10it/s] 81%|████████  | 354/437 [00:07<00:01, 47.10it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.17it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.17it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.14it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.12it/s] 87%|████████▋ | 379/437 [00:08<00:01, 47.04it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.09it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.13it/s] 90%|█████████ | 394/437 [00:08<00:00, 46.98it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.04it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.05it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.05it/s] 95%|█████████▍| 414/437 [00:08<00:00, 47.11it/s] 96%|█████████▌| 419/437 [00:08<00:00, 47.13it/s] 97%|█████████▋| 424/437 [00:08<00:00, 47.08it/s] 98%|█████████▊| 429/437 [00:09<00:00, 47.05it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.18it/s]100%|██████████| 437/437 [00:09<00:00, 47.25it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:06:48,268 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:48,268 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:48,268 >>   eval_loss               =     1.0574
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:48,268 >>   eval_runtime            = 0:00:09.27
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:48,268 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:48,268 >>   eval_samples_per_second =    376.588
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:48,268 >>   eval_steps_per_second   =     47.127
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:06:48,268 >>   perplexity              =     2.8788
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:06:54,951 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:06:54,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:06:54,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:06:54,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:06:54,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:06:55,598 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:06:55,599 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:06:56,169 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:06:57,224 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:06:57,224 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:07:00,100 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:07:00,104 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:07:00,104 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:07:00,104 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:07:00,104 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:07:00,750 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:07:00,751 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:07:01,336 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:07:01,499 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:07:01,500 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.51it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.50it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.48it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.46it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.45it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.50it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.51it/s]Extractor Predicting: 24it [00:16,  1.52it/s]Extractor Predicting: 25it [00:17,  1.21it/s]Extractor Predicting: 26it [00:18,  1.27it/s]Extractor Predicting: 27it [00:18,  1.34it/s]Extractor Predicting: 28it [00:19,  1.38it/s]Extractor Predicting: 29it [00:19,  1.43it/s]Extractor Predicting: 30it [00:20,  1.41it/s]Extractor Predicting: 31it [00:21,  1.44it/s]Extractor Predicting: 32it [00:22,  1.47it/s]Extractor Predicting: 33it [00:22,  1.48it/s]Extractor Predicting: 34it [00:23,  1.47it/s]Extractor Predicting: 35it [00:24,  1.48it/s]Extractor Predicting: 36it [00:24,  1.48it/s]Extractor Predicting: 37it [00:25,  1.48it/s]Extractor Predicting: 38it [00:26,  1.48it/s]Extractor Predicting: 39it [00:26,  1.49it/s]Extractor Predicting: 40it [00:27,  1.50it/s]Extractor Predicting: 41it [00:28,  1.51it/s]Extractor Predicting: 42it [00:28,  1.48it/s]Extractor Predicting: 43it [00:29,  1.50it/s]Extractor Predicting: 44it [00:30,  1.49it/s]Extractor Predicting: 45it [00:30,  1.47it/s]Extractor Predicting: 46it [00:31,  1.47it/s]Extractor Predicting: 47it [00:32,  1.48it/s]Extractor Predicting: 48it [00:32,  1.48it/s]Extractor Predicting: 49it [00:33,  1.49it/s]Extractor Predicting: 50it [00:34,  1.51it/s]Extractor Predicting: 51it [00:34,  1.47it/s]Extractor Predicting: 52it [00:35,  1.46it/s]Extractor Predicting: 53it [00:36,  1.36it/s]Extractor Predicting: 54it [00:37,  1.38it/s]Extractor Predicting: 55it [00:37,  1.41it/s]Extractor Predicting: 56it [00:38,  1.43it/s]Extractor Predicting: 57it [00:39,  1.44it/s]Extractor Predicting: 58it [00:39,  1.47it/s]Extractor Predicting: 59it [00:40,  1.45it/s]Extractor Predicting: 60it [00:41,  1.43it/s]Extractor Predicting: 61it [00:41,  1.42it/s]Extractor Predicting: 62it [00:42,  1.42it/s]Extractor Predicting: 63it [00:43,  1.43it/s]Extractor Predicting: 64it [00:44,  1.40it/s]Extractor Predicting: 65it [00:44,  1.41it/s]Extractor Predicting: 66it [00:45,  1.39it/s]Extractor Predicting: 67it [00:46,  1.40it/s]Extractor Predicting: 68it [00:46,  1.38it/s]Extractor Predicting: 69it [00:47,  1.39it/s]Extractor Predicting: 70it [00:48,  1.38it/s]Extractor Predicting: 71it [00:49,  1.39it/s]Extractor Predicting: 72it [00:49,  1.37it/s]Extractor Predicting: 73it [00:50,  1.39it/s]Extractor Predicting: 74it [00:51,  1.36it/s]Extractor Predicting: 75it [00:51,  1.40it/s]Extractor Predicting: 76it [00:52,  1.41it/s]Extractor Predicting: 77it [00:53,  1.39it/s]Extractor Predicting: 78it [00:54,  1.42it/s]Extractor Predicting: 79it [00:54,  1.39it/s]Extractor Predicting: 80it [00:55,  1.37it/s]Extractor Predicting: 81it [00:56,  1.37it/s]Extractor Predicting: 82it [00:56,  1.41it/s]Extractor Predicting: 83it [00:57,  1.42it/s]Extractor Predicting: 84it [00:58,  1.43it/s]Extractor Predicting: 85it [00:59,  1.42it/s]Extractor Predicting: 86it [00:59,  1.39it/s]Extractor Predicting: 87it [01:00,  1.41it/s]Extractor Predicting: 88it [01:01,  1.45it/s]Extractor Predicting: 89it [01:01,  1.50it/s]Extractor Predicting: 90it [01:02,  1.49it/s]Extractor Predicting: 91it [01:03,  1.55it/s]Extractor Predicting: 92it [01:03,  1.60it/s]Extractor Predicting: 93it [01:04,  1.63it/s]Extractor Predicting: 94it [01:04,  1.66it/s]Extractor Predicting: 95it [01:05,  1.60it/s]Extractor Predicting: 96it [01:06,  1.63it/s]Extractor Predicting: 97it [01:06,  1.59it/s]Extractor Predicting: 98it [01:07,  1.58it/s]Extractor Predicting: 99it [01:07,  1.63it/s]Extractor Predicting: 100it [01:08,  1.64it/s]Extractor Predicting: 101it [01:09,  1.61it/s]Extractor Predicting: 102it [01:09,  1.56it/s]Extractor Predicting: 103it [01:10,  1.56it/s]Extractor Predicting: 104it [01:11,  1.52it/s]Extractor Predicting: 105it [01:11,  1.53it/s]Extractor Predicting: 106it [01:12,  1.55it/s]Extractor Predicting: 107it [01:13,  1.52it/s]Extractor Predicting: 108it [01:13,  1.56it/s]Extractor Predicting: 109it [01:14,  1.57it/s]Extractor Predicting: 110it [01:15,  1.59it/s]Extractor Predicting: 111it [01:15,  1.60it/s]Extractor Predicting: 112it [01:16,  1.60it/s]Extractor Predicting: 113it [01:16,  1.63it/s]Extractor Predicting: 114it [01:17,  1.58it/s]Extractor Predicting: 115it [01:18,  1.61it/s]Extractor Predicting: 116it [01:18,  1.57it/s]Extractor Predicting: 117it [01:19,  1.54it/s]Extractor Predicting: 118it [01:20,  1.54it/s]Extractor Predicting: 119it [01:20,  1.51it/s]Extractor Predicting: 120it [01:21,  1.48it/s]Extractor Predicting: 121it [01:22,  1.48it/s]Extractor Predicting: 122it [01:22,  1.48it/s]Extractor Predicting: 123it [01:23,  1.47it/s]Extractor Predicting: 124it [01:24,  1.45it/s]Extractor Predicting: 125it [01:24,  1.45it/s]Extractor Predicting: 126it [01:25,  1.43it/s]Extractor Predicting: 127it [01:26,  1.43it/s]Extractor Predicting: 128it [01:27,  1.44it/s]Extractor Predicting: 129it [01:27,  1.45it/s]Extractor Predicting: 130it [01:28,  1.48it/s]Extractor Predicting: 131it [01:29,  1.45it/s]Extractor Predicting: 132it [01:29,  1.44it/s]Extractor Predicting: 133it [01:30,  1.43it/s]Extractor Predicting: 134it [01:31,  1.43it/s]Extractor Predicting: 135it [01:32,  1.32it/s]Extractor Predicting: 136it [01:32,  1.34it/s]Extractor Predicting: 137it [01:33,  1.36it/s]Extractor Predicting: 138it [01:34,  1.40it/s]Extractor Predicting: 139it [01:34,  1.41it/s]Extractor Predicting: 140it [01:35,  1.43it/s]Extractor Predicting: 141it [01:36,  1.40it/s]Extractor Predicting: 142it [01:37,  1.41it/s]Extractor Predicting: 143it [01:37,  1.43it/s]Extractor Predicting: 144it [01:38,  1.47it/s]Extractor Predicting: 144it [01:38,  1.46it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:47,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:47,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:47,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:47,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:47,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:08:47,457 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:08:47,462 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:08:47,723 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:08:48,761 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:08:48,761 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:50,553 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:50,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:50,557 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:50,557 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:08:50,557 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:08:50,897 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:08:50,898 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:08:51,163 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:08:51,318 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:08:51,318 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4925496688741722,
  "recall": 0.17038946162657503,
  "score": 0.2531914893617021,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.52it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.46it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:16,  1.51it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.50it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.48it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:20,  1.43it/s]Extractor Predicting: 31it [00:20,  1.44it/s]Extractor Predicting: 32it [00:21,  1.44it/s]Extractor Predicting: 33it [00:22,  1.47it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:23,  1.49it/s]Extractor Predicting: 36it [00:24,  1.51it/s]Extractor Predicting: 37it [00:24,  1.44it/s]Extractor Predicting: 38it [00:25,  1.47it/s]Extractor Predicting: 39it [00:26,  1.47it/s]Extractor Predicting: 40it [00:26,  1.49it/s]Extractor Predicting: 41it [00:27,  1.50it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:28,  1.50it/s]Extractor Predicting: 44it [00:29,  1.50it/s]Extractor Predicting: 45it [00:30,  1.49it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:31,  1.51it/s]Extractor Predicting: 48it [00:32,  1.51it/s]Extractor Predicting: 49it [00:32,  1.50it/s]Extractor Predicting: 50it [00:33,  1.49it/s]Extractor Predicting: 51it [00:34,  1.51it/s]Extractor Predicting: 52it [00:34,  1.50it/s]Extractor Predicting: 53it [00:35,  1.50it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:36,  1.46it/s]Extractor Predicting: 56it [00:37,  1.48it/s]Extractor Predicting: 57it [00:38,  1.51it/s]Extractor Predicting: 58it [00:38,  1.55it/s]Extractor Predicting: 59it [00:39,  1.58it/s]Extractor Predicting: 60it [00:40,  1.56it/s]Extractor Predicting: 61it [00:40,  1.54it/s]Extractor Predicting: 62it [00:41,  1.55it/s]Extractor Predicting: 63it [00:42,  1.55it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:43,  1.51it/s]Extractor Predicting: 66it [00:44,  1.50it/s]Extractor Predicting: 67it [00:44,  1.51it/s]Extractor Predicting: 68it [00:45,  1.57it/s]Extractor Predicting: 69it [00:45,  1.54it/s]Extractor Predicting: 70it [00:46,  1.54it/s]Extractor Predicting: 71it [00:47,  1.53it/s]Extractor Predicting: 72it [00:47,  1.54it/s]Extractor Predicting: 73it [00:48,  1.50it/s]Extractor Predicting: 74it [00:49,  1.50it/s]Extractor Predicting: 75it [00:49,  1.51it/s]Extractor Predicting: 76it [00:50,  1.52it/s]Extractor Predicting: 77it [00:51,  1.49it/s]Extractor Predicting: 78it [00:51,  1.51it/s]Extractor Predicting: 79it [00:52,  1.50it/s]Extractor Predicting: 80it [00:53,  1.51it/s]Extractor Predicting: 81it [00:53,  1.50it/s]Extractor Predicting: 82it [00:54,  1.49it/s]Extractor Predicting: 83it [00:55,  1.50it/s]Extractor Predicting: 84it [00:55,  1.51it/s]Extractor Predicting: 85it [00:56,  1.53it/s]Extractor Predicting: 86it [00:57,  1.50it/s]Extractor Predicting: 87it [00:57,  1.50it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:59,  1.52it/s]Extractor Predicting: 90it [00:59,  1.51it/s]Extractor Predicting: 91it [01:00,  1.50it/s]Extractor Predicting: 92it [01:01,  1.47it/s]Extractor Predicting: 93it [01:01,  1.51it/s]Extractor Predicting: 94it [01:02,  1.48it/s]Extractor Predicting: 95it [01:03,  1.48it/s]Extractor Predicting: 96it [01:03,  1.50it/s]Extractor Predicting: 97it [01:04,  1.50it/s]Extractor Predicting: 98it [01:05,  1.50it/s]Extractor Predicting: 99it [01:05,  1.51it/s]Extractor Predicting: 100it [01:06,  1.53it/s]Extractor Predicting: 101it [01:07,  1.54it/s]Extractor Predicting: 102it [01:07,  1.53it/s]Extractor Predicting: 103it [01:08,  1.49it/s]Extractor Predicting: 104it [01:09,  1.47it/s]Extractor Predicting: 105it [01:09,  1.48it/s]Extractor Predicting: 106it [01:10,  1.47it/s]Extractor Predicting: 107it [01:11,  1.47it/s]Extractor Predicting: 108it [01:11,  1.49it/s]Extractor Predicting: 109it [01:12,  1.48it/s]Extractor Predicting: 110it [01:13,  1.49it/s]Extractor Predicting: 111it [01:14,  1.47it/s]Extractor Predicting: 112it [01:14,  1.45it/s]Extractor Predicting: 113it [01:15,  1.45it/s]Extractor Predicting: 114it [01:16,  1.45it/s]Extractor Predicting: 115it [01:16,  1.46it/s]Extractor Predicting: 116it [01:17,  1.50it/s]Extractor Predicting: 117it [01:18,  1.51it/s]Extractor Predicting: 118it [01:18,  1.51it/s]Extractor Predicting: 119it [01:19,  1.51it/s]Extractor Predicting: 120it [01:20,  1.51it/s]Extractor Predicting: 121it [01:20,  1.53it/s]Extractor Predicting: 122it [01:21,  1.54it/s]Extractor Predicting: 123it [01:21,  1.56it/s]Extractor Predicting: 124it [01:22,  1.56it/s]Extractor Predicting: 125it [01:23,  1.41it/s]Extractor Predicting: 126it [01:24,  1.44it/s]Extractor Predicting: 127it [01:24,  1.48it/s]Extractor Predicting: 128it [01:25,  1.48it/s]Extractor Predicting: 129it [01:26,  1.46it/s]Extractor Predicting: 130it [01:26,  1.45it/s]Extractor Predicting: 131it [01:27,  1.50it/s]Extractor Predicting: 132it [01:28,  1.51it/s]Extractor Predicting: 133it [01:28,  1.52it/s]Extractor Predicting: 134it [01:29,  1.52it/s]Extractor Predicting: 135it [01:30,  1.53it/s]Extractor Predicting: 136it [01:30,  1.53it/s]Extractor Predicting: 137it [01:31,  1.53it/s]Extractor Predicting: 138it [01:32,  1.51it/s]Extractor Predicting: 139it [01:32,  1.52it/s]Extractor Predicting: 140it [01:33,  1.57it/s]Extractor Predicting: 141it [01:33,  1.57it/s]Extractor Predicting: 142it [01:34,  1.55it/s]Extractor Predicting: 143it [01:35,  1.52it/s]Extractor Predicting: 144it [01:35,  1.53it/s]Extractor Predicting: 145it [01:36,  1.54it/s]Extractor Predicting: 146it [01:37,  1.57it/s]Extractor Predicting: 147it [01:37,  1.58it/s]Extractor Predicting: 148it [01:38,  1.53it/s]Extractor Predicting: 149it [01:39,  1.54it/s]Extractor Predicting: 150it [01:39,  1.54it/s]Extractor Predicting: 151it [01:40,  1.53it/s]Extractor Predicting: 152it [01:41,  1.54it/s]Extractor Predicting: 153it [01:41,  1.51it/s]Extractor Predicting: 154it [01:42,  1.54it/s]Extractor Predicting: 155it [01:43,  1.55it/s]Extractor Predicting: 156it [01:43,  1.58it/s]Extractor Predicting: 157it [01:44,  1.58it/s]Extractor Predicting: 158it [01:44,  1.63it/s]Extractor Predicting: 159it [01:45,  1.59it/s]Extractor Predicting: 160it [01:46,  1.55it/s]Extractor Predicting: 161it [01:46,  1.53it/s]Extractor Predicting: 162it [01:47,  1.52it/s]Extractor Predicting: 163it [01:48,  1.54it/s]Extractor Predicting: 164it [01:48,  1.54it/s]Extractor Predicting: 165it [01:49,  1.57it/s]Extractor Predicting: 166it [01:50,  1.58it/s]Extractor Predicting: 167it [01:50,  1.58it/s]Extractor Predicting: 168it [01:51,  1.53it/s]Extractor Predicting: 169it [01:52,  1.53it/s]Extractor Predicting: 170it [01:52,  1.52it/s]Extractor Predicting: 171it [01:53,  1.53it/s]Extractor Predicting: 172it [01:53,  1.53it/s]Extractor Predicting: 173it [01:54,  1.52it/s]Extractor Predicting: 174it [01:55,  1.52it/s]Extractor Predicting: 175it [01:56,  1.49it/s]Extractor Predicting: 176it [01:56,  1.46it/s]Extractor Predicting: 177it [01:57,  1.51it/s]Extractor Predicting: 178it [01:58,  1.50it/s]Extractor Predicting: 179it [01:58,  1.53it/s]Extractor Predicting: 180it [01:59,  1.55it/s]Extractor Predicting: 181it [01:59,  1.52it/s]Extractor Predicting: 182it [02:00,  1.52it/s]Extractor Predicting: 183it [02:01,  1.52it/s]Extractor Predicting: 184it [02:01,  1.50it/s]Extractor Predicting: 185it [02:02,  1.49it/s]Extractor Predicting: 186it [02:03,  1.49it/s]Extractor Predicting: 187it [02:03,  1.50it/s]Extractor Predicting: 188it [02:04,  1.48it/s]Extractor Predicting: 189it [02:05,  1.48it/s]Extractor Predicting: 190it [02:06,  1.48it/s]Extractor Predicting: 191it [02:06,  1.51it/s]Extractor Predicting: 192it [02:07,  1.55it/s]Extractor Predicting: 193it [02:07,  1.49it/s]Extractor Predicting: 194it [02:08,  1.47it/s]Extractor Predicting: 195it [02:09,  1.51it/s]Extractor Predicting: 196it [02:09,  1.51it/s]Extractor Predicting: 197it [02:10,  1.52it/s]Extractor Predicting: 198it [02:11,  1.50it/s]Extractor Predicting: 199it [02:11,  1.52it/s]Extractor Predicting: 200it [02:12,  1.58it/s]Extractor Predicting: 201it [02:13,  1.59it/s]Extractor Predicting: 202it [02:13,  1.58it/s]Extractor Predicting: 203it [02:14,  1.55it/s]Extractor Predicting: 204it [02:15,  1.54it/s]Extractor Predicting: 205it [02:15,  1.55it/s]Extractor Predicting: 206it [02:16,  1.51it/s]Extractor Predicting: 207it [02:17,  1.52it/s]Extractor Predicting: 208it [02:17,  1.49it/s]Extractor Predicting: 209it [02:18,  1.51it/s]Extractor Predicting: 210it [02:19,  1.51it/s]Extractor Predicting: 211it [02:19,  1.50it/s]Extractor Predicting: 212it [02:20,  1.52it/s]Extractor Predicting: 213it [02:21,  1.51it/s]Extractor Predicting: 214it [02:21,  1.49it/s]Extractor Predicting: 215it [02:22,  1.49it/s]Extractor Predicting: 216it [02:23,  1.52it/s]Extractor Predicting: 217it [02:23,  1.53it/s]Extractor Predicting: 218it [02:24,  1.50it/s]Extractor Predicting: 219it [02:25,  1.49it/s]Extractor Predicting: 220it [02:25,  1.51it/s]Extractor Predicting: 221it [02:26,  1.51it/s]Extractor Predicting: 222it [02:27,  1.51it/s]Extractor Predicting: 223it [02:27,  1.53it/s]Extractor Predicting: 224it [02:28,  1.53it/s]Extractor Predicting: 225it [02:29,  1.52it/s]Extractor Predicting: 226it [02:29,  1.50it/s]Extractor Predicting: 227it [02:30,  1.51it/s]Extractor Predicting: 228it [02:31,  1.47it/s]Extractor Predicting: 229it [02:31,  1.50it/s]Extractor Predicting: 230it [02:32,  1.52it/s]Extractor Predicting: 231it [02:33,  1.51it/s]Extractor Predicting: 232it [02:33,  1.52it/s]Extractor Predicting: 233it [02:34,  1.52it/s]Extractor Predicting: 234it [02:34,  1.53it/s]Extractor Predicting: 235it [02:35,  1.54it/s]Extractor Predicting: 236it [02:36,  1.55it/s]Extractor Predicting: 237it [02:36,  1.53it/s]Extractor Predicting: 238it [02:37,  1.53it/s]Extractor Predicting: 239it [02:38,  1.51it/s]Extractor Predicting: 240it [02:39,  1.39it/s]Extractor Predicting: 241it [02:39,  1.43it/s]Extractor Predicting: 242it [02:40,  1.49it/s]Extractor Predicting: 243it [02:41,  1.50it/s]Extractor Predicting: 244it [02:41,  1.50it/s]Extractor Predicting: 245it [02:42,  1.50it/s]Extractor Predicting: 246it [02:43,  1.50it/s]Extractor Predicting: 247it [02:43,  1.52it/s]Extractor Predicting: 248it [02:44,  1.52it/s]Extractor Predicting: 249it [02:44,  1.53it/s]Extractor Predicting: 250it [02:45,  1.55it/s]Extractor Predicting: 251it [02:46,  1.50it/s]Extractor Predicting: 252it [02:46,  1.50it/s]Extractor Predicting: 253it [02:47,  1.55it/s]Extractor Predicting: 254it [02:48,  1.53it/s]Extractor Predicting: 255it [02:48,  1.51it/s]Extractor Predicting: 256it [02:49,  1.51it/s]Extractor Predicting: 257it [02:50,  1.52it/s]Extractor Predicting: 258it [02:50,  1.55it/s]Extractor Predicting: 259it [02:51,  1.56it/s]Extractor Predicting: 260it [02:52,  1.56it/s]Extractor Predicting: 261it [02:52,  1.54it/s]Extractor Predicting: 262it [02:53,  1.55it/s]Extractor Predicting: 263it [02:54,  1.55it/s]Extractor Predicting: 264it [02:54,  1.56it/s]Extractor Predicting: 265it [02:55,  1.57it/s]Extractor Predicting: 266it [02:55,  1.61it/s]Extractor Predicting: 267it [02:56,  1.56it/s]Extractor Predicting: 268it [02:57,  1.55it/s]Extractor Predicting: 269it [02:57,  1.57it/s]Extractor Predicting: 270it [02:58,  1.57it/s]Extractor Predicting: 271it [02:59,  1.59it/s]Extractor Predicting: 272it [02:59,  1.62it/s]Extractor Predicting: 273it [03:00,  1.59it/s]Extractor Predicting: 274it [03:00,  1.58it/s]Extractor Predicting: 275it [03:01,  1.57it/s]Extractor Predicting: 276it [03:02,  1.57it/s]Extractor Predicting: 277it [03:02,  1.57it/s]Extractor Predicting: 278it [03:03,  1.56it/s]Extractor Predicting: 279it [03:04,  1.54it/s]Extractor Predicting: 280it [03:04,  1.55it/s]Extractor Predicting: 281it [03:05,  1.53it/s]Extractor Predicting: 281it [03:05,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:03,061 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:03,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:03,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:03,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:03,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:12:03,348 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:12:03,349 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:12:03,609 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:12:04,638 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:12:04,638 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:05,953 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:05,955 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:05,955 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:05,955 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:12:05,955 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:12:06,269 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:12:06,273 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:12:06,539 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:12:06,696 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:12:06,696 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.45269140755600135,
  "recall": 0.20080083049087943,
  "score": 0.27820012327922744,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.34it/s]Extractor Predicting: 3it [00:02,  1.40it/s]Extractor Predicting: 4it [00:02,  1.40it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:03,  1.83it/s]Extractor Predicting: 6it [00:03,  1.57it/s]
[INFO|configuration_utils.py:515] 2023-08-28 16:12:11,170 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:12:11,171 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:12:11,176 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:12:11,176 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 16:12:11,179 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:12:15,058 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 16:12:15,058 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 16:12:15,074 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:12:15,074 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:12:15,092 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:12:15,098 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:12:15,098 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:12:15,098 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:12:15,098 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:12:15,098 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:12:15,098 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.14893617021276595,
  "recall": 0.027237354085603113,
  "score": 0.046052631578947366,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 16:12:15,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:16,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:16,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:17,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:18,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:19,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:20,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:20,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:21,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:23,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:23,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:24,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:25,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:26,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:27,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:27,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:28,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:29,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:29,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:30,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:46, 16.19s/it][WARNING|generation_utils.py:914] 2023-08-28 16:12:31,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:32,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:33,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:33,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:34,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:35,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:36,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:37,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:37,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:38,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:39,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:40,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:41,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:41,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:42,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:43,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:44,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:45,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:45,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:46,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:32<03:28, 16.07s/it][WARNING|generation_utils.py:914] 2023-08-28 16:12:47,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:48,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:49,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:49,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:50,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:51,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:51,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:52,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:53,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:54,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:54,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:55,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:56,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:57,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:58,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:58,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:12:59,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:00,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:01,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:01,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:02,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:47<03:11, 15.93s/it][WARNING|generation_utils.py:914] 2023-08-28 16:13:03,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:03,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:04,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:05,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:05,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:06,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:07,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:07,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:08,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:09,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:09,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:10,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:11,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:11,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:12,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:13,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:13,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:14,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:15,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:15,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:16,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:01<02:46, 15.10s/it][WARNING|generation_utils.py:914] 2023-08-28 16:13:17,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:17,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:18,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:19,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:20,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:20,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:21,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:22,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:22,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:23,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:24,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:25,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:25,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:26,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:27,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:27,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:28,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:29,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:30,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:30,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:31,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:16<02:31, 15.10s/it][WARNING|generation_utils.py:914] 2023-08-28 16:13:32,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:32,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:33,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:34,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:35,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:35,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:36,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:37,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:37,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:38,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:39,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:40,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:40,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:41,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:42,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:43,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:43,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:44,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:45,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:46,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:46,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:47,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:48,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:33<02:22, 15.78s/it][WARNING|generation_utils.py:914] 2023-08-28 16:13:49,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:49,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:50,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:51,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:52,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:52,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:53,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:54,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:55,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:56,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:56,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:57,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:58,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:59,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:13:59,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:00,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:01,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:02,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:03,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:04,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:04,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:05,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:06,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:51<02:11, 16.44s/it][WARNING|generation_utils.py:914] 2023-08-28 16:14:07,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:07,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:08,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:09,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:09,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:10,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:11,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:12,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:12,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:13,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:14,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:15,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:15,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:16,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:17,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:17,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:18,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:19,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:20,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:20,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:21,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:22,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:07<01:54, 16.34s/it][WARNING|generation_utils.py:914] 2023-08-28 16:14:23,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:24,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:24,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:25,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:26,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:27,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:28,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:29,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:29,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:30,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:31,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:32,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:33,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:34,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:34,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:35,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:36,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:37,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:38,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:38,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:39,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:40,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:26<01:41, 16.90s/it][WARNING|generation_utils.py:914] 2023-08-28 16:14:41,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:41,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:42,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:43,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:43,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:44,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:44,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:45,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:46,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:46,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:47,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:48,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:48,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:49,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:50,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:50,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:51,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:52,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:52,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:53,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:38<01:17, 15.58s/it][WARNING|generation_utils.py:914] 2023-08-28 16:14:53,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:54,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:55,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:56,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:57,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:58,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:58,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:14:59,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:00,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:01,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:02,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:02,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:03,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:04,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:04,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:05,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:06,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:07,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:08,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:08,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:09,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:55<01:03, 15.86s/it][WARNING|generation_utils.py:914] 2023-08-28 16:15:10,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:11,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:11,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:12,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:13,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:14,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:14,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:15,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:16,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:17,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:18,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:18,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:19,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:20,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:20,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:21,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:22,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:23,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:23,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:24,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:25,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:25,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:11<00:47, 15.97s/it][WARNING|generation_utils.py:914] 2023-08-28 16:15:26,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:27,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:28,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:28,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:29,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:30,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:31,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:31,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:32,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:33,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:34,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:35,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:35,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:36,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:37,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:37,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:38,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:39,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:39,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:40,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:41,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:42,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:27<00:32, 16.01s/it][WARNING|generation_utils.py:914] 2023-08-28 16:15:42,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:43,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:44,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:45,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:45,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:46,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:46,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:47,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:48,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:49,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:49,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:50,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:51,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:51,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:52,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:53,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:54,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:54,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:55,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:56,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:57,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:57,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:58,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:43<00:16, 16.09s/it][WARNING|generation_utils.py:914] 2023-08-28 16:15:59,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:15:59,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:00,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:01,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:02,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:03,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:03,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:04,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:05,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:06,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:07,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:07,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:08,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:09,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:10,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:11,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:11,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:12,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:13,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:14,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:15,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:16:15,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:01<00:00, 16.54s/it]Generating: 100%|██████████| 15/15 [04:01<00:00, 16.09s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:21,826 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:21,828 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:21,828 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:21,828 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:21,828 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:16:22,457 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:16:22,458 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:16:23,032 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:16:24,114 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:16:24,114 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:26,909 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:26,913 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:26,913 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:26,913 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:16:26,913 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:16:27,543 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:16:27,544 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:16:28,117 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:16:28,282 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:16:28,282 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.9625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : main subject .', 'success_rate': 0.946875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : participant in .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : platform . Context : The game is a spin on " The Legend of Zelda " and is released worldwide at different times based upon the game . Head Entity : The Legend of Zelda , Tail Entity : Nintendo .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.8928571428571429, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8355978260869565, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8835227272727273, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8821022727272727, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9421875, 'errors': {'', "('Sony Computer Entertainment', 'operating system', '', 'The operating system was released by Sony Computer Entertainment on January 22 , 2003 .')"}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : position held .', 'success_rate': 0.8650568181818182, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : religion . Context : Later in the year , the church founded the Council of Trent and appointed a new clergy to administer it . Head Entity : Council of Trent , Tail Entity : Catholic .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : religion .', 'success_rate': 0.8934659090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 9928
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10028, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.32it/s]Extractor Estimating: 2it [00:01,  1.34it/s]Extractor Estimating: 3it [00:02,  1.37it/s]Extractor Estimating: 4it [00:02,  1.44it/s]Extractor Estimating: 5it [00:03,  1.42it/s]Extractor Estimating: 6it [00:04,  1.46it/s]Extractor Estimating: 7it [00:04,  1.45it/s]Extractor Estimating: 8it [00:05,  1.51it/s]Extractor Estimating: 9it [00:06,  1.55it/s]Extractor Estimating: 10it [00:06,  1.57it/s]Extractor Estimating: 11it [00:07,  1.48it/s]Extractor Estimating: 12it [00:08,  1.54it/s]Extractor Estimating: 13it [00:08,  1.54it/s]Extractor Estimating: 14it [00:09,  1.52it/s]Extractor Estimating: 15it [00:10,  1.53it/s]Extractor Estimating: 16it [00:10,  1.55it/s]Extractor Estimating: 17it [00:11,  1.52it/s]Extractor Estimating: 18it [00:12,  1.52it/s]Extractor Estimating: 19it [00:12,  1.53it/s]Extractor Estimating: 20it [00:13,  1.61it/s]Extractor Estimating: 21it [00:13,  1.54it/s]Extractor Estimating: 22it [00:14,  1.57it/s]Extractor Estimating: 23it [00:15,  1.59it/s]Extractor Estimating: 24it [00:15,  1.62it/s]Extractor Estimating: 25it [00:16,  1.53it/s]Extractor Estimating: 26it [00:17,  1.55it/s]Extractor Estimating: 27it [00:17,  1.52it/s]Extractor Estimating: 28it [00:18,  1.47it/s]Extractor Estimating: 29it [00:19,  1.50it/s]Extractor Estimating: 30it [00:19,  1.49it/s]Extractor Estimating: 31it [00:20,  1.56it/s]Extractor Estimating: 32it [00:21,  1.48it/s]Extractor Estimating: 33it [00:21,  1.46it/s]Extractor Estimating: 34it [00:23,  1.10it/s]Extractor Estimating: 35it [00:23,  1.19it/s]Extractor Estimating: 36it [00:24,  1.26it/s]Extractor Estimating: 37it [00:25,  1.31it/s]Extractor Estimating: 38it [00:25,  1.39it/s]Extractor Estimating: 39it [00:26,  1.38it/s]Extractor Estimating: 40it [00:27,  1.41it/s]Extractor Estimating: 41it [00:28,  1.41it/s]Extractor Estimating: 42it [00:28,  1.43it/s]Extractor Estimating: 43it [00:29,  1.46it/s]Extractor Estimating: 44it [00:30,  1.47it/s]Extractor Estimating: 45it [00:30,  1.35it/s]Extractor Estimating: 46it [00:31,  1.41it/s]Extractor Estimating: 47it [00:32,  1.39it/s]Extractor Estimating: 48it [00:33,  1.42it/s]Extractor Estimating: 49it [00:33,  1.42it/s]Extractor Estimating: 50it [00:34,  1.32it/s]Extractor Estimating: 51it [00:35,  1.42it/s]Extractor Estimating: 52it [00:35,  1.49it/s]Extractor Estimating: 53it [00:36,  1.57it/s]Extractor Estimating: 54it [00:37,  1.56it/s]Extractor Estimating: 55it [00:37,  1.58it/s]Extractor Estimating: 56it [00:38,  1.59it/s]Extractor Estimating: 57it [00:38,  1.59it/s]Extractor Estimating: 58it [00:39,  1.55it/s]Extractor Estimating: 59it [00:40,  1.60it/s]Extractor Estimating: 60it [00:40,  1.63it/s]Extractor Estimating: 61it [00:41,  1.64it/s]Extractor Estimating: 62it [00:41,  1.64it/s]Extractor Estimating: 63it [00:42,  1.62it/s]Extractor Estimating: 64it [00:43,  1.64it/s]Extractor Estimating: 65it [00:43,  1.61it/s]Extractor Estimating: 66it [00:44,  1.64it/s]Extractor Estimating: 67it [00:44,  1.66it/s]Extractor Estimating: 68it [00:45,  1.68it/s]Extractor Estimating: 69it [00:46,  1.64it/s]Extractor Estimating: 70it [00:46,  1.65it/s]Extractor Estimating: 71it [00:47,  1.68it/s]Extractor Estimating: 72it [00:48,  1.62it/s]Extractor Estimating: 73it [00:48,  1.59it/s]Extractor Estimating: 74it [00:49,  1.60it/s]Extractor Estimating: 75it [00:49,  1.59it/s]Extractor Estimating: 76it [00:50,  1.61it/s]Extractor Estimating: 77it [00:51,  1.60it/s]Extractor Estimating: 78it [00:51,  1.61it/s]Extractor Estimating: 79it [00:52,  1.60it/s]Extractor Estimating: 80it [00:53,  1.63it/s]Extractor Estimating: 81it [00:53,  1.63it/s]Extractor Estimating: 82it [00:54,  1.61it/s]Extractor Estimating: 83it [00:54,  1.58it/s]Extractor Estimating: 84it [00:55,  1.62it/s]Extractor Estimating: 85it [00:56,  1.67it/s]Extractor Estimating: 86it [00:56,  1.65it/s]Extractor Estimating: 87it [00:57,  1.69it/s]Extractor Estimating: 88it [00:57,  1.66it/s]Extractor Estimating: 89it [00:58,  1.68it/s]Extractor Estimating: 90it [00:59,  1.71it/s]Extractor Estimating: 91it [00:59,  1.64it/s]Extractor Estimating: 92it [01:00,  1.66it/s]Extractor Estimating: 93it [01:00,  1.70it/s]Extractor Estimating: 94it [01:01,  1.72it/s]Extractor Estimating: 95it [01:01,  1.72it/s]Extractor Estimating: 96it [01:02,  1.70it/s]Extractor Estimating: 97it [01:03,  1.70it/s]Extractor Estimating: 98it [01:03,  1.66it/s]Extractor Estimating: 99it [01:04,  1.60it/s]Extractor Estimating: 100it [01:05,  1.60it/s]Extractor Estimating: 101it [01:05,  1.65it/s]Extractor Estimating: 102it [01:06,  1.70it/s]Extractor Estimating: 103it [01:06,  1.74it/s]Extractor Estimating: 104it [01:07,  1.78it/s]Extractor Estimating: 105it [01:07,  1.85it/s]Extractor Estimating: 106it [01:08,  1.90it/s]Extractor Estimating: 107it [01:08,  1.93it/s]Extractor Estimating: 108it [01:09,  1.93it/s]Extractor Estimating: 109it [01:09,  1.86it/s]Extractor Estimating: 110it [01:10,  1.87it/s]Extractor Estimating: 111it [01:10,  1.88it/s]Extractor Estimating: 112it [01:11,  1.80it/s]Extractor Estimating: 113it [01:12,  1.70it/s]Extractor Estimating: 114it [01:12,  1.68it/s]Extractor Estimating: 115it [01:13,  1.74it/s]Extractor Estimating: 116it [01:13,  1.76it/s]Extractor Estimating: 117it [01:14,  1.75it/s]Extractor Estimating: 118it [01:14,  1.79it/s]Extractor Estimating: 119it [01:15,  1.77it/s]Extractor Estimating: 120it [01:16,  1.78it/s]Extractor Estimating: 121it [01:16,  1.83it/s]Extractor Estimating: 122it [01:17,  1.81it/s]Extractor Estimating: 123it [01:17,  1.77it/s]Extractor Estimating: 124it [01:18,  1.83it/s]Extractor Estimating: 125it [01:18,  1.83it/s]Extractor Estimating: 126it [01:19,  1.83it/s]Extractor Estimating: 127it [01:20,  1.72it/s]Extractor Estimating: 128it [01:20,  1.73it/s]Extractor Estimating: 129it [01:21,  1.78it/s]Extractor Estimating: 130it [01:21,  1.80it/s]Extractor Estimating: 131it [01:22,  1.76it/s]Extractor Estimating: 132it [01:22,  1.75it/s]Extractor Estimating: 133it [01:23,  1.77it/s]Extractor Estimating: 134it [01:23,  1.79it/s]Extractor Estimating: 135it [01:24,  1.82it/s]Extractor Estimating: 136it [01:25,  1.80it/s]Extractor Estimating: 137it [01:25,  1.85it/s]Extractor Estimating: 138it [01:26,  1.90it/s]Extractor Estimating: 139it [01:26,  1.87it/s]Extractor Estimating: 140it [01:27,  1.79it/s]Extractor Estimating: 141it [01:27,  1.74it/s]Extractor Estimating: 142it [01:28,  1.66it/s]Extractor Estimating: 143it [01:29,  1.68it/s]Extractor Estimating: 144it [01:29,  1.61it/s]Extractor Estimating: 145it [01:30,  1.59it/s]Extractor Estimating: 146it [01:30,  1.66it/s]Extractor Estimating: 147it [01:31,  1.64it/s]Extractor Estimating: 148it [01:32,  1.72it/s]Extractor Estimating: 149it [01:32,  1.63it/s]Extractor Estimating: 150it [01:33,  1.68it/s]Extractor Estimating: 151it [01:33,  1.62it/s]Extractor Estimating: 152it [01:34,  1.54it/s]Extractor Estimating: 153it [01:35,  1.47it/s]Extractor Estimating: 154it [01:36,  1.49it/s]Extractor Estimating: 155it [01:36,  1.53it/s]Extractor Estimating: 156it [01:37,  1.50it/s]Extractor Estimating: 157it [01:38,  1.40it/s]Extractor Estimating: 158it [01:38,  1.41it/s]Extractor Estimating: 159it [01:39,  1.40it/s]Extractor Estimating: 160it [01:40,  1.41it/s]Extractor Estimating: 161it [01:41,  1.41it/s]Extractor Estimating: 162it [01:41,  1.41it/s]Extractor Estimating: 163it [01:42,  1.38it/s]Extractor Estimating: 164it [01:43,  1.39it/s]Extractor Estimating: 165it [01:43,  1.44it/s]Extractor Estimating: 166it [01:44,  1.42it/s]Extractor Estimating: 167it [01:45,  1.46it/s]Extractor Estimating: 168it [01:45,  1.50it/s]Extractor Estimating: 169it [01:46,  1.47it/s]Extractor Estimating: 170it [01:47,  1.43it/s]Extractor Estimating: 171it [01:48,  1.42it/s]Extractor Estimating: 172it [01:48,  1.40it/s]Extractor Estimating: 173it [01:49,  1.44it/s]Extractor Estimating: 174it [01:50,  1.46it/s]Extractor Estimating: 175it [01:50,  1.48it/s]Extractor Estimating: 176it [01:51,  1.53it/s]Extractor Estimating: 177it [01:52,  1.53it/s]Extractor Estimating: 178it [01:52,  1.53it/s]Extractor Estimating: 179it [01:53,  1.54it/s]Extractor Estimating: 180it [01:53,  1.57it/s]Extractor Estimating: 181it [01:54,  1.57it/s]Extractor Estimating: 182it [01:55,  1.59it/s]Extractor Estimating: 183it [01:55,  1.65it/s]Extractor Estimating: 184it [01:56,  1.67it/s]Extractor Estimating: 185it [01:56,  1.61it/s]Extractor Estimating: 186it [01:57,  1.65it/s]Extractor Estimating: 187it [01:58,  1.59it/s]Extractor Estimating: 188it [01:58,  1.59it/s]Extractor Estimating: 189it [01:59,  1.62it/s]Extractor Estimating: 190it [02:00,  1.63it/s]Extractor Estimating: 191it [02:00,  1.72it/s]Extractor Estimating: 192it [02:01,  1.71it/s]Extractor Estimating: 193it [02:01,  1.70it/s]Extractor Estimating: 194it [02:02,  1.64it/s]Extractor Estimating: 195it [02:03,  1.66it/s]Extractor Estimating: 196it [02:03,  1.64it/s]Extractor Estimating: 197it [02:04,  1.65it/s]Extractor Estimating: 198it [02:04,  1.61it/s]Extractor Estimating: 199it [02:05,  1.66it/s]Extractor Estimating: 200it [02:06,  1.67it/s]Extractor Estimating: 201it [02:06,  1.42it/s]Extractor Estimating: 202it [02:07,  1.43it/s]Extractor Estimating: 203it [02:08,  1.38it/s]Extractor Estimating: 204it [02:09,  1.33it/s]Extractor Estimating: 205it [02:10,  1.29it/s]Extractor Estimating: 206it [02:10,  1.30it/s]Extractor Estimating: 207it [02:11,  1.34it/s]Extractor Estimating: 208it [02:12,  1.31it/s]Extractor Estimating: 209it [02:13,  1.29it/s]Extractor Estimating: 210it [02:13,  1.33it/s]Extractor Estimating: 211it [02:15,  1.04it/s]Extractor Estimating: 212it [02:16,  1.08it/s]Extractor Estimating: 213it [02:16,  1.18it/s]Extractor Estimating: 214it [02:17,  1.23it/s]Extractor Estimating: 215it [02:18,  1.26it/s]Extractor Estimating: 216it [02:18,  1.32it/s]Extractor Estimating: 217it [02:19,  1.26it/s]Extractor Estimating: 218it [02:20,  1.28it/s]Extractor Estimating: 219it [02:21,  1.33it/s]Extractor Estimating: 220it [02:21,  1.35it/s]Extractor Estimating: 221it [02:22,  1.33it/s]Extractor Estimating: 222it [02:23,  1.38it/s]Extractor Estimating: 223it [02:24,  1.39it/s]Extractor Estimating: 224it [02:25,  1.26it/s]Extractor Estimating: 225it [02:25,  1.28it/s]Extractor Estimating: 226it [02:26,  1.43it/s]Extractor Estimating: 227it [02:26,  1.58it/s]Extractor Estimating: 228it [02:27,  1.70it/s]Extractor Estimating: 229it [02:27,  1.77it/s]Extractor Estimating: 230it [02:28,  1.87it/s]Extractor Estimating: 231it [02:28,  1.86it/s]Extractor Estimating: 232it [02:29,  1.82it/s]Extractor Estimating: 233it [02:29,  1.87it/s]Extractor Estimating: 234it [02:30,  1.89it/s]Extractor Estimating: 235it [02:31,  1.85it/s]Extractor Estimating: 236it [02:31,  1.86it/s]Extractor Estimating: 237it [02:32,  1.87it/s]Extractor Estimating: 238it [02:32,  1.90it/s]Extractor Estimating: 239it [02:33,  1.84it/s]Extractor Estimating: 240it [02:33,  1.89it/s]Extractor Estimating: 241it [02:34,  1.80it/s]Extractor Estimating: 242it [02:34,  1.86it/s]Extractor Estimating: 243it [02:35,  1.90it/s]Extractor Estimating: 244it [02:35,  1.87it/s]Extractor Estimating: 245it [02:36,  1.75it/s]Extractor Estimating: 246it [02:36,  1.80it/s]Extractor Estimating: 247it [02:37,  1.82it/s]Extractor Estimating: 248it [02:38,  1.90it/s]Extractor Estimating: 249it [02:38,  1.87it/s]Extractor Estimating: 250it [02:39,  1.81it/s]Extractor Estimating: 251it [02:39,  1.79it/s]Extractor Estimating: 252it [02:40,  1.71it/s]Extractor Estimating: 253it [02:41,  1.65it/s]Extractor Estimating: 254it [02:41,  1.55it/s]Extractor Estimating: 255it [02:42,  1.50it/s]Extractor Estimating: 256it [02:43,  1.47it/s]Extractor Estimating: 257it [02:43,  1.45it/s]Extractor Estimating: 258it [02:44,  1.46it/s]Extractor Estimating: 259it [02:45,  1.46it/s]Extractor Estimating: 260it [02:45,  1.52it/s]Extractor Estimating: 261it [02:46,  1.51it/s]Extractor Estimating: 262it [02:47,  1.52it/s]Extractor Estimating: 263it [02:47,  1.56it/s]Extractor Estimating: 264it [02:48,  1.53it/s]Extractor Estimating: 265it [02:49,  1.55it/s]Extractor Estimating: 266it [02:49,  1.56it/s]Extractor Estimating: 267it [02:50,  1.57it/s]Extractor Estimating: 268it [02:50,  1.57it/s]Extractor Estimating: 269it [02:51,  1.55it/s]Extractor Estimating: 270it [02:52,  1.55it/s]Extractor Estimating: 271it [02:52,  1.53it/s]Extractor Estimating: 272it [02:53,  1.54it/s]Extractor Estimating: 273it [02:54,  1.56it/s]Extractor Estimating: 274it [02:54,  1.47it/s]Extractor Estimating: 275it [02:55,  1.50it/s]Extractor Estimating: 276it [02:56,  1.57it/s]Extractor Estimating: 277it [02:56,  1.64it/s]Extractor Estimating: 278it [02:57,  1.65it/s]Extractor Estimating: 279it [02:57,  1.67it/s]Extractor Estimating: 280it [02:58,  1.65it/s]Extractor Estimating: 281it [02:59,  1.65it/s]Extractor Estimating: 282it [02:59,  1.54it/s]Extractor Estimating: 283it [03:00,  1.57it/s]Extractor Estimating: 284it [03:01,  1.57it/s]Extractor Estimating: 285it [03:01,  1.55it/s]Extractor Estimating: 286it [03:02,  1.55it/s]Extractor Estimating: 287it [03:03,  1.59it/s]Extractor Estimating: 288it [03:03,  1.60it/s]Extractor Estimating: 289it [03:04,  1.66it/s]Extractor Estimating: 290it [03:04,  1.63it/s]Extractor Estimating: 291it [03:05,  1.62it/s]Extractor Estimating: 292it [03:06,  1.65it/s]Extractor Estimating: 293it [03:06,  1.57it/s]Extractor Estimating: 294it [03:07,  1.59it/s]Extractor Estimating: 295it [03:07,  1.60it/s]Extractor Estimating: 296it [03:08,  1.65it/s]Extractor Estimating: 297it [03:09,  1.70it/s]Extractor Estimating: 298it [03:09,  1.68it/s]Extractor Estimating: 299it [03:10,  1.68it/s]Extractor Estimating: 300it [03:10,  1.73it/s]Extractor Estimating: 301it [03:11,  1.74it/s]Extractor Estimating: 302it [03:12,  1.72it/s]Extractor Estimating: 303it [03:12,  1.69it/s]Extractor Estimating: 304it [03:13,  1.73it/s]Extractor Estimating: 305it [03:13,  1.66it/s]Extractor Estimating: 306it [03:14,  1.60it/s]Extractor Estimating: 307it [03:15,  1.61it/s]Extractor Estimating: 308it [03:15,  1.51it/s]Extractor Estimating: 309it [03:16,  1.56it/s]Extractor Estimating: 310it [03:17,  1.59it/s]Extractor Estimating: 311it [03:17,  1.63it/s]Extractor Estimating: 312it [03:18,  1.69it/s]Extractor Estimating: 313it [03:18,  1.61it/s]Extractor Estimating: 314it [03:19,  1.66it/s]Extractor Estimating: 315it [03:20,  1.68it/s]Extractor Estimating: 316it [03:20,  1.65it/s]Extractor Estimating: 317it [03:21,  1.67it/s]Extractor Estimating: 318it [03:21,  1.68it/s]Extractor Estimating: 319it [03:22,  1.69it/s]Extractor Estimating: 320it [03:22,  1.70it/s]Extractor Estimating: 321it [03:23,  1.68it/s]Extractor Estimating: 322it [03:24,  1.62it/s]Extractor Estimating: 323it [03:24,  1.68it/s]Extractor Estimating: 324it [03:25,  1.69it/s]Extractor Estimating: 325it [03:26,  1.67it/s]Extractor Estimating: 326it [03:26,  1.65it/s]Extractor Estimating: 327it [03:27,  1.60it/s]Extractor Estimating: 328it [03:27,  1.64it/s]Extractor Estimating: 329it [03:28,  1.62it/s]Extractor Estimating: 330it [03:29,  1.63it/s]Extractor Estimating: 331it [03:29,  1.70it/s]Extractor Estimating: 332it [03:30,  1.72it/s]Extractor Estimating: 333it [03:30,  1.71it/s]Extractor Estimating: 334it [03:31,  1.68it/s]Extractor Estimating: 335it [03:31,  1.70it/s]Extractor Estimating: 336it [03:32,  1.70it/s]Extractor Estimating: 337it [03:33,  1.69it/s]Extractor Estimating: 338it [03:33,  1.71it/s]Extractor Estimating: 339it [03:34,  1.67it/s]Extractor Estimating: 340it [03:34,  1.68it/s]Extractor Estimating: 341it [03:35,  1.66it/s]Extractor Estimating: 342it [03:36,  1.64it/s]Extractor Estimating: 343it [03:36,  1.63it/s]Extractor Estimating: 344it [03:37,  1.66it/s]Extractor Estimating: 345it [03:37,  1.68it/s]Extractor Estimating: 346it [03:38,  1.68it/s]Extractor Estimating: 347it [03:39,  1.62it/s]Extractor Estimating: 348it [03:39,  1.68it/s]Extractor Estimating: 349it [03:40,  1.69it/s]Extractor Estimating: 350it [03:40,  1.69it/s]Extractor Estimating: 351it [03:41,  1.66it/s]Extractor Estimating: 352it [03:42,  1.64it/s]Extractor Estimating: 353it [03:42,  1.60it/s]Extractor Estimating: 354it [03:43,  1.54it/s]Extractor Estimating: 355it [03:44,  1.53it/s]Extractor Estimating: 356it [03:45,  1.45it/s]Extractor Estimating: 357it [03:45,  1.52it/s]Extractor Estimating: 358it [03:46,  1.55it/s]Extractor Estimating: 359it [03:46,  1.59it/s]Extractor Estimating: 360it [03:47,  1.53it/s]Extractor Estimating: 361it [03:48,  1.47it/s]Extractor Estimating: 362it [03:48,  1.48it/s]Extractor Estimating: 363it [03:49,  1.54it/s]Extractor Estimating: 364it [03:50,  1.50it/s]Extractor Estimating: 365it [03:50,  1.48it/s]Extractor Estimating: 366it [03:51,  1.52it/s]Extractor Estimating: 367it [03:52,  1.51it/s]Extractor Estimating: 368it [03:52,  1.52it/s]Extractor Estimating: 369it [03:53,  1.59it/s]Extractor Estimating: 370it [03:54,  1.54it/s]Extractor Estimating: 371it [03:54,  1.57it/s]Extractor Estimating: 372it [03:55,  1.58it/s]Extractor Estimating: 373it [03:56,  1.51it/s]Extractor Estimating: 374it [03:56,  1.49it/s]Extractor Estimating: 375it [03:57,  1.47it/s]Extractor Estimating: 375it [03:57,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:20:37,110 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:20:37,115 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:20:37,115 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:20:37,115 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:20:37,115 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:20:37,520 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:20:37,521 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:20:37,785 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:20:38,845 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:20:38,845 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:20:40,133 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:20:40,140 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:20:40,140 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:20:40,140 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:20:40,140 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:20:40,478 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:20:40,479 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:20:40,745 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:20:40,913 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:20:40,913 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 18:44:31,897 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 18:44:31,988 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7496 mean pseudo reward: 0.954388844863117
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 19255
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19355, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19355, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.154, loss:634.3210
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.099, loss:595.8129
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.094, loss:588.6265
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.118, loss:555.6040
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.115, loss:576.3962
>> valid entity prec:0.5689, rec:0.5614, f1:0.5652
>> valid relation prec:0.3308, rec:0.1484, f1:0.2049
>> valid relation with NER prec:0.3308, rec:0.1484, f1:0.2049
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.503, loss:578.5731
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.117, loss:525.7484
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.119, loss:543.1544
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.110, loss:590.9341
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.107, loss:569.5453
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6226, rec:0.5678, f1:0.5939
>> valid relation prec:0.3716, rec:0.1390, f1:0.2023
>> valid relation with NER prec:0.3716, rec:0.1390, f1:0.2023
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.555, loss:546.7372
g_step 1200, step 261, avg_time 1.098, loss:572.0785
g_step 1300, step 48, avg_time 1.099, loss:500.6994
g_step 1400, step 148, avg_time 1.133, loss:505.6001
g_step 1500, step 248, avg_time 1.159, loss:537.3230
>> valid entity prec:0.5943, rec:0.5514, f1:0.5720
>> valid relation prec:0.3459, rec:0.1599, f1:0.2187
>> valid relation with NER prec:0.3459, rec:0.1599, f1:0.2187
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.478, loss:508.1269
g_step 1700, step 135, avg_time 1.114, loss:492.6297
g_step 1800, step 235, avg_time 1.112, loss:475.5098
g_step 1900, step 22, avg_time 1.117, loss:482.0321
g_step 2000, step 122, avg_time 1.112, loss:442.5820
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5372, rec:0.5436, f1:0.5404
>> valid relation prec:0.3080, rec:0.1450, f1:0.1972
>> valid relation with NER prec:0.3080, rec:0.1450, f1:0.1972
g_step 2100, step 222, avg_time 2.496, loss:460.0341
g_step 2200, step 9, avg_time 1.107, loss:477.7890
g_step 2300, step 109, avg_time 1.110, loss:434.0172
g_step 2400, step 209, avg_time 1.101, loss:437.7437
g_step 2500, step 309, avg_time 1.121, loss:454.6655
>> valid entity prec:0.6193, rec:0.5403, f1:0.5771
>> valid relation prec:0.3366, rec:0.1398, f1:0.1976
>> valid relation with NER prec:0.3366, rec:0.1398, f1:0.1976
g_step 2600, step 96, avg_time 2.493, loss:384.9510
g_step 2700, step 196, avg_time 1.108, loss:418.1860
g_step 2800, step 296, avg_time 1.101, loss:411.9070
g_step 2900, step 83, avg_time 1.116, loss:389.9302
g_step 3000, step 183, avg_time 1.092, loss:388.5106
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5969, rec:0.5532, f1:0.5742
>> valid relation prec:0.3327, rec:0.1456, f1:0.2025
>> valid relation with NER prec:0.3327, rec:0.1456, f1:0.2025
g_step 3100, step 283, avg_time 2.507, loss:416.2150
g_step 3200, step 70, avg_time 1.117, loss:384.6718
g_step 3300, step 170, avg_time 1.115, loss:350.8258
g_step 3400, step 270, avg_time 1.095, loss:384.1314
g_step 3500, step 57, avg_time 1.098, loss:347.2577
>> valid entity prec:0.6397, rec:0.5051, f1:0.5645
>> valid relation prec:0.3181, rec:0.1272, f1:0.1817
>> valid relation with NER prec:0.3181, rec:0.1272, f1:0.1817
g_step 3600, step 157, avg_time 2.500, loss:345.5704
g_step 3700, step 257, avg_time 1.115, loss:381.8024
g_step 3800, step 44, avg_time 1.115, loss:348.4847
g_step 3900, step 144, avg_time 1.108, loss:328.6749
g_step 4000, step 244, avg_time 1.118, loss:351.3586
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6284, rec:0.5225, f1:0.5706
>> valid relation prec:0.3503, rec:0.1241, f1:0.1832
>> valid relation with NER prec:0.3503, rec:0.1241, f1:0.1832
g_step 4100, step 31, avg_time 2.501, loss:337.0474
g_step 4200, step 131, avg_time 1.116, loss:332.3346
g_step 4300, step 231, avg_time 1.097, loss:331.4913
g_step 4400, step 18, avg_time 1.112, loss:340.7034
g_step 4500, step 118, avg_time 1.102, loss:297.9739
>> valid entity prec:0.5997, rec:0.5469, f1:0.5721
>> valid relation prec:0.3121, rec:0.1415, f1:0.1948
>> valid relation with NER prec:0.3121, rec:0.1415, f1:0.1948
g_step 4600, step 218, avg_time 2.496, loss:312.1840
g_step 4700, step 5, avg_time 1.114, loss:360.3220
g_step 4800, step 105, avg_time 1.106, loss:280.6191
g_step 4900, step 205, avg_time 1.116, loss:313.9016
g_step 5000, step 305, avg_time 1.109, loss:314.4579
learning rate was adjusted to 0.0008
>> valid entity prec:0.5880, rec:0.5385, f1:0.5622
>> valid relation prec:0.3087, rec:0.1352, f1:0.1881
>> valid relation with NER prec:0.3087, rec:0.1352, f1:0.1881
g_step 5100, step 92, avg_time 2.493, loss:278.4356
g_step 5200, step 192, avg_time 1.113, loss:294.0630
g_step 5300, step 292, avg_time 1.118, loss:323.1307
g_step 5400, step 79, avg_time 1.098, loss:287.2360
g_step 5500, step 179, avg_time 1.135, loss:281.3866
>> valid entity prec:0.6024, rec:0.5518, f1:0.5760
>> valid relation prec:0.2937, rec:0.1444, f1:0.1936
>> valid relation with NER prec:0.2937, rec:0.1444, f1:0.1936
g_step 5600, step 279, avg_time 2.488, loss:294.8521
g_step 5700, step 66, avg_time 1.113, loss:270.3253
g_step 5800, step 166, avg_time 1.113, loss:281.2809
g_step 5900, step 266, avg_time 1.117, loss:287.0559
g_step 6000, step 53, avg_time 1.114, loss:272.2473
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6182, rec:0.5171, f1:0.5631
>> valid relation prec:0.2823, rec:0.1278, f1:0.1759
>> valid relation with NER prec:0.2823, rec:0.1278, f1:0.1759
g_step 6100, step 153, avg_time 2.496, loss:260.0165
g_step 6200, step 253, avg_time 1.118, loss:259.9634
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 18:44:31 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 18:44:31 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_18-44-31_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 18:44:32 - WARNING - datasets.builder -   Using custom data configuration default-257ac802a6047ae2
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-257ac802a6047ae2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 18:44:33,826 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:44:33,827 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:44:33,828 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:44:33,829 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:44:33,889 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:44:33,918 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:44:33,918 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:44:33,918 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:44:33,918 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:44:33,918 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:44:33,919 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 18:44:34,228 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:44:37,345 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 18:44:37,357 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-257ac802a6047ae2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.93ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.85ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.29ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.54ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.67ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.73ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.78ba/s]100%|██████████| 8/8 [00:01<00:00,  5.64ba/s]100%|██████████| 8/8 [00:01<00:00,  4.79ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.79ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.59ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.94ba/s]100%|██████████| 4/4 [00:00<00:00,  5.08ba/s]100%|██████████| 4/4 [00:00<00:00,  4.37ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.37ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.26ba/s] 50%|█████     | 4/8 [00:00<00:00,  8.64ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  9.42ba/s]100%|██████████| 8/8 [00:00<00:00, 10.76ba/s]100%|██████████| 8/8 [00:00<00:00,  9.94ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.03ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.74ba/s]100%|██████████| 4/4 [00:00<00:00, 11.03ba/s]
[INFO|trainer.py:414] 2023-08-28 18:44:41,774 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 18:44:41,801 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 18:44:41,801 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-28 18:44:41,801 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 18:44:41,801 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 18:44:41,801 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 18:44:41,802 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 18:44:41,802 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:59,  3.26it/s]  0%|          | 2/585 [00:00<02:52,  3.38it/s]  1%|          | 3/585 [00:00<02:50,  3.42it/s]  1%|          | 4/585 [00:01<02:49,  3.43it/s]  1%|          | 5/585 [00:01<02:48,  3.45it/s]  1%|          | 6/585 [00:01<02:47,  3.45it/s]  1%|          | 7/585 [00:02<02:48,  3.42it/s]  1%|▏         | 8/585 [00:02<02:47,  3.44it/s]  2%|▏         | 9/585 [00:02<02:47,  3.45it/s]  2%|▏         | 10/585 [00:02<02:46,  3.45it/s]  2%|▏         | 11/585 [00:03<02:46,  3.46it/s]  2%|▏         | 12/585 [00:03<02:45,  3.46it/s]  2%|▏         | 13/585 [00:03<02:45,  3.46it/s]  2%|▏         | 14/585 [00:04<02:44,  3.47it/s]  3%|▎         | 15/585 [00:04<02:44,  3.47it/s]  3%|▎         | 16/585 [00:04<02:44,  3.47it/s]  3%|▎         | 17/585 [00:04<02:43,  3.47it/s]  3%|▎         | 18/585 [00:05<02:45,  3.43it/s]  3%|▎         | 19/585 [00:05<02:44,  3.44it/s]  3%|▎         | 20/585 [00:05<02:43,  3.45it/s]  4%|▎         | 21/585 [00:06<02:43,  3.45it/s]  4%|▍         | 22/585 [00:06<02:43,  3.45it/s]  4%|▍         | 23/585 [00:06<02:42,  3.45it/s]  4%|▍         | 24/585 [00:06<02:42,  3.46it/s]  4%|▍         | 25/585 [00:07<02:42,  3.46it/s]  4%|▍         | 26/585 [00:07<02:41,  3.45it/s]  5%|▍         | 27/585 [00:07<02:41,  3.45it/s]  5%|▍         | 28/585 [00:08<02:41,  3.46it/s]  5%|▍         | 29/585 [00:08<02:40,  3.46it/s]  5%|▌         | 30/585 [00:08<02:42,  3.41it/s]  5%|▌         | 31/585 [00:09<02:41,  3.42it/s]  5%|▌         | 32/585 [00:09<02:41,  3.43it/s]  6%|▌         | 33/585 [00:09<02:40,  3.44it/s]  6%|▌         | 34/585 [00:09<02:39,  3.45it/s]  6%|▌         | 35/585 [00:10<02:39,  3.45it/s]  6%|▌         | 36/585 [00:10<02:39,  3.45it/s]  6%|▋         | 37/585 [00:10<02:38,  3.45it/s]  6%|▋         | 38/585 [00:11<02:38,  3.45it/s]  7%|▋         | 39/585 [00:11<02:38,  3.45it/s]  7%|▋         | 40/585 [00:11<02:37,  3.45it/s]  7%|▋         | 41/585 [00:11<02:37,  3.45it/s]  7%|▋         | 42/585 [00:12<02:37,  3.45it/s]  7%|▋         | 43/585 [00:12<02:36,  3.45it/s]  8%|▊         | 44/585 [00:12<02:36,  3.45it/s]  8%|▊         | 45/585 [00:13<02:36,  3.45it/s]  8%|▊         | 46/585 [00:13<02:35,  3.46it/s]  8%|▊         | 47/585 [00:13<02:36,  3.44it/s]  8%|▊         | 48/585 [00:13<02:35,  3.45it/s]  8%|▊         | 49/585 [00:14<02:35,  3.45it/s]  9%|▊         | 50/585 [00:14<02:35,  3.45it/s]  9%|▊         | 51/585 [00:14<02:34,  3.45it/s]  9%|▉         | 52/585 [00:15<02:34,  3.45it/s]  9%|▉         | 53/585 [00:15<02:34,  3.45it/s]  9%|▉         | 54/585 [00:15<02:33,  3.45it/s]  9%|▉         | 55/585 [00:15<02:33,  3.45it/s] 10%|▉         | 56/585 [00:16<02:33,  3.46it/s] 10%|▉         | 57/585 [00:16<02:32,  3.46it/s] 10%|▉         | 58/585 [00:16<02:32,  3.46it/s] 10%|█         | 59/585 [00:17<02:32,  3.46it/s] 10%|█         | 60/585 [00:17<02:31,  3.46it/s] 10%|█         | 61/585 [00:17<02:31,  3.45it/s] 11%|█         | 62/585 [00:17<02:31,  3.45it/s] 11%|█         | 63/585 [00:18<02:31,  3.45it/s] 11%|█         | 64/585 [00:18<02:30,  3.46it/s] 11%|█         | 65/585 [00:18<02:32,  3.41it/s] 11%|█▏        | 66/585 [00:19<02:31,  3.42it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.44it/s] 12%|█▏        | 69/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 71/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 72/585 [00:20<02:28,  3.45it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.45it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.45it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.45it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.46it/s] 14%|█▎        | 79/585 [00:22<02:26,  3.45it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.45it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.45it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.45it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.42it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.43it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.44it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.44it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.44it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.45it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.45it/s] 15%|█▌        | 90/585 [00:26<02:23,  3.45it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.45it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.45it/s] 16%|█▌        | 93/585 [00:26<02:22,  3.45it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.45it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.45it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 98/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.45it/s] 17%|█▋        | 100/585 [00:29<02:36,  3.10it/s] 17%|█▋        | 101/585 [00:29<02:31,  3.20it/s] 17%|█▋        | 102/585 [00:29<02:27,  3.27it/s] 18%|█▊        | 103/585 [00:29<02:24,  3.33it/s] 18%|█▊        | 104/585 [00:30<02:23,  3.36it/s] 18%|█▊        | 105/585 [00:30<02:21,  3.39it/s] 18%|█▊        | 106/585 [00:30<02:20,  3.41it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.42it/s] 18%|█▊        | 108/585 [00:31<02:19,  3.43it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.44it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.44it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.45it/s] 19%|█▉        | 114/585 [00:33<02:16,  3.45it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.45it/s] 20%|█▉        | 116/585 [00:33<02:16,  3.45it/s] 20%|██        | 117/585 [00:34<02:15,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 18:45:15,890 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:45:15,890 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 18:45:15,890 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.19it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.81it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.93it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.32it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.71it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.45it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.15it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.95it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.96it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.85it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.98it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.93it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.91it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.96it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.90it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.87it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.86it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.64it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.73it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.80it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.92it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.79it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.86it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.91it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.82it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.80it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.76it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.75it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.81it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.81it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.72it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.81it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.89it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.84it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.92it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.88it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.83it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.96it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.92it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.75it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.68it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.76it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.75it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.81it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.84it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.81it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.85it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.88it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.75it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.84it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.73it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.85it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.88it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.80it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.78it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.85it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.86it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.86it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.82it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.74it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.84it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.85it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.75it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.78it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.83it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.78it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.88it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.64it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.73it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.83it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.85it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.82it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.86it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.85it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.76it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.88it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.75it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.82it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.83it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.82it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.83it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.90it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.76it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.78it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.85it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.78it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.78it/s][A 20%|██        | 117/585 [00:43<02:15,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:45:25,259 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 18:45:25,315 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:45:31,839 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:45:31,952 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:45:31,973 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:00<1:02:38,  8.05s/it] 20%|██        | 119/585 [01:00<44:32,  5.74s/it]   21%|██        | 120/585 [01:00<31:47,  4.10s/it] 21%|██        | 121/585 [01:01<22:52,  2.96s/it] 21%|██        | 122/585 [01:01<16:38,  2.16s/it] 21%|██        | 123/585 [01:01<12:17,  1.60s/it] 21%|██        | 124/585 [01:01<09:15,  1.20s/it] 21%|██▏       | 125/585 [01:02<07:07,  1.08it/s] 22%|██▏       | 126/585 [01:02<05:38,  1.36it/s] 22%|██▏       | 127/585 [01:02<04:36,  1.66it/s] 22%|██▏       | 128/585 [01:03<03:52,  1.97it/s] 22%|██▏       | 129/585 [01:03<03:21,  2.26it/s] 22%|██▏       | 130/585 [01:03<03:05,  2.46it/s] 22%|██▏       | 131/585 [01:04<02:48,  2.69it/s] 23%|██▎       | 132/585 [01:04<02:37,  2.88it/s] 23%|██▎       | 133/585 [01:04<02:28,  3.04it/s] 23%|██▎       | 134/585 [01:04<02:23,  3.15it/s] 23%|██▎       | 135/585 [01:05<02:18,  3.24it/s] 23%|██▎       | 136/585 [01:05<02:16,  3.30it/s] 23%|██▎       | 137/585 [01:05<02:13,  3.35it/s] 24%|██▎       | 138/585 [01:06<02:12,  3.38it/s] 24%|██▍       | 139/585 [01:06<02:11,  3.40it/s] 24%|██▍       | 140/585 [01:06<02:10,  3.41it/s] 24%|██▍       | 141/585 [01:06<02:11,  3.39it/s] 24%|██▍       | 142/585 [01:07<02:10,  3.41it/s] 24%|██▍       | 143/585 [01:07<02:09,  3.42it/s] 25%|██▍       | 144/585 [01:07<02:08,  3.43it/s] 25%|██▍       | 145/585 [01:08<02:07,  3.44it/s] 25%|██▍       | 146/585 [01:08<02:07,  3.44it/s] 25%|██▌       | 147/585 [01:08<02:07,  3.44it/s] 25%|██▌       | 148/585 [01:08<02:06,  3.45it/s] 25%|██▌       | 149/585 [01:09<02:06,  3.45it/s] 26%|██▌       | 150/585 [01:09<02:06,  3.45it/s] 26%|██▌       | 151/585 [01:09<02:05,  3.45it/s] 26%|██▌       | 152/585 [01:10<02:06,  3.43it/s] 26%|██▌       | 153/585 [01:10<02:05,  3.44it/s] 26%|██▋       | 154/585 [01:10<02:05,  3.44it/s] 26%|██▋       | 155/585 [01:10<02:04,  3.44it/s] 27%|██▋       | 156/585 [01:11<02:04,  3.45it/s] 27%|██▋       | 157/585 [01:11<02:04,  3.45it/s] 27%|██▋       | 158/585 [01:11<02:03,  3.45it/s] 27%|██▋       | 159/585 [01:12<02:03,  3.45it/s] 27%|██▋       | 160/585 [01:12<02:03,  3.45it/s] 28%|██▊       | 161/585 [01:12<02:02,  3.45it/s] 28%|██▊       | 162/585 [01:13<02:02,  3.45it/s] 28%|██▊       | 163/585 [01:13<02:02,  3.45it/s] 28%|██▊       | 164/585 [01:13<02:01,  3.45it/s] 28%|██▊       | 165/585 [01:13<02:01,  3.45it/s] 28%|██▊       | 166/585 [01:14<02:01,  3.45it/s] 29%|██▊       | 167/585 [01:14<02:01,  3.45it/s] 29%|██▊       | 168/585 [01:14<02:00,  3.45it/s] 29%|██▉       | 169/585 [01:15<02:01,  3.43it/s] 29%|██▉       | 170/585 [01:15<02:00,  3.44it/s] 29%|██▉       | 171/585 [01:15<02:00,  3.44it/s] 29%|██▉       | 172/585 [01:15<01:59,  3.45it/s] 30%|██▉       | 173/585 [01:16<01:59,  3.45it/s] 30%|██▉       | 174/585 [01:16<01:59,  3.45it/s] 30%|██▉       | 175/585 [01:16<01:58,  3.45it/s] 30%|███       | 176/585 [01:17<01:58,  3.45it/s] 30%|███       | 177/585 [01:17<01:58,  3.45it/s] 30%|███       | 178/585 [01:17<01:57,  3.45it/s] 31%|███       | 179/585 [01:17<01:57,  3.45it/s] 31%|███       | 180/585 [01:18<01:58,  3.41it/s] 31%|███       | 181/585 [01:18<01:57,  3.42it/s] 31%|███       | 182/585 [01:18<01:57,  3.43it/s] 31%|███▏      | 183/585 [01:19<01:56,  3.44it/s] 31%|███▏      | 184/585 [01:19<01:56,  3.44it/s] 32%|███▏      | 185/585 [01:19<01:56,  3.45it/s] 32%|███▏      | 186/585 [01:19<01:55,  3.44it/s] 32%|███▏      | 187/585 [01:20<01:55,  3.44it/s] 32%|███▏      | 188/585 [01:20<01:55,  3.44it/s] 32%|███▏      | 189/585 [01:20<01:54,  3.45it/s] 32%|███▏      | 190/585 [01:21<01:54,  3.45it/s] 33%|███▎      | 191/585 [01:21<01:58,  3.32it/s] 33%|███▎      | 192/585 [01:21<01:57,  3.36it/s] 33%|███▎      | 193/585 [01:22<01:55,  3.38it/s] 33%|███▎      | 194/585 [01:22<01:54,  3.40it/s] 33%|███▎      | 195/585 [01:22<01:54,  3.42it/s] 34%|███▎      | 196/585 [01:22<01:53,  3.43it/s] 34%|███▎      | 197/585 [01:23<01:52,  3.44it/s] 34%|███▍      | 198/585 [01:23<01:52,  3.44it/s] 34%|███▍      | 199/585 [01:23<01:52,  3.44it/s] 34%|███▍      | 200/585 [01:24<01:51,  3.45it/s] 34%|███▍      | 201/585 [01:24<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:24<01:57,  3.25it/s] 35%|███▍      | 203/585 [01:25<01:55,  3.30it/s] 35%|███▍      | 204/585 [01:25<01:53,  3.35it/s] 35%|███▌      | 205/585 [01:25<01:52,  3.38it/s] 35%|███▌      | 206/585 [01:25<01:51,  3.40it/s] 35%|███▌      | 207/585 [01:26<01:50,  3.41it/s] 36%|███▌      | 208/585 [01:26<01:50,  3.42it/s] 36%|███▌      | 209/585 [01:26<01:49,  3.43it/s] 36%|███▌      | 210/585 [01:27<01:49,  3.44it/s] 36%|███▌      | 211/585 [01:27<01:48,  3.44it/s] 36%|███▌      | 212/585 [01:27<01:48,  3.44it/s] 36%|███▋      | 213/585 [01:27<01:48,  3.43it/s] 37%|███▋      | 214/585 [01:28<01:47,  3.44it/s] 37%|███▋      | 215/585 [01:28<01:47,  3.44it/s] 37%|███▋      | 216/585 [01:28<01:47,  3.44it/s] 37%|███▋      | 217/585 [01:29<01:46,  3.45it/s] 37%|███▋      | 218/585 [01:29<01:46,  3.45it/s] 37%|███▋      | 219/585 [01:29<01:46,  3.45it/s] 38%|███▊      | 220/585 [01:29<01:45,  3.45it/s] 38%|███▊      | 221/585 [01:30<01:45,  3.44it/s] 38%|███▊      | 222/585 [01:30<01:45,  3.45it/s] 38%|███▊      | 223/585 [01:30<01:44,  3.45it/s] 38%|███▊      | 224/585 [01:31<01:45,  3.43it/s] 38%|███▊      | 225/585 [01:31<01:44,  3.44it/s] 39%|███▊      | 226/585 [01:31<01:44,  3.44it/s] 39%|███▉      | 227/585 [01:31<01:43,  3.44it/s] 39%|███▉      | 228/585 [01:32<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:32<01:43,  3.45it/s] 39%|███▉      | 230/585 [01:32<01:42,  3.45it/s] 39%|███▉      | 231/585 [01:33<01:42,  3.45it/s] 40%|███▉      | 232/585 [01:33<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:33<01:42,  3.45it/s] 40%|████      | 234/585 [01:34<01:41,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 18:46:15,881 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:46:15,881 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 18:46:15,881 >>   Batch size = 8
{'eval_loss': 1.092336654663086, 'eval_runtime': 9.3375, 'eval_samples_per_second': 373.975, 'eval_steps_per_second': 46.8, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.22it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.00it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.15it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.38it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.80it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.69it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.48it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.02it/s][A
 11%|█         | 48/437 [00:00<00:08, 46.89it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.99it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.99it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.06it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.10it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.06it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.12it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.12it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.91it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.85it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.81it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.90it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.05it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.05it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.99it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.11it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.93it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.91it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.87it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.72it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 43.81it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 44.72it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 45.38it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 45.86it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.23it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.50it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.76it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.79it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.59it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.66it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.64it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.83it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.86it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.92it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.01it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.10it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.02it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.90it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.84it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.79it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.74it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.93it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.89it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.03it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.10it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.09it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.92it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.96it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 38.90it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 41.01it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 42.61it/s][A
 69%|██████▉   | 303/437 [00:06<00:03, 43.81it/s][A
 70%|███████   | 308/437 [00:06<00:02, 44.82it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 45.54it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.04it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.40it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.00it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.14it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.38it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.49it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.69it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.91it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.93it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.05it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.02it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.81it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.78it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.77it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.78it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.88it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.89it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.90it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.03it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.10it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.97it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.92it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.78it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.76it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.76it/s][A 40%|████      | 234/585 [01:43<01:41,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:46:25,305 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 18:46:25,336 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:46:31,061 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:46:31,106 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:46:31,128 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:56<40:03,  6.87s/it] 40%|████      | 236/585 [01:56<28:28,  4.90s/it] 41%|████      | 237/585 [01:56<20:22,  3.51s/it] 41%|████      | 238/585 [01:57<14:43,  2.55s/it] 41%|████      | 239/585 [01:57<10:46,  1.87s/it] 41%|████      | 240/585 [01:57<08:01,  1.39s/it] 41%|████      | 241/585 [01:57<06:05,  1.06s/it] 41%|████▏     | 242/585 [01:58<04:44,  1.20it/s] 42%|████▏     | 243/585 [01:58<03:48,  1.50it/s] 42%|████▏     | 244/585 [01:58<03:08,  1.81it/s] 42%|████▏     | 245/585 [01:59<02:41,  2.11it/s] 42%|████▏     | 246/585 [01:59<02:22,  2.39it/s] 42%|████▏     | 247/585 [01:59<02:09,  2.62it/s] 42%|████▏     | 248/585 [01:59<01:59,  2.82it/s] 43%|████▎     | 249/585 [02:00<01:52,  2.99it/s] 43%|████▎     | 250/585 [02:00<01:47,  3.12it/s] 43%|████▎     | 251/585 [02:00<01:43,  3.21it/s] 43%|████▎     | 252/585 [02:01<01:41,  3.29it/s] 43%|████▎     | 253/585 [02:01<01:39,  3.34it/s] 43%|████▎     | 254/585 [02:01<01:38,  3.37it/s] 44%|████▎     | 255/585 [02:02<01:37,  3.40it/s] 44%|████▍     | 256/585 [02:02<01:36,  3.42it/s] 44%|████▍     | 257/585 [02:02<01:35,  3.43it/s] 44%|████▍     | 258/585 [02:02<01:35,  3.42it/s] 44%|████▍     | 259/585 [02:03<01:34,  3.43it/s] 44%|████▍     | 260/585 [02:03<01:34,  3.44it/s] 45%|████▍     | 261/585 [02:03<01:34,  3.44it/s] 45%|████▍     | 262/585 [02:04<01:33,  3.45it/s] 45%|████▍     | 263/585 [02:04<01:33,  3.45it/s] 45%|████▌     | 264/585 [02:04<01:32,  3.45it/s] 45%|████▌     | 265/585 [02:04<01:32,  3.45it/s] 45%|████▌     | 266/585 [02:05<01:32,  3.45it/s] 46%|████▌     | 267/585 [02:05<01:32,  3.45it/s] 46%|████▌     | 268/585 [02:05<01:31,  3.45it/s] 46%|████▌     | 269/585 [02:06<01:31,  3.44it/s] 46%|████▌     | 270/585 [02:06<01:31,  3.45it/s] 46%|████▋     | 271/585 [02:06<01:30,  3.45it/s] 46%|████▋     | 272/585 [02:06<01:30,  3.45it/s] 47%|████▋     | 273/585 [02:07<01:30,  3.45it/s] 47%|████▋     | 274/585 [02:07<01:30,  3.45it/s] 47%|████▋     | 275/585 [02:07<01:29,  3.45it/s] 47%|████▋     | 276/585 [02:08<01:29,  3.45it/s] 47%|████▋     | 277/585 [02:08<01:29,  3.45it/s] 48%|████▊     | 278/585 [02:08<01:28,  3.46it/s] 48%|████▊     | 279/585 [02:08<01:28,  3.46it/s] 48%|████▊     | 280/585 [02:09<01:28,  3.45it/s] 48%|████▊     | 281/585 [02:09<01:28,  3.45it/s] 48%|████▊     | 282/585 [02:09<01:27,  3.45it/s] 48%|████▊     | 283/585 [02:10<01:27,  3.45it/s] 49%|████▊     | 284/585 [02:10<01:27,  3.45it/s] 49%|████▊     | 285/585 [02:10<01:26,  3.45it/s] 49%|████▉     | 286/585 [02:10<01:26,  3.45it/s] 49%|████▉     | 287/585 [02:11<01:26,  3.45it/s] 49%|████▉     | 288/585 [02:11<01:26,  3.45it/s] 49%|████▉     | 289/585 [02:11<01:25,  3.45it/s] 50%|████▉     | 290/585 [02:12<01:25,  3.45it/s] 50%|████▉     | 291/585 [02:12<01:28,  3.31it/s] 50%|████▉     | 292/585 [02:12<01:27,  3.35it/s] 50%|█████     | 293/585 [02:13<01:26,  3.38it/s] 50%|█████     | 294/585 [02:13<01:25,  3.40it/s] 50%|█████     | 295/585 [02:13<01:24,  3.42it/s] 51%|█████     | 296/585 [02:13<01:24,  3.43it/s] 51%|█████     | 297/585 [02:14<01:23,  3.44it/s] 51%|█████     | 298/585 [02:14<01:23,  3.44it/s] 51%|█████     | 299/585 [02:14<01:23,  3.44it/s] 51%|█████▏    | 300/585 [02:15<01:22,  3.45it/s] 51%|█████▏    | 301/585 [02:15<01:22,  3.45it/s] 52%|█████▏    | 302/585 [02:15<01:22,  3.45it/s] 52%|█████▏    | 303/585 [02:15<01:21,  3.45it/s] 52%|█████▏    | 304/585 [02:16<01:21,  3.45it/s] 52%|█████▏    | 305/585 [02:16<01:21,  3.43it/s] 52%|█████▏    | 306/585 [02:16<01:21,  3.43it/s] 52%|█████▏    | 307/585 [02:17<01:20,  3.44it/s] 53%|█████▎    | 308/585 [02:17<01:20,  3.45it/s] 53%|█████▎    | 309/585 [02:17<01:20,  3.45it/s] 53%|█████▎    | 310/585 [02:17<01:19,  3.45it/s] 53%|█████▎    | 311/585 [02:18<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:18<01:19,  3.45it/s] 54%|█████▎    | 313/585 [02:18<01:18,  3.45it/s] 54%|█████▎    | 314/585 [02:19<01:18,  3.45it/s] 54%|█████▍    | 315/585 [02:19<01:18,  3.45it/s] 54%|█████▍    | 316/585 [02:19<01:18,  3.42it/s] 54%|█████▍    | 317/585 [02:20<01:18,  3.43it/s] 54%|█████▍    | 318/585 [02:20<01:17,  3.44it/s] 55%|█████▍    | 319/585 [02:20<01:17,  3.44it/s] 55%|█████▍    | 320/585 [02:20<01:16,  3.45it/s] 55%|█████▍    | 321/585 [02:21<01:16,  3.45it/s] 55%|█████▌    | 322/585 [02:21<01:16,  3.45it/s] 55%|█████▌    | 323/585 [02:21<01:15,  3.45it/s] 55%|█████▌    | 324/585 [02:22<01:15,  3.45it/s] 56%|█████▌    | 325/585 [02:22<01:15,  3.45it/s] 56%|█████▌    | 326/585 [02:22<01:15,  3.45it/s] 56%|█████▌    | 327/585 [02:22<01:15,  3.43it/s] 56%|█████▌    | 328/585 [02:23<01:14,  3.44it/s] 56%|█████▌    | 329/585 [02:23<01:14,  3.44it/s] 56%|█████▋    | 330/585 [02:23<01:13,  3.45it/s] 57%|█████▋    | 331/585 [02:24<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:24<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:24<01:13,  3.45it/s] 57%|█████▋    | 334/585 [02:24<01:12,  3.45it/s] 57%|█████▋    | 335/585 [02:25<01:12,  3.45it/s] 57%|█████▋    | 336/585 [02:25<01:12,  3.45it/s] 58%|█████▊    | 337/585 [02:25<01:11,  3.45it/s] 58%|█████▊    | 338/585 [02:26<01:11,  3.44it/s] 58%|█████▊    | 339/585 [02:26<01:11,  3.44it/s] 58%|█████▊    | 340/585 [02:26<01:11,  3.44it/s] 58%|█████▊    | 341/585 [02:26<01:10,  3.44it/s] 58%|█████▊    | 342/585 [02:27<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:27<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:27<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:28<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:28<01:09,  3.45it/s] 59%|█████▉    | 347/585 [02:28<01:08,  3.45it/s] 59%|█████▉    | 348/585 [02:29<01:08,  3.45it/s] 60%|█████▉    | 349/585 [02:29<01:12,  3.26it/s] 60%|█████▉    | 350/585 [02:29<01:10,  3.31it/s] 60%|██████    | 351/585 [02:29<01:09,  3.36it/s][INFO|trainer.py:2140] 2023-08-28 18:47:11,795 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:47:11,795 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 18:47:11,795 >>   Batch size = 8
{'eval_loss': 1.1070468425750732, 'eval_runtime': 9.412, 'eval_samples_per_second': 371.014, 'eval_steps_per_second': 46.43, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.63it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.93it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.06it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.48it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.89it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.50it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.22it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.90it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.83it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.92it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.82it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.02it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.09it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.90it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.98it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.88it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.75it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.70it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.77it/s][A
 24%|██▎       | 103/437 [00:02<00:08, 37.27it/s][A
 25%|██▍       | 108/437 [00:02<00:08, 39.73it/s][A
 26%|██▌       | 113/437 [00:02<00:07, 41.72it/s][A
 27%|██▋       | 118/437 [00:02<00:07, 43.26it/s][A
 28%|██▊       | 123/437 [00:02<00:07, 44.35it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 45.13it/s][A
 30%|███       | 133/437 [00:02<00:06, 45.79it/s][A
 32%|███▏      | 138/437 [00:03<00:06, 46.18it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 45.90it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 45.95it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.18it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.25it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.79it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.90it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.99it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.08it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.04it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.83it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.66it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.49it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.80it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.83it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.89it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.01it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.99it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.95it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.91it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.63it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 45.17it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 45.68it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.10it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.44it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.67it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.80it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.93it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.85it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.59it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.60it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.51it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.78it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.80it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.82it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.97it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.91it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.81it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.71it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.63it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.55it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.64it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.72it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.86it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.01it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.05it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.00it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.95it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.69it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.79it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.73it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.67it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.83it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.99it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.99it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.99it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.84it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.82it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.81it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.76it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.76it/s][A 60%|██████    | 351/585 [02:39<01:09,  3.36it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:47:21,237 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 18:47:21,249 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:47:24,153 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:47:24,213 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:47:24,227 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:49<24:09,  6.22s/it] 60%|██████    | 353/585 [02:50<17:10,  4.44s/it] 61%|██████    | 354/585 [02:50<12:18,  3.20s/it] 61%|██████    | 355/585 [02:50<08:54,  2.32s/it] 61%|██████    | 356/585 [02:51<06:32,  1.71s/it] 61%|██████    | 357/585 [02:51<04:53,  1.29s/it] 61%|██████    | 358/585 [02:51<03:44,  1.01it/s] 61%|██████▏   | 359/585 [02:52<02:55,  1.29it/s] 62%|██████▏   | 360/585 [02:52<02:21,  1.58it/s] 62%|██████▏   | 361/585 [02:52<01:58,  1.89it/s] 62%|██████▏   | 362/585 [02:52<01:41,  2.19it/s] 62%|██████▏   | 363/585 [02:53<01:30,  2.46it/s] 62%|██████▏   | 364/585 [02:53<01:22,  2.69it/s] 62%|██████▏   | 365/585 [02:53<01:16,  2.88it/s] 63%|██████▎   | 366/585 [02:54<01:12,  3.03it/s] 63%|██████▎   | 367/585 [02:54<01:09,  3.15it/s] 63%|██████▎   | 368/585 [02:54<01:07,  3.24it/s] 63%|██████▎   | 369/585 [02:54<01:05,  3.30it/s] 63%|██████▎   | 370/585 [02:55<01:04,  3.35it/s] 63%|██████▎   | 371/585 [02:55<01:03,  3.38it/s] 64%|██████▎   | 372/585 [02:55<01:02,  3.40it/s] 64%|██████▍   | 373/585 [02:56<01:06,  3.17it/s] 64%|██████▍   | 374/585 [02:56<01:05,  3.24it/s] 64%|██████▍   | 375/585 [02:56<01:04,  3.23it/s] 64%|██████▍   | 376/585 [02:57<01:03,  3.30it/s] 64%|██████▍   | 377/585 [02:57<01:02,  3.35it/s] 65%|██████▍   | 378/585 [02:57<01:01,  3.38it/s] 65%|██████▍   | 379/585 [02:57<01:00,  3.40it/s] 65%|██████▍   | 380/585 [02:58<01:00,  3.42it/s] 65%|██████▌   | 381/585 [02:58<00:59,  3.43it/s] 65%|██████▌   | 382/585 [02:58<00:59,  3.43it/s] 65%|██████▌   | 383/585 [02:59<00:58,  3.44it/s] 66%|██████▌   | 384/585 [02:59<00:58,  3.44it/s] 66%|██████▌   | 385/585 [02:59<00:58,  3.44it/s] 66%|██████▌   | 386/585 [02:59<00:58,  3.40it/s] 66%|██████▌   | 387/585 [03:00<00:57,  3.42it/s] 66%|██████▋   | 388/585 [03:00<00:57,  3.43it/s] 66%|██████▋   | 389/585 [03:00<00:57,  3.43it/s] 67%|██████▋   | 390/585 [03:01<00:56,  3.44it/s] 67%|██████▋   | 391/585 [03:01<00:56,  3.44it/s] 67%|██████▋   | 392/585 [03:01<00:56,  3.44it/s] 67%|██████▋   | 393/585 [03:01<00:55,  3.45it/s] 67%|██████▋   | 394/585 [03:02<00:55,  3.45it/s] 68%|██████▊   | 395/585 [03:02<00:55,  3.45it/s] 68%|██████▊   | 396/585 [03:02<00:54,  3.45it/s] 68%|██████▊   | 397/585 [03:03<00:55,  3.39it/s] 68%|██████▊   | 398/585 [03:03<00:54,  3.41it/s] 68%|██████▊   | 399/585 [03:03<00:54,  3.42it/s] 68%|██████▊   | 400/585 [03:04<00:54,  3.42it/s] 69%|██████▊   | 401/585 [03:04<00:53,  3.43it/s] 69%|██████▊   | 402/585 [03:04<00:53,  3.44it/s] 69%|██████▉   | 403/585 [03:04<00:52,  3.44it/s] 69%|██████▉   | 404/585 [03:05<00:52,  3.44it/s] 69%|██████▉   | 405/585 [03:05<00:52,  3.44it/s] 69%|██████▉   | 406/585 [03:05<00:51,  3.44it/s] 70%|██████▉   | 407/585 [03:06<00:51,  3.45it/s] 70%|██████▉   | 408/585 [03:06<00:52,  3.34it/s] 70%|██████▉   | 409/585 [03:06<00:52,  3.37it/s] 70%|███████   | 410/585 [03:06<00:51,  3.39it/s] 70%|███████   | 411/585 [03:07<00:51,  3.41it/s] 70%|███████   | 412/585 [03:07<00:50,  3.42it/s] 71%|███████   | 413/585 [03:07<00:50,  3.43it/s] 71%|███████   | 414/585 [03:08<00:49,  3.43it/s] 71%|███████   | 415/585 [03:08<00:49,  3.44it/s] 71%|███████   | 416/585 [03:08<00:49,  3.44it/s] 71%|███████▏  | 417/585 [03:08<00:48,  3.44it/s] 71%|███████▏  | 418/585 [03:09<00:48,  3.44it/s] 72%|███████▏  | 419/585 [03:09<00:50,  3.31it/s] 72%|███████▏  | 420/585 [03:09<00:49,  3.35it/s] 72%|███████▏  | 421/585 [03:10<00:48,  3.38it/s] 72%|███████▏  | 422/585 [03:10<00:47,  3.40it/s] 72%|███████▏  | 423/585 [03:10<00:47,  3.41it/s] 72%|███████▏  | 424/585 [03:11<00:47,  3.42it/s] 73%|███████▎  | 425/585 [03:11<00:46,  3.43it/s] 73%|███████▎  | 426/585 [03:11<00:46,  3.43it/s] 73%|███████▎  | 427/585 [03:11<00:45,  3.44it/s] 73%|███████▎  | 428/585 [03:12<00:45,  3.44it/s] 73%|███████▎  | 429/585 [03:12<00:45,  3.44it/s] 74%|███████▎  | 430/585 [03:12<00:45,  3.40it/s] 74%|███████▎  | 431/585 [03:13<00:45,  3.41it/s] 74%|███████▍  | 432/585 [03:13<00:44,  3.42it/s] 74%|███████▍  | 433/585 [03:13<00:44,  3.43it/s] 74%|███████▍  | 434/585 [03:13<00:43,  3.43it/s] 74%|███████▍  | 435/585 [03:14<00:43,  3.44it/s] 75%|███████▍  | 436/585 [03:14<00:43,  3.44it/s] 75%|███████▍  | 437/585 [03:14<00:42,  3.44it/s] 75%|███████▍  | 438/585 [03:15<00:42,  3.44it/s] 75%|███████▌  | 439/585 [03:15<00:42,  3.44it/s] 75%|███████▌  | 440/585 [03:15<00:42,  3.44it/s] 75%|███████▌  | 441/585 [03:15<00:41,  3.45it/s] 76%|███████▌  | 442/585 [03:16<00:41,  3.45it/s] 76%|███████▌  | 443/585 [03:16<00:41,  3.45it/s] 76%|███████▌  | 444/585 [03:16<00:40,  3.44it/s] 76%|███████▌  | 445/585 [03:17<00:40,  3.45it/s] 76%|███████▌  | 446/585 [03:17<00:40,  3.45it/s] 76%|███████▋  | 447/585 [03:17<00:40,  3.45it/s] 77%|███████▋  | 448/585 [03:18<00:43,  3.18it/s] 77%|███████▋  | 449/585 [03:18<00:41,  3.25it/s] 77%|███████▋  | 450/585 [03:18<00:40,  3.31it/s] 77%|███████▋  | 451/585 [03:18<00:40,  3.35it/s] 77%|███████▋  | 452/585 [03:19<00:39,  3.38it/s] 77%|███████▋  | 453/585 [03:19<00:38,  3.40it/s] 78%|███████▊  | 454/585 [03:19<00:38,  3.42it/s] 78%|███████▊  | 455/585 [03:20<00:37,  3.43it/s] 78%|███████▊  | 456/585 [03:20<00:37,  3.44it/s] 78%|███████▊  | 457/585 [03:20<00:37,  3.44it/s] 78%|███████▊  | 458/585 [03:21<00:36,  3.45it/s] 78%|███████▊  | 459/585 [03:21<00:37,  3.40it/s] 79%|███████▊  | 460/585 [03:21<00:36,  3.41it/s] 79%|███████▉  | 461/585 [03:21<00:36,  3.43it/s] 79%|███████▉  | 462/585 [03:22<00:35,  3.43it/s] 79%|███████▉  | 463/585 [03:22<00:35,  3.44it/s] 79%|███████▉  | 464/585 [03:22<00:35,  3.44it/s] 79%|███████▉  | 465/585 [03:23<00:34,  3.44it/s] 80%|███████▉  | 466/585 [03:23<00:34,  3.44it/s] 80%|███████▉  | 467/585 [03:23<00:34,  3.44it/s] 80%|████████  | 468/585 [03:23<00:33,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 18:48:05,771 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:48:05,771 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 18:48:05,771 >>   Batch size = 8
{'eval_loss': 1.1177582740783691, 'eval_runtime': 9.4352, 'eval_samples_per_second': 370.103, 'eval_steps_per_second': 46.316, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.34it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.66it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.44it/s][A
  5%|▌         | 22/437 [00:00<00:08, 47.41it/s][A
  6%|▌         | 27/437 [00:00<00:08, 46.99it/s][A
  7%|▋         | 32/437 [00:00<00:08, 47.06it/s][A
  8%|▊         | 37/437 [00:00<00:08, 47.01it/s][A
 10%|▉         | 42/437 [00:00<00:08, 46.89it/s][A
 11%|█         | 47/437 [00:00<00:08, 46.78it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 46.61it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 46.73it/s][A
 14%|█▍        | 62/437 [00:01<00:07, 46.94it/s][A
 15%|█▌        | 67/437 [00:01<00:07, 46.83it/s][A
 16%|█▋        | 72/437 [00:01<00:07, 46.96it/s][A
 18%|█▊        | 77/437 [00:01<00:07, 47.01it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 47.07it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 46.76it/s][A
 21%|██        | 92/437 [00:01<00:07, 46.80it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 46.57it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 46.77it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 46.90it/s][A
 26%|██▌       | 112/437 [00:02<00:06, 46.93it/s][A
 27%|██▋       | 117/437 [00:02<00:06, 46.92it/s][A
 28%|██▊       | 122/437 [00:02<00:06, 47.07it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 47.02it/s][A
 30%|███       | 132/437 [00:02<00:06, 46.91it/s][A
 31%|███▏      | 137/437 [00:02<00:06, 46.76it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 46.67it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 46.74it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 46.83it/s][A
 36%|███▌      | 157/437 [00:03<00:05, 46.81it/s][A
 37%|███▋      | 162/437 [00:03<00:05, 46.94it/s][A
 38%|███▊      | 167/437 [00:03<00:05, 47.07it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 47.10it/s][A
 41%|████      | 177/437 [00:03<00:05, 46.99it/s][A
 42%|████▏     | 182/437 [00:03<00:05, 47.00it/s][A
 43%|████▎     | 187/437 [00:03<00:05, 46.86it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 46.83it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 46.80it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 46.82it/s][A
 47%|████▋     | 207/437 [00:04<00:04, 46.88it/s][A
 49%|████▊     | 212/437 [00:04<00:04, 46.98it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 47.02it/s][A
 51%|█████     | 222/437 [00:04<00:04, 47.02it/s][A
 52%|█████▏    | 227/437 [00:04<00:04, 46.88it/s][A
 53%|█████▎    | 232/437 [00:04<00:04, 46.84it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 46.78it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 46.79it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 46.84it/s][A
 58%|█████▊    | 252/437 [00:05<00:03, 46.89it/s][A
 59%|█████▉    | 257/437 [00:05<00:03, 46.97it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 47.02it/s][A
 61%|██████    | 267/437 [00:05<00:03, 46.95it/s][A
 62%|██████▏   | 272/437 [00:05<00:03, 46.92it/s][A
 63%|██████▎   | 277/437 [00:05<00:03, 46.87it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 46.79it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 46.86it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 46.87it/s][A
 68%|██████▊   | 297/437 [00:06<00:02, 46.91it/s][A
 69%|██████▉   | 302/437 [00:06<00:02, 47.03it/s][A
 70%|███████   | 307/437 [00:06<00:02, 46.81it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 46.92it/s][A
 73%|███████▎  | 317/437 [00:06<00:02, 46.94it/s][A
 74%|███████▎  | 322/437 [00:06<00:02, 46.88it/s][A
 75%|███████▍  | 327/437 [00:06<00:02, 46.86it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 46.86it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 46.88it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 46.93it/s][A
 79%|███████▉  | 347/437 [00:07<00:01, 47.01it/s][A
 81%|████████  | 352/437 [00:07<00:01, 46.86it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 46.90it/s][A
 83%|████████▎ | 362/437 [00:07<00:01, 46.91it/s][A
 84%|████████▍ | 367/437 [00:07<00:01, 46.82it/s][A
 85%|████████▌ | 372/437 [00:07<00:01, 46.85it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 46.87it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 46.84it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 46.92it/s][A
 90%|████████▉ | 392/437 [00:08<00:00, 47.01it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 46.88it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 47.00it/s][A
 93%|█████████▎| 407/437 [00:08<00:00, 46.98it/s][A
 94%|█████████▍| 412/437 [00:08<00:00, 46.87it/s][A
 95%|█████████▌| 417/437 [00:08<00:00, 46.78it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 46.89it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 45.26it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 45.78it/s][A
100%|██████████| 437/437 [00:09<00:00, 46.18it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.18it/s][A 80%|████████  | 468/585 [03:33<00:33,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:48:15,169 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 18:48:15,242 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:48:19,791 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:48:19,837 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:48:19,875 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:48<14:51,  7.69s/it] 80%|████████  | 470/585 [03:49<10:29,  5.47s/it] 81%|████████  | 471/585 [03:49<07:26,  3.92s/it] 81%|████████  | 472/585 [03:49<05:19,  2.83s/it] 81%|████████  | 473/585 [03:50<03:51,  2.07s/it] 81%|████████  | 474/585 [03:50<02:50,  1.53s/it] 81%|████████  | 475/585 [03:50<02:07,  1.16s/it] 81%|████████▏ | 476/585 [03:50<01:37,  1.11it/s] 82%|████████▏ | 477/585 [03:51<01:17,  1.40it/s] 82%|████████▏ | 478/585 [03:51<01:02,  1.70it/s] 82%|████████▏ | 479/585 [03:51<00:52,  2.01it/s] 82%|████████▏ | 480/585 [03:52<00:45,  2.30it/s] 82%|████████▏ | 481/585 [03:52<00:40,  2.55it/s] 82%|████████▏ | 482/585 [03:52<00:37,  2.77it/s] 83%|████████▎ | 483/585 [03:52<00:34,  2.95it/s] 83%|████████▎ | 484/585 [03:53<00:32,  3.08it/s] 83%|████████▎ | 485/585 [03:53<00:31,  3.19it/s] 83%|████████▎ | 486/585 [03:53<00:30,  3.27it/s] 83%|████████▎ | 487/585 [03:54<00:29,  3.33it/s] 83%|████████▎ | 488/585 [03:54<00:28,  3.37it/s] 84%|████████▎ | 489/585 [03:54<00:28,  3.40it/s] 84%|████████▍ | 490/585 [03:54<00:27,  3.42it/s] 84%|████████▍ | 491/585 [03:55<00:27,  3.43it/s] 84%|████████▍ | 492/585 [03:55<00:27,  3.43it/s] 84%|████████▍ | 493/585 [03:55<00:26,  3.43it/s] 84%|████████▍ | 494/585 [03:56<00:27,  3.26it/s] 85%|████████▍ | 495/585 [03:56<00:27,  3.31it/s] 85%|████████▍ | 496/585 [03:56<00:26,  3.36it/s] 85%|████████▍ | 497/585 [03:57<00:25,  3.39it/s] 85%|████████▌ | 498/585 [03:57<00:25,  3.41it/s] 85%|████████▌ | 499/585 [03:57<00:25,  3.43it/s] 85%|████████▌ | 500/585 [03:57<00:24,  3.44it/s]                                                  85%|████████▌ | 500/585 [03:57<00:24,  3.44it/s] 86%|████████▌ | 501/585 [03:58<00:24,  3.45it/s] 86%|████████▌ | 502/585 [03:58<00:24,  3.45it/s] 86%|████████▌ | 503/585 [03:58<00:26,  3.13it/s] 86%|████████▌ | 504/585 [03:59<00:25,  3.22it/s] 86%|████████▋ | 505/585 [03:59<00:24,  3.29it/s] 86%|████████▋ | 506/585 [03:59<00:23,  3.34it/s] 87%|████████▋ | 507/585 [04:00<00:23,  3.38it/s] 87%|████████▋ | 508/585 [04:00<00:22,  3.40it/s] 87%|████████▋ | 509/585 [04:00<00:22,  3.42it/s] 87%|████████▋ | 510/585 [04:00<00:21,  3.44it/s] 87%|████████▋ | 511/585 [04:01<00:21,  3.45it/s] 88%|████████▊ | 512/585 [04:01<00:21,  3.45it/s] 88%|████████▊ | 513/585 [04:01<00:20,  3.45it/s] 88%|████████▊ | 514/585 [04:02<00:20,  3.45it/s] 88%|████████▊ | 515/585 [04:02<00:20,  3.46it/s] 88%|████████▊ | 516/585 [04:02<00:19,  3.46it/s] 88%|████████▊ | 517/585 [04:02<00:19,  3.46it/s] 89%|████████▊ | 518/585 [04:03<00:19,  3.46it/s] 89%|████████▊ | 519/585 [04:03<00:19,  3.46it/s] 89%|████████▉ | 520/585 [04:03<00:18,  3.46it/s] 89%|████████▉ | 521/585 [04:04<00:18,  3.45it/s] 89%|████████▉ | 522/585 [04:04<00:18,  3.45it/s] 89%|████████▉ | 523/585 [04:04<00:17,  3.45it/s] 90%|████████▉ | 524/585 [04:04<00:17,  3.46it/s] 90%|████████▉ | 525/585 [04:05<00:17,  3.46it/s] 90%|████████▉ | 526/585 [04:05<00:17,  3.46it/s] 90%|█████████ | 527/585 [04:05<00:16,  3.46it/s] 90%|█████████ | 528/585 [04:06<00:16,  3.46it/s] 90%|█████████ | 529/585 [04:06<00:16,  3.46it/s] 91%|█████████ | 530/585 [04:06<00:15,  3.46it/s] 91%|█████████ | 531/585 [04:06<00:15,  3.46it/s] 91%|█████████ | 532/585 [04:07<00:15,  3.41it/s] 91%|█████████ | 533/585 [04:07<00:15,  3.42it/s] 91%|█████████▏| 534/585 [04:07<00:14,  3.43it/s] 91%|█████████▏| 535/585 [04:08<00:14,  3.44it/s] 92%|█████████▏| 536/585 [04:08<00:14,  3.45it/s] 92%|█████████▏| 537/585 [04:08<00:13,  3.45it/s] 92%|█████████▏| 538/585 [04:08<00:13,  3.45it/s] 92%|█████████▏| 539/585 [04:09<00:13,  3.45it/s] 92%|█████████▏| 540/585 [04:09<00:13,  3.45it/s] 92%|█████████▏| 541/585 [04:09<00:12,  3.46it/s] 93%|█████████▎| 542/585 [04:10<00:12,  3.46it/s] 93%|█████████▎| 543/585 [04:10<00:12,  3.43it/s] 93%|█████████▎| 544/585 [04:10<00:11,  3.44it/s] 93%|█████████▎| 545/585 [04:11<00:11,  3.44it/s] 93%|█████████▎| 546/585 [04:11<00:11,  3.44it/s] 94%|█████████▎| 547/585 [04:11<00:11,  3.45it/s] 94%|█████████▎| 548/585 [04:11<00:10,  3.45it/s] 94%|█████████▍| 549/585 [04:12<00:10,  3.45it/s] 94%|█████████▍| 550/585 [04:12<00:10,  3.45it/s] 94%|█████████▍| 551/585 [04:12<00:09,  3.45it/s] 94%|█████████▍| 552/585 [04:13<00:09,  3.45it/s] 95%|█████████▍| 553/585 [04:13<00:09,  3.45it/s] 95%|█████████▍| 554/585 [04:13<00:09,  3.44it/s] 95%|█████████▍| 555/585 [04:13<00:08,  3.44it/s] 95%|█████████▌| 556/585 [04:14<00:08,  3.44it/s] 95%|█████████▌| 557/585 [04:14<00:08,  3.44it/s] 95%|█████████▌| 558/585 [04:14<00:07,  3.44it/s] 96%|█████████▌| 559/585 [04:15<00:07,  3.45it/s] 96%|█████████▌| 560/585 [04:15<00:07,  3.45it/s] 96%|█████████▌| 561/585 [04:15<00:06,  3.45it/s] 96%|█████████▌| 562/585 [04:15<00:06,  3.45it/s] 96%|█████████▌| 563/585 [04:16<00:06,  3.45it/s] 96%|█████████▋| 564/585 [04:16<00:06,  3.46it/s] 97%|█████████▋| 565/585 [04:16<00:06,  2.91it/s] 97%|█████████▋| 566/585 [04:17<00:06,  3.05it/s] 97%|█████████▋| 567/585 [04:17<00:05,  3.16it/s] 97%|█████████▋| 568/585 [04:17<00:05,  3.25it/s] 97%|█████████▋| 569/585 [04:18<00:04,  3.31it/s] 97%|█████████▋| 570/585 [04:18<00:04,  3.36it/s] 98%|█████████▊| 571/585 [04:18<00:04,  3.39it/s] 98%|█████████▊| 572/585 [04:19<00:03,  3.41it/s] 98%|█████████▊| 573/585 [04:19<00:03,  3.42it/s] 98%|█████████▊| 574/585 [04:19<00:03,  3.44it/s] 98%|█████████▊| 575/585 [04:19<00:03,  3.14it/s] 98%|█████████▊| 576/585 [04:20<00:02,  3.22it/s] 99%|█████████▊| 577/585 [04:20<00:02,  3.29it/s] 99%|█████████▉| 578/585 [04:20<00:02,  3.33it/s] 99%|█████████▉| 579/585 [04:21<00:01,  3.36it/s] 99%|█████████▉| 580/585 [04:21<00:01,  3.39it/s] 99%|█████████▉| 581/585 [04:21<00:01,  3.41it/s] 99%|█████████▉| 582/585 [04:22<00:00,  3.43it/s]100%|█████████▉| 583/585 [04:22<00:00,  3.44it/s]100%|█████████▉| 584/585 [04:22<00:00,  3.45it/s]100%|██████████| 585/585 [04:22<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 18:49:04,769 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:49:04,769 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 18:49:04,769 >>   Batch size = 8
{'eval_loss': 1.1254130601882935, 'eval_runtime': 9.3469, 'eval_samples_per_second': 373.599, 'eval_steps_per_second': 46.753, 'epoch': 4.0}
{'loss': 0.517, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.52it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.14it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.25it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.57it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.98it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.92it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.77it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.34it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.00it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.01it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.95it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.09it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.07it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.19it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.22it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.17it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.08it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.96it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.95it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.98it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.00it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.05it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.12it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.15it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.21it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.12it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.01it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 42.71it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 44.06it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 45.01it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 45.64it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.10it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.42it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.68it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.83it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.59it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.62it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.72it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.86it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.06it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.01it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.13it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.17it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.06it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.88it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.78it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.77it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.89it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.06it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.00it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.11it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.17it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.09it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.04it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.85it/s][A
 65%|██████▍   | 283/437 [00:06<00:04, 36.20it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 38.98it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 41.10it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 42.80it/s][A
 69%|██████▉   | 303/437 [00:06<00:03, 44.06it/s][A
 70%|███████   | 308/437 [00:06<00:02, 44.94it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 45.59it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.11it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 45.89it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.12it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.44it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.67it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.81it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.93it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.01it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.12it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.09it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.91it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.78it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.80it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.94it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.01it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.97it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 45.56it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.13it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.38it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.61it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.67it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.67it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.76it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.94it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.94it/s][A100%|██████████| 585/585 [04:32<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:49:15,285 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 18:49:16,255 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:49:26,513 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:49:27,090 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:49:27,404 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:49:40,458 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:49:40,466 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117 (score: 1.092336654663086).
                                                 100%|██████████| 585/585 [05:02<00:00,  3.45it/s]100%|██████████| 585/585 [05:02<00:00,  1.93it/s]
[INFO|trainer.py:1894] 2023-08-28 18:49:44,351 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 18:49:44,362 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:49:49,294 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:49:49,311 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:49:49,318 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:49:49,489 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:49,489 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:49,489 >>   train_loss               =     0.5132
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:49,489 >>   train_runtime            = 0:05:02.54
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:49,489 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:49,489 >>   train_samples_per_second =    123.932
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:49,489 >>   train_steps_per_second   =      1.934
{'eval_loss': 1.1336555480957031, 'eval_runtime': 9.4412, 'eval_samples_per_second': 369.868, 'eval_steps_per_second': 46.286, 'epoch': 5.0}
{'train_runtime': 302.5452, 'train_samples_per_second': 123.932, 'train_steps_per_second': 1.934, 'train_loss': 0.5132412837101863, 'epoch': 5.0}
08/28/2023 18:49:49 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:49:49,519 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:49:49,519 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 18:49:49,519 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 59.48it/s]  3%|▎         | 12/437 [00:00<00:08, 52.08it/s]  4%|▍         | 18/437 [00:00<00:08, 50.00it/s]  5%|▌         | 24/437 [00:00<00:08, 49.17it/s]  7%|▋         | 29/437 [00:00<00:08, 48.76it/s]  8%|▊         | 34/437 [00:00<00:08, 48.36it/s]  9%|▉         | 39/437 [00:00<00:08, 48.03it/s] 10%|█         | 44/437 [00:00<00:08, 47.87it/s] 11%|█         | 49/437 [00:01<00:08, 47.66it/s] 12%|█▏        | 54/437 [00:01<00:08, 47.58it/s] 14%|█▎        | 59/437 [00:01<00:07, 47.45it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.47it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.52it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.55it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.65it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.72it/s] 20%|██        | 89/437 [00:01<00:07, 47.63it/s] 22%|██▏       | 94/437 [00:01<00:07, 47.55it/s] 23%|██▎       | 99/437 [00:02<00:07, 47.51it/s] 24%|██▍       | 104/437 [00:02<00:07, 47.53it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.49it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.34it/s] 27%|██▋       | 119/437 [00:02<00:06, 47.33it/s] 28%|██▊       | 124/437 [00:02<00:06, 47.42it/s] 30%|██▉       | 129/437 [00:02<00:06, 47.45it/s] 31%|███       | 134/437 [00:02<00:06, 47.49it/s] 32%|███▏      | 139/437 [00:02<00:06, 45.54it/s] 33%|███▎      | 144/437 [00:03<00:06, 46.22it/s] 34%|███▍      | 149/437 [00:03<00:06, 46.68it/s] 35%|███▌      | 154/437 [00:03<00:06, 46.92it/s] 36%|███▋      | 159/437 [00:03<00:05, 47.11it/s] 38%|███▊      | 164/437 [00:03<00:05, 47.24it/s] 39%|███▊      | 169/437 [00:03<00:05, 47.29it/s] 40%|███▉      | 174/437 [00:03<00:05, 47.40it/s] 41%|████      | 179/437 [00:03<00:05, 47.35it/s] 42%|████▏     | 184/437 [00:03<00:05, 47.30it/s] 43%|████▎     | 189/437 [00:03<00:05, 47.34it/s] 44%|████▍     | 194/437 [00:04<00:05, 47.49it/s] 46%|████▌     | 199/437 [00:04<00:04, 47.62it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.66it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.60it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.62it/s] 50%|█████     | 219/437 [00:04<00:04, 47.57it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.53it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.42it/s] 54%|█████▎    | 234/437 [00:04<00:04, 47.41it/s] 55%|█████▍    | 239/437 [00:05<00:04, 47.34it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.42it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.56it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.61it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.70it/s] 60%|██████    | 264/437 [00:05<00:03, 47.72it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.58it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.46it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.43it/s] 65%|██████▍   | 284/437 [00:05<00:03, 47.34it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.40it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.48it/s] 68%|██████▊   | 299/437 [00:06<00:02, 47.54it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.63it/s] 71%|███████   | 309/437 [00:06<00:02, 47.71it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.68it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.71it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.55it/s] 75%|███████▌  | 329/437 [00:06<00:02, 47.49it/s] 76%|███████▋  | 334/437 [00:07<00:02, 47.42it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.41it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.53it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.60it/s] 81%|████████  | 354/437 [00:07<00:01, 47.57it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.58it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.56it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.52it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.48it/s] 87%|████████▋ | 379/437 [00:07<00:01, 47.33it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.23it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.30it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.35it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.48it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.60it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.65it/s] 95%|█████████▍| 414/437 [00:08<00:00, 47.66it/s] 96%|█████████▌| 419/437 [00:08<00:00, 47.55it/s] 97%|█████████▋| 424/437 [00:08<00:00, 47.47it/s] 98%|█████████▊| 429/437 [00:09<00:00, 47.43it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.35it/s]100%|██████████| 437/437 [00:09<00:00, 47.56it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:49:58,729 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:58,729 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:58,729 >>   eval_loss               =     1.0923
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:58,729 >>   eval_runtime            = 0:00:09.21
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:58,729 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:58,729 >>   eval_samples_per_second =    379.139
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:58,729 >>   eval_steps_per_second   =     47.447
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:49:58,729 >>   perplexity              =     2.9812
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:50:07,250 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:50:07,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:50:07,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:50:07,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:50:07,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:50:09,622 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:50:09,623 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:50:10,324 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:50:11,531 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:50:11,532 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:50:15,789 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:50:15,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:50:15,792 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:50:15,792 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:50:15,792 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:50:16,872 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:50:16,873 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:50:17,573 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:50:18,252 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:50:18,414 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.38it/s]Extractor Predicting: 6it [00:04,  1.41it/s]Extractor Predicting: 7it [00:04,  1.41it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:07,  1.39it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:09,  1.43it/s]Extractor Predicting: 14it [00:09,  1.43it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:11,  1.45it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.43it/s]Extractor Predicting: 19it [00:13,  1.49it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:15,  1.49it/s]Extractor Predicting: 23it [00:15,  1.51it/s]Extractor Predicting: 24it [00:16,  1.52it/s]Extractor Predicting: 25it [00:17,  1.52it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:18,  1.51it/s]Extractor Predicting: 28it [00:19,  1.50it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:20,  1.46it/s]Extractor Predicting: 31it [00:21,  1.39it/s]Extractor Predicting: 32it [00:22,  1.43it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:23,  1.44it/s]Extractor Predicting: 35it [00:24,  1.45it/s]Extractor Predicting: 36it [00:24,  1.45it/s]Extractor Predicting: 37it [00:25,  1.46it/s]Extractor Predicting: 38it [00:26,  1.46it/s]Extractor Predicting: 39it [00:26,  1.47it/s]Extractor Predicting: 40it [00:27,  1.48it/s]Extractor Predicting: 41it [00:28,  1.48it/s]Extractor Predicting: 42it [00:28,  1.46it/s]Extractor Predicting: 43it [00:29,  1.48it/s]Extractor Predicting: 44it [00:30,  1.48it/s]Extractor Predicting: 45it [00:30,  1.45it/s]Extractor Predicting: 46it [00:31,  1.45it/s]Extractor Predicting: 47it [00:32,  1.46it/s]Extractor Predicting: 48it [00:32,  1.46it/s]Extractor Predicting: 49it [00:33,  1.47it/s]Extractor Predicting: 50it [00:34,  1.48it/s]Extractor Predicting: 51it [00:35,  1.45it/s]Extractor Predicting: 52it [00:35,  1.44it/s]Extractor Predicting: 53it [00:36,  1.45it/s]Extractor Predicting: 54it [00:37,  1.45it/s]Extractor Predicting: 55it [00:37,  1.43it/s]Extractor Predicting: 56it [00:38,  1.45it/s]Extractor Predicting: 57it [00:39,  1.45it/s]Extractor Predicting: 58it [00:39,  1.48it/s]Extractor Predicting: 59it [00:40,  1.47it/s]Extractor Predicting: 60it [00:41,  1.45it/s]Extractor Predicting: 61it [00:42,  1.36it/s]Extractor Predicting: 62it [00:42,  1.38it/s]Extractor Predicting: 63it [00:43,  1.42it/s]Extractor Predicting: 64it [00:44,  1.39it/s]Extractor Predicting: 65it [00:44,  1.37it/s]Extractor Predicting: 66it [00:45,  1.36it/s]Extractor Predicting: 67it [00:46,  1.38it/s]Extractor Predicting: 68it [00:47,  1.38it/s]Extractor Predicting: 69it [00:47,  1.39it/s]Extractor Predicting: 70it [00:48,  1.35it/s]Extractor Predicting: 71it [00:49,  1.36it/s]Extractor Predicting: 72it [00:50,  1.35it/s]Extractor Predicting: 73it [00:50,  1.38it/s]Extractor Predicting: 74it [00:51,  1.36it/s]Extractor Predicting: 75it [00:52,  1.34it/s]Extractor Predicting: 76it [00:53,  1.36it/s]Extractor Predicting: 77it [00:53,  1.36it/s]Extractor Predicting: 78it [00:54,  1.39it/s]Extractor Predicting: 79it [00:55,  1.37it/s]Extractor Predicting: 80it [00:56,  1.30it/s]Extractor Predicting: 81it [00:56,  1.32it/s]Extractor Predicting: 82it [00:57,  1.37it/s]Extractor Predicting: 83it [00:58,  1.39it/s]Extractor Predicting: 84it [00:58,  1.42it/s]Extractor Predicting: 85it [00:59,  1.23it/s]Extractor Predicting: 86it [01:00,  1.26it/s]Extractor Predicting: 87it [01:01,  1.31it/s]Extractor Predicting: 88it [01:01,  1.37it/s]Extractor Predicting: 89it [01:02,  1.38it/s]Extractor Predicting: 90it [01:03,  1.41it/s]Extractor Predicting: 91it [01:03,  1.48it/s]Extractor Predicting: 92it [01:04,  1.55it/s]Extractor Predicting: 93it [01:05,  1.60it/s]Extractor Predicting: 94it [01:05,  1.63it/s]Extractor Predicting: 95it [01:06,  1.58it/s]Extractor Predicting: 96it [01:06,  1.62it/s]Extractor Predicting: 97it [01:07,  1.57it/s]Extractor Predicting: 98it [01:08,  1.57it/s]Extractor Predicting: 99it [01:08,  1.63it/s]Extractor Predicting: 100it [01:09,  1.63it/s]Extractor Predicting: 101it [01:10,  1.60it/s]Extractor Predicting: 102it [01:10,  1.55it/s]Extractor Predicting: 103it [01:11,  1.55it/s]Extractor Predicting: 104it [01:12,  1.52it/s]Extractor Predicting: 105it [01:12,  1.52it/s]Extractor Predicting: 106it [01:13,  1.54it/s]Extractor Predicting: 107it [01:14,  1.52it/s]Extractor Predicting: 108it [01:14,  1.55it/s]Extractor Predicting: 109it [01:15,  1.57it/s]Extractor Predicting: 110it [01:15,  1.59it/s]Extractor Predicting: 111it [01:16,  1.60it/s]Extractor Predicting: 112it [01:17,  1.60it/s]Extractor Predicting: 113it [01:17,  1.63it/s]Extractor Predicting: 114it [01:18,  1.57it/s]Extractor Predicting: 115it [01:19,  1.60it/s]Extractor Predicting: 116it [01:19,  1.44it/s]Extractor Predicting: 117it [01:20,  1.44it/s]Extractor Predicting: 118it [01:21,  1.46it/s]Extractor Predicting: 119it [01:22,  1.39it/s]Extractor Predicting: 120it [01:22,  1.39it/s]Extractor Predicting: 121it [01:23,  1.41it/s]Extractor Predicting: 122it [01:24,  1.43it/s]Extractor Predicting: 123it [01:24,  1.42it/s]Extractor Predicting: 124it [01:25,  1.41it/s]Extractor Predicting: 125it [01:26,  1.42it/s]Extractor Predicting: 126it [01:26,  1.40it/s]Extractor Predicting: 127it [01:27,  1.40it/s]Extractor Predicting: 128it [01:28,  1.41it/s]Extractor Predicting: 129it [01:29,  1.43it/s]Extractor Predicting: 130it [01:29,  1.46it/s]Extractor Predicting: 131it [01:30,  1.43it/s]Extractor Predicting: 132it [01:31,  1.42it/s]Extractor Predicting: 133it [01:31,  1.42it/s]Extractor Predicting: 134it [01:32,  1.42it/s]Extractor Predicting: 135it [01:33,  1.43it/s]Extractor Predicting: 136it [01:33,  1.42it/s]Extractor Predicting: 137it [01:34,  1.43it/s]Extractor Predicting: 138it [01:35,  1.45it/s]Extractor Predicting: 139it [01:36,  1.45it/s]Extractor Predicting: 140it [01:36,  1.47it/s]Extractor Predicting: 141it [01:37,  1.44it/s]Extractor Predicting: 142it [01:38,  1.44it/s]Extractor Predicting: 143it [01:38,  1.45it/s]Extractor Predicting: 144it [01:39,  1.47it/s]Extractor Predicting: 144it [01:39,  1.45it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:52:16,968 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:52:17,065 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:52:17,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:52:17,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:52:17,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:52:18,044 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:52:18,045 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:52:18,825 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:52:19,889 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:52:19,889 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:52:23,345 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:52:23,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:52:23,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:52:23,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:52:23,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:52:24,399 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:52:24,400 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:52:25,315 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:52:25,878 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:52:25,878 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5327783558792925,
  "recall": 0.1466208476517755,
  "score": 0.2299573321356389,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.45it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.46it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.51it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.52it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.43it/s]Extractor Predicting: 23it [00:15,  1.45it/s]Extractor Predicting: 24it [00:16,  1.48it/s]Extractor Predicting: 25it [00:16,  1.50it/s]Extractor Predicting: 26it [00:17,  1.45it/s]Extractor Predicting: 27it [00:18,  1.44it/s]Extractor Predicting: 28it [00:19,  1.44it/s]Extractor Predicting: 29it [00:19,  1.43it/s]Extractor Predicting: 30it [00:20,  1.41it/s]Extractor Predicting: 31it [00:21,  1.39it/s]Extractor Predicting: 32it [00:21,  1.40it/s]Extractor Predicting: 33it [00:22,  1.44it/s]Extractor Predicting: 34it [00:23,  1.45it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.45it/s]Extractor Predicting: 37it [00:25,  1.51it/s]Extractor Predicting: 38it [00:25,  1.52it/s]Extractor Predicting: 39it [00:26,  1.52it/s]Extractor Predicting: 40it [00:27,  1.52it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:29,  1.51it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:30,  1.49it/s]Extractor Predicting: 46it [00:31,  1.49it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:32,  1.51it/s]Extractor Predicting: 49it [00:33,  1.50it/s]Extractor Predicting: 50it [00:33,  1.50it/s]Extractor Predicting: 51it [00:34,  1.51it/s]Extractor Predicting: 52it [00:35,  1.51it/s]Extractor Predicting: 53it [00:35,  1.51it/s]Extractor Predicting: 54it [00:36,  1.51it/s]Extractor Predicting: 55it [00:37,  1.49it/s]Extractor Predicting: 56it [00:37,  1.50it/s]Extractor Predicting: 57it [00:38,  1.53it/s]Extractor Predicting: 58it [00:39,  1.56it/s]Extractor Predicting: 59it [00:39,  1.59it/s]Extractor Predicting: 60it [00:40,  1.57it/s]Extractor Predicting: 61it [00:41,  1.54it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:42,  1.55it/s]Extractor Predicting: 64it [00:43,  1.51it/s]Extractor Predicting: 65it [00:43,  1.51it/s]Extractor Predicting: 66it [00:44,  1.51it/s]Extractor Predicting: 67it [00:45,  1.34it/s]Extractor Predicting: 68it [00:45,  1.44it/s]Extractor Predicting: 69it [00:46,  1.45it/s]Extractor Predicting: 70it [00:47,  1.47it/s]Extractor Predicting: 71it [00:47,  1.49it/s]Extractor Predicting: 72it [00:48,  1.44it/s]Extractor Predicting: 73it [00:49,  1.44it/s]Extractor Predicting: 74it [00:49,  1.45it/s]Extractor Predicting: 75it [00:50,  1.47it/s]Extractor Predicting: 76it [00:51,  1.49it/s]Extractor Predicting: 77it [00:52,  1.22it/s]Extractor Predicting: 78it [00:53,  1.30it/s]Extractor Predicting: 79it [00:53,  1.36it/s]Extractor Predicting: 80it [00:54,  1.40it/s]Extractor Predicting: 81it [00:55,  1.19it/s]Extractor Predicting: 82it [00:56,  1.26it/s]Extractor Predicting: 83it [00:56,  1.33it/s]Extractor Predicting: 84it [00:57,  1.38it/s]Extractor Predicting: 85it [00:58,  1.40it/s]Extractor Predicting: 86it [00:58,  1.41it/s]Extractor Predicting: 87it [00:59,  1.44it/s]Extractor Predicting: 88it [01:00,  1.45it/s]Extractor Predicting: 89it [01:00,  1.49it/s]Extractor Predicting: 90it [01:01,  1.43it/s]Extractor Predicting: 91it [01:02,  1.45it/s]Extractor Predicting: 92it [01:03,  1.44it/s]Extractor Predicting: 93it [01:03,  1.48it/s]Extractor Predicting: 94it [01:04,  1.46it/s]Extractor Predicting: 95it [01:05,  1.46it/s]Extractor Predicting: 96it [01:05,  1.49it/s]Extractor Predicting: 97it [01:06,  1.37it/s]Extractor Predicting: 98it [01:07,  1.40it/s]Extractor Predicting: 99it [01:07,  1.44it/s]Extractor Predicting: 100it [01:08,  1.47it/s]Extractor Predicting: 101it [01:09,  1.49it/s]Extractor Predicting: 102it [01:09,  1.50it/s]Extractor Predicting: 103it [01:10,  1.46it/s]Extractor Predicting: 104it [01:11,  1.45it/s]Extractor Predicting: 105it [01:11,  1.45it/s]Extractor Predicting: 106it [01:12,  1.45it/s]Extractor Predicting: 107it [01:13,  1.45it/s]Extractor Predicting: 108it [01:14,  1.47it/s]Extractor Predicting: 109it [01:14,  1.46it/s]Extractor Predicting: 110it [01:15,  1.46it/s]Extractor Predicting: 111it [01:16,  1.45it/s]Extractor Predicting: 112it [01:16,  1.43it/s]Extractor Predicting: 113it [01:17,  1.43it/s]Extractor Predicting: 114it [01:18,  1.43it/s]Extractor Predicting: 115it [01:18,  1.42it/s]Extractor Predicting: 116it [01:19,  1.47it/s]Extractor Predicting: 117it [01:20,  1.48it/s]Extractor Predicting: 118it [01:20,  1.49it/s]Extractor Predicting: 119it [01:21,  1.49it/s]Extractor Predicting: 120it [01:22,  1.35it/s]Extractor Predicting: 121it [01:23,  1.40it/s]Extractor Predicting: 122it [01:23,  1.45it/s]Extractor Predicting: 123it [01:24,  1.49it/s]Extractor Predicting: 124it [01:25,  1.52it/s]Extractor Predicting: 125it [01:25,  1.51it/s]Extractor Predicting: 126it [01:26,  1.51it/s]Extractor Predicting: 127it [01:26,  1.54it/s]Extractor Predicting: 128it [01:27,  1.52it/s]Extractor Predicting: 129it [01:28,  1.50it/s]Extractor Predicting: 130it [01:29,  1.47it/s]Extractor Predicting: 131it [01:29,  1.51it/s]Extractor Predicting: 132it [01:30,  1.51it/s]Extractor Predicting: 133it [01:30,  1.53it/s]Extractor Predicting: 134it [01:31,  1.54it/s]Extractor Predicting: 135it [01:32,  1.54it/s]Extractor Predicting: 136it [01:32,  1.54it/s]Extractor Predicting: 137it [01:33,  1.54it/s]Extractor Predicting: 138it [01:34,  1.51it/s]Extractor Predicting: 139it [01:34,  1.53it/s]Extractor Predicting: 140it [01:35,  1.57it/s]Extractor Predicting: 141it [01:36,  1.58it/s]Extractor Predicting: 142it [01:36,  1.55it/s]Extractor Predicting: 143it [01:37,  1.54it/s]Extractor Predicting: 144it [01:38,  1.54it/s]Extractor Predicting: 145it [01:38,  1.54it/s]Extractor Predicting: 146it [01:39,  1.56it/s]Extractor Predicting: 147it [01:39,  1.58it/s]Extractor Predicting: 148it [01:40,  1.52it/s]Extractor Predicting: 149it [01:41,  1.54it/s]Extractor Predicting: 150it [01:41,  1.53it/s]Extractor Predicting: 151it [01:42,  1.53it/s]Extractor Predicting: 152it [01:43,  1.53it/s]Extractor Predicting: 153it [01:43,  1.51it/s]Extractor Predicting: 154it [01:44,  1.54it/s]Extractor Predicting: 155it [01:45,  1.55it/s]Extractor Predicting: 156it [01:45,  1.57it/s]Extractor Predicting: 157it [01:46,  1.57it/s]Extractor Predicting: 158it [01:47,  1.62it/s]Extractor Predicting: 159it [01:47,  1.59it/s]Extractor Predicting: 160it [01:48,  1.55it/s]Extractor Predicting: 161it [01:49,  1.53it/s]Extractor Predicting: 162it [01:49,  1.51it/s]Extractor Predicting: 163it [01:50,  1.53it/s]Extractor Predicting: 164it [01:51,  1.54it/s]Extractor Predicting: 165it [01:51,  1.56it/s]Extractor Predicting: 166it [01:52,  1.56it/s]Extractor Predicting: 167it [01:52,  1.56it/s]Extractor Predicting: 168it [01:53,  1.52it/s]Extractor Predicting: 169it [01:54,  1.52it/s]Extractor Predicting: 170it [01:54,  1.52it/s]Extractor Predicting: 171it [01:55,  1.52it/s]Extractor Predicting: 172it [01:56,  1.53it/s]Extractor Predicting: 173it [01:56,  1.51it/s]Extractor Predicting: 174it [01:57,  1.51it/s]Extractor Predicting: 175it [01:58,  1.49it/s]Extractor Predicting: 176it [01:59,  1.46it/s]Extractor Predicting: 177it [01:59,  1.50it/s]Extractor Predicting: 178it [02:00,  1.50it/s]Extractor Predicting: 179it [02:00,  1.52it/s]Extractor Predicting: 180it [02:01,  1.55it/s]Extractor Predicting: 181it [02:02,  1.51it/s]Extractor Predicting: 182it [02:02,  1.52it/s]Extractor Predicting: 183it [02:03,  1.51it/s]Extractor Predicting: 184it [02:04,  1.50it/s]Extractor Predicting: 185it [02:04,  1.49it/s]Extractor Predicting: 186it [02:05,  1.48it/s]Extractor Predicting: 187it [02:06,  1.49it/s]Extractor Predicting: 188it [02:06,  1.47it/s]Extractor Predicting: 189it [02:07,  1.47it/s]Extractor Predicting: 190it [02:08,  1.48it/s]Extractor Predicting: 191it [02:08,  1.49it/s]Extractor Predicting: 192it [02:09,  1.53it/s]Extractor Predicting: 193it [02:10,  1.34it/s]Extractor Predicting: 194it [02:11,  1.36it/s]Extractor Predicting: 195it [02:11,  1.42it/s]Extractor Predicting: 196it [02:12,  1.36it/s]Extractor Predicting: 197it [02:13,  1.40it/s]Extractor Predicting: 198it [02:14,  1.41it/s]Extractor Predicting: 199it [02:14,  1.44it/s]Extractor Predicting: 200it [02:15,  1.50it/s]Extractor Predicting: 201it [02:15,  1.53it/s]Extractor Predicting: 202it [02:16,  1.54it/s]Extractor Predicting: 203it [02:17,  1.51it/s]Extractor Predicting: 204it [02:17,  1.50it/s]Extractor Predicting: 205it [02:18,  1.51it/s]Extractor Predicting: 206it [02:19,  1.50it/s]Extractor Predicting: 207it [02:19,  1.51it/s]Extractor Predicting: 208it [02:20,  1.48it/s]Extractor Predicting: 209it [02:21,  1.49it/s]Extractor Predicting: 210it [02:21,  1.49it/s]Extractor Predicting: 211it [02:22,  1.49it/s]Extractor Predicting: 212it [02:23,  1.51it/s]Extractor Predicting: 213it [02:23,  1.50it/s]Extractor Predicting: 214it [02:24,  1.48it/s]Extractor Predicting: 215it [02:25,  1.48it/s]Extractor Predicting: 216it [02:25,  1.51it/s]Extractor Predicting: 217it [02:26,  1.52it/s]Extractor Predicting: 218it [02:27,  1.51it/s]Extractor Predicting: 219it [02:27,  1.49it/s]Extractor Predicting: 220it [02:28,  1.51it/s]Extractor Predicting: 221it [02:29,  1.51it/s]Extractor Predicting: 222it [02:29,  1.51it/s]Extractor Predicting: 223it [02:30,  1.52it/s]Extractor Predicting: 224it [02:31,  1.52it/s]Extractor Predicting: 225it [02:31,  1.51it/s]Extractor Predicting: 226it [02:32,  1.49it/s]Extractor Predicting: 227it [02:33,  1.51it/s]Extractor Predicting: 228it [02:33,  1.51it/s]Extractor Predicting: 229it [02:34,  1.52it/s]Extractor Predicting: 230it [02:35,  1.53it/s]Extractor Predicting: 231it [02:35,  1.52it/s]Extractor Predicting: 232it [02:36,  1.52it/s]Extractor Predicting: 233it [02:37,  1.51it/s]Extractor Predicting: 234it [02:37,  1.53it/s]Extractor Predicting: 235it [02:38,  1.54it/s]Extractor Predicting: 236it [02:39,  1.54it/s]Extractor Predicting: 237it [02:39,  1.52it/s]Extractor Predicting: 238it [02:40,  1.52it/s]Extractor Predicting: 239it [02:41,  1.50it/s]Extractor Predicting: 240it [02:41,  1.53it/s]Extractor Predicting: 241it [02:42,  1.54it/s]Extractor Predicting: 242it [02:43,  1.56it/s]Extractor Predicting: 243it [02:43,  1.55it/s]Extractor Predicting: 244it [02:44,  1.54it/s]Extractor Predicting: 245it [02:45,  1.53it/s]Extractor Predicting: 246it [02:45,  1.53it/s]Extractor Predicting: 247it [02:46,  1.54it/s]Extractor Predicting: 248it [02:46,  1.54it/s]Extractor Predicting: 249it [02:47,  1.54it/s]Extractor Predicting: 250it [02:48,  1.56it/s]Extractor Predicting: 251it [02:48,  1.52it/s]Extractor Predicting: 252it [02:49,  1.51it/s]Extractor Predicting: 253it [02:50,  1.57it/s]Extractor Predicting: 254it [02:50,  1.55it/s]Extractor Predicting: 255it [02:51,  1.52it/s]Extractor Predicting: 256it [02:52,  1.52it/s]Extractor Predicting: 257it [02:52,  1.53it/s]Extractor Predicting: 258it [02:53,  1.56it/s]Extractor Predicting: 259it [02:54,  1.56it/s]Extractor Predicting: 260it [02:54,  1.57it/s]Extractor Predicting: 261it [02:55,  1.54it/s]Extractor Predicting: 262it [02:56,  1.55it/s]Extractor Predicting: 263it [02:56,  1.55it/s]Extractor Predicting: 264it [02:57,  1.56it/s]Extractor Predicting: 265it [02:57,  1.58it/s]Extractor Predicting: 266it [02:58,  1.61it/s]Extractor Predicting: 267it [02:59,  1.56it/s]Extractor Predicting: 268it [02:59,  1.56it/s]Extractor Predicting: 269it [03:00,  1.57it/s]Extractor Predicting: 270it [03:01,  1.57it/s]Extractor Predicting: 271it [03:01,  1.59it/s]Extractor Predicting: 272it [03:02,  1.62it/s]Extractor Predicting: 273it [03:02,  1.58it/s]Extractor Predicting: 274it [03:03,  1.58it/s]Extractor Predicting: 275it [03:04,  1.56it/s]Extractor Predicting: 276it [03:04,  1.56it/s]Extractor Predicting: 277it [03:05,  1.56it/s]Extractor Predicting: 278it [03:06,  1.56it/s]Extractor Predicting: 279it [03:06,  1.53it/s]Extractor Predicting: 280it [03:07,  1.54it/s]Extractor Predicting: 281it [03:08,  1.52it/s]Extractor Predicting: 281it [03:08,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:43,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:43,774 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:43,774 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:43,774 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:43,774 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:55:44,508 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:55:44,509 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:55:45,084 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:55:46,123 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:55:46,229 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:49,230 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:49,318 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:49,318 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:49,318 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:49,318 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:55:49,976 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:55:49,977 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:55:50,552 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:55:50,695 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:55:50,695 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.45251857968620973,
  "recall": 0.16253892925997332,
  "score": 0.2391707583196945,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.38it/s]Extractor Predicting: 4it [00:02,  1.38it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:03,  1.82it/s]Extractor Predicting: 6it [00:03,  1.56it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:55:55,547 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:55:55,548 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:55:55,581 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:55:55,583 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:55:55,590 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:56:02,602 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:56:02,606 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:56:02,619 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:56:02,620 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:56:02,629 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:56:02,639 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:56:02,639 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:56:02,639 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:56:02,639 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:56:02,639 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:56:02,639 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.19230769230769232,
  "recall": 0.038910505836575876,
  "score": 0.06472491909385113,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:56:02,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:04,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:04,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:06,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:07,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:08,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:08,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:09,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:11,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:11,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:13,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:13,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:14,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:16,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:16,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:17,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:18,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:19,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:20,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:21,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:19<04:35, 19.67s/it][WARNING|generation_utils.py:914] 2023-08-28 18:56:22,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:23,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:24,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:25,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:25,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:26,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:27,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:28,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:29,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:29,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:30,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:31,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:32,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:33,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:33,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:34,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:35,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:36,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:37,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:38,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:35<03:49, 17.68s/it][WARNING|generation_utils.py:914] 2023-08-28 18:56:38,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:39,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:40,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:41,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:41,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:42,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:43,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:44,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:45,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:45,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:46,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:47,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:48,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:48,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:49,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:50,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:51,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:51,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:52,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:53,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:54,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:52<03:23, 16.95s/it][WARNING|generation_utils.py:914] 2023-08-28 18:56:54,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:55,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:56,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:56,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:57,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:58,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:58,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:56:59,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:00,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:00,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:01,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:02,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:02,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:03,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:04,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:04,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:05,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:06,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:07,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:07,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:08,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:06<02:53, 15.81s/it][WARNING|generation_utils.py:914] 2023-08-28 18:57:09,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:09,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:10,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:11,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:11,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:12,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:13,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:13,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:15,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:15,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:16,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:17,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:17,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:18,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:19,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:20,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:20,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:21,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:22,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:23,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:21<02:36, 15.65s/it][WARNING|generation_utils.py:914] 2023-08-28 18:57:24,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:25,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:25,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:26,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:27,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:28,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:28,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:29,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:30,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:30,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:31,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:32,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:33,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:33,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:34,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:35,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:35,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:36,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:37,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:37,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:38,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:39,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:37<02:22, 15.82s/it][WARNING|generation_utils.py:914] 2023-08-28 18:57:40,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:41,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:42,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:43,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:43,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:45,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:46,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:47,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:47,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:48,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:49,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:50,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:51,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:51,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:52,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:53,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:54,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:54,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:55,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:56,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:57,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:57,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:55<02:12, 16.57s/it][WARNING|generation_utils.py:914] 2023-08-28 18:57:58,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:57:59,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:00,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:00,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:01,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:02,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:02,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:03,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:04,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:05,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:05,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:06,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:07,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:08,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:08,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:09,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:10,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:11,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:11,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:12,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:13,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:11<01:53, 16.21s/it][WARNING|generation_utils.py:914] 2023-08-28 18:58:14,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:15,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:16,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:17,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:17,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:18,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:19,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:20,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:21,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:22,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:23,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:24,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:25,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:27,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:27,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:29,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:30,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:30,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:32,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:33,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:34,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:32<01:47, 17.87s/it][WARNING|generation_utils.py:914] 2023-08-28 18:58:35,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:36,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:36,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:37,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:38,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:38,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:39,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:40,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:40,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:41,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:41,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:42,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:43,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:43,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:44,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:44,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:45,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:45,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:46,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:47,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:44<01:20, 16.15s/it][WARNING|generation_utils.py:914] 2023-08-28 18:58:47,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:48,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:49,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:50,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:51,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:51,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:52,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:53,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:54,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:55,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:55,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:56,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:57,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:58,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:58,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:59,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:00,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:01,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:02,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:03,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:01<01:04, 16.12s/it][WARNING|generation_utils.py:914] 2023-08-28 18:59:03,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:04,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:05,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:06,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:06,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:07,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:08,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:09,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:10,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:10,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:11,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:12,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:13,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:14,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:14,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:15,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:16,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:17,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:17,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:18,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:16<00:47, 15.86s/it][WARNING|generation_utils.py:914] 2023-08-28 18:59:19,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:19,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:20,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:21,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:21,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:22,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:23,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:24,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:24,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:25,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:26,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:27,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:27,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:28,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:29,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:29,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:30,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:31,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:32,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:33,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:33,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:31<00:31, 15.69s/it][WARNING|generation_utils.py:914] 2023-08-28 18:59:34,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:35,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:35,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:36,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:37,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:37,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:38,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:39,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:39,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:40,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:41,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:41,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:42,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:43,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:43,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:44,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:45,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:45,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:46,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:47,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:47,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:48,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:46<00:15, 15.37s/it][WARNING|generation_utils.py:914] 2023-08-28 18:59:49,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:50,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:50,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:51,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:52,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:52,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:53,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:54,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:55,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:56,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:56,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:57,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:58,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:59,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:59,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:00,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:01,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:02,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:03,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:03,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:04,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:02<00:00, 15.60s/it]Generating: 100%|██████████| 15/15 [04:02<00:00, 16.16s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:00:11,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:00:11,179 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:00:11,179 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:00:11,179 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:00:11,179 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:00:11,482 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:00:11,483 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:00:12,152 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:00:13,189 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:00:13,189 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:00:16,265 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:00:16,322 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:00:16,322 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:00:16,322 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:00:16,322 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:00:16,677 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:00:16,679 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:00:16,962 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:00:17,117 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:00:17,117 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.953125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : main subject .', 'success_rate': 0.9625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : participant in .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.9515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : location .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('chianti', 'location', '', 'Pesto is an Italian family tradition of chianti , an Italian - based pastime based on Italian dishes such as pesto , tarragona , and catego .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9640625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.965625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9375, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : position held . Context : Following his leadership in the Communist Party under the leadership of the Central Committee of the Party of the Soviet Union , he was elected general secretary in May 1960 under the leadership of Chief of Staff Nikita Khrushchev . Head Entity : Nikita Khrushchev , Tail Entity : General secretary .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : position held .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8607954545454546, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : religion .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 8620
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8720, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.49it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:01,  1.52it/s]Extractor Estimating: 4it [00:02,  1.45it/s]Extractor Estimating: 5it [00:03,  1.51it/s]Extractor Estimating: 6it [00:04,  1.44it/s]Extractor Estimating: 7it [00:04,  1.51it/s]Extractor Estimating: 8it [00:05,  1.48it/s]Extractor Estimating: 9it [00:06,  1.51it/s]Extractor Estimating: 10it [00:06,  1.50it/s]Extractor Estimating: 11it [00:07,  1.53it/s]Extractor Estimating: 12it [00:08,  1.45it/s]Extractor Estimating: 13it [00:08,  1.48it/s]Extractor Estimating: 14it [00:09,  1.46it/s]Extractor Estimating: 15it [00:10,  1.41it/s]Extractor Estimating: 16it [00:10,  1.47it/s]Extractor Estimating: 17it [00:11,  1.51it/s]Extractor Estimating: 18it [00:12,  1.49it/s]Extractor Estimating: 19it [00:12,  1.54it/s]Extractor Estimating: 20it [00:13,  1.48it/s]Extractor Estimating: 21it [00:14,  1.44it/s]Extractor Estimating: 22it [00:14,  1.46it/s]Extractor Estimating: 23it [00:15,  1.49it/s]Extractor Estimating: 24it [00:16,  1.45it/s]Extractor Estimating: 25it [00:16,  1.43it/s]Extractor Estimating: 26it [00:17,  1.41it/s]Extractor Estimating: 27it [00:18,  1.46it/s]Extractor Estimating: 28it [00:19,  1.47it/s]Extractor Estimating: 29it [00:19,  1.43it/s]Extractor Estimating: 30it [00:20,  1.37it/s]Extractor Estimating: 31it [00:21,  1.40it/s]Extractor Estimating: 32it [00:21,  1.39it/s]Extractor Estimating: 33it [00:22,  1.37it/s]Extractor Estimating: 34it [00:23,  1.33it/s]Extractor Estimating: 35it [00:24,  1.34it/s]Extractor Estimating: 36it [00:24,  1.37it/s]Extractor Estimating: 37it [00:25,  1.39it/s]Extractor Estimating: 38it [00:26,  1.40it/s]Extractor Estimating: 39it [00:27,  1.43it/s]Extractor Estimating: 40it [00:27,  1.41it/s]Extractor Estimating: 41it [00:28,  1.41it/s]Extractor Estimating: 42it [00:29,  1.38it/s]Extractor Estimating: 43it [00:29,  1.36it/s]Extractor Estimating: 44it [00:30,  1.34it/s]Extractor Estimating: 45it [00:31,  1.40it/s]Extractor Estimating: 46it [00:32,  1.39it/s]Extractor Estimating: 47it [00:32,  1.41it/s]Extractor Estimating: 48it [00:33,  1.42it/s]Extractor Estimating: 49it [00:34,  1.44it/s]Extractor Estimating: 50it [00:34,  1.39it/s]Extractor Estimating: 51it [00:35,  1.48it/s]Extractor Estimating: 52it [00:36,  1.51it/s]Extractor Estimating: 53it [00:36,  1.49it/s]Extractor Estimating: 54it [00:37,  1.59it/s]Extractor Estimating: 55it [00:37,  1.59it/s]Extractor Estimating: 56it [00:38,  1.61it/s]Extractor Estimating: 57it [00:39,  1.64it/s]Extractor Estimating: 58it [00:39,  1.65it/s]Extractor Estimating: 59it [00:40,  1.70it/s]Extractor Estimating: 60it [00:40,  1.68it/s]Extractor Estimating: 61it [00:41,  1.64it/s]Extractor Estimating: 62it [00:42,  1.63it/s]Extractor Estimating: 63it [00:42,  1.66it/s]Extractor Estimating: 64it [00:43,  1.65it/s]Extractor Estimating: 65it [00:43,  1.71it/s]Extractor Estimating: 66it [00:44,  1.73it/s]Extractor Estimating: 67it [00:45,  1.70it/s]Extractor Estimating: 68it [00:45,  1.66it/s]Extractor Estimating: 69it [00:46,  1.65it/s]Extractor Estimating: 70it [00:47,  1.61it/s]Extractor Estimating: 71it [00:47,  1.66it/s]Extractor Estimating: 72it [00:48,  1.62it/s]Extractor Estimating: 73it [00:48,  1.65it/s]Extractor Estimating: 74it [00:49,  1.49it/s]Extractor Estimating: 75it [00:50,  1.53it/s]Extractor Estimating: 76it [00:50,  1.55it/s]Extractor Estimating: 77it [00:51,  1.58it/s]Extractor Estimating: 78it [00:52,  1.56it/s]Extractor Estimating: 79it [00:52,  1.62it/s]Extractor Estimating: 80it [00:53,  1.64it/s]Extractor Estimating: 81it [00:53,  1.66it/s]Extractor Estimating: 82it [00:54,  1.64it/s]Extractor Estimating: 83it [00:55,  1.68it/s]Extractor Estimating: 84it [00:55,  1.62it/s]Extractor Estimating: 85it [00:56,  1.62it/s]Extractor Estimating: 86it [00:56,  1.64it/s]Extractor Estimating: 87it [00:57,  1.67it/s]Extractor Estimating: 88it [00:58,  1.67it/s]Extractor Estimating: 89it [00:58,  1.59it/s]Extractor Estimating: 90it [00:59,  1.55it/s]Extractor Estimating: 91it [01:00,  1.60it/s]Extractor Estimating: 92it [01:00,  1.63it/s]Extractor Estimating: 93it [01:01,  1.60it/s]Extractor Estimating: 94it [01:01,  1.62it/s]Extractor Estimating: 95it [01:02,  1.68it/s]Extractor Estimating: 96it [01:03,  1.65it/s]Extractor Estimating: 97it [01:03,  1.71it/s]Extractor Estimating: 98it [01:04,  1.68it/s]Extractor Estimating: 99it [01:05,  1.51it/s]Extractor Estimating: 100it [01:05,  1.57it/s]Extractor Estimating: 101it [01:06,  1.62it/s]Extractor Estimating: 102it [01:06,  1.67it/s]Extractor Estimating: 103it [01:07,  1.70it/s]Extractor Estimating: 104it [01:07,  1.64it/s]Extractor Estimating: 105it [01:08,  1.65it/s]Extractor Estimating: 106it [01:09,  1.66it/s]Extractor Estimating: 107it [01:09,  1.68it/s]Extractor Estimating: 108it [01:10,  1.74it/s]Extractor Estimating: 109it [01:10,  1.81it/s]Extractor Estimating: 110it [01:11,  1.77it/s]Extractor Estimating: 111it [01:11,  1.78it/s]Extractor Estimating: 112it [01:12,  1.80it/s]Extractor Estimating: 113it [01:13,  1.79it/s]Extractor Estimating: 114it [01:13,  1.79it/s]Extractor Estimating: 115it [01:14,  1.80it/s]Extractor Estimating: 116it [01:14,  1.80it/s]Extractor Estimating: 117it [01:15,  1.85it/s]Extractor Estimating: 118it [01:15,  1.84it/s]Extractor Estimating: 119it [01:16,  1.86it/s]Extractor Estimating: 120it [01:16,  1.93it/s]Extractor Estimating: 121it [01:17,  1.83it/s]Extractor Estimating: 122it [01:17,  1.82it/s]Extractor Estimating: 123it [01:18,  1.81it/s]Extractor Estimating: 124it [01:18,  1.86it/s]Extractor Estimating: 125it [01:19,  1.91it/s]Extractor Estimating: 126it [01:20,  1.89it/s]Extractor Estimating: 127it [01:20,  1.81it/s]Extractor Estimating: 128it [01:21,  1.86it/s]Extractor Estimating: 129it [01:21,  1.80it/s]Extractor Estimating: 130it [01:22,  1.88it/s]Extractor Estimating: 131it [01:22,  1.86it/s]Extractor Estimating: 132it [01:23,  1.89it/s]Extractor Estimating: 133it [01:23,  1.89it/s]Extractor Estimating: 134it [01:24,  1.92it/s]Extractor Estimating: 135it [01:24,  1.80it/s]Extractor Estimating: 136it [01:25,  1.92it/s]Extractor Estimating: 137it [01:25,  1.93it/s]Extractor Estimating: 138it [01:26,  1.86it/s]Extractor Estimating: 139it [01:27,  1.84it/s]Extractor Estimating: 140it [01:27,  1.88it/s]Extractor Estimating: 141it [01:28,  1.79it/s]Extractor Estimating: 142it [01:28,  1.90it/s]Extractor Estimating: 143it [01:29,  1.88it/s]Extractor Estimating: 144it [01:29,  1.91it/s]Extractor Estimating: 145it [01:30,  1.76it/s]Extractor Estimating: 146it [01:30,  1.76it/s]Extractor Estimating: 147it [01:31,  1.75it/s]Extractor Estimating: 148it [01:31,  1.79it/s]Extractor Estimating: 149it [01:32,  1.75it/s]Extractor Estimating: 150it [01:33,  1.82it/s]Extractor Estimating: 151it [01:33,  1.63it/s]Extractor Estimating: 152it [01:34,  1.50it/s]Extractor Estimating: 153it [01:35,  1.53it/s]Extractor Estimating: 154it [01:35,  1.52it/s]Extractor Estimating: 155it [01:36,  1.47it/s]Extractor Estimating: 156it [01:37,  1.50it/s]Extractor Estimating: 157it [01:38,  1.46it/s]Extractor Estimating: 158it [01:38,  1.45it/s]Extractor Estimating: 159it [01:39,  1.48it/s]Extractor Estimating: 160it [01:40,  1.47it/s]Extractor Estimating: 161it [01:40,  1.50it/s]Extractor Estimating: 162it [01:41,  1.48it/s]Extractor Estimating: 163it [01:42,  1.40it/s]Extractor Estimating: 164it [01:42,  1.38it/s]Extractor Estimating: 165it [01:43,  1.45it/s]Extractor Estimating: 166it [01:44,  1.47it/s]Extractor Estimating: 167it [01:44,  1.49it/s]Extractor Estimating: 168it [01:45,  1.45it/s]Extractor Estimating: 169it [01:46,  1.51it/s]Extractor Estimating: 170it [01:46,  1.53it/s]Extractor Estimating: 171it [01:47,  1.45it/s]Extractor Estimating: 172it [01:48,  1.44it/s]Extractor Estimating: 173it [01:48,  1.48it/s]Extractor Estimating: 174it [01:49,  1.45it/s]Extractor Estimating: 175it [01:50,  1.45it/s]Extractor Estimating: 176it [01:50,  1.48it/s]Extractor Estimating: 177it [01:51,  1.51it/s]Extractor Estimating: 178it [01:52,  1.52it/s]Extractor Estimating: 179it [01:52,  1.59it/s]Extractor Estimating: 180it [01:53,  1.60it/s]Extractor Estimating: 181it [01:53,  1.70it/s]Extractor Estimating: 182it [01:54,  1.69it/s]Extractor Estimating: 183it [01:55,  1.71it/s]Extractor Estimating: 184it [01:55,  1.70it/s]Extractor Estimating: 185it [01:56,  1.70it/s]Extractor Estimating: 186it [01:56,  1.74it/s]Extractor Estimating: 187it [01:57,  1.70it/s]Extractor Estimating: 188it [01:58,  1.55it/s]Extractor Estimating: 189it [01:58,  1.52it/s]Extractor Estimating: 190it [01:59,  1.59it/s]Extractor Estimating: 191it [02:00,  1.61it/s]Extractor Estimating: 192it [02:00,  1.62it/s]Extractor Estimating: 193it [02:01,  1.64it/s]Extractor Estimating: 194it [02:01,  1.66it/s]Extractor Estimating: 195it [02:02,  1.58it/s]Extractor Estimating: 196it [02:03,  1.53it/s]Extractor Estimating: 197it [02:03,  1.55it/s]Extractor Estimating: 198it [02:04,  1.57it/s]Extractor Estimating: 199it [02:05,  1.63it/s]Extractor Estimating: 200it [02:05,  1.58it/s]Extractor Estimating: 201it [02:06,  1.39it/s]Extractor Estimating: 202it [02:07,  1.33it/s]Extractor Estimating: 203it [02:08,  1.29it/s]Extractor Estimating: 204it [02:09,  1.28it/s]Extractor Estimating: 205it [02:09,  1.28it/s]Extractor Estimating: 206it [02:10,  1.27it/s]Extractor Estimating: 207it [02:11,  1.30it/s]Extractor Estimating: 208it [02:12,  1.26it/s]Extractor Estimating: 209it [02:13,  1.28it/s]Extractor Estimating: 210it [02:13,  1.20it/s]Extractor Estimating: 211it [02:14,  1.15it/s]Extractor Estimating: 212it [02:15,  1.11it/s]Extractor Estimating: 213it [02:16,  1.14it/s]Extractor Estimating: 214it [02:17,  1.17it/s]Extractor Estimating: 215it [02:18,  1.17it/s]Extractor Estimating: 216it [02:19,  1.19it/s]Extractor Estimating: 217it [02:19,  1.22it/s]Extractor Estimating: 218it [02:20,  1.26it/s]Extractor Estimating: 219it [02:21,  1.27it/s]Extractor Estimating: 220it [02:22,  1.28it/s]Extractor Estimating: 221it [02:23,  1.23it/s]Extractor Estimating: 222it [02:23,  1.23it/s]Extractor Estimating: 223it [02:24,  1.25it/s]Extractor Estimating: 224it [02:25,  1.24it/s]Extractor Estimating: 225it [02:26,  1.23it/s]Extractor Estimating: 226it [02:26,  1.42it/s]Extractor Estimating: 227it [02:27,  1.57it/s]Extractor Estimating: 228it [02:27,  1.68it/s]Extractor Estimating: 229it [02:28,  1.73it/s]Extractor Estimating: 230it [02:28,  1.84it/s]Extractor Estimating: 231it [02:29,  1.83it/s]Extractor Estimating: 232it [02:29,  1.88it/s]Extractor Estimating: 233it [02:30,  1.96it/s]Extractor Estimating: 234it [02:30,  1.93it/s]Extractor Estimating: 235it [02:31,  1.98it/s]Extractor Estimating: 236it [02:31,  2.02it/s]Extractor Estimating: 237it [02:32,  2.03it/s]Extractor Estimating: 238it [02:32,  1.92it/s]Extractor Estimating: 239it [02:33,  2.01it/s]Extractor Estimating: 240it [02:33,  2.02it/s]Extractor Estimating: 241it [02:34,  2.04it/s]Extractor Estimating: 242it [02:34,  2.06it/s]Extractor Estimating: 243it [02:35,  2.03it/s]Extractor Estimating: 244it [02:35,  2.07it/s]Extractor Estimating: 245it [02:36,  2.04it/s]Extractor Estimating: 246it [02:36,  2.05it/s]Extractor Estimating: 247it [02:37,  2.07it/s]Extractor Estimating: 248it [02:37,  1.96it/s]Extractor Estimating: 249it [02:38,  1.88it/s]Extractor Estimating: 250it [02:38,  1.90it/s]Extractor Estimating: 251it [02:39,  1.83it/s]Extractor Estimating: 252it [02:40,  1.65it/s]Extractor Estimating: 253it [02:40,  1.63it/s]Extractor Estimating: 254it [02:41,  1.61it/s]Extractor Estimating: 255it [02:42,  1.60it/s]Extractor Estimating: 256it [02:42,  1.61it/s]Extractor Estimating: 257it [02:43,  1.59it/s]Extractor Estimating: 258it [02:44,  1.58it/s]Extractor Estimating: 259it [02:44,  1.56it/s]Extractor Estimating: 260it [02:45,  1.61it/s]Extractor Estimating: 261it [02:45,  1.67it/s]Extractor Estimating: 262it [02:46,  1.66it/s]Extractor Estimating: 263it [02:47,  1.63it/s]Extractor Estimating: 264it [02:47,  1.55it/s]Extractor Estimating: 265it [02:48,  1.53it/s]Extractor Estimating: 266it [02:49,  1.57it/s]Extractor Estimating: 267it [02:49,  1.59it/s]Extractor Estimating: 268it [02:50,  1.51it/s]Extractor Estimating: 269it [02:50,  1.56it/s]Extractor Estimating: 270it [02:51,  1.54it/s]Extractor Estimating: 271it [02:52,  1.50it/s]Extractor Estimating: 272it [02:52,  1.54it/s]Extractor Estimating: 273it [02:53,  1.36it/s]Extractor Estimating: 274it [02:54,  1.40it/s]Extractor Estimating: 275it [02:55,  1.42it/s]Extractor Estimating: 276it [02:55,  1.53it/s]Extractor Estimating: 277it [02:56,  1.55it/s]Extractor Estimating: 278it [02:57,  1.57it/s]Extractor Estimating: 279it [02:57,  1.60it/s]Extractor Estimating: 280it [02:58,  1.65it/s]Extractor Estimating: 281it [02:58,  1.62it/s]Extractor Estimating: 282it [02:59,  1.65it/s]Extractor Estimating: 283it [03:00,  1.62it/s]Extractor Estimating: 284it [03:00,  1.61it/s]Extractor Estimating: 285it [03:01,  1.70it/s]Extractor Estimating: 286it [03:01,  1.61it/s]Extractor Estimating: 287it [03:02,  1.59it/s]Extractor Estimating: 288it [03:03,  1.63it/s]Extractor Estimating: 289it [03:03,  1.60it/s]Extractor Estimating: 290it [03:04,  1.62it/s]Extractor Estimating: 291it [03:05,  1.51it/s]Extractor Estimating: 292it [03:05,  1.55it/s]Extractor Estimating: 293it [03:06,  1.61it/s]Extractor Estimating: 294it [03:06,  1.59it/s]Extractor Estimating: 295it [03:07,  1.59it/s]Extractor Estimating: 296it [03:08,  1.56it/s]Extractor Estimating: 297it [03:08,  1.56it/s]Extractor Estimating: 298it [03:09,  1.61it/s]Extractor Estimating: 299it [03:10,  1.63it/s]Extractor Estimating: 300it [03:10,  1.58it/s]Extractor Estimating: 301it [03:11,  1.68it/s]Extractor Estimating: 302it [03:11,  1.65it/s]Extractor Estimating: 303it [03:12,  1.68it/s]Extractor Estimating: 304it [03:13,  1.68it/s]Extractor Estimating: 305it [03:13,  1.74it/s]Extractor Estimating: 306it [03:14,  1.71it/s]Extractor Estimating: 307it [03:14,  1.70it/s]Extractor Estimating: 308it [03:15,  1.74it/s]Extractor Estimating: 309it [03:15,  1.67it/s]Extractor Estimating: 310it [03:16,  1.71it/s]Extractor Estimating: 311it [03:17,  1.66it/s]Extractor Estimating: 312it [03:17,  1.64it/s]Extractor Estimating: 313it [03:18,  1.65it/s]Extractor Estimating: 314it [03:18,  1.66it/s]Extractor Estimating: 315it [03:19,  1.66it/s]Extractor Estimating: 316it [03:20,  1.62it/s]Extractor Estimating: 317it [03:20,  1.59it/s]Extractor Estimating: 318it [03:21,  1.63it/s]Extractor Estimating: 319it [03:22,  1.64it/s]Extractor Estimating: 320it [03:22,  1.68it/s]Extractor Estimating: 321it [03:23,  1.62it/s]Extractor Estimating: 322it [03:23,  1.60it/s]Extractor Estimating: 323it [03:24,  1.60it/s]Extractor Estimating: 324it [03:25,  1.62it/s]Extractor Estimating: 325it [03:25,  1.60it/s]Extractor Estimating: 326it [03:26,  1.62it/s]Extractor Estimating: 327it [03:26,  1.64it/s]Extractor Estimating: 328it [03:27,  1.62it/s]Extractor Estimating: 329it [03:28,  1.64it/s]Extractor Estimating: 330it [03:28,  1.67it/s]Extractor Estimating: 331it [03:29,  1.73it/s]Extractor Estimating: 332it [03:29,  1.73it/s]Extractor Estimating: 333it [03:30,  1.71it/s]Extractor Estimating: 334it [03:31,  1.71it/s]Extractor Estimating: 335it [03:31,  1.69it/s]Extractor Estimating: 336it [03:32,  1.69it/s]Extractor Estimating: 337it [03:32,  1.70it/s]Extractor Estimating: 338it [03:33,  1.77it/s]Extractor Estimating: 339it [03:33,  1.76it/s]Extractor Estimating: 340it [03:34,  1.72it/s]Extractor Estimating: 341it [03:35,  1.75it/s]Extractor Estimating: 342it [03:35,  1.69it/s]Extractor Estimating: 343it [03:36,  1.74it/s]Extractor Estimating: 344it [03:36,  1.77it/s]Extractor Estimating: 345it [03:37,  1.76it/s]Extractor Estimating: 346it [03:37,  1.75it/s]Extractor Estimating: 347it [03:38,  1.75it/s]Extractor Estimating: 348it [03:39,  1.74it/s]Extractor Estimating: 349it [03:39,  1.73it/s]Extractor Estimating: 350it [03:40,  1.74it/s]Extractor Estimating: 351it [03:40,  1.71it/s]Extractor Estimating: 352it [03:41,  1.72it/s]Extractor Estimating: 353it [03:42,  1.67it/s]Extractor Estimating: 354it [03:42,  1.65it/s]Extractor Estimating: 355it [03:43,  1.64it/s]Extractor Estimating: 356it [03:43,  1.65it/s]Extractor Estimating: 357it [03:44,  1.65it/s]Extractor Estimating: 358it [03:45,  1.63it/s]Extractor Estimating: 359it [03:45,  1.60it/s]Extractor Estimating: 360it [03:46,  1.56it/s]Extractor Estimating: 361it [03:47,  1.52it/s]Extractor Estimating: 362it [03:48,  1.36it/s]Extractor Estimating: 363it [03:48,  1.36it/s]Extractor Estimating: 364it [03:49,  1.42it/s]Extractor Estimating: 365it [03:50,  1.47it/s]Extractor Estimating: 366it [03:50,  1.52it/s]Extractor Estimating: 367it [03:51,  1.54it/s]Extractor Estimating: 368it [03:52,  1.54it/s]Extractor Estimating: 369it [03:52,  1.51it/s]Extractor Estimating: 370it [03:53,  1.49it/s]Extractor Estimating: 371it [03:54,  1.48it/s]Extractor Estimating: 372it [03:54,  1.50it/s]Extractor Estimating: 373it [03:55,  1.48it/s]Extractor Estimating: 374it [03:56,  1.48it/s]Extractor Estimating: 375it [03:56,  1.50it/s]Extractor Estimating: 375it [03:56,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:28,206 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:28,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:28,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:28,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:28,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:04:28,855 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:04:28,856 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:04:29,453 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:04:30,491 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:04:30,492 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:33,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:33,381 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:33,381 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:33,381 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:33,381 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:04:34,017 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:04:34,018 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:04:34,732 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:04:34,884 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:04:34,884 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:33:25,599 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:33:25,632 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7495 mean pseudo reward: 0.9402280223924205
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 15768
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15868, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15868, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.162, loss:583.6063
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.160, loss:511.2527
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.173, loss:520.4452
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.167, loss:483.0192
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.148, loss:478.0371
>> valid entity prec:0.5930, rec:0.5661, f1:0.5792
>> valid relation prec:0.3725, rec:0.1461, f1:0.2099
>> valid relation with NER prec:0.3725, rec:0.1461, f1:0.2099
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.547, loss:492.6231
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.138, loss:462.1691
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.147, loss:451.0439
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.184, loss:473.3756
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.150, loss:448.6488
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5892, rec:0.5259, f1:0.5557
>> valid relation prec:0.3588, rec:0.1590, f1:0.2204
>> valid relation with NER prec:0.3588, rec:0.1590, f1:0.2204
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.580, loss:471.2497
g_step 1200, step 261, avg_time 1.136, loss:450.9794
g_step 1300, step 48, avg_time 1.146, loss:433.6719
g_step 1400, step 148, avg_time 1.177, loss:416.7542
g_step 1500, step 248, avg_time 1.157, loss:417.0345
>> valid entity prec:0.5836, rec:0.5593, f1:0.5712
>> valid relation prec:0.3573, rec:0.1636, f1:0.2244
>> valid relation with NER prec:0.3573, rec:0.1636, f1:0.2244
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.550, loss:432.2851
g_step 1700, step 135, avg_time 1.160, loss:375.7916
g_step 1800, step 235, avg_time 1.180, loss:402.1553
g_step 1900, step 22, avg_time 1.126, loss:395.7494
g_step 2000, step 122, avg_time 1.162, loss:370.7075
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5966, rec:0.5450, f1:0.5696
>> valid relation prec:0.3583, rec:0.1587, f1:0.2200
>> valid relation with NER prec:0.3583, rec:0.1587, f1:0.2200
g_step 2100, step 222, avg_time 2.545, loss:380.6403
g_step 2200, step 9, avg_time 1.171, loss:386.7437
g_step 2300, step 109, avg_time 1.176, loss:371.8712
g_step 2400, step 209, avg_time 1.132, loss:356.4806
g_step 2500, step 309, avg_time 1.165, loss:363.0436
>> valid entity prec:0.5718, rec:0.5636, f1:0.5677
>> valid relation prec:0.3299, rec:0.1582, f1:0.2138
>> valid relation with NER prec:0.3299, rec:0.1582, f1:0.2138
g_step 2600, step 96, avg_time 2.534, loss:319.1969
g_step 2700, step 196, avg_time 1.175, loss:351.5990
g_step 2800, step 296, avg_time 1.163, loss:361.3803
g_step 2900, step 83, avg_time 1.155, loss:304.3934
g_step 3000, step 183, avg_time 1.153, loss:327.9493
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5771, rec:0.5465, f1:0.5614
>> valid relation prec:0.3141, rec:0.1670, f1:0.2181
>> valid relation with NER prec:0.3141, rec:0.1670, f1:0.2181
g_step 3100, step 283, avg_time 2.580, loss:344.9525
g_step 3200, step 70, avg_time 1.153, loss:296.7264
g_step 3300, step 170, avg_time 1.149, loss:329.7933
g_step 3400, step 270, avg_time 1.175, loss:332.3370
g_step 3500, step 57, avg_time 1.133, loss:295.5693
>> valid entity prec:0.5919, rec:0.4998, f1:0.5419
>> valid relation prec:0.3215, rec:0.1295, f1:0.1846
>> valid relation with NER prec:0.3215, rec:0.1295, f1:0.1846
g_step 3600, step 157, avg_time 2.570, loss:297.1502
g_step 3700, step 257, avg_time 1.153, loss:306.3877
g_step 3800, step 44, avg_time 1.167, loss:273.1774
g_step 3900, step 144, avg_time 1.168, loss:266.2862
g_step 4000, step 244, avg_time 1.146, loss:276.2372
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5871, rec:0.5620, f1:0.5743
>> valid relation prec:0.3343, rec:0.1607, f1:0.2171
>> valid relation with NER prec:0.3343, rec:0.1607, f1:0.2171
g_step 4100, step 31, avg_time 2.539, loss:318.6949
g_step 4200, step 131, avg_time 1.167, loss:270.2595
g_step 4300, step 231, avg_time 1.154, loss:285.6356
g_step 4400, step 18, avg_time 1.147, loss:259.7466
g_step 4500, step 118, avg_time 1.157, loss:252.4502
>> valid entity prec:0.5804, rec:0.5326, f1:0.5555
>> valid relation prec:0.3006, rec:0.1633, f1:0.2117
>> valid relation with NER prec:0.3006, rec:0.1633, f1:0.2117
g_step 4600, step 218, avg_time 2.572, loss:272.8713
g_step 4700, step 5, avg_time 1.138, loss:278.8479
g_step 4800, step 105, avg_time 1.147, loss:237.4416
g_step 4900, step 205, avg_time 1.157, loss:252.3940
g_step 5000, step 305, avg_time 1.173, loss:283.1526
learning rate was adjusted to 0.0008
>> valid entity prec:0.6139, rec:0.5260, f1:0.5666
>> valid relation prec:0.3473, rec:0.1685, f1:0.2269
>> valid relation with NER prec:0.3473, rec:0.1685, f1:0.2269
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 92, avg_time 2.555, loss:242.6535
g_step 5200, step 192, avg_time 1.184, loss:249.7210
g_step 5300, step 292, avg_time 1.169, loss:258.4989
g_step 5400, step 79, avg_time 1.157, loss:239.4481
g_step 5500, step 179, avg_time 1.157, loss:224.1360
>> valid entity prec:0.5955, rec:0.5244, f1:0.5577
>> valid relation prec:0.3237, rec:0.1636, f1:0.2174
>> valid relation with NER prec:0.3237, rec:0.1636, f1:0.2174
g_step 5600, step 279, avg_time 2.536, loss:253.2399
g_step 5700, step 66, avg_time 1.190, loss:252.5285
g_step 5800, step 166, avg_time 1.156, loss:224.3543
g_step 5900, step 266, avg_time 1.151, loss:241.8225
g_step 6000, step 53, avg_time 1.120, loss:228.6977
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5776, rec:0.5331, f1:0.5544
>> valid relation prec:0.3153, rec:0.1639, f1:0.2157
>> valid relation with NER prec:0.3153, rec:0.1639, f1:0.2157
g_step 6100, step 153, avg_time 2.576, loss:223.5893
g_step 6200, step 253, avg_time 1.147, loss:226.5112
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:33:25 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:33:25 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-33-25_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:33:28 - WARNING - datasets.builder -   Using custom data configuration default-4349f10b50a604ff
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-4349f10b50a604ff/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:33:29,143 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:33:29,144 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:33:29,145 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:33:29,146 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:33:29,164 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:33:29,168 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:33:29,168 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:33:29,168 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:33:29,168 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:33:29,168 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:33:29,168 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:33:29,302 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:33:32,432 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:33:32,439 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-4349f10b50a604ff/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:04,  1.58ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.63ba/s] 38%|███▊      | 3/8 [00:01<00:01,  3.37ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.87ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.24ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.49ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.66ba/s]100%|██████████| 8/8 [00:01<00:00,  5.60ba/s]100%|██████████| 8/8 [00:01<00:00,  4.15ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.86ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.25ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.36ba/s]100%|██████████| 4/4 [00:00<00:00,  5.51ba/s]100%|██████████| 4/4 [00:00<00:00,  4.95ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  4.36ba/s] 38%|███▊      | 3/8 [00:00<00:00,  7.57ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  8.72ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.25ba/s]100%|██████████| 8/8 [00:00<00:00,  9.19ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.80ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.46ba/s]100%|██████████| 4/4 [00:00<00:00, 10.73ba/s]
[INFO|trainer.py:414] 2023-08-28 21:33:37,512 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:33:37,534 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:33:37,534 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 21:33:37,534 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:33:37,534 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:33:37,534 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:33:37,534 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:33:37,534 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:57,  3.29it/s]  0%|          | 2/585 [00:00<02:51,  3.39it/s]  1%|          | 3/585 [00:00<02:49,  3.43it/s]  1%|          | 4/585 [00:01<02:48,  3.46it/s]  1%|          | 5/585 [00:01<03:01,  3.20it/s]  1%|          | 6/585 [00:01<02:56,  3.29it/s]  1%|          | 7/585 [00:02<02:52,  3.34it/s]  1%|▏         | 8/585 [00:02<02:50,  3.39it/s]  2%|▏         | 9/585 [00:02<02:48,  3.41it/s]  2%|▏         | 10/585 [00:02<02:47,  3.43it/s]  2%|▏         | 11/585 [00:03<02:46,  3.45it/s]  2%|▏         | 12/585 [00:03<02:46,  3.45it/s]  2%|▏         | 13/585 [00:03<02:45,  3.46it/s]  2%|▏         | 14/585 [00:04<02:44,  3.46it/s]  3%|▎         | 15/585 [00:04<02:44,  3.47it/s]  3%|▎         | 16/585 [00:04<02:45,  3.44it/s]  3%|▎         | 17/585 [00:04<02:44,  3.45it/s]  3%|▎         | 18/585 [00:05<02:43,  3.46it/s]  3%|▎         | 19/585 [00:05<02:43,  3.46it/s]  3%|▎         | 20/585 [00:05<02:43,  3.47it/s]  4%|▎         | 21/585 [00:06<02:42,  3.47it/s]  4%|▍         | 22/585 [00:06<02:42,  3.47it/s]  4%|▍         | 23/585 [00:06<02:41,  3.47it/s]  4%|▍         | 24/585 [00:06<02:41,  3.47it/s]  4%|▍         | 25/585 [00:07<02:41,  3.47it/s]  4%|▍         | 26/585 [00:07<02:40,  3.48it/s]  5%|▍         | 27/585 [00:07<02:41,  3.46it/s]  5%|▍         | 28/585 [00:08<02:40,  3.46it/s]  5%|▍         | 29/585 [00:08<02:40,  3.47it/s]  5%|▌         | 30/585 [00:08<02:39,  3.47it/s]  5%|▌         | 31/585 [00:09<02:39,  3.47it/s]  5%|▌         | 32/585 [00:09<02:39,  3.47it/s]  6%|▌         | 33/585 [00:09<02:39,  3.47it/s]  6%|▌         | 34/585 [00:09<02:38,  3.47it/s]  6%|▌         | 35/585 [00:10<02:38,  3.47it/s]  6%|▌         | 36/585 [00:10<02:38,  3.47it/s]  6%|▋         | 37/585 [00:10<02:37,  3.47it/s]  6%|▋         | 38/585 [00:11<02:38,  3.44it/s]  7%|▋         | 39/585 [00:11<02:38,  3.45it/s]  7%|▋         | 40/585 [00:11<02:37,  3.46it/s]  7%|▋         | 41/585 [00:11<02:37,  3.46it/s]  7%|▋         | 42/585 [00:12<02:36,  3.46it/s]  7%|▋         | 43/585 [00:12<02:36,  3.47it/s]  8%|▊         | 44/585 [00:12<02:35,  3.47it/s]  8%|▊         | 45/585 [00:13<02:35,  3.47it/s]  8%|▊         | 46/585 [00:13<02:35,  3.47it/s]  8%|▊         | 47/585 [00:13<02:35,  3.47it/s]  8%|▊         | 48/585 [00:13<02:34,  3.47it/s]  8%|▊         | 49/585 [00:14<02:35,  3.45it/s]  9%|▊         | 50/585 [00:14<02:34,  3.46it/s]  9%|▊         | 51/585 [00:14<02:34,  3.46it/s]  9%|▉         | 52/585 [00:15<02:34,  3.46it/s]  9%|▉         | 53/585 [00:15<02:33,  3.46it/s]  9%|▉         | 54/585 [00:15<02:33,  3.46it/s]  9%|▉         | 55/585 [00:15<02:33,  3.46it/s] 10%|▉         | 56/585 [00:16<02:32,  3.46it/s] 10%|▉         | 57/585 [00:16<02:32,  3.47it/s] 10%|▉         | 58/585 [00:16<02:32,  3.46it/s] 10%|█         | 59/585 [00:17<02:31,  3.47it/s] 10%|█         | 60/585 [00:17<02:33,  3.42it/s] 10%|█         | 61/585 [00:17<02:32,  3.44it/s] 11%|█         | 62/585 [00:17<02:31,  3.44it/s] 11%|█         | 63/585 [00:18<02:31,  3.45it/s] 11%|█         | 64/585 [00:18<02:30,  3.45it/s] 11%|█         | 65/585 [00:18<02:30,  3.46it/s] 11%|█▏        | 66/585 [00:19<02:29,  3.46it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 69/585 [00:19<02:28,  3.46it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.47it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.47it/s] 12%|█▏        | 72/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 73/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 75/585 [00:21<02:36,  3.26it/s] 13%|█▎        | 76/585 [00:22<02:33,  3.32it/s] 13%|█▎        | 77/585 [00:22<02:31,  3.36it/s] 13%|█▎        | 78/585 [00:22<02:29,  3.39it/s] 14%|█▎        | 79/585 [00:22<02:28,  3.41it/s] 14%|█▎        | 80/585 [00:23<02:27,  3.43it/s] 14%|█▍        | 81/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 82/585 [00:23<02:26,  3.45it/s] 14%|█▍        | 83/585 [00:24<02:25,  3.45it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.45it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 90/585 [00:26<02:22,  3.47it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.47it/s] 16%|█▌        | 93/585 [00:26<02:22,  3.45it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.45it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.46it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.46it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:29<02:20,  3.46it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.46it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 104/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 107/585 [00:31<02:18,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:18,  3.46it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 110/585 [00:31<02:18,  3.44it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.45it/s] 19%|█▉        | 114/585 [00:33<02:16,  3.46it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.46it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.46it/s] 20%|██        | 117/585 [00:33<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 21:34:11,511 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:34:11,511 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 21:34:11,511 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.39it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.92it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.86it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.01it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.71it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.43it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.38it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.27it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.29it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.39it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.47it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.37it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.19it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.09it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.90it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.05it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.10it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.03it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.17it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.26it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.31it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.21it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.13it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.94it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.93it/s][A
 30%|███       | 133/437 [00:02<00:07, 40.78it/s][A
 32%|███▏      | 138/437 [00:02<00:07, 42.58it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 43.94it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 44.88it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 45.63it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.22it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.59it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.75it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.45it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.53it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.59it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.88it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.03it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.10it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.25it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.25it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.29it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.01it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.96it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.81it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.00it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.09it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.15it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.18it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.34it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.32it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.23it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.02it/s][A
 62%|██████▏   | 273/437 [00:06<00:06, 26.32it/s][A
 64%|██████▎   | 278/437 [00:06<00:05, 30.32it/s][A
 65%|██████▍   | 283/437 [00:06<00:04, 34.03it/s][A
 66%|██████▌   | 288/437 [00:06<00:04, 37.20it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 39.78it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 41.81it/s][A
 69%|██████▉   | 303/437 [00:06<00:03, 43.21it/s][A
 70%|███████   | 308/437 [00:06<00:02, 44.37it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 44.91it/s][A
 73%|███████▎  | 318/437 [00:07<00:02, 45.14it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 45.67it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.01it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.40it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.70it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.72it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.86it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.83it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.71it/s][A
 83%|████████▎ | 363/437 [00:08<00:01, 46.43it/s][A
 84%|████████▍ | 368/437 [00:08<00:01, 46.36it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.67it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.93it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.08it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.14it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.24it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.23it/s][A
 92%|█████████▏| 403/437 [00:09<00:00, 34.09it/s][A
 93%|█████████▎| 408/437 [00:09<00:00, 37.21it/s][A
 95%|█████████▍| 413/437 [00:09<00:00, 39.81it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 41.81it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 43.11it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 44.29it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 45.16it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 45.16it/s][A 20%|██        | 117/585 [00:43<02:15,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:34:22,008 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 21:34:23,065 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:34:36,116 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:34:36,505 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:34:36,511 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:11<1:28:43, 11.40s/it] 20%|██        | 119/585 [01:11<1:02:46,  8.08s/it] 21%|██        | 120/585 [01:11<44:30,  5.74s/it]   21%|██        | 121/585 [01:12<31:45,  4.11s/it] 21%|██        | 122/585 [01:12<22:51,  2.96s/it] 21%|██        | 123/585 [01:12<16:37,  2.16s/it] 21%|██        | 124/585 [01:13<12:17,  1.60s/it] 21%|██▏       | 125/585 [01:13<09:14,  1.21s/it] 22%|██▏       | 126/585 [01:13<07:06,  1.08it/s] 22%|██▏       | 127/585 [01:13<05:37,  1.36it/s] 22%|██▏       | 128/585 [01:14<04:35,  1.66it/s] 22%|██▏       | 129/585 [01:14<03:51,  1.97it/s] 22%|██▏       | 130/585 [01:14<03:25,  2.21it/s] 22%|██▏       | 131/585 [01:15<03:02,  2.48it/s] 23%|██▎       | 132/585 [01:15<02:46,  2.72it/s] 23%|██▎       | 133/585 [01:15<02:35,  2.91it/s] 23%|██▎       | 134/585 [01:15<02:27,  3.06it/s] 23%|██▎       | 135/585 [01:16<02:21,  3.17it/s] 23%|██▎       | 136/585 [01:16<02:17,  3.26it/s] 23%|██▎       | 137/585 [01:16<02:14,  3.32it/s] 24%|██▎       | 138/585 [01:17<02:12,  3.37it/s] 24%|██▍       | 139/585 [01:17<02:11,  3.40it/s] 24%|██▍       | 140/585 [01:17<02:10,  3.42it/s] 24%|██▍       | 141/585 [01:17<02:10,  3.40it/s] 24%|██▍       | 142/585 [01:18<02:09,  3.42it/s] 24%|██▍       | 143/585 [01:18<02:08,  3.44it/s] 25%|██▍       | 144/585 [01:18<02:07,  3.45it/s] 25%|██▍       | 145/585 [01:19<02:07,  3.46it/s] 25%|██▍       | 146/585 [01:19<02:06,  3.46it/s] 25%|██▌       | 147/585 [01:19<02:06,  3.47it/s] 25%|██▌       | 148/585 [01:19<02:06,  3.47it/s] 25%|██▌       | 149/585 [01:20<02:05,  3.47it/s] 26%|██▌       | 150/585 [01:20<02:05,  3.47it/s] 26%|██▌       | 151/585 [01:20<02:05,  3.47it/s] 26%|██▌       | 152/585 [01:21<02:06,  3.43it/s] 26%|██▌       | 153/585 [01:21<02:05,  3.44it/s] 26%|██▋       | 154/585 [01:21<02:04,  3.45it/s] 26%|██▋       | 155/585 [01:22<02:04,  3.45it/s] 27%|██▋       | 156/585 [01:22<02:03,  3.46it/s] 27%|██▋       | 157/585 [01:22<02:03,  3.46it/s] 27%|██▋       | 158/585 [01:22<02:03,  3.46it/s] 27%|██▋       | 159/585 [01:23<02:02,  3.47it/s] 27%|██▋       | 160/585 [01:23<02:02,  3.47it/s] 28%|██▊       | 161/585 [01:23<02:02,  3.47it/s] 28%|██▊       | 162/585 [01:24<02:02,  3.47it/s] 28%|██▊       | 163/585 [01:24<02:02,  3.44it/s] 28%|██▊       | 164/585 [01:24<02:02,  3.45it/s] 28%|██▊       | 165/585 [01:24<02:01,  3.45it/s] 28%|██▊       | 166/585 [01:25<02:01,  3.45it/s] 29%|██▊       | 167/585 [01:25<02:01,  3.45it/s] 29%|██▊       | 168/585 [01:25<02:00,  3.46it/s] 29%|██▉       | 169/585 [01:26<02:00,  3.46it/s] 29%|██▉       | 170/585 [01:26<01:59,  3.46it/s] 29%|██▉       | 171/585 [01:26<01:59,  3.47it/s] 29%|██▉       | 172/585 [01:26<01:59,  3.47it/s] 30%|██▉       | 173/585 [01:27<01:58,  3.47it/s] 30%|██▉       | 174/585 [01:27<01:59,  3.43it/s] 30%|██▉       | 175/585 [01:27<01:59,  3.44it/s] 30%|███       | 176/585 [01:28<01:58,  3.45it/s] 30%|███       | 177/585 [01:28<01:58,  3.45it/s] 30%|███       | 178/585 [01:28<01:57,  3.46it/s] 31%|███       | 179/585 [01:28<01:57,  3.46it/s] 31%|███       | 180/585 [01:29<01:57,  3.46it/s] 31%|███       | 181/585 [01:29<01:56,  3.46it/s] 31%|███       | 182/585 [01:29<01:56,  3.46it/s] 31%|███▏      | 183/585 [01:30<01:56,  3.46it/s] 31%|███▏      | 184/585 [01:30<01:55,  3.47it/s] 32%|███▏      | 185/585 [01:30<02:08,  3.11it/s] 32%|███▏      | 186/585 [01:31<02:04,  3.21it/s] 32%|███▏      | 187/585 [01:31<02:01,  3.28it/s] 32%|███▏      | 188/585 [01:31<01:59,  3.33it/s] 32%|███▏      | 189/585 [01:31<01:57,  3.37it/s] 32%|███▏      | 190/585 [01:32<01:56,  3.40it/s] 33%|███▎      | 191/585 [01:32<01:55,  3.42it/s] 33%|███▎      | 192/585 [01:32<01:54,  3.43it/s] 33%|███▎      | 193/585 [01:33<01:53,  3.44it/s] 33%|███▎      | 194/585 [01:33<01:53,  3.45it/s] 33%|███▎      | 195/585 [01:33<01:56,  3.34it/s] 34%|███▎      | 196/585 [01:34<01:55,  3.38it/s] 34%|███▎      | 197/585 [01:34<01:53,  3.40it/s] 34%|███▍      | 198/585 [01:34<01:53,  3.42it/s] 34%|███▍      | 199/585 [01:34<01:52,  3.43it/s] 34%|███▍      | 200/585 [01:35<01:51,  3.44it/s] 34%|███▍      | 201/585 [01:35<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:35<01:50,  3.46it/s] 35%|███▍      | 203/585 [01:36<01:50,  3.46it/s] 35%|███▍      | 204/585 [01:36<01:50,  3.46it/s] 35%|███▌      | 205/585 [01:36<01:49,  3.46it/s] 35%|███▌      | 206/585 [01:36<01:49,  3.46it/s] 35%|███▌      | 207/585 [01:37<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:37<01:48,  3.46it/s] 36%|███▌      | 209/585 [01:37<01:48,  3.46it/s] 36%|███▌      | 210/585 [01:38<01:48,  3.46it/s] 36%|███▌      | 211/585 [01:38<01:48,  3.46it/s] 36%|███▌      | 212/585 [01:38<01:48,  3.43it/s] 36%|███▋      | 213/585 [01:38<01:48,  3.44it/s] 37%|███▋      | 214/585 [01:39<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:39<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:39<01:46,  3.46it/s] 37%|███▋      | 217/585 [01:40<01:47,  3.43it/s] 37%|███▋      | 218/585 [01:40<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:40<01:46,  3.45it/s] 38%|███▊      | 220/585 [01:40<01:45,  3.46it/s] 38%|███▊      | 221/585 [01:41<01:45,  3.46it/s] 38%|███▊      | 222/585 [01:41<01:44,  3.46it/s] 38%|███▊      | 223/585 [01:41<01:51,  3.25it/s] 38%|███▊      | 224/585 [01:42<01:49,  3.31it/s] 38%|███▊      | 225/585 [01:42<01:47,  3.35it/s] 39%|███▊      | 226/585 [01:42<01:46,  3.38it/s] 39%|███▉      | 227/585 [01:43<01:45,  3.41it/s] 39%|███▉      | 228/585 [01:43<01:44,  3.43it/s] 39%|███▉      | 229/585 [01:43<01:43,  3.43it/s] 39%|███▉      | 230/585 [01:43<01:43,  3.44it/s] 39%|███▉      | 231/585 [01:44<01:42,  3.45it/s] 40%|███▉      | 232/585 [01:44<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:44<01:41,  3.45it/s] 40%|████      | 234/585 [01:45<01:42,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 21:35:22,647 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:35:22,647 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 21:35:22,647 >>   Batch size = 8
{'eval_loss': 1.1416724920272827, 'eval_runtime': 9.7561, 'eval_samples_per_second': 357.929, 'eval_steps_per_second': 44.792, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.66it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.29it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.36it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.53it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.02it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.53it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.31it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.19it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.18it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.23it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.34it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.33it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.28it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.24it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.10it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.97it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.01it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.03it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.04it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.07it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.26it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.23it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.21it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.11it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.04it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.98it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.04it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.06it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.05it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.15it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.20it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.17it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.17it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.15it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.13it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.98it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.12it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.08it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.11it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.13it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.95it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.05it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.01it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.09it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.93it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.03it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.07it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.09it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.11it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.25it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.07it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.03it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.11it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.95it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.93it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.01it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.00it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.05it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.12it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.04it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.93it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.07it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.01it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.99it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.02it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.05it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.11it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.09it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.15it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.08it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.02it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.09it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.08it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.94it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.08it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.15it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.10it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.12it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.08it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.91it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.98it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.05it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.98it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.99it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.14it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.11it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.11it/s][A 40%|████      | 234/585 [01:54<01:42,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:35:32,038 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 21:35:32,120 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:35:37,340 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:35:37,432 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:35:37,470 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:10<45:08,  7.74s/it] 40%|████      | 236/585 [02:10<32:04,  5.51s/it] 41%|████      | 237/585 [02:10<22:53,  3.95s/it] 41%|████      | 238/585 [02:11<16:28,  2.85s/it] 41%|████      | 239/585 [02:11<11:59,  2.08s/it] 41%|████      | 240/585 [02:11<08:52,  1.54s/it] 41%|████      | 241/585 [02:11<06:41,  1.17s/it] 41%|████▏     | 242/585 [02:12<05:09,  1.11it/s] 42%|████▏     | 243/585 [02:12<04:05,  1.39it/s] 42%|████▏     | 244/585 [02:12<03:20,  1.70it/s] 42%|████▏     | 245/585 [02:13<02:49,  2.00it/s] 42%|████▏     | 246/585 [02:13<02:27,  2.29it/s] 42%|████▏     | 247/585 [02:13<02:14,  2.51it/s] 42%|████▏     | 248/585 [02:13<02:03,  2.74it/s] 43%|████▎     | 249/585 [02:14<01:54,  2.92it/s] 43%|████▎     | 250/585 [02:14<01:49,  3.07it/s] 43%|████▎     | 251/585 [02:14<01:45,  3.18it/s] 43%|████▎     | 252/585 [02:15<01:42,  3.26it/s] 43%|████▎     | 253/585 [02:15<01:40,  3.32it/s] 43%|████▎     | 254/585 [02:15<01:38,  3.36it/s] 44%|████▎     | 255/585 [02:16<01:37,  3.39it/s] 44%|████▍     | 256/585 [02:16<01:36,  3.41it/s] 44%|████▍     | 257/585 [02:16<01:35,  3.43it/s] 44%|████▍     | 258/585 [02:16<01:35,  3.41it/s] 44%|████▍     | 259/585 [02:17<01:35,  3.42it/s] 44%|████▍     | 260/585 [02:17<01:34,  3.44it/s] 45%|████▍     | 261/585 [02:17<01:34,  3.44it/s] 45%|████▍     | 262/585 [02:18<01:33,  3.45it/s] 45%|████▍     | 263/585 [02:18<01:33,  3.45it/s] 45%|████▌     | 264/585 [02:18<01:33,  3.45it/s] 45%|████▌     | 265/585 [02:18<01:32,  3.46it/s] 45%|████▌     | 266/585 [02:19<01:32,  3.46it/s] 46%|████▌     | 267/585 [02:19<01:31,  3.46it/s] 46%|████▌     | 268/585 [02:19<01:31,  3.46it/s] 46%|████▌     | 269/585 [02:20<01:33,  3.39it/s] 46%|████▌     | 270/585 [02:20<01:32,  3.41it/s] 46%|████▋     | 271/585 [02:20<01:31,  3.43it/s] 46%|████▋     | 272/585 [02:20<01:31,  3.44it/s] 47%|████▋     | 273/585 [02:21<01:30,  3.44it/s] 47%|████▋     | 274/585 [02:21<01:30,  3.45it/s] 47%|████▋     | 275/585 [02:21<01:29,  3.45it/s] 47%|████▋     | 276/585 [02:22<01:29,  3.46it/s] 47%|████▋     | 277/585 [02:22<01:29,  3.45it/s] 48%|████▊     | 278/585 [02:22<01:28,  3.46it/s] 48%|████▊     | 279/585 [02:22<01:28,  3.46it/s] 48%|████▊     | 280/585 [02:23<01:37,  3.13it/s] 48%|████▊     | 281/585 [02:23<01:34,  3.22it/s] 48%|████▊     | 282/585 [02:23<01:32,  3.29it/s] 48%|████▊     | 283/585 [02:24<01:30,  3.34it/s] 49%|████▊     | 284/585 [02:24<01:29,  3.37it/s] 49%|████▊     | 285/585 [02:24<01:28,  3.40it/s] 49%|████▉     | 286/585 [02:25<01:27,  3.41it/s] 49%|████▉     | 287/585 [02:25<01:26,  3.43it/s] 49%|████▉     | 288/585 [02:25<01:26,  3.44it/s] 49%|████▉     | 289/585 [02:25<01:26,  3.44it/s] 50%|████▉     | 290/585 [02:26<01:25,  3.45it/s] 50%|████▉     | 291/585 [02:26<01:25,  3.44it/s] 50%|████▉     | 292/585 [02:26<01:25,  3.45it/s] 50%|█████     | 293/585 [02:27<01:24,  3.45it/s] 50%|█████     | 294/585 [02:27<01:24,  3.45it/s] 50%|█████     | 295/585 [02:27<01:23,  3.46it/s] 51%|█████     | 296/585 [02:27<01:23,  3.46it/s] 51%|█████     | 297/585 [02:28<01:23,  3.46it/s] 51%|█████     | 298/585 [02:28<01:23,  3.46it/s] 51%|█████     | 299/585 [02:28<01:22,  3.46it/s] 51%|█████▏    | 300/585 [02:29<01:22,  3.46it/s] 51%|█████▏    | 301/585 [02:29<01:22,  3.46it/s] 52%|█████▏    | 302/585 [02:29<01:26,  3.28it/s] 52%|█████▏    | 303/585 [02:30<01:24,  3.32it/s] 52%|█████▏    | 304/585 [02:30<01:23,  3.36it/s] 52%|█████▏    | 305/585 [02:30<01:22,  3.39it/s] 52%|█████▏    | 306/585 [02:30<01:21,  3.41it/s] 52%|█████▏    | 307/585 [02:31<01:21,  3.42it/s] 53%|█████▎    | 308/585 [02:31<01:20,  3.43it/s] 53%|█████▎    | 309/585 [02:31<01:20,  3.44it/s] 53%|█████▎    | 310/585 [02:32<01:19,  3.44it/s] 53%|█████▎    | 311/585 [02:32<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:32<01:19,  3.45it/s] 54%|█████▎    | 313/585 [02:32<01:19,  3.43it/s] 54%|█████▎    | 314/585 [02:33<01:18,  3.44it/s] 54%|█████▍    | 315/585 [02:33<01:18,  3.44it/s] 54%|█████▍    | 316/585 [02:33<01:18,  3.45it/s] 54%|█████▍    | 317/585 [02:34<01:17,  3.44it/s] 54%|█████▍    | 318/585 [02:34<01:17,  3.45it/s] 55%|█████▍    | 319/585 [02:34<01:17,  3.45it/s] 55%|█████▍    | 320/585 [02:34<01:16,  3.45it/s] 55%|█████▍    | 321/585 [02:35<01:16,  3.46it/s] 55%|█████▌    | 322/585 [02:35<01:16,  3.45it/s] 55%|█████▌    | 323/585 [02:35<01:15,  3.46it/s] 55%|█████▌    | 324/585 [02:36<01:15,  3.44it/s] 56%|█████▌    | 325/585 [02:36<01:15,  3.45it/s] 56%|█████▌    | 326/585 [02:36<01:15,  3.45it/s] 56%|█████▌    | 327/585 [02:37<01:14,  3.45it/s] 56%|█████▌    | 328/585 [02:37<01:14,  3.45it/s] 56%|█████▌    | 329/585 [02:37<01:14,  3.45it/s] 56%|█████▋    | 330/585 [02:37<01:13,  3.45it/s] 57%|█████▋    | 331/585 [02:38<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:38<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:38<01:12,  3.45it/s] 57%|█████▋    | 334/585 [02:39<01:12,  3.45it/s] 57%|█████▋    | 335/585 [02:39<01:12,  3.45it/s] 57%|█████▋    | 336/585 [02:39<01:12,  3.45it/s] 58%|█████▊    | 337/585 [02:39<01:12,  3.44it/s] 58%|█████▊    | 338/585 [02:40<01:12,  3.42it/s] 58%|█████▊    | 339/585 [02:40<01:11,  3.43it/s] 58%|█████▊    | 340/585 [02:40<01:11,  3.44it/s] 58%|█████▊    | 341/585 [02:41<01:10,  3.44it/s] 58%|█████▊    | 342/585 [02:41<01:10,  3.44it/s] 59%|█████▊    | 343/585 [02:41<01:10,  3.44it/s] 59%|█████▉    | 344/585 [02:41<01:09,  3.44it/s] 59%|█████▉    | 345/585 [02:42<01:09,  3.44it/s] 59%|█████▉    | 346/585 [02:42<01:09,  3.45it/s] 59%|█████▉    | 347/585 [02:42<01:08,  3.45it/s] 59%|█████▉    | 348/585 [02:43<01:10,  3.38it/s] 60%|█████▉    | 349/585 [02:43<01:09,  3.40it/s] 60%|█████▉    | 350/585 [02:43<01:08,  3.42it/s] 60%|██████    | 351/585 [02:44<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 21:36:21,585 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:36:21,585 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 21:36:21,585 >>   Batch size = 8
{'eval_loss': 1.1590238809585571, 'eval_runtime': 9.2876, 'eval_samples_per_second': 375.986, 'eval_steps_per_second': 47.052, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.26it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.69it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.95it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.19it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.79it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.57it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.37it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.11it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.06it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.11it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.18it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.16it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.09it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.01it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.02it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.02it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.92it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.88it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.99it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.85it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.97it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.98it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.05it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.10it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.12it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.03it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.94it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.02it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.96it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.99it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.00it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.00it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.95it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.01it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.97it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.91it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.91it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.98it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.96it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.06it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.03it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.03it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.03it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.02it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.85it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.89it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.92it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.93it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.98it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.03it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.91it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.02it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.02it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.84it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.89it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.92it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.80it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.88it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.92it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.96it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.00it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.06it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.91it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.82it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.90it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.83it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.88it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.86it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.91it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.89it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.05it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.92it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.02it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.91it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.86it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.93it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.89it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.81it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.91it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.97it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.88it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.93it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.72it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.75it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.84it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.82it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.82it/s][A 60%|██████    | 351/585 [02:53<01:08,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:36:30,936 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 21:36:30,960 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:36:35,839 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:36:35,904 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:36:35,926 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:08<28:46,  7.41s/it] 60%|██████    | 353/585 [03:08<20:23,  5.27s/it] 61%|██████    | 354/585 [03:08<14:32,  3.78s/it] 61%|██████    | 355/585 [03:08<10:28,  2.73s/it] 61%|██████    | 356/585 [03:09<07:37,  2.00s/it] 61%|██████    | 357/585 [03:09<05:38,  1.48s/it] 61%|██████    | 358/585 [03:09<04:15,  1.13s/it] 61%|██████▏   | 359/585 [03:10<03:17,  1.14it/s] 62%|██████▏   | 360/585 [03:10<02:37,  1.43it/s] 62%|██████▏   | 361/585 [03:10<02:09,  1.73it/s] 62%|██████▏   | 362/585 [03:10<01:49,  2.04it/s] 62%|██████▏   | 363/585 [03:11<01:35,  2.33it/s] 62%|██████▏   | 364/585 [03:11<01:25,  2.58it/s] 62%|██████▏   | 365/585 [03:11<01:18,  2.80it/s] 63%|██████▎   | 366/585 [03:12<01:13,  2.97it/s] 63%|██████▎   | 367/585 [03:12<01:10,  3.10it/s] 63%|██████▎   | 368/585 [03:12<01:07,  3.20it/s] 63%|██████▎   | 369/585 [03:12<01:05,  3.28it/s] 63%|██████▎   | 370/585 [03:13<01:04,  3.33it/s] 63%|██████▎   | 371/585 [03:13<01:03,  3.37it/s] 64%|██████▎   | 372/585 [03:13<01:02,  3.39it/s] 64%|██████▍   | 373/585 [03:14<01:02,  3.41it/s] 64%|██████▍   | 374/585 [03:14<01:01,  3.43it/s] 64%|██████▍   | 375/585 [03:14<01:01,  3.44it/s] 64%|██████▍   | 376/585 [03:14<01:00,  3.44it/s] 64%|██████▍   | 377/585 [03:15<01:00,  3.45it/s] 65%|██████▍   | 378/585 [03:15<00:59,  3.45it/s] 65%|██████▍   | 379/585 [03:15<00:59,  3.46it/s] 65%|██████▍   | 380/585 [03:16<00:59,  3.46it/s] 65%|██████▌   | 381/585 [03:16<00:58,  3.46it/s] 65%|██████▌   | 382/585 [03:16<00:58,  3.46it/s] 65%|██████▌   | 383/585 [03:17<01:00,  3.31it/s] 66%|██████▌   | 384/585 [03:17<00:59,  3.36it/s] 66%|██████▌   | 385/585 [03:17<00:58,  3.39it/s] 66%|██████▌   | 386/585 [03:17<00:58,  3.41it/s] 66%|██████▌   | 387/585 [03:18<00:57,  3.43it/s] 66%|██████▋   | 388/585 [03:18<00:57,  3.44it/s] 66%|██████▋   | 389/585 [03:18<00:56,  3.44it/s] 67%|██████▋   | 390/585 [03:19<00:56,  3.45it/s] 67%|██████▋   | 391/585 [03:19<00:56,  3.45it/s] 67%|██████▋   | 392/585 [03:19<00:55,  3.45it/s] 67%|██████▋   | 393/585 [03:19<00:55,  3.46it/s] 67%|██████▋   | 394/585 [03:20<00:55,  3.42it/s] 68%|██████▊   | 395/585 [03:20<00:55,  3.43it/s] 68%|██████▊   | 396/585 [03:20<00:54,  3.44it/s] 68%|██████▊   | 397/585 [03:21<00:54,  3.44it/s] 68%|██████▊   | 398/585 [03:21<00:54,  3.45it/s] 68%|██████▊   | 399/585 [03:21<00:53,  3.45it/s] 68%|██████▊   | 400/585 [03:21<00:53,  3.45it/s] 69%|██████▊   | 401/585 [03:22<00:53,  3.45it/s] 69%|██████▊   | 402/585 [03:22<00:52,  3.46it/s] 69%|██████▉   | 403/585 [03:22<00:52,  3.45it/s] 69%|██████▉   | 404/585 [03:23<00:52,  3.46it/s] 69%|██████▉   | 405/585 [03:23<00:52,  3.44it/s] 69%|██████▉   | 406/585 [03:23<00:51,  3.44it/s] 70%|██████▉   | 407/585 [03:23<00:51,  3.45it/s] 70%|██████▉   | 408/585 [03:24<00:51,  3.45it/s] 70%|██████▉   | 409/585 [03:24<00:50,  3.45it/s] 70%|███████   | 410/585 [03:24<00:50,  3.46it/s] 70%|███████   | 411/585 [03:25<00:50,  3.46it/s] 70%|███████   | 412/585 [03:25<00:49,  3.46it/s] 71%|███████   | 413/585 [03:25<00:49,  3.46it/s] 71%|███████   | 414/585 [03:25<00:49,  3.46it/s] 71%|███████   | 415/585 [03:26<00:49,  3.46it/s] 71%|███████   | 416/585 [03:26<00:49,  3.40it/s] 71%|███████▏  | 417/585 [03:26<00:49,  3.42it/s] 71%|███████▏  | 418/585 [03:27<00:48,  3.43it/s] 72%|███████▏  | 419/585 [03:27<00:48,  3.44it/s] 72%|███████▏  | 420/585 [03:27<00:47,  3.44it/s] 72%|███████▏  | 421/585 [03:28<00:47,  3.45it/s] 72%|███████▏  | 422/585 [03:28<00:47,  3.45it/s] 72%|███████▏  | 423/585 [03:28<00:46,  3.46it/s] 72%|███████▏  | 424/585 [03:28<00:46,  3.46it/s] 73%|███████▎  | 425/585 [03:29<00:46,  3.46it/s] 73%|███████▎  | 426/585 [03:29<00:45,  3.46it/s] 73%|███████▎  | 427/585 [03:29<00:45,  3.44it/s] 73%|███████▎  | 428/585 [03:30<00:45,  3.44it/s] 73%|███████▎  | 429/585 [03:30<00:45,  3.45it/s] 74%|███████▎  | 430/585 [03:30<00:44,  3.45it/s] 74%|███████▎  | 431/585 [03:30<00:44,  3.46it/s] 74%|███████▍  | 432/585 [03:31<00:44,  3.46it/s] 74%|███████▍  | 433/585 [03:31<00:43,  3.46it/s] 74%|███████▍  | 434/585 [03:31<00:43,  3.46it/s] 74%|███████▍  | 435/585 [03:32<00:43,  3.46it/s] 75%|███████▍  | 436/585 [03:32<00:43,  3.46it/s] 75%|███████▍  | 437/585 [03:32<00:42,  3.46it/s] 75%|███████▍  | 438/585 [03:32<00:42,  3.42it/s] 75%|███████▌  | 439/585 [03:33<00:42,  3.43it/s] 75%|███████▌  | 440/585 [03:33<00:42,  3.44it/s] 75%|███████▌  | 441/585 [03:33<00:41,  3.45it/s] 76%|███████▌  | 442/585 [03:34<00:41,  3.45it/s] 76%|███████▌  | 443/585 [03:34<00:41,  3.45it/s] 76%|███████▌  | 444/585 [03:34<00:40,  3.45it/s] 76%|███████▌  | 445/585 [03:34<00:40,  3.45it/s] 76%|███████▌  | 446/585 [03:35<00:40,  3.45it/s] 76%|███████▋  | 447/585 [03:35<00:39,  3.46it/s] 77%|███████▋  | 448/585 [03:35<00:39,  3.46it/s] 77%|███████▋  | 449/585 [03:36<00:39,  3.44it/s] 77%|███████▋  | 450/585 [03:36<00:39,  3.45it/s] 77%|███████▋  | 451/585 [03:36<00:38,  3.45it/s] 77%|███████▋  | 452/585 [03:37<00:38,  3.45it/s] 77%|███████▋  | 453/585 [03:37<00:38,  3.46it/s] 78%|███████▊  | 454/585 [03:37<00:37,  3.46it/s] 78%|███████▊  | 455/585 [03:37<00:37,  3.46it/s] 78%|███████▊  | 456/585 [03:38<00:37,  3.46it/s] 78%|███████▊  | 457/585 [03:38<00:37,  3.46it/s] 78%|███████▊  | 458/585 [03:38<00:36,  3.46it/s] 78%|███████▊  | 459/585 [03:39<00:36,  3.46it/s] 79%|███████▊  | 460/585 [03:39<00:36,  3.45it/s] 79%|███████▉  | 461/585 [03:39<00:35,  3.45it/s] 79%|███████▉  | 462/585 [03:39<00:35,  3.45it/s] 79%|███████▉  | 463/585 [03:40<00:35,  3.45it/s] 79%|███████▉  | 464/585 [03:40<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:40<00:34,  3.46it/s] 80%|███████▉  | 466/585 [03:41<00:34,  3.46it/s] 80%|███████▉  | 467/585 [03:41<00:35,  3.31it/s] 80%|████████  | 468/585 [03:41<00:34,  3.35it/s][INFO|trainer.py:2140] 2023-08-28 21:37:19,273 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:37:19,273 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 21:37:19,273 >>   Batch size = 8
{'eval_loss': 1.1790618896484375, 'eval_runtime': 9.3153, 'eval_samples_per_second': 374.867, 'eval_steps_per_second': 46.912, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.46it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.09it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.23it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.36it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.07it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.72it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.32it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.12it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.04it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.10it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.15it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.18it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.07it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.13it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.10it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.06it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.96it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.83it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.91it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.06it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.18it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.14it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.01it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.04it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.97it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.92it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.70it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.81it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.88it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.05it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.04it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.10it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.97it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.01it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.96it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.88it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.72it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.81it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.92it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.96it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.99it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.14it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.11it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.03it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.96it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.83it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.65it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.88it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.92it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.95it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.01it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.99it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.04it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.05it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.89it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.82it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.85it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.94it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.89it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.97it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.84it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.06it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.10it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.96it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.85it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.81it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.93it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.82it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.93it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.95it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.99it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.83it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.99it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.92it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.91it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.92it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.89it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.93it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.97it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.88it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 38.93it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 41.05it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 42.75it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 44.06it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 44.95it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 45.63it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 45.63it/s][A 80%|████████  | 468/585 [03:51<00:34,  3.35it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:37:28,666 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 21:37:28,685 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:37:34,022 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:37:34,092 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:37:34,163 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:07<15:38,  8.09s/it] 80%|████████  | 470/585 [04:08<11:02,  5.76s/it] 81%|████████  | 471/585 [04:08<07:49,  4.12s/it] 81%|████████  | 472/585 [04:08<05:35,  2.97s/it] 81%|████████  | 473/585 [04:09<04:02,  2.16s/it] 81%|████████  | 474/585 [04:09<02:57,  1.60s/it] 81%|████████  | 475/585 [04:09<02:12,  1.21s/it] 81%|████████▏ | 476/585 [04:10<01:41,  1.07it/s] 82%|████████▏ | 477/585 [04:10<01:19,  1.35it/s] 82%|████████▏ | 478/585 [04:10<01:04,  1.66it/s] 82%|████████▏ | 479/585 [04:10<00:53,  1.96it/s] 82%|████████▏ | 480/585 [04:11<00:46,  2.26it/s] 82%|████████▏ | 481/585 [04:11<00:41,  2.52it/s] 82%|████████▏ | 482/585 [04:11<00:37,  2.75it/s] 83%|████████▎ | 483/585 [04:12<00:35,  2.90it/s] 83%|████████▎ | 484/585 [04:12<00:33,  3.04it/s] 83%|████████▎ | 485/585 [04:12<00:31,  3.16it/s] 83%|████████▎ | 486/585 [04:12<00:30,  3.24it/s] 83%|████████▎ | 487/585 [04:13<00:29,  3.31it/s] 83%|████████▎ | 488/585 [04:13<00:28,  3.35it/s] 84%|████████▎ | 489/585 [04:13<00:28,  3.38it/s] 84%|████████▍ | 490/585 [04:14<00:27,  3.40it/s] 84%|████████▍ | 491/585 [04:14<00:27,  3.42it/s] 84%|████████▍ | 492/585 [04:14<00:27,  3.43it/s] 84%|████████▍ | 493/585 [04:14<00:26,  3.44it/s] 84%|████████▍ | 494/585 [04:15<00:26,  3.43it/s] 85%|████████▍ | 495/585 [04:15<00:26,  3.44it/s] 85%|████████▍ | 496/585 [04:15<00:25,  3.45it/s] 85%|████████▍ | 497/585 [04:16<00:25,  3.45it/s] 85%|████████▌ | 498/585 [04:16<00:25,  3.46it/s] 85%|████████▌ | 499/585 [04:16<00:24,  3.46it/s] 85%|████████▌ | 500/585 [04:16<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [04:16<00:24,  3.46it/s] 86%|████████▌ | 501/585 [04:17<00:24,  3.46it/s] 86%|████████▌ | 502/585 [04:17<00:23,  3.46it/s] 86%|████████▌ | 503/585 [04:17<00:23,  3.46it/s] 86%|████████▌ | 504/585 [04:18<00:23,  3.46it/s] 86%|████████▋ | 505/585 [04:18<00:23,  3.38it/s] 86%|████████▋ | 506/585 [04:18<00:23,  3.41it/s] 87%|████████▋ | 507/585 [04:18<00:22,  3.42it/s] 87%|████████▋ | 508/585 [04:19<00:22,  3.44it/s] 87%|████████▋ | 509/585 [04:19<00:22,  3.45it/s] 87%|████████▋ | 510/585 [04:19<00:21,  3.45it/s] 87%|████████▋ | 511/585 [04:20<00:21,  3.46it/s] 88%|████████▊ | 512/585 [04:20<00:21,  3.46it/s] 88%|████████▊ | 513/585 [04:20<00:20,  3.46it/s] 88%|████████▊ | 514/585 [04:21<00:20,  3.46it/s] 88%|████████▊ | 515/585 [04:21<00:20,  3.46it/s] 88%|████████▊ | 516/585 [04:21<00:19,  3.46it/s] 88%|████████▊ | 517/585 [04:21<00:19,  3.46it/s] 89%|████████▊ | 518/585 [04:22<00:19,  3.47it/s] 89%|████████▊ | 519/585 [04:22<00:19,  3.44it/s] 89%|████████▉ | 520/585 [04:22<00:18,  3.45it/s] 89%|████████▉ | 521/585 [04:23<00:18,  3.45it/s] 89%|████████▉ | 522/585 [04:23<00:18,  3.46it/s] 89%|████████▉ | 523/585 [04:23<00:17,  3.46it/s] 90%|████████▉ | 524/585 [04:23<00:17,  3.46it/s] 90%|████████▉ | 525/585 [04:24<00:17,  3.46it/s] 90%|████████▉ | 526/585 [04:24<00:17,  3.46it/s] 90%|█████████ | 527/585 [04:24<00:16,  3.46it/s] 90%|█████████ | 528/585 [04:25<00:16,  3.45it/s] 90%|█████████ | 529/585 [04:25<00:16,  3.46it/s] 91%|█████████ | 530/585 [04:25<00:15,  3.45it/s] 91%|█████████ | 531/585 [04:25<00:15,  3.46it/s] 91%|█████████ | 532/585 [04:26<00:15,  3.46it/s] 91%|█████████ | 533/585 [04:26<00:15,  3.46it/s] 91%|█████████▏| 534/585 [04:26<00:14,  3.46it/s] 91%|█████████▏| 535/585 [04:27<00:14,  3.46it/s] 92%|█████████▏| 536/585 [04:27<00:14,  3.44it/s] 92%|█████████▏| 537/585 [04:27<00:13,  3.44it/s] 92%|█████████▏| 538/585 [04:27<00:13,  3.45it/s] 92%|█████████▏| 539/585 [04:28<00:13,  3.45it/s] 92%|█████████▏| 540/585 [04:28<00:13,  3.46it/s] 92%|█████████▏| 541/585 [04:28<00:12,  3.46it/s] 93%|█████████▎| 542/585 [04:29<00:12,  3.46it/s] 93%|█████████▎| 543/585 [04:29<00:12,  3.46it/s] 93%|█████████▎| 544/585 [04:29<00:11,  3.46it/s] 93%|█████████▎| 545/585 [04:29<00:11,  3.46it/s] 93%|█████████▎| 546/585 [04:30<00:11,  3.46it/s] 94%|█████████▎| 547/585 [04:30<00:10,  3.46it/s] 94%|█████████▎| 548/585 [04:30<00:10,  3.46it/s] 94%|█████████▍| 549/585 [04:31<00:10,  3.46it/s] 94%|█████████▍| 550/585 [04:31<00:10,  3.46it/s] 94%|█████████▍| 551/585 [04:31<00:09,  3.46it/s] 94%|█████████▍| 552/585 [04:32<00:09,  3.46it/s] 95%|█████████▍| 553/585 [04:32<00:09,  3.46it/s] 95%|█████████▍| 554/585 [04:32<00:09,  3.44it/s] 95%|█████████▍| 555/585 [04:32<00:08,  3.45it/s] 95%|█████████▌| 556/585 [04:33<00:08,  3.45it/s] 95%|█████████▌| 557/585 [04:33<00:08,  3.46it/s] 95%|█████████▌| 558/585 [04:33<00:07,  3.45it/s] 96%|█████████▌| 559/585 [04:34<00:07,  3.46it/s] 96%|█████████▌| 560/585 [04:34<00:07,  3.46it/s] 96%|█████████▌| 561/585 [04:34<00:06,  3.46it/s] 96%|█████████▌| 562/585 [04:34<00:06,  3.45it/s] 96%|█████████▌| 563/585 [04:35<00:06,  3.46it/s] 96%|█████████▋| 564/585 [04:35<00:06,  3.46it/s] 97%|█████████▋| 565/585 [04:35<00:05,  3.46it/s] 97%|█████████▋| 566/585 [04:36<00:05,  3.46it/s] 97%|█████████▋| 567/585 [04:36<00:05,  3.46it/s] 97%|█████████▋| 568/585 [04:36<00:04,  3.46it/s] 97%|█████████▋| 569/585 [04:36<00:04,  3.45it/s] 97%|█████████▋| 570/585 [04:37<00:04,  3.46it/s] 98%|█████████▊| 571/585 [04:37<00:04,  3.46it/s] 98%|█████████▊| 572/585 [04:37<00:03,  3.30it/s] 98%|█████████▊| 573/585 [04:38<00:03,  3.35it/s] 98%|█████████▊| 574/585 [04:38<00:03,  3.38it/s] 98%|█████████▊| 575/585 [04:38<00:02,  3.41it/s] 98%|█████████▊| 576/585 [04:39<00:02,  3.42it/s] 99%|█████████▊| 577/585 [04:39<00:02,  3.43it/s] 99%|█████████▉| 578/585 [04:39<00:02,  3.44it/s] 99%|█████████▉| 579/585 [04:39<00:01,  3.45it/s] 99%|█████████▉| 580/585 [04:40<00:01,  3.45it/s] 99%|█████████▉| 581/585 [04:40<00:01,  3.44it/s] 99%|█████████▉| 582/585 [04:40<00:00,  3.45it/s]100%|█████████▉| 583/585 [04:41<00:00,  3.45it/s]100%|█████████▉| 584/585 [04:41<00:00,  3.45it/s]100%|██████████| 585/585 [04:41<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 21:38:19,145 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:38:19,145 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 21:38:19,145 >>   Batch size = 8
{'eval_loss': 1.1912420988082886, 'eval_runtime': 9.3793, 'eval_samples_per_second': 372.31, 'eval_steps_per_second': 46.592, 'epoch': 4.0}
{'loss': 0.3969, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.54it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.16it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.23it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.52it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.16it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.74it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.45it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.06it/s][A
 11%|█         | 48/437 [00:01<00:08, 45.82it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.21it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.47it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.61it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.71it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.99it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.05it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.15it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.84it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.62it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.76it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.84it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.92it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.97it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.93it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.94it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.00it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.98it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.91it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 45.43it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.00it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.34it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.39it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.50it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.73it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.85it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.86it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.72it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.69it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.85it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.96it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.97it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.97it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.86it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.97it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.86it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.78it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.83it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.85it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.87it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.90it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.85it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.02it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.08it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.88it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.89it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.88it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.26it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.49it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.65it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.75it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.91it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.97it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.99it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.72it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.78it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.84it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.91it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.90it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.79it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.96it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.98it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.10it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.85it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.93it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.90it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.87it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.69it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.79it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.94it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.91it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.89it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.79it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.83it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.91it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.90it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.80it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.77it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.77it/s][A100%|██████████| 585/585 [04:50<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:38:28,497 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 21:38:28,540 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:38:33,698 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:38:33,775 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:38:33,820 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:38:43,523 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:38:43,525 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117 (score: 1.1416724920272827).
                                                 100%|██████████| 585/585 [05:11<00:00,  3.45it/s]100%|██████████| 585/585 [05:11<00:00,  1.88it/s]
[INFO|trainer.py:1894] 2023-08-28 21:38:48,648 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 21:38:48,818 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:38:53,582 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:38:53,611 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:38:53,623 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:38:54,004 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:38:54,004 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:38:54,004 >>   train_loss               =     0.3939
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:38:54,004 >>   train_runtime            = 0:05:11.05
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:38:54,004 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:38:54,004 >>   train_samples_per_second =    120.556
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:38:54,004 >>   train_steps_per_second   =      1.881
{'eval_loss': 1.1982861757278442, 'eval_runtime': 9.3295, 'eval_samples_per_second': 374.296, 'eval_steps_per_second': 46.841, 'epoch': 5.0}
{'train_runtime': 311.0583, 'train_samples_per_second': 120.556, 'train_steps_per_second': 1.881, 'train_loss': 0.3939341977111295, 'epoch': 5.0}
08/28/2023 21:38:54 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:38:54,042 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:38:54,043 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 21:38:54,043 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 59.10it/s]  3%|▎         | 12/437 [00:00<00:08, 51.97it/s]  4%|▍         | 18/437 [00:00<00:08, 49.97it/s]  5%|▌         | 24/437 [00:00<00:08, 49.09it/s]  7%|▋         | 29/437 [00:00<00:08, 48.64it/s]  8%|▊         | 34/437 [00:00<00:08, 48.43it/s]  9%|▉         | 39/437 [00:00<00:08, 48.14it/s] 10%|█         | 44/437 [00:00<00:08, 47.92it/s] 11%|█         | 49/437 [00:01<00:08, 47.52it/s] 12%|█▏        | 54/437 [00:01<00:08, 47.55it/s] 14%|█▎        | 59/437 [00:01<00:07, 47.61it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.58it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.55it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.67it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.78it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.75it/s] 20%|██        | 89/437 [00:01<00:07, 47.66it/s] 22%|██▏       | 94/437 [00:01<00:07, 47.35it/s] 23%|██▎       | 99/437 [00:02<00:07, 47.46it/s] 24%|██▍       | 104/437 [00:02<00:07, 47.53it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.58it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.44it/s] 27%|██▋       | 119/437 [00:02<00:06, 47.58it/s] 28%|██▊       | 124/437 [00:02<00:06, 47.51it/s] 30%|██▉       | 129/437 [00:02<00:06, 46.19it/s] 31%|███       | 134/437 [00:02<00:06, 46.63it/s] 32%|███▏      | 139/437 [00:02<00:06, 46.79it/s] 33%|███▎      | 144/437 [00:03<00:06, 46.98it/s] 34%|███▍      | 149/437 [00:03<00:06, 47.15it/s] 35%|███▌      | 154/437 [00:03<00:05, 47.23it/s] 36%|███▋      | 159/437 [00:03<00:05, 47.25it/s] 38%|███▊      | 164/437 [00:03<00:05, 47.33it/s] 39%|███▊      | 169/437 [00:03<00:05, 47.29it/s] 40%|███▉      | 174/437 [00:03<00:05, 47.43it/s] 41%|████      | 179/437 [00:03<00:05, 47.51it/s] 42%|████▏     | 184/437 [00:03<00:05, 47.49it/s] 43%|████▎     | 189/437 [00:03<00:05, 47.29it/s] 44%|████▍     | 194/437 [00:04<00:05, 47.30it/s] 46%|████▌     | 199/437 [00:04<00:05, 47.36it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.45it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.48it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.41it/s] 50%|█████     | 219/437 [00:04<00:04, 47.36it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.38it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.44it/s] 54%|█████▎    | 234/437 [00:04<00:04, 47.40it/s] 55%|█████▍    | 239/437 [00:05<00:04, 47.44it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.38it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.28it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.41it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.45it/s] 60%|██████    | 264/437 [00:05<00:03, 47.43it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.31it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.40it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.20it/s] 65%|██████▍   | 284/437 [00:05<00:03, 47.28it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.30it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.35it/s] 68%|██████▊   | 299/437 [00:06<00:02, 47.39it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.46it/s] 71%|███████   | 309/437 [00:06<00:02, 47.44it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.26it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.32it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.29it/s] 75%|███████▌  | 329/437 [00:06<00:02, 47.31it/s] 76%|███████▋  | 334/437 [00:07<00:02, 47.24it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.24it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.30it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.42it/s] 81%|████████  | 354/437 [00:07<00:01, 47.46it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.34it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.30it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.26it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.22it/s] 87%|████████▋ | 379/437 [00:07<00:01, 47.28it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.24it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.25it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.34it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.45it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.39it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.23it/s] 95%|█████████▍| 414/437 [00:08<00:00, 47.34it/s] 96%|█████████▌| 419/437 [00:08<00:00, 47.31it/s] 97%|█████████▋| 424/437 [00:08<00:00, 47.23it/s] 98%|█████████▊| 429/437 [00:09<00:00, 47.31it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.14it/s]100%|██████████| 437/437 [00:09<00:00, 47.48it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:39:03,267 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:39:03,268 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:39:03,268 >>   eval_loss               =     1.1417
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:39:03,268 >>   eval_runtime            = 0:00:09.22
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:39:03,268 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:39:03,268 >>   eval_samples_per_second =    378.541
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:39:03,268 >>   eval_steps_per_second   =     47.372
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:39:03,268 >>   perplexity              =      3.132
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:10,847 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:10,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:10,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:10,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:10,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:39:11,446 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:39:11,447 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:39:12,291 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:39:13,319 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:39:13,319 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:17,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:17,804 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:17,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:17,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:17,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:39:19,206 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:39:19,207 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:39:20,126 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:39:20,275 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:39:20,275 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.43it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.45it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:10,  1.44it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.42it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.46it/s]Extractor Predicting: 21it [00:14,  1.47it/s]Extractor Predicting: 22it [00:15,  1.47it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:16,  1.50it/s]Extractor Predicting: 25it [00:17,  1.49it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:18,  1.49it/s]Extractor Predicting: 28it [00:19,  1.48it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:20,  1.42it/s]Extractor Predicting: 31it [00:21,  1.45it/s]Extractor Predicting: 32it [00:21,  1.47it/s]Extractor Predicting: 33it [00:22,  1.47it/s]Extractor Predicting: 34it [00:23,  1.46it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.47it/s]Extractor Predicting: 37it [00:25,  1.46it/s]Extractor Predicting: 38it [00:25,  1.46it/s]Extractor Predicting: 39it [00:26,  1.47it/s]Extractor Predicting: 40it [00:27,  1.47it/s]Extractor Predicting: 41it [00:27,  1.47it/s]Extractor Predicting: 42it [00:28,  1.45it/s]Extractor Predicting: 43it [00:29,  1.47it/s]Extractor Predicting: 44it [00:29,  1.47it/s]Extractor Predicting: 45it [00:30,  1.45it/s]Extractor Predicting: 46it [00:31,  1.45it/s]Extractor Predicting: 47it [00:32,  1.46it/s]Extractor Predicting: 48it [00:32,  1.46it/s]Extractor Predicting: 49it [00:33,  1.47it/s]Extractor Predicting: 50it [00:34,  1.49it/s]Extractor Predicting: 51it [00:34,  1.45it/s]Extractor Predicting: 52it [00:35,  1.44it/s]Extractor Predicting: 53it [00:36,  1.45it/s]Extractor Predicting: 54it [00:36,  1.45it/s]Extractor Predicting: 55it [00:37,  1.44it/s]Extractor Predicting: 56it [00:38,  1.45it/s]Extractor Predicting: 57it [00:38,  1.45it/s]Extractor Predicting: 58it [00:39,  1.48it/s]Extractor Predicting: 59it [00:40,  1.46it/s]Extractor Predicting: 60it [00:41,  1.43it/s]Extractor Predicting: 61it [00:41,  1.42it/s]Extractor Predicting: 62it [00:42,  1.43it/s]Extractor Predicting: 63it [00:43,  1.44it/s]Extractor Predicting: 64it [00:43,  1.41it/s]Extractor Predicting: 65it [00:44,  1.42it/s]Extractor Predicting: 66it [00:45,  1.30it/s]Extractor Predicting: 67it [00:46,  1.33it/s]Extractor Predicting: 68it [00:46,  1.33it/s]Extractor Predicting: 69it [00:47,  1.35it/s]Extractor Predicting: 70it [00:48,  1.36it/s]Extractor Predicting: 71it [00:49,  1.36it/s]Extractor Predicting: 72it [00:49,  1.35it/s]Extractor Predicting: 73it [00:50,  1.37it/s]Extractor Predicting: 74it [00:51,  1.35it/s]Extractor Predicting: 75it [00:52,  1.38it/s]Extractor Predicting: 76it [00:52,  1.39it/s]Extractor Predicting: 77it [00:53,  1.38it/s]Extractor Predicting: 78it [00:54,  1.40it/s]Extractor Predicting: 79it [00:54,  1.37it/s]Extractor Predicting: 80it [00:55,  1.36it/s]Extractor Predicting: 81it [00:56,  1.36it/s]Extractor Predicting: 82it [00:57,  1.39it/s]Extractor Predicting: 83it [00:57,  1.40it/s]Extractor Predicting: 84it [00:58,  1.42it/s]Extractor Predicting: 85it [00:59,  1.41it/s]Extractor Predicting: 86it [00:59,  1.38it/s]Extractor Predicting: 87it [01:00,  1.40it/s]Extractor Predicting: 88it [01:01,  1.42it/s]Extractor Predicting: 89it [01:01,  1.47it/s]Extractor Predicting: 90it [01:02,  1.47it/s]Extractor Predicting: 91it [01:03,  1.52it/s]Extractor Predicting: 92it [01:03,  1.58it/s]Extractor Predicting: 93it [01:04,  1.61it/s]Extractor Predicting: 94it [01:04,  1.63it/s]Extractor Predicting: 95it [01:05,  1.57it/s]Extractor Predicting: 96it [01:06,  1.59it/s]Extractor Predicting: 97it [01:06,  1.55it/s]Extractor Predicting: 98it [01:07,  1.55it/s]Extractor Predicting: 99it [01:08,  1.60it/s]Extractor Predicting: 100it [01:08,  1.61it/s]Extractor Predicting: 101it [01:09,  1.58it/s]Extractor Predicting: 102it [01:10,  1.53it/s]Extractor Predicting: 103it [01:10,  1.53it/s]Extractor Predicting: 104it [01:11,  1.49it/s]Extractor Predicting: 105it [01:12,  1.50it/s]Extractor Predicting: 106it [01:12,  1.51it/s]Extractor Predicting: 107it [01:13,  1.49it/s]Extractor Predicting: 108it [01:14,  1.53it/s]Extractor Predicting: 109it [01:14,  1.55it/s]Extractor Predicting: 110it [01:15,  1.56it/s]Extractor Predicting: 111it [01:16,  1.57it/s]Extractor Predicting: 112it [01:16,  1.57it/s]Extractor Predicting: 113it [01:17,  1.60it/s]Extractor Predicting: 114it [01:17,  1.55it/s]Extractor Predicting: 115it [01:18,  1.58it/s]Extractor Predicting: 116it [01:19,  1.54it/s]Extractor Predicting: 117it [01:19,  1.51it/s]Extractor Predicting: 118it [01:20,  1.51it/s]Extractor Predicting: 119it [01:21,  1.48it/s]Extractor Predicting: 120it [01:22,  1.44it/s]Extractor Predicting: 121it [01:22,  1.45it/s]Extractor Predicting: 122it [01:23,  1.45it/s]Extractor Predicting: 123it [01:24,  1.44it/s]Extractor Predicting: 124it [01:24,  1.42it/s]Extractor Predicting: 125it [01:25,  1.42it/s]Extractor Predicting: 126it [01:26,  1.41it/s]Extractor Predicting: 127it [01:26,  1.40it/s]Extractor Predicting: 128it [01:27,  1.41it/s]Extractor Predicting: 129it [01:28,  1.43it/s]Extractor Predicting: 130it [01:29,  1.47it/s]Extractor Predicting: 131it [01:29,  1.44it/s]Extractor Predicting: 132it [01:30,  1.43it/s]Extractor Predicting: 133it [01:31,  1.42it/s]Extractor Predicting: 134it [01:31,  1.43it/s]Extractor Predicting: 135it [01:32,  1.43it/s]Extractor Predicting: 136it [01:33,  1.43it/s]Extractor Predicting: 137it [01:33,  1.43it/s]Extractor Predicting: 138it [01:34,  1.46it/s]Extractor Predicting: 139it [01:35,  1.46it/s]Extractor Predicting: 140it [01:35,  1.48it/s]Extractor Predicting: 141it [01:36,  1.44it/s]Extractor Predicting: 142it [01:37,  1.43it/s]Extractor Predicting: 143it [01:38,  1.45it/s]Extractor Predicting: 144it [01:38,  1.47it/s]Extractor Predicting: 144it [01:38,  1.46it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:08,716 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:08,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:08,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:08,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:08,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:41:09,403 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:41:09,408 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:41:09,686 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:41:10,694 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:41:10,694 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:12,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:12,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:12,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:12,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:12,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:41:13,616 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:41:13,617 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:41:13,918 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:41:14,065 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:41:14,065 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5238954012623985,
  "recall": 0.16638029782359678,
  "score": 0.2525537926537709,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.51it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.52it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:12,  1.50it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:14,  1.48it/s]Extractor Predicting: 22it [00:14,  1.44it/s]Extractor Predicting: 23it [00:15,  1.46it/s]Extractor Predicting: 24it [00:16,  1.49it/s]Extractor Predicting: 25it [00:16,  1.50it/s]Extractor Predicting: 26it [00:17,  1.45it/s]Extractor Predicting: 27it [00:18,  1.45it/s]Extractor Predicting: 28it [00:18,  1.45it/s]Extractor Predicting: 29it [00:19,  1.43it/s]Extractor Predicting: 30it [00:20,  1.41it/s]Extractor Predicting: 31it [00:21,  1.42it/s]Extractor Predicting: 32it [00:21,  1.41it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:23,  1.46it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:24,  1.49it/s]Extractor Predicting: 37it [00:25,  1.53it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:26,  1.52it/s]Extractor Predicting: 40it [00:27,  1.51it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:29,  1.50it/s]Extractor Predicting: 44it [00:29,  1.50it/s]Extractor Predicting: 45it [00:30,  1.45it/s]Extractor Predicting: 46it [00:31,  1.48it/s]Extractor Predicting: 47it [00:31,  1.49it/s]Extractor Predicting: 48it [00:32,  1.50it/s]Extractor Predicting: 49it [00:33,  1.49it/s]Extractor Predicting: 50it [00:33,  1.48it/s]Extractor Predicting: 51it [00:34,  1.49it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:37,  1.47it/s]Extractor Predicting: 56it [00:37,  1.48it/s]Extractor Predicting: 57it [00:38,  1.51it/s]Extractor Predicting: 58it [00:39,  1.54it/s]Extractor Predicting: 59it [00:39,  1.57it/s]Extractor Predicting: 60it [00:40,  1.54it/s]Extractor Predicting: 61it [00:41,  1.53it/s]Extractor Predicting: 62it [00:41,  1.53it/s]Extractor Predicting: 63it [00:42,  1.54it/s]Extractor Predicting: 64it [00:43,  1.50it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:44,  1.49it/s]Extractor Predicting: 67it [00:45,  1.50it/s]Extractor Predicting: 68it [00:45,  1.56it/s]Extractor Predicting: 69it [00:46,  1.53it/s]Extractor Predicting: 70it [00:47,  1.50it/s]Extractor Predicting: 71it [00:47,  1.50it/s]Extractor Predicting: 72it [00:48,  1.52it/s]Extractor Predicting: 73it [00:49,  1.49it/s]Extractor Predicting: 74it [00:49,  1.49it/s]Extractor Predicting: 75it [00:50,  1.48it/s]Extractor Predicting: 76it [00:51,  1.50it/s]Extractor Predicting: 77it [00:51,  1.48it/s]Extractor Predicting: 78it [00:52,  1.49it/s]Extractor Predicting: 79it [00:53,  1.50it/s]Extractor Predicting: 80it [00:53,  1.50it/s]Extractor Predicting: 81it [00:54,  1.47it/s]Extractor Predicting: 82it [00:55,  1.47it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:56,  1.49it/s]Extractor Predicting: 85it [00:57,  1.52it/s]Extractor Predicting: 86it [00:57,  1.50it/s]Extractor Predicting: 87it [00:58,  1.50it/s]Extractor Predicting: 88it [00:59,  1.49it/s]Extractor Predicting: 89it [00:59,  1.51it/s]Extractor Predicting: 90it [01:00,  1.51it/s]Extractor Predicting: 91it [01:01,  1.38it/s]Extractor Predicting: 92it [01:01,  1.39it/s]Extractor Predicting: 93it [01:02,  1.44it/s]Extractor Predicting: 94it [01:03,  1.43it/s]Extractor Predicting: 95it [01:03,  1.44it/s]Extractor Predicting: 96it [01:04,  1.46it/s]Extractor Predicting: 97it [01:05,  1.48it/s]Extractor Predicting: 98it [01:05,  1.48it/s]Extractor Predicting: 99it [01:06,  1.49it/s]Extractor Predicting: 100it [01:07,  1.52it/s]Extractor Predicting: 101it [01:07,  1.52it/s]Extractor Predicting: 102it [01:08,  1.52it/s]Extractor Predicting: 103it [01:09,  1.48it/s]Extractor Predicting: 104it [01:10,  1.39it/s]Extractor Predicting: 105it [01:10,  1.42it/s]Extractor Predicting: 106it [01:11,  1.30it/s]Extractor Predicting: 107it [01:12,  1.35it/s]Extractor Predicting: 108it [01:13,  1.39it/s]Extractor Predicting: 109it [01:13,  1.41it/s]Extractor Predicting: 110it [01:14,  1.43it/s]Extractor Predicting: 111it [01:15,  1.42it/s]Extractor Predicting: 112it [01:15,  1.42it/s]Extractor Predicting: 113it [01:16,  1.41it/s]Extractor Predicting: 114it [01:17,  1.41it/s]Extractor Predicting: 115it [01:17,  1.42it/s]Extractor Predicting: 116it [01:18,  1.47it/s]Extractor Predicting: 117it [01:19,  1.48it/s]Extractor Predicting: 118it [01:19,  1.48it/s]Extractor Predicting: 119it [01:20,  1.48it/s]Extractor Predicting: 120it [01:21,  1.48it/s]Extractor Predicting: 121it [01:21,  1.49it/s]Extractor Predicting: 122it [01:22,  1.51it/s]Extractor Predicting: 123it [01:23,  1.53it/s]Extractor Predicting: 124it [01:23,  1.54it/s]Extractor Predicting: 125it [01:24,  1.54it/s]Extractor Predicting: 126it [01:25,  1.53it/s]Extractor Predicting: 127it [01:25,  1.54it/s]Extractor Predicting: 128it [01:26,  1.52it/s]Extractor Predicting: 129it [01:27,  1.49it/s]Extractor Predicting: 130it [01:27,  1.48it/s]Extractor Predicting: 131it [01:28,  1.52it/s]Extractor Predicting: 132it [01:29,  1.51it/s]Extractor Predicting: 133it [01:29,  1.52it/s]Extractor Predicting: 134it [01:30,  1.52it/s]Extractor Predicting: 135it [01:31,  1.53it/s]Extractor Predicting: 136it [01:31,  1.52it/s]Extractor Predicting: 137it [01:32,  1.52it/s]Extractor Predicting: 138it [01:33,  1.50it/s]Extractor Predicting: 139it [01:33,  1.51it/s]Extractor Predicting: 140it [01:34,  1.57it/s]Extractor Predicting: 141it [01:34,  1.57it/s]Extractor Predicting: 142it [01:35,  1.54it/s]Extractor Predicting: 143it [01:36,  1.52it/s]Extractor Predicting: 144it [01:36,  1.53it/s]Extractor Predicting: 145it [01:37,  1.54it/s]Extractor Predicting: 146it [01:38,  1.56it/s]Extractor Predicting: 147it [01:38,  1.57it/s]Extractor Predicting: 148it [01:39,  1.51it/s]Extractor Predicting: 149it [01:40,  1.53it/s]Extractor Predicting: 150it [01:40,  1.53it/s]Extractor Predicting: 151it [01:41,  1.52it/s]Extractor Predicting: 152it [01:42,  1.53it/s]Extractor Predicting: 153it [01:42,  1.51it/s]Extractor Predicting: 154it [01:43,  1.54it/s]Extractor Predicting: 155it [01:44,  1.55it/s]Extractor Predicting: 156it [01:44,  1.57it/s]Extractor Predicting: 157it [01:45,  1.56it/s]Extractor Predicting: 158it [01:45,  1.62it/s]Extractor Predicting: 159it [01:46,  1.58it/s]Extractor Predicting: 160it [01:47,  1.55it/s]Extractor Predicting: 161it [01:47,  1.53it/s]Extractor Predicting: 162it [01:48,  1.51it/s]Extractor Predicting: 163it [01:49,  1.54it/s]Extractor Predicting: 164it [01:49,  1.54it/s]Extractor Predicting: 165it [01:50,  1.56it/s]Extractor Predicting: 166it [01:51,  1.58it/s]Extractor Predicting: 167it [01:51,  1.57it/s]Extractor Predicting: 168it [01:52,  1.52it/s]Extractor Predicting: 169it [01:53,  1.53it/s]Extractor Predicting: 170it [01:53,  1.52it/s]Extractor Predicting: 171it [01:54,  1.53it/s]Extractor Predicting: 172it [01:55,  1.53it/s]Extractor Predicting: 173it [01:55,  1.48it/s]Extractor Predicting: 174it [01:56,  1.49it/s]Extractor Predicting: 175it [01:57,  1.47it/s]Extractor Predicting: 176it [01:57,  1.45it/s]Extractor Predicting: 177it [01:58,  1.50it/s]Extractor Predicting: 178it [01:59,  1.49it/s]Extractor Predicting: 179it [01:59,  1.52it/s]Extractor Predicting: 180it [02:00,  1.55it/s]Extractor Predicting: 181it [02:01,  1.51it/s]Extractor Predicting: 182it [02:01,  1.51it/s]Extractor Predicting: 183it [02:02,  1.51it/s]Extractor Predicting: 184it [02:03,  1.36it/s]Extractor Predicting: 185it [02:04,  1.39it/s]Extractor Predicting: 186it [02:04,  1.41it/s]Extractor Predicting: 187it [02:05,  1.44it/s]Extractor Predicting: 188it [02:06,  1.42it/s]Extractor Predicting: 189it [02:06,  1.43it/s]Extractor Predicting: 190it [02:07,  1.44it/s]Extractor Predicting: 191it [02:08,  1.48it/s]Extractor Predicting: 192it [02:08,  1.51it/s]Extractor Predicting: 193it [02:09,  1.46it/s]Extractor Predicting: 194it [02:10,  1.44it/s]Extractor Predicting: 195it [02:10,  1.48it/s]Extractor Predicting: 196it [02:11,  1.46it/s]Extractor Predicting: 197it [02:12,  1.48it/s]Extractor Predicting: 198it [02:12,  1.46it/s]Extractor Predicting: 199it [02:13,  1.38it/s]Extractor Predicting: 200it [02:14,  1.46it/s]Extractor Predicting: 201it [02:14,  1.50it/s]Extractor Predicting: 202it [02:15,  1.51it/s]Extractor Predicting: 203it [02:16,  1.42it/s]Extractor Predicting: 204it [02:17,  1.44it/s]Extractor Predicting: 205it [02:17,  1.47it/s]Extractor Predicting: 206it [02:18,  1.47it/s]Extractor Predicting: 207it [02:19,  1.49it/s]Extractor Predicting: 208it [02:19,  1.46it/s]Extractor Predicting: 209it [02:20,  1.48it/s]Extractor Predicting: 210it [02:21,  1.48it/s]Extractor Predicting: 211it [02:21,  1.48it/s]Extractor Predicting: 212it [02:22,  1.49it/s]Extractor Predicting: 213it [02:23,  1.49it/s]Extractor Predicting: 214it [02:23,  1.47it/s]Extractor Predicting: 215it [02:24,  1.47it/s]Extractor Predicting: 216it [02:25,  1.50it/s]Extractor Predicting: 217it [02:25,  1.51it/s]Extractor Predicting: 218it [02:26,  1.49it/s]Extractor Predicting: 219it [02:27,  1.47it/s]Extractor Predicting: 220it [02:27,  1.49it/s]Extractor Predicting: 221it [02:28,  1.50it/s]Extractor Predicting: 222it [02:29,  1.49it/s]Extractor Predicting: 223it [02:29,  1.52it/s]Extractor Predicting: 224it [02:30,  1.51it/s]Extractor Predicting: 225it [02:31,  1.50it/s]Extractor Predicting: 226it [02:31,  1.48it/s]Extractor Predicting: 227it [02:32,  1.49it/s]Extractor Predicting: 228it [02:33,  1.51it/s]Extractor Predicting: 229it [02:33,  1.52it/s]Extractor Predicting: 230it [02:34,  1.54it/s]Extractor Predicting: 231it [02:35,  1.52it/s]Extractor Predicting: 232it [02:35,  1.52it/s]Extractor Predicting: 233it [02:36,  1.52it/s]Extractor Predicting: 234it [02:37,  1.53it/s]Extractor Predicting: 235it [02:37,  1.53it/s]Extractor Predicting: 236it [02:38,  1.53it/s]Extractor Predicting: 237it [02:39,  1.52it/s]Extractor Predicting: 238it [02:39,  1.52it/s]Extractor Predicting: 239it [02:40,  1.51it/s]Extractor Predicting: 240it [02:40,  1.54it/s]Extractor Predicting: 241it [02:41,  1.54it/s]Extractor Predicting: 242it [02:42,  1.56it/s]Extractor Predicting: 243it [02:42,  1.55it/s]Extractor Predicting: 244it [02:43,  1.55it/s]Extractor Predicting: 245it [02:44,  1.54it/s]Extractor Predicting: 246it [02:44,  1.53it/s]Extractor Predicting: 247it [02:45,  1.55it/s]Extractor Predicting: 248it [02:46,  1.54it/s]Extractor Predicting: 249it [02:46,  1.55it/s]Extractor Predicting: 250it [02:47,  1.57it/s]Extractor Predicting: 251it [02:48,  1.52it/s]Extractor Predicting: 252it [02:48,  1.52it/s]Extractor Predicting: 253it [02:49,  1.56it/s]Extractor Predicting: 254it [02:50,  1.54it/s]Extractor Predicting: 255it [02:50,  1.52it/s]Extractor Predicting: 256it [02:51,  1.51it/s]Extractor Predicting: 257it [02:52,  1.52it/s]Extractor Predicting: 258it [02:52,  1.55it/s]Extractor Predicting: 259it [02:53,  1.57it/s]Extractor Predicting: 260it [02:53,  1.57it/s]Extractor Predicting: 261it [02:54,  1.54it/s]Extractor Predicting: 262it [02:55,  1.55it/s]Extractor Predicting: 263it [02:55,  1.55it/s]Extractor Predicting: 264it [02:56,  1.57it/s]Extractor Predicting: 265it [02:57,  1.58it/s]Extractor Predicting: 266it [02:57,  1.61it/s]Extractor Predicting: 267it [02:58,  1.56it/s]Extractor Predicting: 268it [02:59,  1.40it/s]Extractor Predicting: 269it [02:59,  1.45it/s]Extractor Predicting: 270it [03:00,  1.48it/s]Extractor Predicting: 271it [03:01,  1.52it/s]Extractor Predicting: 272it [03:01,  1.56it/s]Extractor Predicting: 273it [03:02,  1.54it/s]Extractor Predicting: 274it [03:03,  1.55it/s]Extractor Predicting: 275it [03:03,  1.54it/s]Extractor Predicting: 276it [03:04,  1.54it/s]Extractor Predicting: 277it [03:05,  1.54it/s]Extractor Predicting: 278it [03:05,  1.54it/s]Extractor Predicting: 279it [03:06,  1.51it/s]Extractor Predicting: 280it [03:07,  1.52it/s]Extractor Predicting: 281it [03:07,  1.49it/s]Extractor Predicting: 281it [03:07,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:33,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:33,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:33,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:33,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:33,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:44:33,878 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:44:33,879 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:44:34,548 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:44:35,583 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:44:35,583 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:39,028 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:39,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:39,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:39,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:39,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:44:39,726 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:44:39,733 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:44:40,312 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:44:40,480 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:44:40,480 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4559868780754511,
  "recall": 0.24736764051609075,
  "score": 0.3207383905393712,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.34it/s]Extractor Predicting: 3it [00:02,  1.38it/s]Extractor Predicting: 4it [00:02,  1.40it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:03,  1.83it/s]Extractor Predicting: 6it [00:03,  1.57it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2,
  "recall": 0.04669260700389105,
  "score": 0.07570977917981073,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_0', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:12, 12.92s/it]Extractor Predicting: 2it [00:14,  6.01s/it]Extractor Predicting: 3it [00:15,  3.78s/it]Extractor Predicting: 4it [00:15,  2.50s/it]Extractor Predicting: 5it [00:17,  2.20s/it]Extractor Predicting: 6it [00:19,  2.03s/it]Extractor Predicting: 7it [00:19,  1.56s/it]Extractor Predicting: 8it [00:20,  1.24s/it]Extractor Predicting: 9it [00:20,  1.02s/it]Extractor Predicting: 10it [00:21,  1.14it/s]Extractor Predicting: 11it [00:21,  1.29it/s]Extractor Predicting: 12it [00:22,  1.41it/s]Extractor Predicting: 13it [00:23,  1.45it/s]Extractor Predicting: 14it [00:23,  1.48it/s]Extractor Predicting: 15it [00:24,  1.48it/s]Extractor Predicting: 16it [00:25,  1.52it/s]Extractor Predicting: 17it [00:25,  1.53it/s]Extractor Predicting: 18it [00:26,  1.52it/s]Extractor Predicting: 19it [00:27,  1.49it/s]Extractor Predicting: 20it [00:28,  1.33it/s]Extractor Predicting: 21it [00:28,  1.40it/s]Extractor Predicting: 22it [00:29,  1.47it/s]Extractor Predicting: 23it [00:29,  1.49it/s]Extractor Predicting: 24it [00:30,  1.51it/s]Extractor Predicting: 25it [00:31,  1.53it/s]Extractor Predicting: 26it [00:31,  1.54it/s]Extractor Predicting: 27it [00:32,  1.54it/s]Extractor Predicting: 28it [00:33,  1.54it/s]Extractor Predicting: 29it [00:33,  1.52it/s]Extractor Predicting: 30it [00:34,  1.54it/s]Extractor Predicting: 31it [00:35,  1.55it/s]Extractor Predicting: 32it [00:35,  1.56it/s]Extractor Predicting: 33it [00:36,  1.57it/s]Extractor Predicting: 34it [00:36,  1.55it/s]Extractor Predicting: 35it [00:37,  1.54it/s]Extractor Predicting: 36it [00:38,  1.51it/s]Extractor Predicting: 37it [00:38,  1.52it/s]Extractor Predicting: 38it [00:39,  1.50it/s]Extractor Predicting: 39it [00:40,  1.53it/s]Extractor Predicting: 40it [00:40,  1.54it/s]Extractor Predicting: 41it [00:41,  1.51it/s]Extractor Predicting: 42it [00:42,  1.49it/s]Extractor Predicting: 43it [00:42,  1.52it/s]Extractor Predicting: 44it [00:43,  1.54it/s]Extractor Predicting: 45it [00:44,  1.55it/s]Extractor Predicting: 46it [00:44,  1.56it/s]Extractor Predicting: 47it [00:45,  1.56it/s]Extractor Predicting: 48it [00:46,  1.55it/s]Extractor Predicting: 49it [00:46,  1.54it/s]Extractor Predicting: 50it [00:47,  1.45it/s]Extractor Predicting: 51it [00:48,  1.49it/s]Extractor Predicting: 52it [00:48,  1.48it/s]Extractor Predicting: 53it [00:49,  1.47it/s]Extractor Predicting: 54it [00:50,  1.51it/s]Extractor Predicting: 55it [00:50,  1.56it/s]Extractor Predicting: 56it [00:51,  1.53it/s]Extractor Predicting: 57it [00:52,  1.52it/s]Extractor Predicting: 58it [00:52,  1.56it/s]Extractor Predicting: 59it [00:53,  1.21it/s]Extractor Predicting: 60it [00:54,  1.30it/s]Extractor Predicting: 61it [00:55,  1.38it/s]Extractor Predicting: 62it [00:55,  1.43it/s]Extractor Predicting: 63it [00:56,  1.47it/s]Extractor Predicting: 64it [00:57,  1.50it/s]Extractor Predicting: 65it [00:57,  1.56it/s]Extractor Predicting: 66it [00:58,  1.57it/s]Extractor Predicting: 67it [00:58,  1.58it/s]Extractor Predicting: 68it [00:59,  1.55it/s]Extractor Predicting: 69it [01:00,  1.56it/s]Extractor Predicting: 70it [01:00,  1.55it/s]Extractor Predicting: 71it [01:01,  1.55it/s]Extractor Predicting: 72it [01:02,  1.56it/s]Extractor Predicting: 73it [01:02,  1.54it/s]Extractor Predicting: 74it [01:03,  1.51it/s]Extractor Predicting: 75it [01:04,  1.47it/s]Extractor Predicting: 76it [01:04,  1.49it/s]Extractor Predicting: 77it [01:05,  1.53it/s]Extractor Predicting: 78it [01:06,  1.52it/s]Extractor Predicting: 79it [01:06,  1.51it/s]Extractor Predicting: 80it [01:07,  1.54it/s]Extractor Predicting: 81it [01:08,  1.55it/s]Extractor Predicting: 82it [01:08,  1.56it/s]Extractor Predicting: 83it [01:09,  1.58it/s]Extractor Predicting: 84it [01:10,  1.58it/s]Extractor Predicting: 85it [01:10,  1.60it/s]Extractor Predicting: 86it [01:11,  1.57it/s]Extractor Predicting: 87it [01:11,  1.56it/s]Extractor Predicting: 88it [01:12,  1.59it/s]Extractor Predicting: 89it [01:13,  1.60it/s]Extractor Predicting: 90it [01:13,  1.55it/s]Extractor Predicting: 91it [01:14,  1.54it/s]Extractor Predicting: 92it [01:15,  1.51it/s]Extractor Predicting: 93it [01:15,  1.57it/s]Extractor Predicting: 94it [01:16,  1.59it/s]Extractor Predicting: 95it [01:17,  1.57it/s]Extractor Predicting: 96it [01:17,  1.59it/s]Extractor Predicting: 97it [01:18,  1.61it/s]Extractor Predicting: 98it [01:18,  1.59it/s]Extractor Predicting: 99it [01:19,  1.56it/s]Extractor Predicting: 100it [01:20,  1.57it/s]Extractor Predicting: 101it [01:20,  1.57it/s]Extractor Predicting: 102it [01:21,  1.55it/s]Extractor Predicting: 103it [01:22,  1.56it/s]Extractor Predicting: 104it [01:22,  1.57it/s]Extractor Predicting: 105it [01:23,  1.55it/s]Extractor Predicting: 106it [01:24,  1.49it/s]Extractor Predicting: 107it [01:24,  1.50it/s]Extractor Predicting: 108it [01:25,  1.51it/s]Extractor Predicting: 109it [01:26,  1.53it/s]Extractor Predicting: 110it [01:26,  1.53it/s]Extractor Predicting: 111it [01:27,  1.55it/s]Extractor Predicting: 112it [01:27,  1.59it/s]Extractor Predicting: 113it [01:28,  1.58it/s]Extractor Predicting: 114it [01:29,  1.43it/s]Extractor Predicting: 115it [01:30,  1.46it/s]Extractor Predicting: 116it [01:30,  1.49it/s]Extractor Predicting: 117it [01:31,  1.53it/s]Extractor Predicting: 118it [01:32,  1.40it/s]Extractor Predicting: 119it [01:32,  1.45it/s]Extractor Predicting: 120it [01:33,  1.47it/s]Extractor Predicting: 121it [01:34,  1.48it/s]Extractor Predicting: 122it [01:34,  1.48it/s]Extractor Predicting: 123it [01:35,  1.53it/s]Extractor Predicting: 124it [01:36,  1.55it/s]Extractor Predicting: 125it [01:36,  1.57it/s]Extractor Predicting: 126it [01:37,  1.54it/s]Extractor Predicting: 127it [01:38,  1.52it/s]Extractor Predicting: 128it [01:38,  1.55it/s]Extractor Predicting: 129it [01:39,  1.57it/s]Extractor Predicting: 130it [01:39,  1.61it/s]Extractor Predicting: 131it [01:40,  1.58it/s]Extractor Predicting: 132it [01:41,  1.57it/s]Extractor Predicting: 133it [01:41,  1.57it/s]Extractor Predicting: 134it [01:42,  1.57it/s]Extractor Predicting: 135it [01:43,  1.59it/s]Extractor Predicting: 136it [01:43,  1.59it/s]Extractor Predicting: 137it [01:44,  1.60it/s]Extractor Predicting: 138it [01:45,  1.55it/s]Extractor Predicting: 139it [01:45,  1.53it/s]Extractor Predicting: 140it [01:46,  1.57it/s]Extractor Predicting: 141it [01:46,  1.57it/s]Extractor Predicting: 142it [01:47,  1.60it/s]Extractor Predicting: 143it [01:48,  1.60it/s]Extractor Predicting: 144it [01:48,  1.61it/s]Extractor Predicting: 145it [01:49,  1.58it/s]Extractor Predicting: 146it [01:50,  1.57it/s]Extractor Predicting: 147it [01:50,  1.58it/s]Extractor Predicting: 148it [01:51,  1.60it/s]Extractor Predicting: 149it [01:51,  1.61it/s]Extractor Predicting: 150it [01:52,  1.55it/s]Extractor Predicting: 151it [01:53,  1.56it/s]Extractor Predicting: 152it [01:53,  1.57it/s]Extractor Predicting: 153it [01:54,  1.54it/s]Extractor Predicting: 154it [01:55,  1.56it/s]Extractor Predicting: 155it [01:55,  1.53it/s]Extractor Predicting: 156it [01:56,  1.52it/s]Extractor Predicting: 157it [01:57,  1.53it/s]Extractor Predicting: 158it [01:57,  1.54it/s]Extractor Predicting: 159it [01:58,  1.56it/s]Extractor Predicting: 160it [01:59,  1.59it/s]Extractor Predicting: 161it [01:59,  1.60it/s]Extractor Predicting: 162it [02:00,  1.61it/s]Extractor Predicting: 163it [02:00,  1.53it/s]Extractor Predicting: 163it [02:00,  1.35it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.33819241982507287,
  "recall": 0.026715799170888992,
  "score": 0.04951974386339381,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:05,  1.62it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:12,  1.65it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:14,  1.60it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:17,  1.65it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:18,  1.66it/s]Extractor Predicting: 32it [00:19,  1.66it/s]Extractor Predicting: 33it [00:20,  1.65it/s]Extractor Predicting: 34it [00:20,  1.64it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:22,  1.64it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.62it/s]Extractor Predicting: 39it [00:23,  1.67it/s]Extractor Predicting: 40it [00:24,  1.65it/s]Extractor Predicting: 41it [00:25,  1.54it/s]Extractor Predicting: 42it [00:25,  1.56it/s]Extractor Predicting: 43it [00:26,  1.55it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:27,  1.54it/s]Extractor Predicting: 46it [00:28,  1.53it/s]Extractor Predicting: 47it [00:29,  1.51it/s]Extractor Predicting: 48it [00:29,  1.54it/s]Extractor Predicting: 49it [00:30,  1.52it/s]Extractor Predicting: 50it [00:31,  1.54it/s]Extractor Predicting: 51it [00:31,  1.54it/s]Extractor Predicting: 52it [00:32,  1.53it/s]Extractor Predicting: 53it [00:33,  1.52it/s]Extractor Predicting: 54it [00:34,  1.20it/s]Extractor Predicting: 55it [00:34,  1.30it/s]Extractor Predicting: 56it [00:35,  1.40it/s]Extractor Predicting: 57it [00:36,  1.43it/s]Extractor Predicting: 58it [00:36,  1.46it/s]Extractor Predicting: 59it [00:37,  1.44it/s]Extractor Predicting: 60it [00:38,  1.47it/s]Extractor Predicting: 61it [00:38,  1.51it/s]Extractor Predicting: 62it [00:39,  1.52it/s]Extractor Predicting: 63it [00:40,  1.47it/s]Extractor Predicting: 64it [00:40,  1.49it/s]Extractor Predicting: 65it [00:41,  1.51it/s]Extractor Predicting: 66it [00:42,  1.41it/s]Extractor Predicting: 67it [00:42,  1.45it/s]Extractor Predicting: 68it [00:43,  1.45it/s]Extractor Predicting: 69it [00:44,  1.47it/s]Extractor Predicting: 70it [00:44,  1.49it/s]Extractor Predicting: 71it [00:45,  1.51it/s]Extractor Predicting: 72it [00:46,  1.50it/s]Extractor Predicting: 73it [00:46,  1.50it/s]Extractor Predicting: 74it [00:47,  1.54it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:48,  1.55it/s]Extractor Predicting: 77it [00:49,  1.58it/s]Extractor Predicting: 78it [00:50,  1.55it/s]Extractor Predicting: 79it [00:50,  1.51it/s]Extractor Predicting: 80it [00:51,  1.55it/s]Extractor Predicting: 81it [00:53,  1.21s/it]Extractor Predicting: 82it [00:54,  1.03s/it]Extractor Predicting: 83it [00:55,  1.10it/s]Extractor Predicting: 84it [00:55,  1.17it/s]Extractor Predicting: 85it [00:56,  1.26it/s]Extractor Predicting: 86it [00:57,  1.34it/s]Extractor Predicting: 87it [00:57,  1.41it/s]Extractor Predicting: 88it [00:58,  1.41it/s]Extractor Predicting: 89it [00:59,  1.48it/s]Extractor Predicting: 90it [00:59,  1.51it/s]Extractor Predicting: 91it [01:00,  1.55it/s]Extractor Predicting: 92it [01:00,  1.56it/s]Extractor Predicting: 93it [01:01,  1.56it/s]Extractor Predicting: 94it [01:02,  1.54it/s]Extractor Predicting: 95it [01:02,  1.56it/s]Extractor Predicting: 96it [01:03,  1.59it/s]Extractor Predicting: 97it [01:04,  1.45it/s]Extractor Predicting: 98it [01:05,  1.44it/s]Extractor Predicting: 99it [01:05,  1.48it/s]Extractor Predicting: 100it [01:06,  1.50it/s]Extractor Predicting: 101it [01:06,  1.51it/s]Extractor Predicting: 102it [01:07,  1.54it/s]Extractor Predicting: 103it [01:08,  1.53it/s]Extractor Predicting: 104it [01:08,  1.56it/s]Extractor Predicting: 105it [01:09,  1.56it/s]Extractor Predicting: 106it [01:10,  1.52it/s]Extractor Predicting: 107it [01:10,  1.50it/s]Extractor Predicting: 108it [01:11,  1.54it/s]Extractor Predicting: 109it [01:12,  1.48it/s]Extractor Predicting: 110it [01:12,  1.51it/s]Extractor Predicting: 111it [01:13,  1.50it/s]Extractor Predicting: 112it [01:14,  1.52it/s]Extractor Predicting: 113it [01:14,  1.51it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:16,  1.52it/s]Extractor Predicting: 116it [01:16,  1.50it/s]Extractor Predicting: 117it [01:17,  1.52it/s]Extractor Predicting: 118it [01:18,  1.56it/s]Extractor Predicting: 119it [01:18,  1.56it/s]Extractor Predicting: 120it [01:19,  1.60it/s]Extractor Predicting: 121it [01:19,  1.59it/s]Extractor Predicting: 122it [01:20,  1.62it/s]Extractor Predicting: 123it [01:21,  1.62it/s]Extractor Predicting: 124it [01:21,  1.64it/s]Extractor Predicting: 125it [01:22,  1.63it/s]Extractor Predicting: 126it [01:22,  1.60it/s]Extractor Predicting: 127it [01:23,  1.60it/s]Extractor Predicting: 128it [01:24,  1.58it/s]Extractor Predicting: 129it [01:24,  1.58it/s]Extractor Predicting: 130it [01:25,  1.58it/s]Extractor Predicting: 131it [01:26,  1.63it/s]Extractor Predicting: 132it [01:26,  1.61it/s]Extractor Predicting: 133it [01:27,  1.61it/s]Extractor Predicting: 134it [01:27,  1.64it/s]Extractor Predicting: 135it [01:28,  1.64it/s]Extractor Predicting: 136it [01:29,  1.63it/s]Extractor Predicting: 137it [01:29,  1.67it/s]Extractor Predicting: 138it [01:30,  1.66it/s]Extractor Predicting: 139it [01:30,  1.65it/s]Extractor Predicting: 140it [01:31,  1.63it/s]Extractor Predicting: 141it [01:32,  1.64it/s]Extractor Predicting: 142it [01:32,  1.62it/s]Extractor Predicting: 143it [01:33,  1.59it/s]Extractor Predicting: 144it [01:34,  1.60it/s]Extractor Predicting: 145it [01:34,  1.55it/s]Extractor Predicting: 146it [01:35,  1.50it/s]Extractor Predicting: 147it [01:36,  1.52it/s]Extractor Predicting: 148it [01:36,  1.50it/s]Extractor Predicting: 149it [01:37,  1.53it/s]Extractor Predicting: 150it [01:38,  1.53it/s]Extractor Predicting: 151it [01:38,  1.54it/s]Extractor Predicting: 152it [01:39,  1.55it/s]Extractor Predicting: 153it [01:40,  1.57it/s]Extractor Predicting: 154it [01:40,  1.58it/s]Extractor Predicting: 155it [01:41,  1.61it/s]Extractor Predicting: 156it [01:41,  1.56it/s]Extractor Predicting: 157it [01:42,  1.56it/s]Extractor Predicting: 158it [01:43,  1.57it/s]Extractor Predicting: 159it [01:43,  1.56it/s]Extractor Predicting: 160it [01:44,  1.60it/s]Extractor Predicting: 161it [01:45,  1.50it/s]Extractor Predicting: 162it [01:45,  1.56it/s]Extractor Predicting: 163it [01:46,  1.59it/s]Extractor Predicting: 164it [01:47,  1.56it/s]Extractor Predicting: 165it [01:47,  1.62it/s]Extractor Predicting: 166it [01:48,  1.63it/s]Extractor Predicting: 167it [01:48,  1.62it/s]Extractor Predicting: 168it [01:49,  1.67it/s]Extractor Predicting: 169it [01:49,  1.68it/s]Extractor Predicting: 170it [01:50,  1.63it/s]Extractor Predicting: 171it [01:51,  1.62it/s]Extractor Predicting: 172it [01:51,  1.62it/s]Extractor Predicting: 173it [01:52,  1.59it/s]Extractor Predicting: 174it [01:53,  1.63it/s]Extractor Predicting: 175it [01:53,  1.62it/s]Extractor Predicting: 176it [01:54,  1.60it/s]Extractor Predicting: 177it [01:55,  1.59it/s]Extractor Predicting: 178it [01:55,  1.58it/s]Extractor Predicting: 179it [01:56,  1.66it/s]Extractor Predicting: 180it [01:56,  1.60it/s]Extractor Predicting: 181it [01:57,  1.60it/s]Extractor Predicting: 182it [01:58,  1.59it/s]Extractor Predicting: 183it [01:58,  1.59it/s]Extractor Predicting: 184it [01:59,  1.58it/s]Extractor Predicting: 185it [01:59,  1.62it/s]Extractor Predicting: 186it [02:00,  1.59it/s]Extractor Predicting: 187it [02:01,  1.57it/s]Extractor Predicting: 188it [02:01,  1.56it/s]Extractor Predicting: 189it [02:02,  1.54it/s]Extractor Predicting: 190it [02:03,  1.38it/s]Extractor Predicting: 191it [02:04,  1.42it/s]Extractor Predicting: 192it [02:04,  1.47it/s]Extractor Predicting: 193it [02:05,  1.53it/s]Extractor Predicting: 194it [02:06,  1.55it/s]Extractor Predicting: 195it [02:06,  1.53it/s]Extractor Predicting: 196it [02:07,  1.55it/s]Extractor Predicting: 197it [02:07,  1.55it/s]Extractor Predicting: 198it [02:08,  1.57it/s]Extractor Predicting: 199it [02:09,  1.56it/s]Extractor Predicting: 200it [02:09,  1.56it/s]Extractor Predicting: 201it [02:10,  1.57it/s]Extractor Predicting: 202it [02:11,  1.57it/s]Extractor Predicting: 203it [02:11,  1.57it/s]Extractor Predicting: 204it [02:12,  1.57it/s]Extractor Predicting: 205it [02:13,  1.57it/s]Extractor Predicting: 206it [02:13,  1.56it/s]Extractor Predicting: 207it [02:14,  1.58it/s]Extractor Predicting: 208it [02:14,  1.58it/s]Extractor Predicting: 209it [02:15,  1.58it/s]Extractor Predicting: 210it [02:16,  1.57it/s]Extractor Predicting: 211it [02:16,  1.57it/s]Extractor Predicting: 212it [02:17,  1.57it/s]Extractor Predicting: 213it [02:18,  1.57it/s]Extractor Predicting: 214it [02:18,  1.57it/s]Extractor Predicting: 215it [02:19,  1.52it/s]Extractor Predicting: 216it [02:20,  1.56it/s]Extractor Predicting: 217it [02:20,  1.58it/s]Extractor Predicting: 218it [02:21,  1.59it/s]Extractor Predicting: 219it [02:21,  1.56it/s]Extractor Predicting: 220it [02:22,  1.58it/s]Extractor Predicting: 221it [02:23,  1.56it/s]Extractor Predicting: 222it [02:23,  1.58it/s]Extractor Predicting: 223it [02:24,  1.52it/s]Extractor Predicting: 224it [02:25,  1.53it/s]Extractor Predicting: 225it [02:25,  1.51it/s]Extractor Predicting: 226it [02:26,  1.52it/s]Extractor Predicting: 227it [02:27,  1.49it/s]Extractor Predicting: 228it [02:28,  1.42it/s]Extractor Predicting: 229it [02:28,  1.44it/s]Extractor Predicting: 230it [02:29,  1.47it/s]Extractor Predicting: 231it [02:30,  1.48it/s]Extractor Predicting: 232it [02:30,  1.49it/s]Extractor Predicting: 233it [02:31,  1.50it/s]Extractor Predicting: 234it [02:31,  1.52it/s]Extractor Predicting: 235it [02:32,  1.52it/s]Extractor Predicting: 236it [02:33,  1.53it/s]Extractor Predicting: 237it [02:33,  1.53it/s]Extractor Predicting: 238it [02:34,  1.58it/s]Extractor Predicting: 239it [02:35,  1.60it/s]Extractor Predicting: 240it [02:35,  1.65it/s]Extractor Predicting: 241it [02:36,  1.64it/s]Extractor Predicting: 242it [02:36,  1.63it/s]Extractor Predicting: 243it [02:37,  1.59it/s]Extractor Predicting: 244it [02:38,  1.60it/s]Extractor Predicting: 245it [02:38,  1.63it/s]Extractor Predicting: 246it [02:39,  1.61it/s]Extractor Predicting: 247it [02:40,  1.62it/s]Extractor Predicting: 248it [02:40,  1.64it/s]Extractor Predicting: 249it [02:41,  1.70it/s]Extractor Predicting: 250it [02:41,  1.70it/s]Extractor Predicting: 251it [02:42,  1.76it/s]Extractor Predicting: 252it [02:42,  1.76it/s]Extractor Predicting: 253it [02:43,  1.73it/s]Extractor Predicting: 254it [02:44,  1.74it/s]Extractor Predicting: 255it [02:44,  1.68it/s]Extractor Predicting: 256it [02:45,  1.73it/s]Extractor Predicting: 257it [02:45,  1.68it/s]Extractor Predicting: 258it [02:46,  1.64it/s]Extractor Predicting: 259it [02:47,  1.59it/s]Extractor Predicting: 260it [02:47,  1.65it/s]Extractor Predicting: 261it [02:48,  1.61it/s]Extractor Predicting: 262it [02:48,  1.62it/s]Extractor Predicting: 263it [02:49,  1.66it/s]Extractor Predicting: 264it [02:50,  1.67it/s]Extractor Predicting: 265it [02:50,  1.66it/s]Extractor Predicting: 266it [02:51,  1.69it/s]Extractor Predicting: 267it [02:51,  1.71it/s]Extractor Predicting: 268it [02:52,  1.70it/s]Extractor Predicting: 269it [02:53,  1.65it/s]Extractor Predicting: 270it [02:53,  1.67it/s]Extractor Predicting: 271it [02:54,  1.65it/s]Extractor Predicting: 272it [02:54,  1.63it/s]Extractor Predicting: 273it [02:55,  1.68it/s]Extractor Predicting: 274it [02:56,  1.69it/s]Extractor Predicting: 275it [02:56,  1.67it/s]Extractor Predicting: 276it [02:57,  1.65it/s]Extractor Predicting: 277it [02:57,  1.66it/s]Extractor Predicting: 278it [02:58,  1.68it/s]Extractor Predicting: 279it [02:59,  1.62it/s]Extractor Predicting: 280it [02:59,  1.68it/s]Extractor Predicting: 281it [03:00,  1.69it/s]Extractor Predicting: 282it [03:00,  1.71it/s]Extractor Predicting: 283it [03:01,  1.72it/s]Extractor Predicting: 284it [03:02,  1.72it/s]Extractor Predicting: 285it [03:02,  1.72it/s]Extractor Predicting: 286it [03:03,  1.71it/s]Extractor Predicting: 287it [03:03,  1.71it/s]Extractor Predicting: 288it [03:04,  1.66it/s]Extractor Predicting: 289it [03:05,  1.66it/s]Extractor Predicting: 290it [03:05,  1.60it/s]Extractor Predicting: 291it [03:06,  1.61it/s]Extractor Predicting: 292it [03:06,  1.64it/s]Extractor Predicting: 293it [03:07,  1.65it/s]Extractor Predicting: 294it [03:08,  1.48it/s]Extractor Predicting: 295it [03:08,  1.53it/s]Extractor Predicting: 296it [03:09,  1.50it/s]Extractor Predicting: 297it [03:10,  1.52it/s]Extractor Predicting: 298it [03:10,  1.49it/s]Extractor Predicting: 299it [03:11,  1.51it/s]Extractor Predicting: 300it [03:12,  1.54it/s]Extractor Predicting: 301it [03:12,  1.56it/s]Extractor Predicting: 302it [03:13,  1.57it/s]Extractor Predicting: 303it [03:14,  1.53it/s]Extractor Predicting: 304it [03:14,  1.54it/s]Extractor Predicting: 305it [03:15,  1.58it/s]Extractor Predicting: 306it [03:16,  1.54it/s]Extractor Predicting: 307it [03:16,  1.51it/s]Extractor Predicting: 308it [03:17,  1.51it/s]Extractor Predicting: 309it [03:18,  1.55it/s]Extractor Predicting: 310it [03:18,  1.56it/s]Extractor Predicting: 311it [03:19,  1.57it/s]Extractor Predicting: 312it [03:19,  1.56it/s]Extractor Predicting: 313it [03:20,  1.53it/s]Extractor Predicting: 314it [03:21,  1.51it/s]Extractor Predicting: 315it [03:21,  1.51it/s]Extractor Predicting: 316it [03:22,  1.51it/s]Extractor Predicting: 317it [03:23,  1.52it/s]Extractor Predicting: 318it [03:23,  1.53it/s]Extractor Predicting: 319it [03:24,  1.54it/s]Extractor Predicting: 320it [03:25,  1.54it/s]Extractor Predicting: 321it [03:25,  1.60it/s]Extractor Predicting: 322it [03:26,  1.59it/s]Extractor Predicting: 323it [03:27,  1.57it/s]Extractor Predicting: 324it [03:27,  1.56it/s]Extractor Predicting: 325it [03:28,  1.55it/s]Extractor Predicting: 326it [03:29,  1.54it/s]Extractor Predicting: 327it [03:29,  1.50it/s]Extractor Predicting: 328it [03:30,  1.52it/s]Extractor Predicting: 329it [03:30,  1.56it/s]Extractor Predicting: 330it [03:31,  1.59it/s]Extractor Predicting: 331it [03:32,  1.50it/s]Extractor Predicting: 331it [03:32,  1.56it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3879668049792531,
  "recall": 0.09431345353675451,
  "score": 0.15173952733542956,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.50it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.49it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:13,  1.49it/s]Extractor Predicting: 22it [00:14,  1.46it/s]Extractor Predicting: 23it [00:15,  1.45it/s]Extractor Predicting: 24it [00:15,  1.46it/s]Extractor Predicting: 25it [00:16,  1.47it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.53it/s]Extractor Predicting: 32it [00:21,  1.55it/s]Extractor Predicting: 33it [00:21,  1.57it/s]Extractor Predicting: 34it [00:22,  1.55it/s]Extractor Predicting: 35it [00:23,  1.57it/s]Extractor Predicting: 36it [00:23,  1.57it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:30,  1.55it/s]Extractor Predicting: 47it [00:30,  1.52it/s]Extractor Predicting: 48it [00:31,  1.52it/s]Extractor Predicting: 49it [00:32,  1.55it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:33,  1.58it/s]Extractor Predicting: 52it [00:34,  1.57it/s]Extractor Predicting: 53it [00:34,  1.48it/s]Extractor Predicting: 54it [00:35,  1.52it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:36,  1.61it/s]Extractor Predicting: 57it [00:37,  1.66it/s]Extractor Predicting: 58it [00:37,  1.72it/s]Extractor Predicting: 59it [00:38,  1.80it/s]Extractor Predicting: 60it [00:38,  1.89it/s]Extractor Predicting: 61it [00:39,  1.92it/s]Extractor Predicting: 62it [00:39,  1.91it/s]Extractor Predicting: 63it [00:40,  1.93it/s]Extractor Predicting: 64it [00:40,  1.92it/s]Extractor Predicting: 65it [00:41,  1.91it/s]Extractor Predicting: 66it [00:41,  1.91it/s]Extractor Predicting: 67it [00:42,  1.90it/s]Extractor Predicting: 68it [00:42,  1.92it/s]Extractor Predicting: 69it [00:43,  1.95it/s]Extractor Predicting: 70it [00:43,  1.91it/s]Extractor Predicting: 71it [00:44,  1.91it/s]Extractor Predicting: 72it [00:44,  1.93it/s]Extractor Predicting: 73it [00:45,  1.97it/s]Extractor Predicting: 74it [00:45,  1.99it/s]Extractor Predicting: 75it [00:46,  1.97it/s]Extractor Predicting: 76it [00:46,  1.95it/s]Extractor Predicting: 77it [00:47,  2.00it/s]Extractor Predicting: 78it [00:47,  1.94it/s]Extractor Predicting: 79it [00:48,  1.93it/s]Extractor Predicting: 80it [00:49,  1.91it/s]Extractor Predicting: 81it [00:49,  1.90it/s]Extractor Predicting: 82it [00:50,  1.92it/s]Extractor Predicting: 83it [00:50,  1.93it/s]Extractor Predicting: 84it [00:51,  1.94it/s]Extractor Predicting: 85it [00:51,  1.95it/s]Extractor Predicting: 86it [00:52,  1.81it/s]Extractor Predicting: 87it [00:52,  1.74it/s]Extractor Predicting: 88it [00:53,  1.69it/s]Extractor Predicting: 89it [00:54,  1.53it/s]Extractor Predicting: 90it [00:54,  1.57it/s]Extractor Predicting: 91it [00:55,  1.57it/s]Extractor Predicting: 92it [00:56,  1.58it/s]Extractor Predicting: 93it [00:56,  1.60it/s]Extractor Predicting: 94it [00:57,  1.59it/s]Extractor Predicting: 95it [00:57,  1.63it/s]Extractor Predicting: 96it [00:58,  1.63it/s]Extractor Predicting: 97it [00:59,  1.64it/s]Extractor Predicting: 98it [00:59,  1.64it/s]Extractor Predicting: 99it [01:00,  1.62it/s]Extractor Predicting: 100it [01:01,  1.57it/s]Extractor Predicting: 101it [01:01,  1.60it/s]Extractor Predicting: 102it [01:02,  1.58it/s]Extractor Predicting: 103it [01:03,  1.55it/s]Extractor Predicting: 104it [01:03,  1.55it/s]Extractor Predicting: 105it [01:04,  1.54it/s]Extractor Predicting: 106it [01:04,  1.53it/s]Extractor Predicting: 107it [01:05,  1.49it/s]Extractor Predicting: 108it [01:06,  1.47it/s]Extractor Predicting: 108it [01:06,  1.63it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7244186046511628,
  "recall": 0.09979176677879224,
  "score": 0.17541883711107983,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:48, 16.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:35, 16.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:49<03:19, 16.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:07<03:09, 17.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:24<02:51, 17.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:40<02:30, 16.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:58<02:15, 16.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:15<01:58, 16.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:31<01:39, 16.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:45<01:19, 15.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:01<01:03, 15.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:21<00:51, 17.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:41<00:36, 18.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:58<00:17, 17.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:17<00:00, 18.05s/it]Generating: 100%|██████████| 15/15 [04:17<00:00, 17.15s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : characters . Context : Later in Life , he played the title character , a young son of a Scottish novelist . Head Entity : son of a Scottish novelist , Tail Entity : Harry .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : characters .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 393, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 472, 'raw': 608}
{'target': 600, 'success': 499, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 576, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.7838541666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : made from material .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8315217391304348, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : cast member .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : follows .', 'success_rate': 0.8059895833333334, 'errors': {'', "('The Day After', 'follows', '', 'His next big album as a pop singer was his 2008 solo album , The Day After .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 499, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 579, 'raw': 736}
{'target': 600, 'success': 601, 'raw': 768}
{'prompt': 'Relation : league .', 'success_rate': 0.7825520833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.859375, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 395, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 487, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 528, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 575, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 622, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7475961538461539, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : residence . Context : Later in life he studied at the Conservatory of Fine Arts at Oxford University studying at the Renaissance and from 1907 to 1908 at the Conservatory of Fine Arts in London , where he wrote many works on abstract sculpture . Head Entity : Joseph S. S. S. , Tail Entity : Oxford University .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 89, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 385, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 478, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 531, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 604, 'raw': 800}
{'prompt': 'Relation : residence .', 'success_rate': 0.755, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 587, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.765, 'errors': {'', "('Nepal', 'shares border with', '', 'It is popular with mountain goats , bison and snowshoes of European descent , and their home is near the border with Nepal .')", 'not enough values to unpack (expected 2, got 1)', "('Orange Is The New Black', 'shares border with', '', 'She is best known for her portrayal of the character of the fictional character Lorna in the 1995 crime drama Orange Is The New Black .')"}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 454, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 581, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.7890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 13184
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13284, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.13it/s]Extractor Estimating: 2it [00:01,  1.34it/s]Extractor Estimating: 3it [00:02,  1.35it/s]Extractor Estimating: 4it [00:02,  1.41it/s]Extractor Estimating: 5it [00:03,  1.40it/s]Extractor Estimating: 6it [00:04,  1.42it/s]Extractor Estimating: 7it [00:05,  1.42it/s]Extractor Estimating: 8it [00:05,  1.48it/s]Extractor Estimating: 9it [00:06,  1.49it/s]Extractor Estimating: 10it [00:06,  1.50it/s]Extractor Estimating: 11it [00:07,  1.50it/s]Extractor Estimating: 12it [00:08,  1.53it/s]Extractor Estimating: 13it [00:08,  1.52it/s]Extractor Estimating: 14it [00:10,  1.02s/it]Extractor Estimating: 15it [00:11,  1.10it/s]Extractor Estimating: 16it [00:12,  1.20it/s]Extractor Estimating: 17it [00:12,  1.27it/s]Extractor Estimating: 18it [00:13,  1.31it/s]Extractor Estimating: 19it [00:14,  1.38it/s]Extractor Estimating: 20it [00:14,  1.40it/s]Extractor Estimating: 21it [00:15,  1.43it/s]Extractor Estimating: 22it [00:16,  1.47it/s]Extractor Estimating: 23it [00:16,  1.53it/s]Extractor Estimating: 24it [00:17,  1.51it/s]Extractor Estimating: 25it [00:18,  1.50it/s]Extractor Estimating: 26it [00:18,  1.54it/s]Extractor Estimating: 27it [00:19,  1.54it/s]Extractor Estimating: 28it [00:20,  1.50it/s]Extractor Estimating: 29it [00:20,  1.51it/s]Extractor Estimating: 30it [00:21,  1.49it/s]Extractor Estimating: 31it [00:22,  1.50it/s]Extractor Estimating: 32it [00:22,  1.53it/s]Extractor Estimating: 33it [00:23,  1.55it/s]Extractor Estimating: 34it [00:23,  1.57it/s]Extractor Estimating: 35it [00:24,  1.53it/s]Extractor Estimating: 36it [00:25,  1.56it/s]Extractor Estimating: 37it [00:25,  1.58it/s]Extractor Estimating: 38it [00:26,  1.62it/s]Extractor Estimating: 39it [00:27,  1.55it/s]Extractor Estimating: 40it [00:27,  1.54it/s]Extractor Estimating: 41it [00:28,  1.49it/s]Extractor Estimating: 42it [00:29,  1.48it/s]Extractor Estimating: 43it [00:29,  1.48it/s]Extractor Estimating: 44it [00:30,  1.53it/s]Extractor Estimating: 45it [00:31,  1.54it/s]Extractor Estimating: 46it [00:31,  1.57it/s]Extractor Estimating: 47it [00:32,  1.61it/s]Extractor Estimating: 48it [00:32,  1.55it/s]Extractor Estimating: 49it [00:33,  1.53it/s]Extractor Estimating: 50it [00:34,  1.46it/s]Extractor Estimating: 51it [00:35,  1.51it/s]Extractor Estimating: 52it [00:35,  1.54it/s]Extractor Estimating: 53it [00:36,  1.56it/s]Extractor Estimating: 54it [00:36,  1.61it/s]Extractor Estimating: 55it [00:37,  1.58it/s]Extractor Estimating: 56it [00:38,  1.53it/s]Extractor Estimating: 57it [00:38,  1.55it/s]Extractor Estimating: 58it [00:39,  1.53it/s]Extractor Estimating: 59it [00:40,  1.56it/s]Extractor Estimating: 60it [00:40,  1.57it/s]Extractor Estimating: 61it [00:41,  1.54it/s]Extractor Estimating: 62it [00:42,  1.54it/s]Extractor Estimating: 63it [00:42,  1.57it/s]Extractor Estimating: 64it [00:43,  1.56it/s]Extractor Estimating: 65it [00:43,  1.63it/s]Extractor Estimating: 66it [00:44,  1.52it/s]Extractor Estimating: 67it [00:45,  1.54it/s]Extractor Estimating: 68it [00:45,  1.53it/s]Extractor Estimating: 69it [00:46,  1.50it/s]Extractor Estimating: 70it [00:47,  1.52it/s]Extractor Estimating: 71it [00:47,  1.52it/s]Extractor Estimating: 72it [00:48,  1.47it/s]Extractor Estimating: 73it [00:49,  1.43it/s]Extractor Estimating: 74it [00:49,  1.51it/s]Extractor Estimating: 75it [00:50,  1.54it/s]Extractor Estimating: 76it [00:51,  1.53it/s]Extractor Estimating: 77it [00:51,  1.57it/s]Extractor Estimating: 78it [00:52,  1.52it/s]Extractor Estimating: 79it [00:53,  1.52it/s]Extractor Estimating: 80it [00:53,  1.54it/s]Extractor Estimating: 81it [00:54,  1.54it/s]Extractor Estimating: 82it [00:55,  1.53it/s]Extractor Estimating: 83it [00:55,  1.51it/s]Extractor Estimating: 84it [00:56,  1.51it/s]Extractor Estimating: 85it [00:57,  1.49it/s]Extractor Estimating: 86it [00:57,  1.49it/s]Extractor Estimating: 87it [00:58,  1.45it/s]Extractor Estimating: 88it [00:59,  1.47it/s]Extractor Estimating: 89it [00:59,  1.44it/s]Extractor Estimating: 90it [01:00,  1.44it/s]Extractor Estimating: 91it [01:01,  1.49it/s]Extractor Estimating: 92it [01:01,  1.51it/s]Extractor Estimating: 93it [01:02,  1.53it/s]Extractor Estimating: 94it [01:03,  1.52it/s]Extractor Estimating: 95it [01:03,  1.46it/s]Extractor Estimating: 96it [01:04,  1.47it/s]Extractor Estimating: 97it [01:05,  1.45it/s]Extractor Estimating: 98it [01:06,  1.41it/s]Extractor Estimating: 99it [01:06,  1.41it/s]Extractor Estimating: 100it [01:07,  1.43it/s]Extractor Estimating: 101it [01:08,  1.47it/s]Extractor Estimating: 102it [01:08,  1.45it/s]Extractor Estimating: 103it [01:09,  1.48it/s]Extractor Estimating: 104it [01:10,  1.46it/s]Extractor Estimating: 105it [01:10,  1.51it/s]Extractor Estimating: 106it [01:11,  1.56it/s]Extractor Estimating: 107it [01:12,  1.48it/s]Extractor Estimating: 108it [01:12,  1.46it/s]Extractor Estimating: 109it [01:13,  1.47it/s]Extractor Estimating: 110it [01:14,  1.48it/s]Extractor Estimating: 111it [01:14,  1.50it/s]Extractor Estimating: 112it [01:15,  1.55it/s]Extractor Estimating: 113it [01:16,  1.55it/s]Extractor Estimating: 114it [01:16,  1.51it/s]Extractor Estimating: 115it [01:17,  1.49it/s]Extractor Estimating: 116it [01:18,  1.51it/s]Extractor Estimating: 117it [01:19,  1.36it/s]Extractor Estimating: 118it [01:19,  1.43it/s]Extractor Estimating: 119it [01:20,  1.46it/s]Extractor Estimating: 120it [01:21,  1.44it/s]Extractor Estimating: 121it [01:21,  1.43it/s]Extractor Estimating: 122it [01:22,  1.42it/s]Extractor Estimating: 123it [01:23,  1.44it/s]Extractor Estimating: 124it [01:23,  1.47it/s]Extractor Estimating: 125it [01:24,  1.47it/s]Extractor Estimating: 126it [01:25,  1.51it/s]Extractor Estimating: 127it [01:25,  1.53it/s]Extractor Estimating: 128it [01:26,  1.55it/s]Extractor Estimating: 129it [01:26,  1.61it/s]Extractor Estimating: 130it [01:27,  1.50it/s]Extractor Estimating: 131it [01:28,  1.54it/s]Extractor Estimating: 132it [01:28,  1.56it/s]Extractor Estimating: 133it [01:29,  1.57it/s]Extractor Estimating: 134it [01:30,  1.59it/s]Extractor Estimating: 135it [01:30,  1.59it/s]Extractor Estimating: 136it [01:31,  1.57it/s]Extractor Estimating: 137it [01:32,  1.52it/s]Extractor Estimating: 138it [01:32,  1.52it/s]Extractor Estimating: 139it [01:33,  1.50it/s]Extractor Estimating: 140it [01:34,  1.36it/s]Extractor Estimating: 141it [01:35,  1.41it/s]Extractor Estimating: 142it [01:35,  1.48it/s]Extractor Estimating: 143it [01:36,  1.54it/s]Extractor Estimating: 144it [01:36,  1.54it/s]Extractor Estimating: 145it [01:37,  1.51it/s]Extractor Estimating: 146it [01:38,  1.51it/s]Extractor Estimating: 147it [01:38,  1.52it/s]Extractor Estimating: 148it [01:39,  1.53it/s]Extractor Estimating: 149it [01:40,  1.52it/s]Extractor Estimating: 150it [01:40,  1.50it/s]Extractor Estimating: 151it [01:41,  1.44it/s]Extractor Estimating: 152it [01:42,  1.44it/s]Extractor Estimating: 153it [01:42,  1.47it/s]Extractor Estimating: 154it [01:43,  1.48it/s]Extractor Estimating: 155it [01:44,  1.49it/s]Extractor Estimating: 156it [01:44,  1.46it/s]Extractor Estimating: 157it [01:45,  1.53it/s]Extractor Estimating: 158it [01:46,  1.49it/s]Extractor Estimating: 159it [01:46,  1.50it/s]Extractor Estimating: 160it [01:47,  1.53it/s]Extractor Estimating: 161it [01:48,  1.56it/s]Extractor Estimating: 162it [01:48,  1.61it/s]Extractor Estimating: 163it [01:49,  1.58it/s]Extractor Estimating: 164it [01:50,  1.56it/s]Extractor Estimating: 165it [01:50,  1.46it/s]Extractor Estimating: 166it [01:51,  1.43it/s]Extractor Estimating: 167it [01:52,  1.46it/s]Extractor Estimating: 168it [01:52,  1.44it/s]Extractor Estimating: 169it [01:53,  1.43it/s]Extractor Estimating: 170it [01:54,  1.44it/s]Extractor Estimating: 171it [01:55,  1.42it/s]Extractor Estimating: 172it [01:55,  1.49it/s]Extractor Estimating: 173it [01:56,  1.47it/s]Extractor Estimating: 174it [01:57,  1.45it/s]Extractor Estimating: 175it [01:57,  1.45it/s]Extractor Estimating: 176it [01:58,  1.45it/s]Extractor Estimating: 177it [01:59,  1.48it/s]Extractor Estimating: 178it [01:59,  1.52it/s]Extractor Estimating: 179it [02:00,  1.54it/s]Extractor Estimating: 180it [02:00,  1.54it/s]Extractor Estimating: 181it [02:01,  1.59it/s]Extractor Estimating: 182it [02:02,  1.53it/s]Extractor Estimating: 183it [02:04,  1.05s/it]Extractor Estimating: 184it [02:05,  1.00it/s]Extractor Estimating: 185it [02:05,  1.11it/s]Extractor Estimating: 186it [02:06,  1.21it/s]Extractor Estimating: 187it [02:07,  1.27it/s]Extractor Estimating: 188it [02:07,  1.28it/s]Extractor Estimating: 189it [02:08,  1.38it/s]Extractor Estimating: 190it [02:09,  1.43it/s]Extractor Estimating: 191it [02:09,  1.45it/s]Extractor Estimating: 192it [02:10,  1.45it/s]Extractor Estimating: 193it [02:11,  1.46it/s]Extractor Estimating: 194it [02:11,  1.47it/s]Extractor Estimating: 195it [02:12,  1.45it/s]Extractor Estimating: 196it [02:13,  1.49it/s]Extractor Estimating: 197it [02:13,  1.55it/s]Extractor Estimating: 198it [02:14,  1.52it/s]Extractor Estimating: 199it [02:15,  1.47it/s]Extractor Estimating: 200it [02:15,  1.44it/s]Extractor Estimating: 201it [02:16,  1.48it/s]Extractor Estimating: 202it [02:17,  1.53it/s]Extractor Estimating: 203it [02:17,  1.61it/s]Extractor Estimating: 204it [02:18,  1.66it/s]Extractor Estimating: 205it [02:18,  1.66it/s]Extractor Estimating: 206it [02:19,  1.59it/s]Extractor Estimating: 207it [02:20,  1.59it/s]Extractor Estimating: 208it [02:20,  1.67it/s]Extractor Estimating: 209it [02:21,  1.59it/s]Extractor Estimating: 210it [02:22,  1.60it/s]Extractor Estimating: 211it [02:22,  1.59it/s]Extractor Estimating: 212it [02:23,  1.65it/s]Extractor Estimating: 213it [02:23,  1.66it/s]Extractor Estimating: 214it [02:24,  1.67it/s]Extractor Estimating: 215it [02:25,  1.64it/s]Extractor Estimating: 216it [02:25,  1.53it/s]Extractor Estimating: 217it [02:26,  1.56it/s]Extractor Estimating: 218it [02:27,  1.53it/s]Extractor Estimating: 219it [02:27,  1.58it/s]Extractor Estimating: 220it [02:28,  1.62it/s]Extractor Estimating: 221it [02:28,  1.60it/s]Extractor Estimating: 222it [02:29,  1.62it/s]Extractor Estimating: 223it [02:30,  1.58it/s]Extractor Estimating: 224it [02:30,  1.60it/s]Extractor Estimating: 225it [02:31,  1.62it/s]Extractor Estimating: 226it [02:31,  1.64it/s]Extractor Estimating: 227it [02:32,  1.74it/s]Extractor Estimating: 228it [02:32,  1.78it/s]Extractor Estimating: 229it [02:33,  1.79it/s]Extractor Estimating: 230it [02:34,  1.83it/s]Extractor Estimating: 231it [02:34,  1.76it/s]Extractor Estimating: 232it [02:35,  1.81it/s]Extractor Estimating: 233it [02:35,  1.74it/s]Extractor Estimating: 234it [02:36,  1.76it/s]Extractor Estimating: 235it [02:36,  1.75it/s]Extractor Estimating: 236it [02:37,  1.69it/s]Extractor Estimating: 237it [02:38,  1.71it/s]Extractor Estimating: 238it [02:38,  1.66it/s]Extractor Estimating: 239it [02:39,  1.61it/s]Extractor Estimating: 240it [02:40,  1.67it/s]Extractor Estimating: 241it [02:40,  1.70it/s]Extractor Estimating: 242it [02:41,  1.72it/s]Extractor Estimating: 243it [02:41,  1.76it/s]Extractor Estimating: 244it [02:42,  1.75it/s]Extractor Estimating: 245it [02:42,  1.76it/s]Extractor Estimating: 246it [02:43,  1.73it/s]Extractor Estimating: 247it [02:43,  1.74it/s]Extractor Estimating: 248it [02:44,  1.68it/s]Extractor Estimating: 249it [02:45,  1.72it/s]Extractor Estimating: 250it [02:45,  1.70it/s]Extractor Estimating: 251it [02:46,  1.64it/s]Extractor Estimating: 252it [02:47,  1.64it/s]Extractor Estimating: 253it [02:47,  1.63it/s]Extractor Estimating: 254it [02:48,  1.63it/s]Extractor Estimating: 255it [02:48,  1.60it/s]Extractor Estimating: 256it [02:49,  1.59it/s]Extractor Estimating: 257it [02:50,  1.63it/s]Extractor Estimating: 258it [02:50,  1.68it/s]Extractor Estimating: 259it [02:51,  1.66it/s]Extractor Estimating: 260it [02:52,  1.60it/s]Extractor Estimating: 261it [02:52,  1.60it/s]Extractor Estimating: 262it [02:53,  1.61it/s]Extractor Estimating: 263it [02:53,  1.61it/s]Extractor Estimating: 264it [02:54,  1.62it/s]Extractor Estimating: 265it [02:55,  1.59it/s]Extractor Estimating: 266it [02:55,  1.59it/s]Extractor Estimating: 267it [02:56,  1.63it/s]Extractor Estimating: 268it [02:57,  1.58it/s]Extractor Estimating: 269it [02:57,  1.61it/s]Extractor Estimating: 270it [02:58,  1.49it/s]Extractor Estimating: 271it [02:59,  1.50it/s]Extractor Estimating: 272it [02:59,  1.48it/s]Extractor Estimating: 273it [03:00,  1.52it/s]Extractor Estimating: 274it [03:00,  1.61it/s]Extractor Estimating: 275it [03:01,  1.61it/s]Extractor Estimating: 276it [03:02,  1.58it/s]Extractor Estimating: 277it [03:02,  1.58it/s]Extractor Estimating: 278it [03:03,  1.58it/s]Extractor Estimating: 279it [03:04,  1.53it/s]Extractor Estimating: 280it [03:04,  1.47it/s]Extractor Estimating: 281it [03:05,  1.47it/s]Extractor Estimating: 282it [03:06,  1.40it/s]Extractor Estimating: 283it [03:07,  1.41it/s]Extractor Estimating: 284it [03:07,  1.39it/s]Extractor Estimating: 285it [03:08,  1.40it/s]Extractor Estimating: 286it [03:09,  1.40it/s]Extractor Estimating: 287it [03:09,  1.44it/s]Extractor Estimating: 288it [03:10,  1.51it/s]Extractor Estimating: 289it [03:11,  1.44it/s]Extractor Estimating: 290it [03:12,  1.39it/s]Extractor Estimating: 291it [03:12,  1.42it/s]Extractor Estimating: 292it [03:13,  1.45it/s]Extractor Estimating: 293it [03:13,  1.50it/s]Extractor Estimating: 294it [03:14,  1.53it/s]Extractor Estimating: 295it [03:15,  1.49it/s]Extractor Estimating: 296it [03:15,  1.47it/s]Extractor Estimating: 297it [03:16,  1.50it/s]Extractor Estimating: 298it [03:17,  1.45it/s]Extractor Estimating: 299it [03:18,  1.46it/s]Extractor Estimating: 300it [03:18,  1.43it/s]Extractor Estimating: 301it [03:19,  1.49it/s]Extractor Estimating: 302it [03:20,  1.42it/s]Extractor Estimating: 303it [03:20,  1.45it/s]Extractor Estimating: 304it [03:21,  1.47it/s]Extractor Estimating: 305it [03:22,  1.44it/s]Extractor Estimating: 306it [03:22,  1.42it/s]Extractor Estimating: 307it [03:23,  1.45it/s]Extractor Estimating: 308it [03:24,  1.39it/s]Extractor Estimating: 309it [03:24,  1.45it/s]Extractor Estimating: 310it [03:25,  1.45it/s]Extractor Estimating: 311it [03:26,  1.49it/s]Extractor Estimating: 312it [03:26,  1.49it/s]Extractor Estimating: 313it [03:27,  1.54it/s]Extractor Estimating: 314it [03:28,  1.52it/s]Extractor Estimating: 315it [03:28,  1.53it/s]Extractor Estimating: 316it [03:29,  1.55it/s]Extractor Estimating: 317it [03:30,  1.53it/s]Extractor Estimating: 318it [03:30,  1.54it/s]Extractor Estimating: 319it [03:31,  1.58it/s]Extractor Estimating: 320it [03:32,  1.54it/s]Extractor Estimating: 321it [03:32,  1.54it/s]Extractor Estimating: 322it [03:33,  1.51it/s]Extractor Estimating: 323it [03:34,  1.52it/s]Extractor Estimating: 324it [03:34,  1.58it/s]Extractor Estimating: 325it [03:35,  1.57it/s]Extractor Estimating: 326it [03:35,  1.63it/s]Extractor Estimating: 327it [03:36,  1.58it/s]Extractor Estimating: 328it [03:37,  1.58it/s]Extractor Estimating: 329it [03:37,  1.65it/s]Extractor Estimating: 330it [03:38,  1.68it/s]Extractor Estimating: 331it [03:38,  1.72it/s]Extractor Estimating: 332it [03:39,  1.66it/s]Extractor Estimating: 333it [03:40,  1.68it/s]Extractor Estimating: 334it [03:40,  1.68it/s]Extractor Estimating: 335it [03:41,  1.69it/s]Extractor Estimating: 336it [03:41,  1.72it/s]Extractor Estimating: 337it [03:42,  1.69it/s]Extractor Estimating: 338it [03:43,  1.64it/s]Extractor Estimating: 339it [03:43,  1.64it/s]Extractor Estimating: 340it [03:44,  1.69it/s]Extractor Estimating: 341it [03:44,  1.71it/s]Extractor Estimating: 342it [03:45,  1.69it/s]Extractor Estimating: 343it [03:45,  1.74it/s]Extractor Estimating: 344it [03:46,  1.74it/s]Extractor Estimating: 345it [03:47,  1.69it/s]Extractor Estimating: 346it [03:47,  1.68it/s]Extractor Estimating: 347it [03:48,  1.69it/s]Extractor Estimating: 348it [03:48,  1.67it/s]Extractor Estimating: 349it [03:49,  1.68it/s]Extractor Estimating: 350it [03:50,  1.66it/s]Extractor Estimating: 351it [03:50,  1.63it/s]Extractor Estimating: 352it [03:51,  1.60it/s]Extractor Estimating: 353it [03:52,  1.60it/s]Extractor Estimating: 354it [03:52,  1.58it/s]Extractor Estimating: 355it [03:53,  1.62it/s]Extractor Estimating: 356it [03:53,  1.62it/s]Extractor Estimating: 357it [03:54,  1.61it/s]Extractor Estimating: 358it [03:55,  1.52it/s]Extractor Estimating: 359it [03:56,  1.51it/s]Extractor Estimating: 360it [03:56,  1.49it/s]Extractor Estimating: 361it [03:57,  1.50it/s]Extractor Estimating: 362it [03:58,  1.50it/s]Extractor Estimating: 363it [03:58,  1.52it/s]Extractor Estimating: 364it [03:59,  1.55it/s]Extractor Estimating: 365it [03:59,  1.54it/s]Extractor Estimating: 366it [04:00,  1.52it/s]Extractor Estimating: 367it [04:01,  1.50it/s]Extractor Estimating: 368it [04:01,  1.50it/s]Extractor Estimating: 369it [04:02,  1.51it/s]Extractor Estimating: 370it [04:03,  1.53it/s]Extractor Estimating: 371it [04:03,  1.50it/s]Extractor Estimating: 372it [04:04,  1.49it/s]Extractor Estimating: 373it [04:05,  1.54it/s]Extractor Estimating: 374it [04:05,  1.55it/s]Extractor Estimating: 375it [04:06,  1.79it/s]Extractor Estimating: 375it [04:06,  1.52it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7450 mean pseudo reward: 0.9379077023281539
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 25277
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25377, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25377, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.351, loss:962.5047
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.149, loss:907.8044
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.075, loss:915.0687
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 89, avg_time 1.097, loss:909.1975
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 189, avg_time 1.076, loss:871.9768
>> valid entity prec:0.4821, rec:0.3600, f1:0.4122
>> valid relation prec:0.0771, rec:0.0071, f1:0.0131
>> valid relation with NER prec:0.0771, rec:0.0071, f1:0.0131
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 289, avg_time 2.614, loss:899.6617
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 78, avg_time 1.082, loss:874.6392
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 178, avg_time 1.092, loss:900.1494
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 278, avg_time 1.087, loss:903.8163
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 67, avg_time 1.075, loss:868.2957
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4796, rec:0.4076, f1:0.4407
>> valid relation prec:0.0588, rec:0.0074, f1:0.0131
>> valid relation with NER prec:0.0588, rec:0.0074, f1:0.0131
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 167, avg_time 2.624, loss:857.1703
g_step 1200, step 267, avg_time 1.071, loss:878.7949
g_step 1300, step 56, avg_time 1.086, loss:869.1750
g_step 1400, step 156, avg_time 1.090, loss:840.7899
g_step 1500, step 256, avg_time 1.086, loss:832.1614
>> valid entity prec:0.5088, rec:0.4326, f1:0.4676
>> valid relation prec:0.0778, rec:0.0076, f1:0.0139
>> valid relation with NER prec:0.0778, rec:0.0076, f1:0.0139
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 45, avg_time 2.620, loss:794.8776
g_step 1700, step 145, avg_time 1.083, loss:790.3480
g_step 1800, step 245, avg_time 1.079, loss:845.2454
g_step 1900, step 34, avg_time 1.081, loss:792.7438
g_step 2000, step 134, avg_time 1.087, loss:752.0349
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5178, rec:0.4058, f1:0.4550
>> valid relation prec:0.1250, rec:0.0125, f1:0.0226
>> valid relation with NER prec:0.1250, rec:0.0125, f1:0.0226
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 234, avg_time 2.603, loss:795.5799
g_step 2200, step 23, avg_time 1.102, loss:774.9873
g_step 2300, step 123, avg_time 1.082, loss:768.1984
g_step 2400, step 223, avg_time 1.087, loss:743.5216
g_step 2500, step 12, avg_time 1.080, loss:722.1538
>> valid entity prec:0.4818, rec:0.4749, f1:0.4783
>> valid relation prec:0.0535, rec:0.0062, f1:0.0112
>> valid relation with NER prec:0.0535, rec:0.0062, f1:0.0112
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 112, avg_time 2.622, loss:695.0802
g_step 2700, step 212, avg_time 1.083, loss:743.3604
g_step 2800, step 1, avg_time 1.084, loss:726.4229
g_step 2900, step 101, avg_time 1.085, loss:682.2992
g_step 3000, step 201, avg_time 1.095, loss:668.1160
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4852, rec:0.3315, f1:0.3939
>> valid relation prec:0.1586, rec:0.0129, f1:0.0239
>> valid relation with NER prec:0.1586, rec:0.0129, f1:0.0239
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 301, avg_time 2.601, loss:732.0331
g_step 3200, step 90, avg_time 1.089, loss:634.5631
g_step 3300, step 190, avg_time 1.086, loss:679.4998
g_step 3400, step 290, avg_time 1.085, loss:678.2564
g_step 3500, step 79, avg_time 1.075, loss:635.0643
>> valid entity prec:0.5070, rec:0.4011, f1:0.4479
>> valid relation prec:0.1588, rec:0.0194, f1:0.0345
>> valid relation with NER prec:0.1588, rec:0.0194, f1:0.0345
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 179, avg_time 2.617, loss:631.5328
g_step 3700, step 279, avg_time 1.082, loss:677.7755
g_step 3800, step 68, avg_time 1.081, loss:622.8535
g_step 3900, step 168, avg_time 1.082, loss:626.8478
g_step 4000, step 268, avg_time 1.097, loss:635.3167
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5049, rec:0.3391, f1:0.4057
>> valid relation prec:0.1048, rec:0.0101, f1:0.0185
>> valid relation with NER prec:0.1048, rec:0.0101, f1:0.0185
g_step 4100, step 57, avg_time 2.606, loss:607.2435
g_step 4200, step 157, avg_time 1.083, loss:598.3441
g_step 4300, step 257, avg_time 1.075, loss:607.9866
g_step 4400, step 46, avg_time 1.090, loss:586.1757
g_step 4500, step 146, avg_time 1.085, loss:550.5433
>> valid entity prec:0.4680, rec:0.3306, f1:0.3875
>> valid relation prec:0.0698, rec:0.0090, f1:0.0159
>> valid relation with NER prec:0.0698, rec:0.0090, f1:0.0159
g_step 4600, step 246, avg_time 2.610, loss:594.9784
g_step 4700, step 35, avg_time 1.094, loss:573.4990
g_step 4800, step 135, avg_time 1.101, loss:562.7594
g_step 4900, step 235, avg_time 1.081, loss:566.2165
g_step 5000, step 24, avg_time 1.086, loss:558.3560
learning rate was adjusted to 0.0008
>> valid entity prec:0.4846, rec:0.4199, f1:0.4499
>> valid relation prec:0.0827, rec:0.0097, f1:0.0173
>> valid relation with NER prec:0.0827, rec:0.0097, f1:0.0173
g_step 5100, step 124, avg_time 2.626, loss:529.7880
g_step 5200, step 224, avg_time 1.096, loss:544.6995
g_step 5300, step 13, avg_time 1.089, loss:545.2954
g_step 5400, step 113, avg_time 1.077, loss:531.7011
g_step 5500, step 213, avg_time 1.090, loss:521.5194
>> valid entity prec:0.4606, rec:0.4065, f1:0.4319
>> valid relation prec:0.0560, rec:0.0090, f1:0.0155
>> valid relation with NER prec:0.0560, rec:0.0090, f1:0.0155
g_step 5600, step 2, avg_time 2.630, loss:505.3807
g_step 5700, step 102, avg_time 1.100, loss:473.2251
g_step 5800, step 202, avg_time 1.084, loss:503.8499
g_step 5900, step 302, avg_time 1.075, loss:526.8028
g_step 6000, step 91, avg_time 1.085, loss:471.2262
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4793, rec:0.4039, f1:0.4384
>> valid relation prec:0.0498, rec:0.0090, f1:0.0152
>> valid relation with NER prec:0.0498, rec:0.0090, f1:0.0152
g_step 6100, step 191, avg_time 2.620, loss:504.2997
g_step 6200, step 291, avg_time 1.083, loss:498.9098
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:26:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:26:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-26-12_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:26:13 - WARNING - datasets.builder -   Using custom data configuration default-9e9f4c2c4300ae95
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-9e9f4c2c4300ae95/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:26:13,333 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:26:13,334 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:26:13,334 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:26:13,335 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:26:13,345 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:26:13,351 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:26:13,352 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:26:13,352 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:26:13,352 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:26:13,352 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:26:13,352 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:26:13,471 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:26:16,554 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:26:16,559 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-9e9f4c2c4300ae95/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 00:26:16 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14ab763e8cb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.06ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.92ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.25ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.40ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.78ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.06ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.23ba/s]100%|██████████| 8/8 [00:01<00:00,  5.05ba/s]100%|██████████| 8/8 [00:01<00:00,  4.37ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.07ba/s] 40%|████      | 2/5 [00:00<00:00,  4.35ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.42ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.45ba/s]100%|██████████| 5/5 [00:00<00:00,  5.05ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.19ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.53ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.83ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.99ba/s]100%|██████████| 8/8 [00:00<00:00, 10.43ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.20ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.59ba/s]100%|██████████| 5/5 [00:00<00:00, 11.90ba/s]100%|██████████| 5/5 [00:00<00:00, 11.16ba/s]
[INFO|trainer.py:414] 2023-08-29 00:26:21,008 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:26:21,018 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:26:21,018 >>   Num examples = 7524
[INFO|trainer.py:1149] 2023-08-29 00:26:21,018 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:26:21,018 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:26:21,018 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:26:21,018 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:26:21,018 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:56,  3.34it/s]  0%|          | 2/590 [00:00<02:51,  3.43it/s]  1%|          | 3/590 [00:00<02:49,  3.45it/s]  1%|          | 4/590 [00:01<02:48,  3.47it/s]  1%|          | 5/590 [00:01<02:48,  3.48it/s]  1%|          | 6/590 [00:01<02:47,  3.48it/s]  1%|          | 7/590 [00:02<02:48,  3.47it/s]  1%|▏         | 8/590 [00:02<02:47,  3.47it/s]  2%|▏         | 9/590 [00:02<02:47,  3.47it/s]  2%|▏         | 10/590 [00:02<02:46,  3.48it/s]  2%|▏         | 11/590 [00:03<02:46,  3.48it/s]  2%|▏         | 12/590 [00:03<02:46,  3.48it/s]  2%|▏         | 13/590 [00:03<02:45,  3.48it/s]  2%|▏         | 14/590 [00:04<02:45,  3.48it/s]  3%|▎         | 15/590 [00:04<02:45,  3.48it/s]  3%|▎         | 16/590 [00:04<02:44,  3.48it/s]  3%|▎         | 17/590 [00:04<02:44,  3.48it/s]  3%|▎         | 18/590 [00:05<02:44,  3.47it/s]  3%|▎         | 19/590 [00:05<02:44,  3.47it/s]  3%|▎         | 20/590 [00:05<02:43,  3.48it/s]  4%|▎         | 21/590 [00:06<02:43,  3.48it/s]  4%|▎         | 22/590 [00:06<02:43,  3.48it/s]  4%|▍         | 23/590 [00:06<02:42,  3.48it/s]  4%|▍         | 24/590 [00:06<02:42,  3.48it/s]  4%|▍         | 25/590 [00:07<02:42,  3.48it/s]  4%|▍         | 26/590 [00:07<02:41,  3.48it/s]  5%|▍         | 27/590 [00:07<02:41,  3.48it/s]  5%|▍         | 28/590 [00:08<02:41,  3.48it/s]  5%|▍         | 29/590 [00:08<02:41,  3.47it/s]  5%|▌         | 30/590 [00:08<02:41,  3.47it/s]  5%|▌         | 31/590 [00:08<02:40,  3.48it/s]  5%|▌         | 32/590 [00:09<02:40,  3.48it/s]  6%|▌         | 33/590 [00:09<02:40,  3.48it/s]  6%|▌         | 34/590 [00:09<02:39,  3.48it/s]  6%|▌         | 35/590 [00:10<02:39,  3.48it/s]  6%|▌         | 36/590 [00:10<02:38,  3.48it/s]  6%|▋         | 37/590 [00:10<02:38,  3.48it/s]  6%|▋         | 38/590 [00:10<02:38,  3.48it/s]  7%|▋         | 39/590 [00:11<02:38,  3.49it/s]  7%|▋         | 40/590 [00:11<02:38,  3.47it/s]  7%|▋         | 41/590 [00:11<02:37,  3.47it/s]  7%|▋         | 42/590 [00:12<02:37,  3.48it/s]  7%|▋         | 43/590 [00:12<02:37,  3.47it/s]  7%|▋         | 44/590 [00:12<02:37,  3.47it/s]  8%|▊         | 45/590 [00:12<02:36,  3.48it/s]  8%|▊         | 46/590 [00:13<02:36,  3.47it/s]  8%|▊         | 47/590 [00:13<02:36,  3.48it/s]  8%|▊         | 48/590 [00:13<02:35,  3.48it/s]  8%|▊         | 49/590 [00:14<02:35,  3.48it/s]  8%|▊         | 50/590 [00:14<02:35,  3.48it/s]  9%|▊         | 51/590 [00:14<02:35,  3.47it/s]  9%|▉         | 52/590 [00:14<02:34,  3.47it/s]  9%|▉         | 53/590 [00:15<02:34,  3.47it/s]  9%|▉         | 54/590 [00:15<02:34,  3.48it/s]  9%|▉         | 55/590 [00:15<02:33,  3.48it/s]  9%|▉         | 56/590 [00:16<02:33,  3.48it/s] 10%|▉         | 57/590 [00:16<02:33,  3.48it/s] 10%|▉         | 58/590 [00:16<02:32,  3.48it/s] 10%|█         | 59/590 [00:16<02:32,  3.48it/s] 10%|█         | 60/590 [00:17<02:32,  3.48it/s] 10%|█         | 61/590 [00:17<02:31,  3.48it/s] 11%|█         | 62/590 [00:17<02:54,  3.02it/s] 11%|█         | 63/590 [00:18<02:47,  3.15it/s] 11%|█         | 64/590 [00:18<02:42,  3.24it/s] 11%|█         | 65/590 [00:18<02:38,  3.31it/s] 11%|█         | 66/590 [00:19<02:36,  3.35it/s] 11%|█▏        | 67/590 [00:19<02:34,  3.39it/s] 12%|█▏        | 68/590 [00:19<02:32,  3.42it/s] 12%|█▏        | 69/590 [00:19<02:31,  3.44it/s] 12%|█▏        | 70/590 [00:20<02:30,  3.45it/s] 12%|█▏        | 71/590 [00:20<02:30,  3.46it/s] 12%|█▏        | 72/590 [00:20<02:30,  3.45it/s] 12%|█▏        | 73/590 [00:21<02:29,  3.46it/s] 13%|█▎        | 74/590 [00:21<02:29,  3.46it/s] 13%|█▎        | 75/590 [00:21<02:28,  3.47it/s] 13%|█▎        | 76/590 [00:22<02:28,  3.47it/s] 13%|█▎        | 77/590 [00:22<02:27,  3.47it/s] 13%|█▎        | 78/590 [00:22<02:27,  3.47it/s] 13%|█▎        | 79/590 [00:22<02:27,  3.47it/s] 14%|█▎        | 80/590 [00:23<02:26,  3.48it/s] 14%|█▎        | 81/590 [00:23<02:26,  3.47it/s] 14%|█▍        | 82/590 [00:23<02:26,  3.48it/s] 14%|█▍        | 83/590 [00:24<02:25,  3.48it/s] 14%|█▍        | 84/590 [00:24<02:25,  3.48it/s] 14%|█▍        | 85/590 [00:24<02:25,  3.48it/s] 15%|█▍        | 86/590 [00:24<02:24,  3.48it/s] 15%|█▍        | 87/590 [00:25<02:24,  3.48it/s] 15%|█▍        | 88/590 [00:25<02:24,  3.48it/s] 15%|█▌        | 89/590 [00:25<02:23,  3.48it/s] 15%|█▌        | 90/590 [00:26<02:23,  3.48it/s] 15%|█▌        | 91/590 [00:26<02:23,  3.48it/s] 16%|█▌        | 92/590 [00:26<02:23,  3.48it/s] 16%|█▌        | 93/590 [00:26<02:23,  3.47it/s] 16%|█▌        | 94/590 [00:27<02:22,  3.47it/s] 16%|█▌        | 95/590 [00:27<02:22,  3.47it/s] 16%|█▋        | 96/590 [00:27<02:22,  3.47it/s] 16%|█▋        | 97/590 [00:28<02:22,  3.47it/s] 17%|█▋        | 98/590 [00:28<02:21,  3.47it/s] 17%|█▋        | 99/590 [00:28<02:21,  3.47it/s] 17%|█▋        | 100/590 [00:28<02:21,  3.47it/s] 17%|█▋        | 101/590 [00:29<02:20,  3.47it/s] 17%|█▋        | 102/590 [00:29<02:20,  3.47it/s] 17%|█▋        | 103/590 [00:29<02:20,  3.47it/s] 18%|█▊        | 104/590 [00:30<02:20,  3.47it/s] 18%|█▊        | 105/590 [00:30<02:19,  3.47it/s] 18%|█▊        | 106/590 [00:30<02:19,  3.47it/s] 18%|█▊        | 107/590 [00:30<02:19,  3.47it/s] 18%|█▊        | 108/590 [00:31<02:18,  3.47it/s] 18%|█▊        | 109/590 [00:31<02:18,  3.47it/s] 19%|█▊        | 110/590 [00:31<02:18,  3.48it/s] 19%|█▉        | 111/590 [00:32<02:18,  3.47it/s] 19%|█▉        | 112/590 [00:32<02:17,  3.47it/s] 19%|█▉        | 113/590 [00:32<02:17,  3.47it/s] 19%|█▉        | 114/590 [00:32<02:17,  3.46it/s] 19%|█▉        | 115/590 [00:33<02:22,  3.34it/s] 20%|█▉        | 116/590 [00:33<02:20,  3.38it/s] 20%|█▉        | 117/590 [00:33<02:18,  3.41it/s] 20%|██        | 118/590 [00:34<02:04,  3.80it/s][INFO|trainer.py:2140] 2023-08-29 00:26:55,063 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:26:55,063 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 00:26:55,063 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.96it/s][A
  2%|▏         | 12/543 [00:00<00:10, 51.43it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.37it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.84it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.32it/s][A
  6%|▌         | 33/543 [00:00<00:10, 48.08it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.65it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.26it/s][A
  9%|▉         | 48/543 [00:00<00:10, 47.03it/s][A
 10%|▉         | 53/543 [00:01<00:10, 46.98it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.07it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.16it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.15it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.08it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.07it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 46.93it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.88it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.76it/s][A
 18%|█▊        | 98/543 [00:02<00:10, 44.32it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 45.18it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 45.89it/s][A
 21%|██        | 113/543 [00:02<00:09, 46.40it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 46.79it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 46.98it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.21it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 47.27it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.81it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.61it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.80it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 46.99it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.20it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.34it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.33it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.39it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 47.48it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 47.25it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 47.01it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.86it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 46.76it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.00it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.11it/s][A
 39%|███▉      | 213/543 [00:04<00:06, 47.20it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.30it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.27it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 47.28it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 47.27it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 47.09it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 46.93it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 46.95it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.03it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.11it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.25it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.39it/s][A
 50%|█████     | 273/543 [00:05<00:05, 47.28it/s][A
 51%|█████     | 278/543 [00:05<00:05, 47.30it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 47.19it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 47.01it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 46.82it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 46.80it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 46.91it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.14it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.29it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.34it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 47.24it/s][A
 60%|██████    | 328/543 [00:06<00:04, 47.23it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 47.14it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 46.95it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 46.80it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 46.86it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.01it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 47.07it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.17it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 47.19it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 47.21it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 47.24it/s][A
 71%|███████   | 383/543 [00:08<00:03, 47.07it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 47.01it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 46.91it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 46.88it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.00it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.12it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 47.19it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 47.18it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 47.15it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 47.23it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 46.97it/s][A
 81%|████████  | 438/543 [00:09<00:02, 47.15it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 47.03it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 46.93it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.00it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 47.12it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 47.10it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 47.12it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 47.19it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 47.22it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 47.18it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 47.11it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.01it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 46.98it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 46.89it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 47.00it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 47.10it/s][A
 95%|█████████▌| 518/543 [00:10<00:00, 47.20it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 47.18it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 47.17it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 47.14it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 47.04it/s][A
100%|██████████| 543/543 [00:11<00:00, 46.86it/s][A                                                 
                                                 [A 20%|██        | 118/590 [00:45<02:04,  3.80it/s]
100%|██████████| 543/543 [00:11<00:00, 46.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:27:06,631 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-29 00:27:06,650 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:27:08,913 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:27:08,930 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:27:08,940 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [00:52<45:35,  5.81s/it] 20%|██        | 120/590 [00:53<32:31,  4.15s/it] 21%|██        | 121/590 [00:53<23:23,  2.99s/it] 21%|██        | 122/590 [00:53<17:00,  2.18s/it] 21%|██        | 123/590 [00:53<12:33,  1.61s/it] 21%|██        | 124/590 [00:54<09:26,  1.22s/it] 21%|██        | 125/590 [00:54<07:15,  1.07it/s] 21%|██▏       | 126/590 [00:54<05:44,  1.35it/s] 22%|██▏       | 127/590 [00:55<04:40,  1.65it/s] 22%|██▏       | 128/590 [00:55<03:55,  1.96it/s] 22%|██▏       | 129/590 [00:55<03:24,  2.26it/s] 22%|██▏       | 130/590 [00:55<03:02,  2.52it/s] 22%|██▏       | 131/590 [00:56<02:47,  2.75it/s] 22%|██▏       | 132/590 [00:56<02:36,  2.93it/s] 23%|██▎       | 133/590 [00:56<02:28,  3.08it/s] 23%|██▎       | 134/590 [00:57<02:23,  3.19it/s] 23%|██▎       | 135/590 [00:57<02:19,  3.25it/s] 23%|██▎       | 136/590 [00:57<02:16,  3.32it/s] 23%|██▎       | 137/590 [00:57<02:14,  3.37it/s] 23%|██▎       | 138/590 [00:58<02:13,  3.40it/s] 24%|██▎       | 139/590 [00:58<02:11,  3.42it/s] 24%|██▎       | 140/590 [00:58<02:11,  3.43it/s] 24%|██▍       | 141/590 [00:59<02:10,  3.44it/s] 24%|██▍       | 142/590 [00:59<02:09,  3.45it/s] 24%|██▍       | 143/590 [00:59<02:09,  3.46it/s] 24%|██▍       | 144/590 [00:59<02:08,  3.47it/s] 25%|██▍       | 145/590 [01:00<02:08,  3.47it/s] 25%|██▍       | 146/590 [01:00<02:08,  3.46it/s] 25%|██▍       | 147/590 [01:00<02:07,  3.47it/s] 25%|██▌       | 148/590 [01:01<02:07,  3.47it/s] 25%|██▌       | 149/590 [01:01<02:07,  3.47it/s] 25%|██▌       | 150/590 [01:01<02:06,  3.47it/s] 26%|██▌       | 151/590 [01:02<02:06,  3.48it/s] 26%|██▌       | 152/590 [01:02<02:06,  3.47it/s] 26%|██▌       | 153/590 [01:02<02:05,  3.47it/s] 26%|██▌       | 154/590 [01:02<02:05,  3.48it/s] 26%|██▋       | 155/590 [01:03<02:05,  3.48it/s] 26%|██▋       | 156/590 [01:03<02:04,  3.47it/s] 27%|██▋       | 157/590 [01:03<02:04,  3.47it/s] 27%|██▋       | 158/590 [01:04<02:04,  3.47it/s] 27%|██▋       | 159/590 [01:04<02:04,  3.47it/s] 27%|██▋       | 160/590 [01:04<02:04,  3.46it/s] 27%|██▋       | 161/590 [01:04<02:03,  3.46it/s] 27%|██▋       | 162/590 [01:05<02:03,  3.46it/s] 28%|██▊       | 163/590 [01:05<02:03,  3.46it/s] 28%|██▊       | 164/590 [01:05<02:02,  3.46it/s] 28%|██▊       | 165/590 [01:06<02:02,  3.47it/s] 28%|██▊       | 166/590 [01:06<02:02,  3.47it/s] 28%|██▊       | 167/590 [01:06<02:01,  3.47it/s] 28%|██▊       | 168/590 [01:06<02:02,  3.46it/s] 29%|██▊       | 169/590 [01:07<02:01,  3.46it/s] 29%|██▉       | 170/590 [01:07<02:01,  3.47it/s] 29%|██▉       | 171/590 [01:07<02:01,  3.46it/s] 29%|██▉       | 172/590 [01:08<02:00,  3.46it/s] 29%|██▉       | 173/590 [01:08<02:00,  3.46it/s] 29%|██▉       | 174/590 [01:08<01:59,  3.47it/s] 30%|██▉       | 175/590 [01:08<01:59,  3.47it/s] 30%|██▉       | 176/590 [01:09<01:59,  3.46it/s] 30%|███       | 177/590 [01:09<01:59,  3.46it/s] 30%|███       | 178/590 [01:09<01:59,  3.46it/s] 30%|███       | 179/590 [01:10<01:58,  3.46it/s] 31%|███       | 180/590 [01:10<01:58,  3.46it/s] 31%|███       | 181/590 [01:10<01:58,  3.46it/s] 31%|███       | 182/590 [01:10<01:57,  3.46it/s] 31%|███       | 183/590 [01:11<01:57,  3.46it/s] 31%|███       | 184/590 [01:11<01:57,  3.46it/s] 31%|███▏      | 185/590 [01:11<01:57,  3.46it/s] 32%|███▏      | 186/590 [01:12<01:56,  3.46it/s] 32%|███▏      | 187/590 [01:12<01:56,  3.46it/s] 32%|███▏      | 188/590 [01:12<01:55,  3.47it/s] 32%|███▏      | 189/590 [01:12<01:55,  3.46it/s] 32%|███▏      | 190/590 [01:13<01:55,  3.46it/s] 32%|███▏      | 191/590 [01:13<01:55,  3.46it/s] 33%|███▎      | 192/590 [01:13<01:54,  3.46it/s] 33%|███▎      | 193/590 [01:14<01:54,  3.46it/s] 33%|███▎      | 194/590 [01:14<01:54,  3.46it/s] 33%|███▎      | 195/590 [01:14<01:53,  3.47it/s] 33%|███▎      | 196/590 [01:14<01:53,  3.46it/s] 33%|███▎      | 197/590 [01:15<01:53,  3.47it/s] 34%|███▎      | 198/590 [01:15<01:53,  3.47it/s] 34%|███▎      | 199/590 [01:15<01:52,  3.47it/s] 34%|███▍      | 200/590 [01:16<01:52,  3.47it/s] 34%|███▍      | 201/590 [01:16<01:52,  3.46it/s] 34%|███▍      | 202/590 [01:16<01:52,  3.46it/s] 34%|███▍      | 203/590 [01:17<01:51,  3.46it/s] 35%|███▍      | 204/590 [01:17<01:51,  3.47it/s] 35%|███▍      | 205/590 [01:17<01:51,  3.46it/s] 35%|███▍      | 206/590 [01:17<01:50,  3.47it/s] 35%|███▌      | 207/590 [01:18<01:50,  3.46it/s] 35%|███▌      | 208/590 [01:18<01:50,  3.46it/s] 35%|███▌      | 209/590 [01:18<01:50,  3.46it/s] 36%|███▌      | 210/590 [01:19<01:49,  3.46it/s] 36%|███▌      | 211/590 [01:19<01:49,  3.46it/s] 36%|███▌      | 212/590 [01:19<01:49,  3.45it/s] 36%|███▌      | 213/590 [01:19<01:48,  3.46it/s] 36%|███▋      | 214/590 [01:20<01:48,  3.46it/s] 36%|███▋      | 215/590 [01:20<01:48,  3.46it/s] 37%|███▋      | 216/590 [01:20<01:47,  3.46it/s] 37%|███▋      | 217/590 [01:21<01:47,  3.46it/s] 37%|███▋      | 218/590 [01:21<01:47,  3.46it/s] 37%|███▋      | 219/590 [01:21<01:47,  3.46it/s] 37%|███▋      | 220/590 [01:21<01:46,  3.46it/s] 37%|███▋      | 221/590 [01:22<01:46,  3.46it/s] 38%|███▊      | 222/590 [01:22<01:46,  3.47it/s] 38%|███▊      | 223/590 [01:22<01:46,  3.46it/s] 38%|███▊      | 224/590 [01:23<01:45,  3.46it/s] 38%|███▊      | 225/590 [01:23<01:45,  3.45it/s] 38%|███▊      | 226/590 [01:23<01:45,  3.46it/s] 38%|███▊      | 227/590 [01:23<01:44,  3.46it/s] 39%|███▊      | 228/590 [01:24<01:44,  3.46it/s] 39%|███▉      | 229/590 [01:24<01:44,  3.46it/s] 39%|███▉      | 230/590 [01:24<01:44,  3.45it/s] 39%|███▉      | 231/590 [01:25<01:44,  3.45it/s] 39%|███▉      | 232/590 [01:25<01:43,  3.45it/s] 39%|███▉      | 233/590 [01:25<01:43,  3.45it/s] 40%|███▉      | 234/590 [01:25<01:42,  3.46it/s] 40%|███▉      | 235/590 [01:26<01:42,  3.46it/s] 40%|████      | 236/590 [01:26<01:32,  3.84it/s][INFO|trainer.py:2140] 2023-08-29 00:27:47,484 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:27:47,484 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 00:27:47,484 >>   Batch size = 8
{'eval_loss': 0.9708923697471619, 'eval_runtime': 11.5432, 'eval_samples_per_second': 376.152, 'eval_steps_per_second': 47.041, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.64it/s][A
  2%|▏         | 12/543 [00:00<00:10, 50.87it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.18it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.47it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.00it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.72it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.42it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.06it/s][A
  9%|▉         | 48/543 [00:00<00:10, 46.80it/s][A
 10%|▉         | 53/543 [00:01<00:10, 45.56it/s][A
 11%|█         | 58/543 [00:01<00:10, 46.16it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 46.58it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 43.88it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 44.88it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 45.54it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 45.91it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.39it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.67it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.68it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 46.81it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 46.74it/s][A
 21%|██        | 113/543 [00:02<00:09, 46.85it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.01it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 46.95it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.03it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 47.14it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 47.17it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.99it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 47.00it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 46.85it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 46.78it/s][A
 30%|███       | 163/543 [00:03<00:08, 46.79it/s][A
 31%|███       | 168/543 [00:03<00:08, 46.82it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 46.80it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 46.83it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.79it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 46.69it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.72it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 46.80it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 46.73it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 44.50it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 45.25it/s][A
 40%|████      | 218/543 [00:04<00:07, 45.90it/s][A
 41%|████      | 223/543 [00:04<00:06, 46.33it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 46.61it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.78it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.79it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 46.94it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 46.72it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 46.72it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 46.83it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 46.91it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 46.98it/s][A
 50%|█████     | 273/543 [00:05<00:05, 47.10it/s][A
 51%|█████     | 278/543 [00:05<00:05, 47.16it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 47.11it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 47.16it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 46.99it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 46.91it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 46.79it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 46.87it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 46.95it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.00it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 47.15it/s][A
 60%|██████    | 328/543 [00:07<00:04, 47.04it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 47.14it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 47.02it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 46.87it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 46.94it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 46.97it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 46.96it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.02it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 47.13it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 47.17it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 47.16it/s][A
 71%|███████   | 383/543 [00:08<00:03, 47.09it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 46.92it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 46.84it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 46.94it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 46.99it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 46.96it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 47.03it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 47.13it/s][A
 78%|███████▊  | 423/543 [00:09<00:02, 47.18it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 47.14it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 46.87it/s][A
 81%|████████  | 438/543 [00:09<00:02, 46.82it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 46.91it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 46.95it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 46.94it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 46.96it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 47.07it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 47.09it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 47.07it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 47.00it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 46.85it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 46.92it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 46.94it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 46.92it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 46.97it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 46.95it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 46.94it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 47.07it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 47.09it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 47.02it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 46.99it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 46.97it/s][A
100%|██████████| 543/543 [00:11<00:00, 46.86it/s][A                                                 
                                                 [A 40%|████      | 236/590 [01:38<01:32,  3.84it/s]
100%|██████████| 543/543 [00:11<00:00, 46.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:27:59,101 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-29 00:27:59,120 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:28:01,395 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:28:01,414 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:28:01,421 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [01:45<34:20,  5.84s/it] 40%|████      | 238/590 [01:45<24:29,  4.17s/it] 41%|████      | 239/590 [01:45<17:35,  3.01s/it] 41%|████      | 240/590 [01:46<12:47,  2.19s/it] 41%|████      | 241/590 [01:46<09:25,  1.62s/it] 41%|████      | 242/590 [01:46<07:04,  1.22s/it] 41%|████      | 243/590 [01:47<05:26,  1.06it/s] 41%|████▏     | 244/590 [01:47<04:17,  1.34it/s] 42%|████▏     | 245/590 [01:47<03:29,  1.64it/s] 42%|████▏     | 246/590 [01:47<02:56,  1.95it/s] 42%|████▏     | 247/590 [01:48<02:32,  2.25it/s] 42%|████▏     | 248/590 [01:48<02:16,  2.51it/s] 42%|████▏     | 249/590 [01:48<02:04,  2.73it/s] 42%|████▏     | 250/590 [01:49<01:56,  2.92it/s] 43%|████▎     | 251/590 [01:49<01:50,  3.07it/s] 43%|████▎     | 252/590 [01:49<01:46,  3.18it/s] 43%|████▎     | 253/590 [01:49<01:43,  3.26it/s] 43%|████▎     | 254/590 [01:50<01:41,  3.32it/s] 43%|████▎     | 255/590 [01:50<01:39,  3.36it/s] 43%|████▎     | 256/590 [01:50<01:38,  3.39it/s] 44%|████▎     | 257/590 [01:51<01:37,  3.41it/s] 44%|████▎     | 258/590 [01:51<01:36,  3.43it/s] 44%|████▍     | 259/590 [01:51<01:36,  3.43it/s] 44%|████▍     | 260/590 [01:51<01:36,  3.43it/s] 44%|████▍     | 261/590 [01:52<01:35,  3.44it/s] 44%|████▍     | 262/590 [01:52<01:35,  3.44it/s] 45%|████▍     | 263/590 [01:52<01:34,  3.45it/s] 45%|████▍     | 264/590 [01:53<01:34,  3.45it/s] 45%|████▍     | 265/590 [01:53<01:34,  3.46it/s] 45%|████▌     | 266/590 [01:53<01:33,  3.46it/s] 45%|████▌     | 267/590 [01:53<01:33,  3.46it/s] 45%|████▌     | 268/590 [01:54<01:33,  3.46it/s] 46%|████▌     | 269/590 [01:54<01:32,  3.46it/s] 46%|████▌     | 270/590 [01:54<01:32,  3.46it/s] 46%|████▌     | 271/590 [01:55<01:32,  3.46it/s] 46%|████▌     | 272/590 [01:55<01:31,  3.46it/s] 46%|████▋     | 273/590 [01:55<01:31,  3.46it/s] 46%|████▋     | 274/590 [01:56<01:31,  3.46it/s] 47%|████▋     | 275/590 [01:56<01:30,  3.46it/s] 47%|████▋     | 276/590 [01:56<01:30,  3.46it/s] 47%|████▋     | 277/590 [01:56<01:30,  3.46it/s] 47%|████▋     | 278/590 [01:57<01:30,  3.46it/s] 47%|████▋     | 279/590 [01:57<01:29,  3.46it/s] 47%|████▋     | 280/590 [01:57<01:29,  3.46it/s] 48%|████▊     | 281/590 [01:58<01:29,  3.46it/s] 48%|████▊     | 282/590 [01:58<01:28,  3.46it/s] 48%|████▊     | 283/590 [01:58<01:28,  3.46it/s] 48%|████▊     | 284/590 [01:58<01:28,  3.45it/s] 48%|████▊     | 285/590 [01:59<01:28,  3.45it/s] 48%|████▊     | 286/590 [01:59<01:27,  3.46it/s] 49%|████▊     | 287/590 [01:59<01:27,  3.46it/s] 49%|████▉     | 288/590 [02:00<01:27,  3.46it/s] 49%|████▉     | 289/590 [02:00<01:26,  3.46it/s] 49%|████▉     | 290/590 [02:00<01:26,  3.46it/s] 49%|████▉     | 291/590 [02:00<01:26,  3.46it/s] 49%|████▉     | 292/590 [02:01<01:26,  3.46it/s] 50%|████▉     | 293/590 [02:01<01:25,  3.46it/s] 50%|████▉     | 294/590 [02:01<01:25,  3.46it/s] 50%|█████     | 295/590 [02:02<01:25,  3.45it/s] 50%|█████     | 296/590 [02:02<01:25,  3.46it/s] 50%|█████     | 297/590 [02:02<01:24,  3.45it/s] 51%|█████     | 298/590 [02:02<01:24,  3.45it/s] 51%|█████     | 299/590 [02:03<01:24,  3.46it/s] 51%|█████     | 300/590 [02:03<01:23,  3.46it/s] 51%|█████     | 301/590 [02:03<01:23,  3.46it/s] 51%|█████     | 302/590 [02:04<01:23,  3.47it/s] 51%|█████▏    | 303/590 [02:04<01:22,  3.46it/s] 52%|█████▏    | 304/590 [02:04<01:22,  3.46it/s] 52%|█████▏    | 305/590 [02:04<01:22,  3.46it/s] 52%|█████▏    | 306/590 [02:05<01:22,  3.46it/s] 52%|█████▏    | 307/590 [02:05<01:21,  3.46it/s] 52%|█████▏    | 308/590 [02:05<01:21,  3.46it/s] 52%|█████▏    | 309/590 [02:06<01:21,  3.46it/s] 53%|█████▎    | 310/590 [02:06<01:20,  3.46it/s] 53%|█████▎    | 311/590 [02:06<01:20,  3.46it/s] 53%|█████▎    | 312/590 [02:06<01:20,  3.46it/s] 53%|█████▎    | 313/590 [02:07<01:19,  3.46it/s] 53%|█████▎    | 314/590 [02:07<01:19,  3.46it/s] 53%|█████▎    | 315/590 [02:07<01:19,  3.46it/s] 54%|█████▎    | 316/590 [02:08<01:19,  3.46it/s] 54%|█████▎    | 317/590 [02:08<01:18,  3.46it/s] 54%|█████▍    | 318/590 [02:08<01:18,  3.46it/s] 54%|█████▍    | 319/590 [02:09<01:18,  3.46it/s] 54%|█████▍    | 320/590 [02:09<01:17,  3.46it/s] 54%|█████▍    | 321/590 [02:09<01:17,  3.46it/s] 55%|█████▍    | 322/590 [02:09<01:17,  3.46it/s] 55%|█████▍    | 323/590 [02:10<01:17,  3.46it/s] 55%|█████▍    | 324/590 [02:10<01:16,  3.46it/s] 55%|█████▌    | 325/590 [02:10<01:16,  3.46it/s] 55%|█████▌    | 326/590 [02:11<01:16,  3.46it/s] 55%|█████▌    | 327/590 [02:11<01:15,  3.47it/s] 56%|█████▌    | 328/590 [02:11<01:16,  3.43it/s] 56%|█████▌    | 329/590 [02:11<01:15,  3.44it/s] 56%|█████▌    | 330/590 [02:12<01:15,  3.45it/s] 56%|█████▌    | 331/590 [02:12<01:15,  3.45it/s] 56%|█████▋    | 332/590 [02:12<01:14,  3.46it/s] 56%|█████▋    | 333/590 [02:13<01:14,  3.46it/s] 57%|█████▋    | 334/590 [02:13<01:14,  3.46it/s] 57%|█████▋    | 335/590 [02:13<01:13,  3.46it/s] 57%|█████▋    | 336/590 [02:13<01:13,  3.46it/s] 57%|█████▋    | 337/590 [02:14<01:13,  3.46it/s] 57%|█████▋    | 338/590 [02:14<01:12,  3.46it/s] 57%|█████▋    | 339/590 [02:14<01:12,  3.44it/s] 58%|█████▊    | 340/590 [02:15<01:12,  3.45it/s] 58%|█████▊    | 341/590 [02:15<01:12,  3.46it/s] 58%|█████▊    | 342/590 [02:15<01:11,  3.45it/s] 58%|█████▊    | 343/590 [02:15<01:11,  3.46it/s] 58%|█████▊    | 344/590 [02:16<01:11,  3.46it/s] 58%|█████▊    | 345/590 [02:16<01:10,  3.46it/s] 59%|█████▊    | 346/590 [02:16<01:10,  3.46it/s] 59%|█████▉    | 347/590 [02:17<01:10,  3.46it/s] 59%|█████▉    | 348/590 [02:17<01:09,  3.46it/s] 59%|█████▉    | 349/590 [02:17<01:09,  3.46it/s] 59%|█████▉    | 350/590 [02:17<01:09,  3.46it/s] 59%|█████▉    | 351/590 [02:18<01:09,  3.46it/s] 60%|█████▉    | 352/590 [02:18<01:08,  3.46it/s] 60%|█████▉    | 353/590 [02:18<01:08,  3.46it/s] 60%|██████    | 354/590 [02:19<01:01,  3.84it/s][INFO|trainer.py:2140] 2023-08-29 00:28:40,062 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:28:40,062 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 00:28:40,062 >>   Batch size = 8
{'eval_loss': 0.9760839939117432, 'eval_runtime': 11.6033, 'eval_samples_per_second': 374.205, 'eval_steps_per_second': 46.797, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.85it/s][A
  2%|▏         | 12/543 [00:00<00:10, 50.96it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.19it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.50it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.10it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.77it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.67it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.19it/s][A
  9%|▉         | 48/543 [00:00<00:10, 46.89it/s][A
 10%|▉         | 53/543 [00:01<00:10, 46.99it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.04it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.03it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.05it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.08it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.06it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.10it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.88it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.74it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.86it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 46.73it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 46.89it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.00it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.12it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.09it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.07it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 46.98it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.84it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.79it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.79it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 46.75it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 46.76it/s][A
 30%|███       | 163/543 [00:03<00:08, 46.80it/s][A
 31%|███       | 168/543 [00:03<00:08, 46.83it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 46.87it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 46.87it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.70it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.75it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.79it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 46.86it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 46.97it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.08it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 47.02it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.02it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.02it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 46.88it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.83it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.83it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 45.30it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 45.92it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 46.32it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 46.49it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 46.71it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 46.78it/s][A
 50%|█████     | 273/543 [00:05<00:05, 46.81it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.68it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 46.48it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 46.40it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 46.34it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 46.36it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 46.47it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 46.57it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 46.77it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 46.96it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 46.97it/s][A
 60%|██████    | 328/543 [00:06<00:04, 46.89it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 46.75it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 46.73it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 46.84it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 46.92it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 46.97it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 46.99it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.05it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 46.95it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 44.18it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 45.12it/s][A
 71%|███████   | 383/543 [00:08<00:03, 41.48it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 43.08it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 44.28it/s][A
 73%|███████▎  | 398/543 [00:08<00:06, 23.40it/s][A
 74%|███████▍  | 403/543 [00:08<00:05, 27.63it/s][A
 75%|███████▌  | 408/543 [00:09<00:04, 31.58it/s][A
 76%|███████▌  | 413/543 [00:09<00:03, 35.09it/s][A
 77%|███████▋  | 418/543 [00:09<00:03, 38.02it/s][A
 78%|███████▊  | 423/543 [00:09<00:02, 40.23it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 42.27it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 43.62it/s][A
 81%|████████  | 438/543 [00:09<00:02, 44.56it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 44.78it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 45.42it/s][A
 83%|████████▎ | 453/543 [00:10<00:01, 45.95it/s][A
 84%|████████▍ | 458/543 [00:10<00:01, 46.28it/s][A
 85%|████████▌ | 463/543 [00:10<00:01, 46.54it/s][A
 86%|████████▌ | 468/543 [00:10<00:01, 46.69it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.88it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 46.94it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 46.86it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 46.69it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 46.67it/s][A
 92%|█████████▏| 498/543 [00:11<00:00, 46.76it/s][A
 93%|█████████▎| 503/543 [00:11<00:00, 46.87it/s][A
 94%|█████████▎| 508/543 [00:11<00:00, 46.89it/s][A
 94%|█████████▍| 513/543 [00:11<00:00, 46.96it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 47.11it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 47.09it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 47.00it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 46.85it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 46.77it/s][A
100%|██████████| 543/543 [00:11<00:00, 46.66it/s][A                                                 
                                                 [A 60%|██████    | 354/590 [02:31<01:01,  3.84it/s]
100%|██████████| 543/543 [00:11<00:00, 46.66it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:28:52,054 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-29 00:28:52,070 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:28:54,172 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:28:54,190 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:28:54,204 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [02:37<22:45,  5.81s/it] 60%|██████    | 356/590 [02:38<16:12,  4.15s/it] 61%|██████    | 357/590 [02:38<11:37,  2.99s/it] 61%|██████    | 358/590 [02:38<08:26,  2.18s/it] 61%|██████    | 359/590 [02:38<06:12,  1.61s/it] 61%|██████    | 360/590 [02:39<04:39,  1.22s/it] 61%|██████    | 361/590 [02:39<03:34,  1.07it/s] 61%|██████▏   | 362/590 [02:39<02:49,  1.35it/s] 62%|██████▏   | 363/590 [02:40<02:17,  1.65it/s] 62%|██████▏   | 364/590 [02:40<01:55,  1.96it/s] 62%|██████▏   | 365/590 [02:40<01:39,  2.25it/s] 62%|██████▏   | 366/590 [02:40<01:28,  2.52it/s] 62%|██████▏   | 367/590 [02:41<01:21,  2.74it/s] 62%|██████▏   | 368/590 [02:41<01:15,  2.93it/s] 63%|██████▎   | 369/590 [02:41<01:12,  3.07it/s] 63%|██████▎   | 370/590 [02:42<01:09,  3.18it/s] 63%|██████▎   | 371/590 [02:42<01:07,  3.26it/s] 63%|██████▎   | 372/590 [02:42<01:05,  3.32it/s] 63%|██████▎   | 373/590 [02:42<01:04,  3.36it/s] 63%|██████▎   | 374/590 [02:43<01:03,  3.39it/s] 64%|██████▎   | 375/590 [02:43<01:02,  3.42it/s] 64%|██████▎   | 376/590 [02:43<01:02,  3.43it/s] 64%|██████▍   | 377/590 [02:44<01:01,  3.44it/s] 64%|██████▍   | 378/590 [02:44<01:01,  3.44it/s] 64%|██████▍   | 379/590 [02:44<01:01,  3.45it/s] 64%|██████▍   | 380/590 [02:45<01:00,  3.45it/s] 65%|██████▍   | 381/590 [02:45<01:00,  3.46it/s] 65%|██████▍   | 382/590 [02:45<01:00,  3.46it/s] 65%|██████▍   | 383/590 [02:45<00:59,  3.46it/s] 65%|██████▌   | 384/590 [02:46<00:59,  3.47it/s] 65%|██████▌   | 385/590 [02:46<00:59,  3.47it/s] 65%|██████▌   | 386/590 [02:46<00:58,  3.47it/s] 66%|██████▌   | 387/590 [02:47<00:58,  3.47it/s] 66%|██████▌   | 388/590 [02:47<00:58,  3.47it/s] 66%|██████▌   | 389/590 [02:47<00:58,  3.46it/s] 66%|██████▌   | 390/590 [02:47<00:57,  3.46it/s] 66%|██████▋   | 391/590 [02:48<00:57,  3.46it/s] 66%|██████▋   | 392/590 [02:48<00:57,  3.46it/s] 67%|██████▋   | 393/590 [02:48<00:56,  3.47it/s] 67%|██████▋   | 394/590 [02:49<00:56,  3.47it/s] 67%|██████▋   | 395/590 [02:49<00:56,  3.47it/s] 67%|██████▋   | 396/590 [02:49<00:55,  3.47it/s] 67%|██████▋   | 397/590 [02:49<00:55,  3.46it/s] 67%|██████▋   | 398/590 [02:50<00:55,  3.46it/s] 68%|██████▊   | 399/590 [02:50<00:55,  3.46it/s] 68%|██████▊   | 400/590 [02:50<00:54,  3.46it/s] 68%|██████▊   | 401/590 [02:51<00:54,  3.46it/s] 68%|██████▊   | 402/590 [02:51<00:54,  3.46it/s] 68%|██████▊   | 403/590 [02:51<00:54,  3.46it/s] 68%|██████▊   | 404/590 [02:51<00:53,  3.46it/s] 69%|██████▊   | 405/590 [02:52<00:53,  3.47it/s] 69%|██████▉   | 406/590 [02:52<00:53,  3.47it/s] 69%|██████▉   | 407/590 [02:52<00:52,  3.47it/s] 69%|██████▉   | 408/590 [02:53<00:52,  3.46it/s] 69%|██████▉   | 409/590 [02:53<00:52,  3.46it/s] 69%|██████▉   | 410/590 [02:53<00:51,  3.46it/s] 70%|██████▉   | 411/590 [02:53<00:51,  3.46it/s] 70%|██████▉   | 412/590 [02:54<00:51,  3.46it/s] 70%|███████   | 413/590 [02:54<00:51,  3.46it/s] 70%|███████   | 414/590 [02:54<00:50,  3.47it/s] 70%|███████   | 415/590 [02:55<00:50,  3.46it/s] 71%|███████   | 416/590 [02:55<00:50,  3.46it/s] 71%|███████   | 417/590 [02:55<00:49,  3.46it/s] 71%|███████   | 418/590 [02:55<00:49,  3.46it/s] 71%|███████   | 419/590 [02:56<00:49,  3.47it/s] 71%|███████   | 420/590 [02:56<00:49,  3.46it/s] 71%|███████▏  | 421/590 [02:56<00:48,  3.46it/s] 72%|███████▏  | 422/590 [02:57<00:48,  3.47it/s] 72%|███████▏  | 423/590 [02:57<00:48,  3.46it/s] 72%|███████▏  | 424/590 [02:57<00:47,  3.46it/s] 72%|███████▏  | 425/590 [02:57<00:47,  3.46it/s] 72%|███████▏  | 426/590 [02:58<00:47,  3.45it/s] 72%|███████▏  | 427/590 [02:58<00:47,  3.46it/s] 73%|███████▎  | 428/590 [02:58<00:46,  3.46it/s] 73%|███████▎  | 429/590 [02:59<00:46,  3.46it/s] 73%|███████▎  | 430/590 [02:59<00:46,  3.46it/s] 73%|███████▎  | 431/590 [02:59<00:45,  3.46it/s] 73%|███████▎  | 432/590 [03:00<00:45,  3.46it/s] 73%|███████▎  | 433/590 [03:00<00:45,  3.46it/s] 74%|███████▎  | 434/590 [03:00<00:45,  3.46it/s] 74%|███████▎  | 435/590 [03:00<00:44,  3.46it/s] 74%|███████▍  | 436/590 [03:01<00:44,  3.46it/s] 74%|███████▍  | 437/590 [03:01<00:44,  3.45it/s] 74%|███████▍  | 438/590 [03:01<00:43,  3.46it/s] 74%|███████▍  | 439/590 [03:02<00:43,  3.46it/s] 75%|███████▍  | 440/590 [03:02<00:43,  3.46it/s] 75%|███████▍  | 441/590 [03:02<00:43,  3.46it/s] 75%|███████▍  | 442/590 [03:02<00:42,  3.46it/s] 75%|███████▌  | 443/590 [03:03<00:42,  3.46it/s] 75%|███████▌  | 444/590 [03:03<00:42,  3.46it/s] 75%|███████▌  | 445/590 [03:03<00:42,  3.45it/s] 76%|███████▌  | 446/590 [03:04<00:41,  3.46it/s] 76%|███████▌  | 447/590 [03:04<00:41,  3.46it/s] 76%|███████▌  | 448/590 [03:04<00:41,  3.45it/s] 76%|███████▌  | 449/590 [03:04<00:40,  3.46it/s] 76%|███████▋  | 450/590 [03:05<00:40,  3.46it/s] 76%|███████▋  | 451/590 [03:05<00:40,  3.46it/s] 77%|███████▋  | 452/590 [03:05<00:39,  3.46it/s] 77%|███████▋  | 453/590 [03:06<00:39,  3.46it/s] 77%|███████▋  | 454/590 [03:06<00:39,  3.46it/s] 77%|███████▋  | 455/590 [03:06<00:38,  3.46it/s] 77%|███████▋  | 456/590 [03:06<00:38,  3.46it/s] 77%|███████▋  | 457/590 [03:07<00:38,  3.46it/s] 78%|███████▊  | 458/590 [03:07<00:38,  3.46it/s] 78%|███████▊  | 459/590 [03:07<00:37,  3.46it/s] 78%|███████▊  | 460/590 [03:08<00:37,  3.46it/s] 78%|███████▊  | 461/590 [03:08<00:37,  3.46it/s] 78%|███████▊  | 462/590 [03:08<00:36,  3.46it/s] 78%|███████▊  | 463/590 [03:08<00:36,  3.46it/s] 79%|███████▊  | 464/590 [03:09<00:36,  3.46it/s] 79%|███████▉  | 465/590 [03:09<00:36,  3.46it/s] 79%|███████▉  | 466/590 [03:09<00:35,  3.46it/s] 79%|███████▉  | 467/590 [03:10<00:35,  3.46it/s] 79%|███████▉  | 468/590 [03:10<00:35,  3.46it/s] 79%|███████▉  | 469/590 [03:10<00:34,  3.46it/s] 80%|███████▉  | 470/590 [03:11<00:34,  3.46it/s] 80%|███████▉  | 471/590 [03:11<00:34,  3.46it/s] 80%|████████  | 472/590 [03:11<00:30,  3.84it/s][INFO|trainer.py:2140] 2023-08-29 00:29:32,511 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:29:32,512 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 00:29:32,512 >>   Batch size = 8
{'eval_loss': 0.9847042560577393, 'eval_runtime': 11.9799, 'eval_samples_per_second': 362.439, 'eval_steps_per_second': 45.326, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.81it/s][A
  2%|▏         | 12/543 [00:00<00:10, 51.01it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.09it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.51it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.08it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.73it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.64it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.13it/s][A
  9%|▉         | 48/543 [00:00<00:10, 46.94it/s][A
 10%|▉         | 53/543 [00:01<00:10, 47.00it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.03it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.08it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.02it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.17it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.19it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.22it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 47.01it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.89it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.86it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 46.85it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 46.86it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.01it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.10it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.05it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.12it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 47.07it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.94it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.90it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.73it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 46.88it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.02it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.13it/s][A
 31%|███       | 168/543 [00:03<00:07, 46.95it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.05it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 47.06it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.95it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.80it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.82it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 46.81it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 46.99it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.09it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 47.03it/s][A
 40%|████      | 218/543 [00:04<00:06, 46.99it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.05it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 46.92it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.88it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.83it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 46.65it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 46.71it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 46.75it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 46.71it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 46.80it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 46.88it/s][A
 50%|█████     | 273/543 [00:05<00:05, 46.75it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.83it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 46.79it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 46.77it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 46.87it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 46.88it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 46.88it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.05it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.06it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 46.95it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 46.92it/s][A
 60%|██████    | 328/543 [00:06<00:04, 46.88it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 44.18it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 45.03it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 45.70it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 46.15it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 46.50it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 46.73it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 46.68it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 46.78it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 46.48it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 46.45it/s][A
 71%|███████   | 383/543 [00:08<00:03, 46.52it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 46.56it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 46.59it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 46.66it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 46.71it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 46.72it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 46.67it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 46.47it/s][A
 78%|███████▊  | 423/543 [00:09<00:02, 46.62it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 46.67it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 46.74it/s][A
 81%|████████  | 438/543 [00:09<00:02, 46.90it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 46.94it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 46.98it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.11it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 47.07it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 46.87it/s][A
 86%|████████▌ | 468/543 [00:10<00:01, 46.82it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 40.81it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 42.51it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 43.78it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 44.78it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 45.40it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 46.01it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 46.38it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 46.67it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 46.30it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 46.42it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 46.56it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 46.72it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 46.71it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 46.95it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.07it/s][A                                                 
                                                 [A 80%|████████  | 472/590 [03:23<00:30,  3.84it/s]
100%|██████████| 543/543 [00:11<00:00, 47.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:29:44,431 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-29 00:29:44,677 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:29:47,146 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:29:47,172 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:29:47,183 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [03:32<12:43,  6.52s/it] 80%|████████  | 474/590 [03:32<08:59,  4.66s/it] 81%|████████  | 475/590 [03:33<06:24,  3.34s/it] 81%|████████  | 476/590 [03:33<04:36,  2.43s/it] 81%|████████  | 477/590 [03:33<03:21,  1.79s/it] 81%|████████  | 478/590 [03:34<02:29,  1.34s/it] 81%|████████  | 479/590 [03:34<01:53,  1.02s/it] 81%|████████▏ | 480/590 [03:34<01:28,  1.25it/s] 82%|████████▏ | 481/590 [03:34<01:10,  1.55it/s] 82%|████████▏ | 482/590 [03:35<00:58,  1.85it/s] 82%|████████▏ | 483/590 [03:35<00:49,  2.16it/s] 82%|████████▏ | 484/590 [03:35<00:43,  2.43it/s] 82%|████████▏ | 485/590 [03:36<00:39,  2.67it/s] 82%|████████▏ | 486/590 [03:36<00:36,  2.87it/s] 83%|████████▎ | 487/590 [03:36<00:34,  3.03it/s] 83%|████████▎ | 488/590 [03:36<00:32,  3.15it/s] 83%|████████▎ | 489/590 [03:37<00:31,  3.24it/s] 83%|████████▎ | 490/590 [03:37<00:30,  3.31it/s] 83%|████████▎ | 491/590 [03:37<00:29,  3.35it/s] 83%|████████▎ | 492/590 [03:38<00:28,  3.39it/s] 84%|████████▎ | 493/590 [03:38<00:28,  3.42it/s] 84%|████████▎ | 494/590 [03:38<00:27,  3.44it/s] 84%|████████▍ | 495/590 [03:38<00:27,  3.45it/s] 84%|████████▍ | 496/590 [03:39<00:27,  3.45it/s] 84%|████████▍ | 497/590 [03:39<00:26,  3.46it/s] 84%|████████▍ | 498/590 [03:39<00:26,  3.46it/s] 85%|████████▍ | 499/590 [03:40<00:26,  3.46it/s] 85%|████████▍ | 500/590 [03:40<00:25,  3.47it/s]                                                  85%|████████▍ | 500/590 [03:40<00:25,  3.47it/s] 85%|████████▍ | 501/590 [03:40<00:25,  3.47it/s] 85%|████████▌ | 502/590 [03:40<00:25,  3.47it/s] 85%|████████▌ | 503/590 [03:41<00:25,  3.47it/s] 85%|████████▌ | 504/590 [03:41<00:24,  3.47it/s] 86%|████████▌ | 505/590 [03:41<00:24,  3.47it/s] 86%|████████▌ | 506/590 [03:42<00:24,  3.47it/s] 86%|████████▌ | 507/590 [03:42<00:24,  3.44it/s] 86%|████████▌ | 508/590 [03:42<00:23,  3.45it/s] 86%|████████▋ | 509/590 [03:43<00:23,  3.45it/s] 86%|████████▋ | 510/590 [03:43<00:23,  3.43it/s] 87%|████████▋ | 511/590 [03:43<00:22,  3.44it/s] 87%|████████▋ | 512/590 [03:43<00:22,  3.45it/s] 87%|████████▋ | 513/590 [03:44<00:22,  3.46it/s] 87%|████████▋ | 514/590 [03:44<00:21,  3.46it/s] 87%|████████▋ | 515/590 [03:44<00:21,  3.46it/s] 87%|████████▋ | 516/590 [03:45<00:21,  3.46it/s] 88%|████████▊ | 517/590 [03:45<00:21,  3.46it/s] 88%|████████▊ | 518/590 [03:45<00:20,  3.46it/s] 88%|████████▊ | 519/590 [03:45<00:20,  3.46it/s] 88%|████████▊ | 520/590 [03:46<00:20,  3.46it/s] 88%|████████▊ | 521/590 [03:46<00:19,  3.46it/s] 88%|████████▊ | 522/590 [03:46<00:19,  3.46it/s] 89%|████████▊ | 523/590 [03:47<00:19,  3.46it/s] 89%|████████▉ | 524/590 [03:47<00:19,  3.46it/s] 89%|████████▉ | 525/590 [03:47<00:18,  3.46it/s] 89%|████████▉ | 526/590 [03:47<00:18,  3.46it/s] 89%|████████▉ | 527/590 [03:48<00:18,  3.46it/s] 89%|████████▉ | 528/590 [03:48<00:17,  3.46it/s] 90%|████████▉ | 529/590 [03:48<00:17,  3.46it/s] 90%|████████▉ | 530/590 [03:49<00:17,  3.47it/s] 90%|█████████ | 531/590 [03:49<00:17,  3.46it/s] 90%|█████████ | 532/590 [03:49<00:16,  3.46it/s] 90%|█████████ | 533/590 [03:49<00:16,  3.46it/s] 91%|█████████ | 534/590 [03:50<00:16,  3.46it/s] 91%|█████████ | 535/590 [03:50<00:15,  3.47it/s] 91%|█████████ | 536/590 [03:50<00:15,  3.47it/s] 91%|█████████ | 537/590 [03:51<00:15,  3.47it/s] 91%|█████████ | 538/590 [03:51<00:15,  3.46it/s] 91%|█████████▏| 539/590 [03:51<00:14,  3.47it/s] 92%|█████████▏| 540/590 [03:51<00:14,  3.47it/s] 92%|█████████▏| 541/590 [03:52<00:14,  3.46it/s] 92%|█████████▏| 542/590 [03:52<00:13,  3.46it/s] 92%|█████████▏| 543/590 [03:52<00:13,  3.45it/s] 92%|█████████▏| 544/590 [03:53<00:13,  3.45it/s] 92%|█████████▏| 545/590 [03:53<00:13,  3.46it/s] 93%|█████████▎| 546/590 [03:53<00:12,  3.46it/s] 93%|█████████▎| 547/590 [03:53<00:12,  3.47it/s] 93%|█████████▎| 548/590 [03:54<00:12,  3.47it/s] 93%|█████████▎| 549/590 [03:54<00:11,  3.47it/s] 93%|█████████▎| 550/590 [03:54<00:11,  3.47it/s] 93%|█████████▎| 551/590 [03:55<00:11,  3.46it/s] 94%|█████████▎| 552/590 [03:55<00:10,  3.46it/s] 94%|█████████▎| 553/590 [03:55<00:10,  3.46it/s] 94%|█████████▍| 554/590 [03:56<00:10,  3.46it/s] 94%|█████████▍| 555/590 [03:56<00:10,  3.46it/s] 94%|█████████▍| 556/590 [03:56<00:09,  3.45it/s] 94%|█████████▍| 557/590 [03:56<00:09,  3.45it/s] 95%|█████████▍| 558/590 [03:57<00:09,  3.45it/s] 95%|█████████▍| 559/590 [03:57<00:08,  3.46it/s] 95%|█████████▍| 560/590 [03:57<00:08,  3.46it/s] 95%|█████████▌| 561/590 [03:58<00:08,  3.46it/s] 95%|█████████▌| 562/590 [03:58<00:08,  3.46it/s] 95%|█████████▌| 563/590 [03:58<00:07,  3.46it/s] 96%|█████████▌| 564/590 [03:58<00:07,  3.46it/s] 96%|█████████▌| 565/590 [03:59<00:07,  3.46it/s] 96%|█████████▌| 566/590 [03:59<00:06,  3.47it/s] 96%|█████████▌| 567/590 [03:59<00:06,  3.46it/s] 96%|█████████▋| 568/590 [04:00<00:06,  3.46it/s] 96%|█████████▋| 569/590 [04:00<00:06,  3.47it/s] 97%|█████████▋| 570/590 [04:00<00:05,  3.47it/s] 97%|█████████▋| 571/590 [04:00<00:05,  3.47it/s] 97%|█████████▋| 572/590 [04:01<00:05,  3.47it/s] 97%|█████████▋| 573/590 [04:01<00:04,  3.47it/s] 97%|█████████▋| 574/590 [04:01<00:04,  3.46it/s] 97%|█████████▋| 575/590 [04:02<00:04,  3.46it/s] 98%|█████████▊| 576/590 [04:02<00:04,  3.46it/s] 98%|█████████▊| 577/590 [04:02<00:03,  3.46it/s] 98%|█████████▊| 578/590 [04:02<00:03,  3.46it/s] 98%|█████████▊| 579/590 [04:03<00:03,  3.46it/s] 98%|█████████▊| 580/590 [04:03<00:02,  3.46it/s] 98%|█████████▊| 581/590 [04:03<00:02,  3.46it/s] 99%|█████████▊| 582/590 [04:04<00:02,  3.46it/s] 99%|█████████▉| 583/590 [04:04<00:02,  3.46it/s] 99%|█████████▉| 584/590 [04:04<00:01,  3.46it/s] 99%|█████████▉| 585/590 [04:04<00:01,  3.46it/s] 99%|█████████▉| 586/590 [04:05<00:01,  3.46it/s] 99%|█████████▉| 587/590 [04:05<00:00,  3.46it/s]100%|█████████▉| 588/590 [04:05<00:00,  3.46it/s]100%|█████████▉| 589/590 [04:06<00:00,  3.46it/s]100%|██████████| 590/590 [04:06<00:00,  3.84it/s][INFO|trainer.py:2140] 2023-08-29 00:30:27,341 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:30:27,341 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 00:30:27,341 >>   Batch size = 8
{'eval_loss': 0.994333803653717, 'eval_runtime': 11.6428, 'eval_samples_per_second': 372.935, 'eval_steps_per_second': 46.638, 'epoch': 4.0}
{'loss': 0.7135, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.54it/s][A
  2%|▏         | 12/543 [00:00<00:10, 50.95it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.24it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.39it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.03it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.73it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.58it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.16it/s][A
  9%|▉         | 48/543 [00:00<00:10, 46.95it/s][A
 10%|▉         | 53/543 [00:01<00:10, 46.88it/s][A
 11%|█         | 58/543 [00:01<00:10, 46.83it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 46.99it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.08it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.17it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.10it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.10it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.91it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.80it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.69it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 46.89it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 46.92it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.05it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.13it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.13it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.12it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 46.96it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.79it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.71it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.80it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 46.98it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.10it/s][A
 30%|███       | 163/543 [00:03<00:08, 46.95it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.00it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 46.98it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 46.93it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.86it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.74it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.75it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 46.89it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.00it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.04it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 47.04it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.02it/s][A
 41%|████      | 223/543 [00:04<00:06, 46.91it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 46.93it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.90it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.87it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 46.99it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 46.95it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 46.86it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.04it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.03it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 46.87it/s][A
 50%|█████     | 273/543 [00:05<00:05, 46.81it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.77it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 46.80it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 46.93it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 46.89it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 46.87it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 46.99it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 46.98it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 46.92it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 46.92it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 46.83it/s][A
 60%|██████    | 328/543 [00:06<00:04, 46.67it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 46.73it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 46.86it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 46.80it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 46.95it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.03it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 46.99it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 46.95it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 46.81it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 46.75it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 46.89it/s][A
 71%|███████   | 383/543 [00:08<00:03, 46.93it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 46.92it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 47.01it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 46.95it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 46.87it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 46.94it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 46.86it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 46.36it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 46.60it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 46.74it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 46.84it/s][A
 81%|████████  | 438/543 [00:09<00:02, 46.86it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 46.92it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 46.91it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 46.78it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 46.81it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 46.70it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 46.80it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.88it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 46.83it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 46.97it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 46.97it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 46.91it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 46.65it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 46.67it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 46.59it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 46.72it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 46.76it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 46.93it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 46.97it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 47.02it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 46.95it/s][A
100%|██████████| 543/543 [00:11<00:00, 46.86it/s][A                                                 
                                                 [A100%|██████████| 590/590 [04:17<00:00,  3.84it/s]
100%|██████████| 543/543 [00:11<00:00, 46.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:30:38,932 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-29 00:30:38,971 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:30:41,238 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:30:41,253 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:30:41,263 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:30:46,037 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:30:46,039 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118 (score: 0.9708923697471619).
                                                 100%|██████████| 590/590 [04:26<00:00,  3.84it/s]100%|██████████| 590/590 [04:26<00:00,  2.21it/s]
[INFO|trainer.py:1894] 2023-08-29 00:30:47,782 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 00:30:47,797 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:30:50,123 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:30:50,153 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:30:50,164 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:30:50,414 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:50,414 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:50,414 >>   train_loss               =     0.7094
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:50,414 >>   train_runtime            = 0:04:26.75
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:50,414 >>   train_samples            =       7524
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:50,414 >>   train_samples_per_second =    141.026
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:30:50,414 >>   train_steps_per_second   =      2.212
{'eval_loss': 0.9982038736343384, 'eval_runtime': 11.5743, 'eval_samples_per_second': 375.14, 'eval_steps_per_second': 46.914, 'epoch': 5.0}
{'train_runtime': 266.7592, 'train_samples_per_second': 141.026, 'train_steps_per_second': 2.212, 'train_loss': 0.709431444588354, 'epoch': 5.0}
08/29/2023 00:30:50 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:30:50,462 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:30:50,462 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 00:30:50,462 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 59.18it/s]  2%|▏         | 12/543 [00:00<00:10, 51.60it/s]  3%|▎         | 18/543 [00:00<00:10, 49.72it/s]  4%|▍         | 24/543 [00:00<00:10, 48.90it/s]  5%|▌         | 29/543 [00:00<00:10, 48.52it/s]  6%|▋         | 34/543 [00:00<00:10, 48.28it/s]  7%|▋         | 39/543 [00:00<00:10, 48.11it/s]  8%|▊         | 44/543 [00:00<00:10, 47.73it/s]  9%|▉         | 49/543 [00:01<00:10, 47.20it/s] 10%|▉         | 54/543 [00:01<00:10, 47.17it/s] 11%|█         | 59/543 [00:01<00:10, 47.25it/s] 12%|█▏        | 64/543 [00:01<00:10, 47.32it/s] 13%|█▎        | 69/543 [00:01<00:10, 47.40it/s] 14%|█▎        | 74/543 [00:01<00:09, 47.46it/s] 15%|█▍        | 79/543 [00:01<00:09, 47.39it/s] 15%|█▌        | 84/543 [00:01<00:09, 47.46it/s] 16%|█▋        | 89/543 [00:01<00:09, 47.34it/s] 17%|█▋        | 94/543 [00:01<00:09, 47.21it/s] 18%|█▊        | 99/543 [00:02<00:09, 47.16it/s] 19%|█▉        | 104/543 [00:02<00:09, 47.03it/s] 20%|██        | 109/543 [00:02<00:09, 47.20it/s] 21%|██        | 114/543 [00:02<00:09, 47.15it/s] 22%|██▏       | 119/543 [00:02<00:08, 47.25it/s] 23%|██▎       | 124/543 [00:02<00:09, 45.17it/s] 24%|██▍       | 129/543 [00:02<00:09, 45.74it/s] 25%|██▍       | 134/543 [00:02<00:08, 46.25it/s] 26%|██▌       | 139/543 [00:02<00:08, 46.48it/s] 27%|██▋       | 144/543 [00:03<00:08, 46.59it/s] 27%|██▋       | 149/543 [00:03<00:08, 46.76it/s] 28%|██▊       | 154/543 [00:03<00:08, 46.90it/s] 29%|██▉       | 159/543 [00:03<00:08, 47.10it/s] 30%|███       | 164/543 [00:03<00:08, 47.19it/s] 31%|███       | 169/543 [00:03<00:07, 47.09it/s] 32%|███▏      | 174/543 [00:03<00:07, 47.09it/s] 33%|███▎      | 179/543 [00:03<00:07, 47.29it/s] 34%|███▍      | 184/543 [00:03<00:07, 47.26it/s] 35%|███▍      | 189/543 [00:03<00:07, 47.14it/s] 36%|███▌      | 194/543 [00:04<00:07, 47.18it/s] 37%|███▋      | 199/543 [00:04<00:07, 47.16it/s] 38%|███▊      | 204/543 [00:04<00:07, 47.02it/s] 38%|███▊      | 209/543 [00:04<00:07, 47.16it/s] 39%|███▉      | 214/543 [00:04<00:06, 47.13it/s] 40%|████      | 219/543 [00:04<00:06, 47.16it/s] 41%|████▏     | 224/543 [00:04<00:06, 47.29it/s] 42%|████▏     | 229/543 [00:04<00:06, 47.30it/s] 43%|████▎     | 234/543 [00:04<00:06, 47.33it/s] 44%|████▍     | 239/543 [00:05<00:06, 47.16it/s] 45%|████▍     | 244/543 [00:05<00:06, 47.09it/s] 46%|████▌     | 249/543 [00:05<00:06, 47.15it/s] 47%|████▋     | 254/543 [00:05<00:06, 47.12it/s] 48%|████▊     | 259/543 [00:05<00:06, 47.14it/s] 49%|████▊     | 264/543 [00:05<00:05, 47.21it/s] 50%|████▉     | 269/543 [00:05<00:05, 47.15it/s] 50%|█████     | 274/543 [00:05<00:05, 47.21it/s] 51%|█████▏    | 279/543 [00:05<00:05, 47.32it/s] 52%|█████▏    | 284/543 [00:06<00:05, 47.27it/s] 53%|█████▎    | 289/543 [00:06<00:05, 47.24it/s] 54%|█████▍    | 294/543 [00:06<00:05, 47.21it/s] 55%|█████▌    | 299/543 [00:06<00:05, 47.18it/s] 56%|█████▌    | 304/543 [00:06<00:05, 47.10it/s] 57%|█████▋    | 309/543 [00:06<00:04, 47.16it/s] 58%|█████▊    | 314/543 [00:06<00:04, 47.22it/s] 59%|█████▊    | 319/543 [00:06<00:04, 47.25it/s] 60%|█████▉    | 324/543 [00:06<00:04, 47.18it/s] 61%|██████    | 329/543 [00:06<00:04, 47.25it/s] 62%|██████▏   | 334/543 [00:07<00:04, 47.19it/s] 62%|██████▏   | 339/543 [00:07<00:04, 47.15it/s] 63%|██████▎   | 344/543 [00:07<00:04, 47.12it/s] 64%|██████▍   | 349/543 [00:07<00:04, 47.09it/s] 65%|██████▌   | 354/543 [00:07<00:04, 47.01it/s] 66%|██████▌   | 359/543 [00:07<00:03, 47.01it/s] 67%|██████▋   | 364/543 [00:07<00:03, 47.07it/s] 68%|██████▊   | 369/543 [00:07<00:03, 47.05it/s] 69%|██████▉   | 374/543 [00:07<00:03, 47.15it/s] 70%|██████▉   | 379/543 [00:08<00:03, 47.23it/s] 71%|███████   | 384/543 [00:08<00:03, 47.20it/s] 72%|███████▏  | 389/543 [00:08<00:03, 47.15it/s] 73%|███████▎  | 394/543 [00:08<00:03, 46.97it/s] 73%|███████▎  | 399/543 [00:08<00:03, 47.06it/s] 74%|███████▍  | 404/543 [00:08<00:02, 46.98it/s] 75%|███████▌  | 409/543 [00:08<00:02, 47.11it/s] 76%|███████▌  | 414/543 [00:08<00:02, 47.04it/s] 77%|███████▋  | 419/543 [00:08<00:02, 47.09it/s] 78%|███████▊  | 424/543 [00:08<00:02, 47.16it/s] 79%|███████▉  | 429/543 [00:09<00:02, 47.12it/s] 80%|███████▉  | 434/543 [00:09<00:02, 47.11it/s] 81%|████████  | 439/543 [00:09<00:02, 47.00it/s] 82%|████████▏ | 444/543 [00:09<00:02, 46.98it/s] 83%|████████▎ | 449/543 [00:09<00:01, 47.03it/s] 84%|████████▎ | 454/543 [00:09<00:01, 47.08it/s] 85%|████████▍ | 459/543 [00:09<00:01, 47.06it/s] 85%|████████▌ | 464/543 [00:09<00:01, 47.05it/s] 86%|████████▋ | 469/543 [00:09<00:01, 47.13it/s] 87%|████████▋ | 474/543 [00:10<00:01, 47.20it/s] 88%|████████▊ | 479/543 [00:10<00:01, 47.14it/s] 89%|████████▉ | 484/543 [00:10<00:01, 47.09it/s] 90%|█████████ | 489/543 [00:10<00:01, 46.96it/s] 91%|█████████ | 494/543 [00:10<00:01, 47.00it/s] 92%|█████████▏| 499/543 [00:10<00:00, 47.01it/s] 93%|█████████▎| 504/543 [00:10<00:00, 47.13it/s] 94%|█████████▎| 509/543 [00:10<00:00, 47.14it/s] 95%|█████████▍| 514/543 [00:10<00:00, 47.08it/s] 96%|█████████▌| 519/543 [00:10<00:00, 47.13it/s] 97%|█████████▋| 524/543 [00:11<00:00, 47.12it/s] 97%|█████████▋| 529/543 [00:11<00:00, 47.07it/s] 98%|█████████▊| 534/543 [00:11<00:00, 47.04it/s] 99%|█████████▉| 539/543 [00:11<00:00, 47.04it/s]100%|██████████| 543/543 [00:11<00:00, 47.19it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:31:01,992 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:31:01,992 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:31:01,992 >>   eval_loss               =     0.9709
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:31:01,992 >>   eval_runtime            = 0:00:11.52
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:31:01,992 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:31:01,992 >>   eval_samples_per_second =    376.582
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:31:01,992 >>   eval_steps_per_second   =     47.094
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:31:01,992 >>   perplexity              =     2.6403
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:31:07,242 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:31:07,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:31:07,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:31:07,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:31:07,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:31:07,523 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:31:07,524 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:31:07,786 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:31:08,811 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:31:08,814 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:31:10,754 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:31:10,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:31:10,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:31:10,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:31:10,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:31:11,107 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:31:11,109 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:31:11,369 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:31:11,522 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:31:11,522 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-472
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-590
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-236
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-354
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.71it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.67it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.72it/s]Extractor Predicting: 11it [00:06,  1.73it/s]Extractor Predicting: 12it [00:07,  1.73it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:09,  1.54it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:11,  1.47it/s]Extractor Predicting: 20it [00:12,  1.45it/s]Extractor Predicting: 21it [00:13,  1.48it/s]Extractor Predicting: 22it [00:13,  1.51it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:15,  1.51it/s]Extractor Predicting: 26it [00:16,  1.50it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:17,  1.51it/s]Extractor Predicting: 29it [00:18,  1.49it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:19,  1.51it/s]Extractor Predicting: 32it [00:20,  1.51it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:21,  1.50it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:23,  1.46it/s]Extractor Predicting: 37it [00:23,  1.46it/s]Extractor Predicting: 38it [00:24,  1.45it/s]Extractor Predicting: 39it [00:25,  1.48it/s]Extractor Predicting: 40it [00:25,  1.49it/s]Extractor Predicting: 41it [00:26,  1.47it/s]Extractor Predicting: 42it [00:27,  1.46it/s]Extractor Predicting: 43it [00:28,  1.41it/s]Extractor Predicting: 44it [00:28,  1.45it/s]Extractor Predicting: 45it [00:29,  1.47it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.50it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:31,  1.49it/s]Extractor Predicting: 50it [00:32,  1.50it/s]Extractor Predicting: 51it [00:33,  1.51it/s]Extractor Predicting: 52it [00:33,  1.50it/s]Extractor Predicting: 53it [00:34,  1.47it/s]Extractor Predicting: 54it [00:35,  1.51it/s]Extractor Predicting: 55it [00:35,  1.54it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:37,  1.49it/s]Extractor Predicting: 58it [00:37,  1.53it/s]Extractor Predicting: 59it [00:38,  1.49it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:39,  1.51it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 64it [00:41,  1.52it/s]Extractor Predicting: 65it [00:42,  1.56it/s]Extractor Predicting: 66it [00:43,  1.55it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:44,  1.52it/s]Extractor Predicting: 69it [00:45,  1.53it/s]Extractor Predicting: 70it [00:45,  1.52it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:47,  1.52it/s]Extractor Predicting: 73it [00:47,  1.50it/s]Extractor Predicting: 74it [00:48,  1.48it/s]Extractor Predicting: 75it [00:49,  1.45it/s]Extractor Predicting: 76it [00:49,  1.47it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.49it/s]Extractor Predicting: 79it [00:51,  1.48it/s]Extractor Predicting: 80it [00:52,  1.50it/s]Extractor Predicting: 81it [00:53,  1.49it/s]Extractor Predicting: 82it [00:53,  1.50it/s]Extractor Predicting: 83it [00:54,  1.51it/s]Extractor Predicting: 84it [00:55,  1.52it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:56,  1.50it/s]Extractor Predicting: 87it [00:57,  1.51it/s]Extractor Predicting: 88it [00:57,  1.53it/s]Extractor Predicting: 89it [00:58,  1.54it/s]Extractor Predicting: 90it [00:59,  1.50it/s]Extractor Predicting: 91it [00:59,  1.48it/s]Extractor Predicting: 92it [01:00,  1.46it/s]Extractor Predicting: 93it [01:01,  1.51it/s]Extractor Predicting: 94it [01:01,  1.52it/s]Extractor Predicting: 95it [01:02,  1.51it/s]Extractor Predicting: 96it [01:03,  1.52it/s]Extractor Predicting: 97it [01:03,  1.54it/s]Extractor Predicting: 98it [01:04,  1.52it/s]Extractor Predicting: 99it [01:05,  1.50it/s]Extractor Predicting: 100it [01:05,  1.51it/s]Extractor Predicting: 101it [01:06,  1.52it/s]Extractor Predicting: 102it [01:07,  1.51it/s]Extractor Predicting: 103it [01:07,  1.51it/s]Extractor Predicting: 104it [01:08,  1.52it/s]Extractor Predicting: 105it [01:09,  1.50it/s]Extractor Predicting: 106it [01:09,  1.51it/s]Extractor Predicting: 107it [01:10,  1.51it/s]Extractor Predicting: 108it [01:11,  1.51it/s]Extractor Predicting: 109it [01:11,  1.51it/s]Extractor Predicting: 110it [01:12,  1.50it/s]Extractor Predicting: 111it [01:13,  1.51it/s]Extractor Predicting: 112it [01:13,  1.53it/s]Extractor Predicting: 113it [01:14,  1.44it/s]Extractor Predicting: 114it [01:15,  1.46it/s]Extractor Predicting: 115it [01:15,  1.47it/s]Extractor Predicting: 116it [01:16,  1.48it/s]Extractor Predicting: 117it [01:17,  1.50it/s]Extractor Predicting: 118it [01:17,  1.46it/s]Extractor Predicting: 119it [01:18,  1.48it/s]Extractor Predicting: 120it [01:19,  1.47it/s]Extractor Predicting: 121it [01:19,  1.46it/s]Extractor Predicting: 122it [01:20,  1.45it/s]Extractor Predicting: 123it [01:21,  1.49it/s]Extractor Predicting: 124it [01:21,  1.51it/s]Extractor Predicting: 125it [01:22,  1.53it/s]Extractor Predicting: 126it [01:23,  1.48it/s]Extractor Predicting: 127it [01:23,  1.47it/s]Extractor Predicting: 128it [01:24,  1.49it/s]Extractor Predicting: 129it [01:25,  1.51it/s]Extractor Predicting: 130it [01:25,  1.55it/s]Extractor Predicting: 131it [01:26,  1.51it/s]Extractor Predicting: 132it [01:27,  1.51it/s]Extractor Predicting: 133it [01:27,  1.51it/s]Extractor Predicting: 134it [01:28,  1.51it/s]Extractor Predicting: 135it [01:29,  1.53it/s]Extractor Predicting: 136it [01:29,  1.53it/s]Extractor Predicting: 137it [01:30,  1.54it/s]Extractor Predicting: 138it [01:31,  1.51it/s]Extractor Predicting: 139it [01:31,  1.50it/s]Extractor Predicting: 140it [01:32,  1.53it/s]Extractor Predicting: 141it [01:33,  1.54it/s]Extractor Predicting: 142it [01:33,  1.56it/s]Extractor Predicting: 143it [01:34,  1.56it/s]Extractor Predicting: 144it [01:34,  1.56it/s]Extractor Predicting: 145it [01:35,  1.53it/s]Extractor Predicting: 146it [01:36,  1.54it/s]Extractor Predicting: 147it [01:36,  1.54it/s]Extractor Predicting: 148it [01:37,  1.55it/s]Extractor Predicting: 149it [01:38,  1.56it/s]Extractor Predicting: 150it [01:38,  1.51it/s]Extractor Predicting: 151it [01:39,  1.51it/s]Extractor Predicting: 152it [01:40,  1.51it/s]Extractor Predicting: 153it [01:40,  1.49it/s]Extractor Predicting: 154it [01:41,  1.50it/s]Extractor Predicting: 155it [01:42,  1.48it/s]Extractor Predicting: 156it [01:42,  1.47it/s]Extractor Predicting: 157it [01:43,  1.48it/s]Extractor Predicting: 158it [01:44,  1.49it/s]Extractor Predicting: 159it [01:44,  1.51it/s]Extractor Predicting: 160it [01:45,  1.53it/s]Extractor Predicting: 161it [01:46,  1.55it/s]Extractor Predicting: 162it [01:46,  1.55it/s]Extractor Predicting: 163it [01:47,  1.47it/s]Extractor Predicting: 163it [01:47,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:08,117 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:08,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:08,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:08,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:08,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:33:08,750 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:33:08,751 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:33:09,321 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:33:10,361 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:33:10,361 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:13,452 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:13,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:13,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:13,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:13,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:33:14,089 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:33:14,090 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:33:14,710 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:33:14,865 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:33:14,865 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.32338308457711445,
  "recall": 0.014970059880239521,
  "score": 0.028615452344265904,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.59it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:14,  1.57it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.59it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.59it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:20,  1.58it/s]Extractor Predicting: 34it [00:21,  1.57it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:26,  1.51it/s]Extractor Predicting: 43it [00:27,  1.50it/s]Extractor Predicting: 44it [00:28,  1.50it/s]Extractor Predicting: 45it [00:28,  1.48it/s]Extractor Predicting: 46it [00:29,  1.47it/s]Extractor Predicting: 47it [00:30,  1.45it/s]Extractor Predicting: 48it [00:30,  1.47it/s]Extractor Predicting: 49it [00:31,  1.46it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:32,  1.48it/s]Extractor Predicting: 52it [00:33,  1.47it/s]Extractor Predicting: 53it [00:34,  1.46it/s]Extractor Predicting: 54it [00:35,  1.42it/s]Extractor Predicting: 55it [00:35,  1.46it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:37,  1.49it/s]Extractor Predicting: 58it [00:37,  1.48it/s]Extractor Predicting: 59it [00:38,  1.45it/s]Extractor Predicting: 60it [00:39,  1.45it/s]Extractor Predicting: 61it [00:39,  1.48it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.43it/s]Extractor Predicting: 64it [00:41,  1.44it/s]Extractor Predicting: 65it [00:42,  1.44it/s]Extractor Predicting: 66it [00:43,  1.43it/s]Extractor Predicting: 67it [00:43,  1.44it/s]Extractor Predicting: 68it [00:44,  1.43it/s]Extractor Predicting: 69it [00:45,  1.44it/s]Extractor Predicting: 70it [00:46,  1.45it/s]Extractor Predicting: 71it [00:46,  1.47it/s]Extractor Predicting: 72it [00:47,  1.46it/s]Extractor Predicting: 73it [00:48,  1.45it/s]Extractor Predicting: 74it [00:48,  1.48it/s]Extractor Predicting: 75it [00:49,  1.48it/s]Extractor Predicting: 76it [00:50,  1.49it/s]Extractor Predicting: 77it [00:50,  1.52it/s]Extractor Predicting: 78it [00:51,  1.51it/s]Extractor Predicting: 79it [00:52,  1.47it/s]Extractor Predicting: 80it [00:52,  1.51it/s]Extractor Predicting: 81it [00:53,  1.39it/s]Extractor Predicting: 82it [00:54,  1.44it/s]Extractor Predicting: 83it [00:54,  1.47it/s]Extractor Predicting: 84it [00:55,  1.44it/s]Extractor Predicting: 85it [00:56,  1.46it/s]Extractor Predicting: 86it [00:56,  1.48it/s]Extractor Predicting: 87it [00:57,  1.50it/s]Extractor Predicting: 88it [00:58,  1.48it/s]Extractor Predicting: 89it [00:58,  1.51it/s]Extractor Predicting: 90it [00:59,  1.52it/s]Extractor Predicting: 91it [01:00,  1.39it/s]Extractor Predicting: 92it [01:01,  1.43it/s]Extractor Predicting: 93it [01:01,  1.46it/s]Extractor Predicting: 94it [01:02,  1.47it/s]Extractor Predicting: 95it [01:02,  1.50it/s]Extractor Predicting: 96it [01:03,  1.53it/s]Extractor Predicting: 97it [01:04,  1.53it/s]Extractor Predicting: 98it [01:04,  1.48it/s]Extractor Predicting: 99it [01:05,  1.49it/s]Extractor Predicting: 100it [01:06,  1.49it/s]Extractor Predicting: 101it [01:07,  1.48it/s]Extractor Predicting: 102it [01:07,  1.50it/s]Extractor Predicting: 103it [01:08,  1.48it/s]Extractor Predicting: 104it [01:08,  1.50it/s]Extractor Predicting: 105it [01:09,  1.49it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:11,  1.47it/s]Extractor Predicting: 109it [01:12,  1.42it/s]Extractor Predicting: 110it [01:13,  1.44it/s]Extractor Predicting: 111it [01:13,  1.44it/s]Extractor Predicting: 112it [01:14,  1.46it/s]Extractor Predicting: 113it [01:15,  1.46it/s]Extractor Predicting: 114it [01:15,  1.46it/s]Extractor Predicting: 115it [01:16,  1.46it/s]Extractor Predicting: 116it [01:17,  1.44it/s]Extractor Predicting: 117it [01:17,  1.45it/s]Extractor Predicting: 118it [01:18,  1.50it/s]Extractor Predicting: 119it [01:19,  1.49it/s]Extractor Predicting: 120it [01:19,  1.53it/s]Extractor Predicting: 121it [01:20,  1.52it/s]Extractor Predicting: 122it [01:21,  1.54it/s]Extractor Predicting: 123it [01:21,  1.56it/s]Extractor Predicting: 124it [01:22,  1.59it/s]Extractor Predicting: 125it [01:23,  1.56it/s]Extractor Predicting: 126it [01:23,  1.53it/s]Extractor Predicting: 127it [01:24,  1.53it/s]Extractor Predicting: 128it [01:25,  1.51it/s]Extractor Predicting: 129it [01:25,  1.51it/s]Extractor Predicting: 130it [01:26,  1.50it/s]Extractor Predicting: 131it [01:27,  1.56it/s]Extractor Predicting: 132it [01:27,  1.54it/s]Extractor Predicting: 133it [01:28,  1.53it/s]Extractor Predicting: 134it [01:28,  1.56it/s]Extractor Predicting: 135it [01:29,  1.55it/s]Extractor Predicting: 136it [01:30,  1.55it/s]Extractor Predicting: 137it [01:30,  1.59it/s]Extractor Predicting: 138it [01:31,  1.56it/s]Extractor Predicting: 139it [01:32,  1.57it/s]Extractor Predicting: 140it [01:32,  1.55it/s]Extractor Predicting: 141it [01:33,  1.56it/s]Extractor Predicting: 142it [01:34,  1.54it/s]Extractor Predicting: 143it [01:34,  1.52it/s]Extractor Predicting: 144it [01:35,  1.53it/s]Extractor Predicting: 145it [01:36,  1.48it/s]Extractor Predicting: 146it [01:36,  1.43it/s]Extractor Predicting: 147it [01:37,  1.46it/s]Extractor Predicting: 148it [01:38,  1.46it/s]Extractor Predicting: 149it [01:38,  1.48it/s]Extractor Predicting: 150it [01:39,  1.48it/s]Extractor Predicting: 151it [01:40,  1.49it/s]Extractor Predicting: 152it [01:40,  1.49it/s]Extractor Predicting: 153it [01:41,  1.52it/s]Extractor Predicting: 154it [01:42,  1.53it/s]Extractor Predicting: 155it [01:42,  1.55it/s]Extractor Predicting: 156it [01:43,  1.50it/s]Extractor Predicting: 157it [01:44,  1.51it/s]Extractor Predicting: 158it [01:44,  1.51it/s]Extractor Predicting: 159it [01:45,  1.51it/s]Extractor Predicting: 160it [01:46,  1.53it/s]Extractor Predicting: 161it [01:46,  1.46it/s]Extractor Predicting: 162it [01:47,  1.51it/s]Extractor Predicting: 163it [01:48,  1.53it/s]Extractor Predicting: 164it [01:48,  1.52it/s]Extractor Predicting: 165it [01:49,  1.57it/s]Extractor Predicting: 166it [01:50,  1.58it/s]Extractor Predicting: 167it [01:50,  1.57it/s]Extractor Predicting: 168it [01:51,  1.61it/s]Extractor Predicting: 169it [01:51,  1.62it/s]Extractor Predicting: 170it [01:52,  1.58it/s]Extractor Predicting: 171it [01:53,  1.57it/s]Extractor Predicting: 172it [01:53,  1.56it/s]Extractor Predicting: 173it [01:54,  1.53it/s]Extractor Predicting: 174it [01:55,  1.57it/s]Extractor Predicting: 175it [01:55,  1.56it/s]Extractor Predicting: 176it [01:56,  1.54it/s]Extractor Predicting: 177it [01:57,  1.53it/s]Extractor Predicting: 178it [01:57,  1.53it/s]Extractor Predicting: 179it [01:58,  1.60it/s]Extractor Predicting: 180it [01:58,  1.57it/s]Extractor Predicting: 181it [01:59,  1.56it/s]Extractor Predicting: 182it [02:00,  1.54it/s]Extractor Predicting: 183it [02:00,  1.54it/s]Extractor Predicting: 184it [02:01,  1.53it/s]Extractor Predicting: 185it [02:02,  1.56it/s]Extractor Predicting: 186it [02:02,  1.54it/s]Extractor Predicting: 187it [02:03,  1.38it/s]Extractor Predicting: 188it [02:04,  1.40it/s]Extractor Predicting: 189it [02:05,  1.42it/s]Extractor Predicting: 190it [02:05,  1.42it/s]Extractor Predicting: 191it [02:06,  1.44it/s]Extractor Predicting: 192it [02:07,  1.47it/s]Extractor Predicting: 193it [02:07,  1.51it/s]Extractor Predicting: 194it [02:08,  1.53it/s]Extractor Predicting: 195it [02:09,  1.50it/s]Extractor Predicting: 196it [02:09,  1.52it/s]Extractor Predicting: 197it [02:10,  1.52it/s]Extractor Predicting: 198it [02:11,  1.53it/s]Extractor Predicting: 199it [02:11,  1.52it/s]Extractor Predicting: 200it [02:12,  1.51it/s]Extractor Predicting: 201it [02:13,  1.52it/s]Extractor Predicting: 202it [02:13,  1.52it/s]Extractor Predicting: 203it [02:14,  1.52it/s]Extractor Predicting: 204it [02:15,  1.53it/s]Extractor Predicting: 205it [02:15,  1.52it/s]Extractor Predicting: 206it [02:16,  1.51it/s]Extractor Predicting: 207it [02:16,  1.53it/s]Extractor Predicting: 208it [02:17,  1.53it/s]Extractor Predicting: 209it [02:18,  1.52it/s]Extractor Predicting: 210it [02:18,  1.52it/s]Extractor Predicting: 211it [02:19,  1.51it/s]Extractor Predicting: 212it [02:20,  1.53it/s]Extractor Predicting: 213it [02:20,  1.51it/s]Extractor Predicting: 214it [02:21,  1.51it/s]Extractor Predicting: 215it [02:22,  1.47it/s]Extractor Predicting: 216it [02:22,  1.50it/s]Extractor Predicting: 217it [02:23,  1.53it/s]Extractor Predicting: 218it [02:24,  1.53it/s]Extractor Predicting: 219it [02:24,  1.51it/s]Extractor Predicting: 220it [02:25,  1.52it/s]Extractor Predicting: 221it [02:26,  1.50it/s]Extractor Predicting: 222it [02:26,  1.52it/s]Extractor Predicting: 223it [02:27,  1.46it/s]Extractor Predicting: 224it [02:28,  1.48it/s]Extractor Predicting: 225it [02:29,  1.46it/s]Extractor Predicting: 226it [02:29,  1.47it/s]Extractor Predicting: 227it [02:30,  1.43it/s]Extractor Predicting: 228it [02:31,  1.39it/s]Extractor Predicting: 229it [02:31,  1.41it/s]Extractor Predicting: 230it [02:32,  1.43it/s]Extractor Predicting: 231it [02:33,  1.43it/s]Extractor Predicting: 232it [02:33,  1.44it/s]Extractor Predicting: 233it [02:34,  1.45it/s]Extractor Predicting: 234it [02:35,  1.46it/s]Extractor Predicting: 235it [02:35,  1.47it/s]Extractor Predicting: 236it [02:36,  1.48it/s]Extractor Predicting: 237it [02:37,  1.48it/s]Extractor Predicting: 238it [02:37,  1.53it/s]Extractor Predicting: 239it [02:38,  1.55it/s]Extractor Predicting: 240it [02:39,  1.61it/s]Extractor Predicting: 241it [02:39,  1.60it/s]Extractor Predicting: 242it [02:40,  1.58it/s]Extractor Predicting: 243it [02:41,  1.55it/s]Extractor Predicting: 244it [02:41,  1.55it/s]Extractor Predicting: 245it [02:42,  1.58it/s]Extractor Predicting: 246it [02:42,  1.56it/s]Extractor Predicting: 247it [02:43,  1.57it/s]Extractor Predicting: 248it [02:44,  1.58it/s]Extractor Predicting: 249it [02:44,  1.63it/s]Extractor Predicting: 250it [02:45,  1.63it/s]Extractor Predicting: 251it [02:45,  1.68it/s]Extractor Predicting: 252it [02:46,  1.70it/s]Extractor Predicting: 253it [02:47,  1.66it/s]Extractor Predicting: 254it [02:47,  1.68it/s]Extractor Predicting: 255it [02:48,  1.63it/s]Extractor Predicting: 256it [02:48,  1.67it/s]Extractor Predicting: 257it [02:49,  1.62it/s]Extractor Predicting: 258it [02:50,  1.58it/s]Extractor Predicting: 259it [02:50,  1.53it/s]Extractor Predicting: 260it [02:51,  1.59it/s]Extractor Predicting: 261it [02:52,  1.56it/s]Extractor Predicting: 262it [02:52,  1.57it/s]Extractor Predicting: 263it [02:53,  1.61it/s]Extractor Predicting: 264it [02:54,  1.62it/s]Extractor Predicting: 265it [02:54,  1.61it/s]Extractor Predicting: 266it [02:55,  1.63it/s]Extractor Predicting: 267it [02:55,  1.65it/s]Extractor Predicting: 268it [02:56,  1.64it/s]Extractor Predicting: 269it [02:57,  1.60it/s]Extractor Predicting: 270it [02:57,  1.62it/s]Extractor Predicting: 271it [02:58,  1.61it/s]Extractor Predicting: 272it [02:59,  1.58it/s]Extractor Predicting: 273it [02:59,  1.63it/s]Extractor Predicting: 274it [03:00,  1.64it/s]Extractor Predicting: 275it [03:00,  1.62it/s]Extractor Predicting: 276it [03:01,  1.61it/s]Extractor Predicting: 277it [03:02,  1.60it/s]Extractor Predicting: 278it [03:02,  1.63it/s]Extractor Predicting: 279it [03:03,  1.57it/s]Extractor Predicting: 280it [03:03,  1.63it/s]Extractor Predicting: 281it [03:04,  1.63it/s]Extractor Predicting: 282it [03:05,  1.64it/s]Extractor Predicting: 283it [03:05,  1.65it/s]Extractor Predicting: 284it [03:06,  1.48it/s]Extractor Predicting: 285it [03:07,  1.53it/s]Extractor Predicting: 286it [03:07,  1.56it/s]Extractor Predicting: 287it [03:08,  1.58it/s]Extractor Predicting: 288it [03:09,  1.56it/s]Extractor Predicting: 289it [03:09,  1.57it/s]Extractor Predicting: 290it [03:10,  1.50it/s]Extractor Predicting: 291it [03:11,  1.53it/s]Extractor Predicting: 292it [03:11,  1.58it/s]Extractor Predicting: 293it [03:12,  1.59it/s]Extractor Predicting: 294it [03:12,  1.60it/s]Extractor Predicting: 295it [03:13,  1.61it/s]Extractor Predicting: 296it [03:14,  1.54it/s]Extractor Predicting: 297it [03:14,  1.53it/s]Extractor Predicting: 298it [03:15,  1.47it/s]Extractor Predicting: 299it [03:16,  1.48it/s]Extractor Predicting: 300it [03:16,  1.50it/s]Extractor Predicting: 301it [03:17,  1.51it/s]Extractor Predicting: 302it [03:18,  1.51it/s]Extractor Predicting: 303it [03:18,  1.48it/s]Extractor Predicting: 304it [03:19,  1.49it/s]Extractor Predicting: 305it [03:20,  1.52it/s]Extractor Predicting: 306it [03:20,  1.48it/s]Extractor Predicting: 307it [03:21,  1.45it/s]Extractor Predicting: 308it [03:22,  1.45it/s]Extractor Predicting: 309it [03:23,  1.48it/s]Extractor Predicting: 310it [03:23,  1.51it/s]Extractor Predicting: 311it [03:24,  1.52it/s]Extractor Predicting: 312it [03:24,  1.51it/s]Extractor Predicting: 313it [03:25,  1.48it/s]Extractor Predicting: 314it [03:26,  1.45it/s]Extractor Predicting: 315it [03:27,  1.45it/s]Extractor Predicting: 316it [03:27,  1.45it/s]Extractor Predicting: 317it [03:28,  1.46it/s]Extractor Predicting: 318it [03:29,  1.47it/s]Extractor Predicting: 319it [03:29,  1.48it/s]Extractor Predicting: 320it [03:30,  1.49it/s]Extractor Predicting: 321it [03:31,  1.54it/s]Extractor Predicting: 322it [03:31,  1.54it/s]Extractor Predicting: 323it [03:32,  1.51it/s]Extractor Predicting: 324it [03:33,  1.50it/s]Extractor Predicting: 325it [03:33,  1.49it/s]Extractor Predicting: 326it [03:34,  1.48it/s]Extractor Predicting: 327it [03:35,  1.46it/s]Extractor Predicting: 328it [03:35,  1.46it/s]Extractor Predicting: 329it [03:36,  1.50it/s]Extractor Predicting: 330it [03:37,  1.53it/s]Extractor Predicting: 331it [03:37,  1.70it/s]Extractor Predicting: 331it [03:37,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:37:01,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:37:01,483 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:37:01,483 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:37:01,483 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:37:01,484 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:37:02,119 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:37:02,120 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:37:02,705 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:37:03,754 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:37:03,754 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:37:06,638 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:37:06,642 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:37:06,642 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:37:06,642 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:37:06,642 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:37:07,277 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:37:07,278 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:37:07,852 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:37:08,004 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:37:08,004 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.45821114369501464,
  "recall": 0.0788046904551759,
  "score": 0.13448090371167296,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.44it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.45it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:08,  1.44it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:13,  1.44it/s]Extractor Predicting: 20it [00:13,  1.42it/s]Extractor Predicting: 21it [00:14,  1.42it/s]Extractor Predicting: 22it [00:15,  1.40it/s]Extractor Predicting: 23it [00:15,  1.39it/s]Extractor Predicting: 24it [00:16,  1.40it/s]Extractor Predicting: 25it [00:17,  1.41it/s]Extractor Predicting: 26it [00:18,  1.45it/s]Extractor Predicting: 27it [00:18,  1.47it/s]Extractor Predicting: 28it [00:19,  1.45it/s]Extractor Predicting: 29it [00:20,  1.45it/s]Extractor Predicting: 30it [00:20,  1.45it/s]Extractor Predicting: 31it [00:21,  1.45it/s]Extractor Predicting: 32it [00:22,  1.47it/s]Extractor Predicting: 33it [00:22,  1.48it/s]Extractor Predicting: 34it [00:23,  1.46it/s]Extractor Predicting: 35it [00:24,  1.48it/s]Extractor Predicting: 36it [00:24,  1.48it/s]Extractor Predicting: 37it [00:25,  1.49it/s]Extractor Predicting: 38it [00:26,  1.50it/s]Extractor Predicting: 39it [00:26,  1.44it/s]Extractor Predicting: 40it [00:27,  1.45it/s]Extractor Predicting: 41it [00:28,  1.46it/s]Extractor Predicting: 42it [00:28,  1.47it/s]Extractor Predicting: 43it [00:29,  1.47it/s]Extractor Predicting: 44it [00:30,  1.47it/s]Extractor Predicting: 45it [00:30,  1.47it/s]Extractor Predicting: 46it [00:31,  1.48it/s]Extractor Predicting: 47it [00:32,  1.46it/s]Extractor Predicting: 48it [00:32,  1.49it/s]Extractor Predicting: 49it [00:33,  1.50it/s]Extractor Predicting: 50it [00:34,  1.40it/s]Extractor Predicting: 51it [00:35,  1.43it/s]Extractor Predicting: 52it [00:35,  1.45it/s]Extractor Predicting: 53it [00:36,  1.40it/s]Extractor Predicting: 54it [00:37,  1.45it/s]Extractor Predicting: 55it [00:37,  1.48it/s]Extractor Predicting: 56it [00:38,  1.54it/s]Extractor Predicting: 57it [00:38,  1.59it/s]Extractor Predicting: 58it [00:39,  1.65it/s]Extractor Predicting: 59it [00:40,  1.72it/s]Extractor Predicting: 60it [00:40,  1.80it/s]Extractor Predicting: 61it [00:41,  1.84it/s]Extractor Predicting: 62it [00:41,  1.84it/s]Extractor Predicting: 63it [00:42,  1.86it/s]Extractor Predicting: 64it [00:42,  1.84it/s]Extractor Predicting: 65it [00:43,  1.83it/s]Extractor Predicting: 66it [00:43,  1.82it/s]Extractor Predicting: 67it [00:44,  1.81it/s]Extractor Predicting: 68it [00:44,  1.83it/s]Extractor Predicting: 69it [00:45,  1.85it/s]Extractor Predicting: 70it [00:46,  1.82it/s]Extractor Predicting: 71it [00:46,  1.82it/s]Extractor Predicting: 72it [00:47,  1.84it/s]Extractor Predicting: 73it [00:47,  1.88it/s]Extractor Predicting: 74it [00:48,  1.90it/s]Extractor Predicting: 75it [00:48,  1.88it/s]Extractor Predicting: 76it [00:49,  1.86it/s]Extractor Predicting: 77it [00:49,  1.91it/s]Extractor Predicting: 78it [00:50,  1.85it/s]Extractor Predicting: 79it [00:50,  1.84it/s]Extractor Predicting: 80it [00:51,  1.83it/s]Extractor Predicting: 81it [00:51,  1.81it/s]Extractor Predicting: 82it [00:52,  1.84it/s]Extractor Predicting: 83it [00:53,  1.84it/s]Extractor Predicting: 84it [00:53,  1.85it/s]Extractor Predicting: 85it [00:54,  1.86it/s]Extractor Predicting: 86it [00:54,  1.73it/s]Extractor Predicting: 87it [00:55,  1.66it/s]Extractor Predicting: 88it [00:56,  1.62it/s]Extractor Predicting: 89it [00:56,  1.61it/s]Extractor Predicting: 90it [00:57,  1.61it/s]Extractor Predicting: 91it [00:57,  1.59it/s]Extractor Predicting: 92it [00:58,  1.57it/s]Extractor Predicting: 93it [00:59,  1.57it/s]Extractor Predicting: 94it [00:59,  1.55it/s]Extractor Predicting: 95it [01:00,  1.59it/s]Extractor Predicting: 96it [01:01,  1.59it/s]Extractor Predicting: 97it [01:01,  1.59it/s]Extractor Predicting: 98it [01:02,  1.59it/s]Extractor Predicting: 99it [01:03,  1.57it/s]Extractor Predicting: 100it [01:03,  1.52it/s]Extractor Predicting: 101it [01:04,  1.55it/s]Extractor Predicting: 102it [01:05,  1.55it/s]Extractor Predicting: 103it [01:05,  1.50it/s]Extractor Predicting: 104it [01:06,  1.49it/s]Extractor Predicting: 105it [01:07,  1.48it/s]Extractor Predicting: 106it [01:07,  1.47it/s]Extractor Predicting: 107it [01:08,  1.46it/s]Extractor Predicting: 108it [01:09,  1.43it/s]Extractor Predicting: 108it [01:09,  1.56it/s]
[INFO|configuration_utils.py:515] 2023-08-29 00:38:18,756 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:38:18,761 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:38:18,765 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:38:18,766 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 00:38:18,768 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:38:21,855 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 00:38:21,855 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 00:38:21,868 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:38:21,869 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:38:21,874 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:38:21,878 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:38:21,878 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:38:21,878 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:38:21,878 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:38:21,878 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:38:21,878 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5458937198067633,
  "recall": 0.03620054460996316,
  "score": 0.06789845275649692,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 00:38:22,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:22,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:23,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:24,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:24,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:25,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:26,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:27,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:27,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:28,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:29,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:30,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:30,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:31,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:32,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:33,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:33,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:34,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:35,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:36,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:37,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:39, 15.67s/it][WARNING|generation_utils.py:914] 2023-08-29 00:38:37,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:38,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:39,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:39,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:40,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:41,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:42,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:42,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:43,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:44,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:45,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:45,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:46,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:47,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:47,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:48,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:49,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:49,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:50,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:51,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:52,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:30<03:20, 15.45s/it][WARNING|generation_utils.py:914] 2023-08-29 00:38:53,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:53,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:54,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:55,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:55,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:56,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:57,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:58,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:58,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:59,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:00,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:01,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:01,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:02,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:03,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:04,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:04,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:05,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:06,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:06,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:07,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:08,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:46<03:07, 15.66s/it][WARNING|generation_utils.py:914] 2023-08-29 00:39:08,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:10,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:11,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:11,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:12,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:13,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:14,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:14,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:15,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:16,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:17,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:18,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:18,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:19,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:20,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:21,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:22,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:23,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:23,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:24,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:25,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:26,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:05<03:03, 16.72s/it][WARNING|generation_utils.py:914] 2023-08-29 00:39:27,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:28,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:29,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:29,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:30,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:31,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:31,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:32,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:33,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:34,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:34,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:35,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:36,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:37,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:37,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:38,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:39,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:40,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:41,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:41,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:42,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:43,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:21<02:47, 16.73s/it][WARNING|generation_utils.py:914] 2023-08-29 00:39:44,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:44,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:45,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:46,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:46,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:47,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:48,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:48,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:49,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:50,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:50,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:51,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:52,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:52,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:53,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:54,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:54,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:55,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:56,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:56,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:57,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:58,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:58,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:37<02:27, 16.34s/it][WARNING|generation_utils.py:914] 2023-08-29 00:39:59,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:00,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:01,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:01,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:02,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:03,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:04,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:05,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:06,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:06,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:07,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:08,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:08,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:09,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:10,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:11,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:11,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:12,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:13,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:14,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:15,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:15,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:54<02:11, 16.46s/it][WARNING|generation_utils.py:914] 2023-08-29 00:40:16,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:16,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:17,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:18,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:19,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:19,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:20,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:21,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:21,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:22,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:23,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:24,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:24,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:25,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:26,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:26,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:27,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:28,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:28,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:29,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:30,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:30,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:09<01:52, 16.02s/it][WARNING|generation_utils.py:914] 2023-08-29 00:40:31,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:32,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:32,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:33,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:34,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:35,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:35,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:36,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:37,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:37,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:38,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:39,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:39,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:40,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:41,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:41,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:42,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:43,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:43,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:44,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:45,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:45,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:24<01:33, 15.67s/it][WARNING|generation_utils.py:914] 2023-08-29 00:40:46,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:47,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:47,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:48,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:49,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:49,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:50,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:51,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:51,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:52,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:52,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:53,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:54,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:54,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:55,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:56,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:56,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:57,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:58,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:58,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:59,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:38<01:15, 15.18s/it][WARNING|generation_utils.py:914] 2023-08-29 00:41:00,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:01,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:01,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:02,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:03,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:03,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:04,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:05,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:05,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:06,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:07,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:07,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:08,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:09,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:10,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:10,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:11,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:12,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:12,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:13,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:14,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:14,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:53<01:00, 15.21s/it][WARNING|generation_utils.py:914] 2023-08-29 00:41:15,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:16,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:17,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:18,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:19,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:20,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:20,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:21,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:22,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:23,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:23,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:24,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:25,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:26,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:26,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:27,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:28,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:29,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:30,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:30,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:31,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:32,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:33,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:12<00:48, 16.21s/it][WARNING|generation_utils.py:914] 2023-08-29 00:41:34,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:34,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:35,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:36,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:37,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:38,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:38,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:39,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:40,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:41,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:41,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:42,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:43,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:44,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:45,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:45,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:46,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:47,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:48,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:49,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:49,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:50,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:51,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:52,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:30<00:33, 16.97s/it][WARNING|generation_utils.py:914] 2023-08-29 00:41:52,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:53,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:54,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:55,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:55,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:56,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:57,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:58,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:58,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:59,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:41:59,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:00,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:01,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:01,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:02,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:03,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:03,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:04,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:05,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:05,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:06,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:07,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:07,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:46<00:16, 16.62s/it][WARNING|generation_utils.py:914] 2023-08-29 00:42:08,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:09,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:10,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:11,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:12,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:12,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:13,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:14,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:15,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:15,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:16,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:17,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:18,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:18,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:19,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:20,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:21,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:22,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:22,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:23,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:24,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:25,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:42:25,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:04<00:00, 16.96s/it]Generating: 100%|██████████| 15/15 [04:04<00:00, 16.29s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:33,028 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:33,038 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:33,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:33,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:33,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:42:33,686 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:42:33,687 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:42:34,252 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:42:35,309 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:42:35,309 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:38,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:38,198 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:38,199 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:38,199 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:38,199 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:42:38,857 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:42:38,858 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:42:39,466 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:42:39,621 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:42:39,621 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : characters . Context : Later in Life , he played the title character , a young son of a Scottish novelist . Head Entity : son of a Scottish novelist , Tail Entity : Harry .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : made from material .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8536931818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : cast member .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : follows .', 'success_rate': 0.8536931818181818, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : league .', 'success_rate': 0.8877840909090909, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8821022727272727, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9002976190476191, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : mother .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.796875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 11597
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11697, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.52it/s]Extractor Estimating: 2it [00:01,  1.52it/s]Extractor Estimating: 3it [00:01,  1.58it/s]Extractor Estimating: 4it [00:02,  1.53it/s]Extractor Estimating: 5it [00:03,  1.50it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.55it/s]Extractor Estimating: 8it [00:05,  1.51it/s]Extractor Estimating: 9it [00:05,  1.49it/s]Extractor Estimating: 10it [00:06,  1.51it/s]Extractor Estimating: 11it [00:07,  1.47it/s]Extractor Estimating: 12it [00:07,  1.49it/s]Extractor Estimating: 13it [00:08,  1.52it/s]Extractor Estimating: 14it [00:09,  1.48it/s]Extractor Estimating: 15it [00:09,  1.48it/s]Extractor Estimating: 16it [00:10,  1.55it/s]Extractor Estimating: 17it [00:11,  1.56it/s]Extractor Estimating: 18it [00:11,  1.54it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:13,  1.52it/s]Extractor Estimating: 21it [00:13,  1.54it/s]Extractor Estimating: 22it [00:14,  1.53it/s]Extractor Estimating: 23it [00:15,  1.55it/s]Extractor Estimating: 24it [00:15,  1.51it/s]Extractor Estimating: 25it [00:16,  1.49it/s]Extractor Estimating: 26it [00:17,  1.42it/s]Extractor Estimating: 27it [00:17,  1.42it/s]Extractor Estimating: 28it [00:18,  1.42it/s]Extractor Estimating: 29it [00:19,  1.44it/s]Extractor Estimating: 30it [00:19,  1.47it/s]Extractor Estimating: 31it [00:20,  1.50it/s]Extractor Estimating: 32it [00:21,  1.49it/s]Extractor Estimating: 33it [00:21,  1.54it/s]Extractor Estimating: 34it [00:22,  1.57it/s]Extractor Estimating: 35it [00:23,  1.57it/s]Extractor Estimating: 36it [00:23,  1.57it/s]Extractor Estimating: 37it [00:24,  1.59it/s]Extractor Estimating: 38it [00:25,  1.57it/s]Extractor Estimating: 39it [00:25,  1.63it/s]Extractor Estimating: 40it [00:26,  1.63it/s]Extractor Estimating: 41it [00:26,  1.64it/s]Extractor Estimating: 42it [00:27,  1.64it/s]Extractor Estimating: 43it [00:28,  1.67it/s]Extractor Estimating: 44it [00:28,  1.62it/s]Extractor Estimating: 45it [00:29,  1.59it/s]Extractor Estimating: 46it [00:29,  1.60it/s]Extractor Estimating: 47it [00:30,  1.57it/s]Extractor Estimating: 48it [00:31,  1.54it/s]Extractor Estimating: 49it [00:31,  1.53it/s]Extractor Estimating: 50it [00:32,  1.54it/s]Extractor Estimating: 51it [00:33,  1.48it/s]Extractor Estimating: 52it [00:33,  1.49it/s]Extractor Estimating: 53it [00:34,  1.50it/s]Extractor Estimating: 54it [00:35,  1.52it/s]Extractor Estimating: 55it [00:35,  1.49it/s]Extractor Estimating: 56it [00:36,  1.48it/s]Extractor Estimating: 57it [00:37,  1.51it/s]Extractor Estimating: 58it [00:38,  1.49it/s]Extractor Estimating: 59it [00:38,  1.50it/s]Extractor Estimating: 60it [00:39,  1.49it/s]Extractor Estimating: 61it [00:39,  1.51it/s]Extractor Estimating: 62it [00:40,  1.48it/s]Extractor Estimating: 63it [00:41,  1.51it/s]Extractor Estimating: 64it [00:42,  1.48it/s]Extractor Estimating: 65it [00:42,  1.47it/s]Extractor Estimating: 66it [00:43,  1.55it/s]Extractor Estimating: 67it [00:43,  1.55it/s]Extractor Estimating: 68it [00:44,  1.56it/s]Extractor Estimating: 69it [00:45,  1.56it/s]Extractor Estimating: 70it [00:45,  1.62it/s]Extractor Estimating: 71it [00:46,  1.60it/s]Extractor Estimating: 72it [00:47,  1.57it/s]Extractor Estimating: 73it [00:47,  1.54it/s]Extractor Estimating: 74it [00:48,  1.54it/s]Extractor Estimating: 75it [00:49,  1.52it/s]Extractor Estimating: 76it [00:49,  1.53it/s]Extractor Estimating: 77it [00:50,  1.56it/s]Extractor Estimating: 78it [00:50,  1.54it/s]Extractor Estimating: 79it [00:51,  1.55it/s]Extractor Estimating: 80it [00:52,  1.52it/s]Extractor Estimating: 81it [00:52,  1.51it/s]Extractor Estimating: 82it [00:53,  1.53it/s]Extractor Estimating: 83it [00:54,  1.52it/s]Extractor Estimating: 84it [00:55,  1.49it/s]Extractor Estimating: 85it [00:55,  1.49it/s]Extractor Estimating: 86it [00:56,  1.40it/s]Extractor Estimating: 87it [00:57,  1.45it/s]Extractor Estimating: 88it [00:57,  1.49it/s]Extractor Estimating: 89it [00:58,  1.50it/s]Extractor Estimating: 90it [00:59,  1.45it/s]Extractor Estimating: 91it [00:59,  1.45it/s]Extractor Estimating: 92it [01:00,  1.47it/s]Extractor Estimating: 93it [01:01,  1.37it/s]Extractor Estimating: 94it [01:02,  1.39it/s]Extractor Estimating: 95it [01:02,  1.42it/s]Extractor Estimating: 96it [01:03,  1.48it/s]Extractor Estimating: 97it [01:03,  1.48it/s]Extractor Estimating: 98it [01:04,  1.41it/s]Extractor Estimating: 99it [01:05,  1.45it/s]Extractor Estimating: 100it [01:06,  1.35it/s]Extractor Estimating: 101it [01:06,  1.42it/s]Extractor Estimating: 102it [01:07,  1.45it/s]Extractor Estimating: 103it [01:08,  1.47it/s]Extractor Estimating: 104it [01:08,  1.46it/s]Extractor Estimating: 105it [01:09,  1.46it/s]Extractor Estimating: 106it [01:10,  1.52it/s]Extractor Estimating: 107it [01:10,  1.56it/s]Extractor Estimating: 108it [01:11,  1.57it/s]Extractor Estimating: 109it [01:12,  1.53it/s]Extractor Estimating: 110it [01:12,  1.55it/s]Extractor Estimating: 111it [01:13,  1.59it/s]Extractor Estimating: 112it [01:13,  1.57it/s]Extractor Estimating: 113it [01:14,  1.55it/s]Extractor Estimating: 114it [01:15,  1.56it/s]Extractor Estimating: 115it [01:15,  1.58it/s]Extractor Estimating: 116it [01:16,  1.52it/s]Extractor Estimating: 117it [01:17,  1.51it/s]Extractor Estimating: 118it [01:17,  1.53it/s]Extractor Estimating: 119it [01:18,  1.53it/s]Extractor Estimating: 120it [01:19,  1.51it/s]Extractor Estimating: 121it [01:19,  1.49it/s]Extractor Estimating: 122it [01:20,  1.52it/s]Extractor Estimating: 123it [01:21,  1.54it/s]Extractor Estimating: 124it [01:21,  1.56it/s]Extractor Estimating: 125it [01:22,  1.53it/s]Extractor Estimating: 126it [01:23,  1.46it/s]Extractor Estimating: 127it [01:23,  1.50it/s]Extractor Estimating: 128it [01:24,  1.48it/s]Extractor Estimating: 129it [01:25,  1.53it/s]Extractor Estimating: 130it [01:25,  1.57it/s]Extractor Estimating: 131it [01:26,  1.58it/s]Extractor Estimating: 132it [01:27,  1.59it/s]Extractor Estimating: 133it [01:27,  1.58it/s]Extractor Estimating: 134it [01:28,  1.57it/s]Extractor Estimating: 135it [01:28,  1.56it/s]Extractor Estimating: 136it [01:29,  1.57it/s]Extractor Estimating: 137it [01:30,  1.63it/s]Extractor Estimating: 138it [01:30,  1.62it/s]Extractor Estimating: 139it [01:31,  1.62it/s]Extractor Estimating: 140it [01:32,  1.62it/s]Extractor Estimating: 141it [01:32,  1.57it/s]Extractor Estimating: 142it [01:33,  1.57it/s]Extractor Estimating: 143it [01:33,  1.61it/s]Extractor Estimating: 144it [01:34,  1.59it/s]Extractor Estimating: 145it [01:35,  1.62it/s]Extractor Estimating: 146it [01:35,  1.62it/s]Extractor Estimating: 147it [01:36,  1.62it/s]Extractor Estimating: 148it [01:36,  1.65it/s]Extractor Estimating: 149it [01:37,  1.63it/s]Extractor Estimating: 150it [01:38,  1.61it/s]Extractor Estimating: 151it [01:38,  1.59it/s]Extractor Estimating: 152it [01:39,  1.55it/s]Extractor Estimating: 153it [01:40,  1.54it/s]Extractor Estimating: 154it [01:40,  1.53it/s]Extractor Estimating: 155it [01:41,  1.51it/s]Extractor Estimating: 156it [01:42,  1.49it/s]Extractor Estimating: 157it [01:42,  1.47it/s]Extractor Estimating: 158it [01:43,  1.50it/s]Extractor Estimating: 159it [01:44,  1.41it/s]Extractor Estimating: 160it [01:45,  1.46it/s]Extractor Estimating: 161it [01:45,  1.48it/s]Extractor Estimating: 162it [01:46,  1.49it/s]Extractor Estimating: 163it [01:46,  1.54it/s]Extractor Estimating: 164it [01:47,  1.55it/s]Extractor Estimating: 165it [01:48,  1.59it/s]Extractor Estimating: 166it [01:48,  1.47it/s]Extractor Estimating: 167it [01:49,  1.49it/s]Extractor Estimating: 168it [01:50,  1.45it/s]Extractor Estimating: 169it [01:51,  1.48it/s]Extractor Estimating: 170it [01:51,  1.47it/s]Extractor Estimating: 171it [01:52,  1.44it/s]Extractor Estimating: 172it [01:53,  1.44it/s]Extractor Estimating: 173it [01:53,  1.42it/s]Extractor Estimating: 174it [01:54,  1.43it/s]Extractor Estimating: 175it [01:55,  1.44it/s]Extractor Estimating: 176it [01:55,  1.49it/s]Extractor Estimating: 177it [01:56,  1.47it/s]Extractor Estimating: 178it [01:57,  1.44it/s]Extractor Estimating: 179it [01:57,  1.51it/s]Extractor Estimating: 180it [01:58,  1.52it/s]Extractor Estimating: 181it [01:59,  1.52it/s]Extractor Estimating: 182it [01:59,  1.56it/s]Extractor Estimating: 183it [02:00,  1.53it/s]Extractor Estimating: 184it [02:01,  1.51it/s]Extractor Estimating: 185it [02:01,  1.50it/s]Extractor Estimating: 186it [02:02,  1.51it/s]Extractor Estimating: 187it [02:03,  1.53it/s]Extractor Estimating: 188it [02:03,  1.45it/s]Extractor Estimating: 189it [02:04,  1.48it/s]Extractor Estimating: 190it [02:05,  1.43it/s]Extractor Estimating: 191it [02:05,  1.46it/s]Extractor Estimating: 192it [02:06,  1.47it/s]Extractor Estimating: 193it [02:07,  1.50it/s]Extractor Estimating: 194it [02:07,  1.50it/s]Extractor Estimating: 195it [02:08,  1.53it/s]Extractor Estimating: 196it [02:09,  1.52it/s]Extractor Estimating: 197it [02:09,  1.56it/s]Extractor Estimating: 198it [02:10,  1.59it/s]Extractor Estimating: 199it [02:11,  1.56it/s]Extractor Estimating: 200it [02:11,  1.58it/s]Extractor Estimating: 201it [02:12,  1.56it/s]Extractor Estimating: 202it [02:12,  1.54it/s]Extractor Estimating: 203it [02:13,  1.57it/s]Extractor Estimating: 204it [02:14,  1.65it/s]Extractor Estimating: 205it [02:14,  1.66it/s]Extractor Estimating: 206it [02:15,  1.69it/s]Extractor Estimating: 207it [02:15,  1.71it/s]Extractor Estimating: 208it [02:16,  1.66it/s]Extractor Estimating: 209it [02:17,  1.70it/s]Extractor Estimating: 210it [02:17,  1.72it/s]Extractor Estimating: 211it [02:18,  1.67it/s]Extractor Estimating: 212it [02:18,  1.67it/s]Extractor Estimating: 213it [02:19,  1.62it/s]Extractor Estimating: 214it [02:20,  1.61it/s]Extractor Estimating: 215it [02:20,  1.71it/s]Extractor Estimating: 216it [02:21,  1.64it/s]Extractor Estimating: 217it [02:21,  1.66it/s]Extractor Estimating: 218it [02:22,  1.65it/s]Extractor Estimating: 219it [02:23,  1.66it/s]Extractor Estimating: 220it [02:23,  1.72it/s]Extractor Estimating: 221it [02:24,  1.76it/s]Extractor Estimating: 222it [02:24,  1.75it/s]Extractor Estimating: 223it [02:25,  1.70it/s]Extractor Estimating: 224it [02:25,  1.74it/s]Extractor Estimating: 225it [02:26,  1.72it/s]Extractor Estimating: 226it [02:27,  1.70it/s]Extractor Estimating: 227it [02:27,  1.71it/s]Extractor Estimating: 228it [02:28,  1.75it/s]Extractor Estimating: 229it [02:28,  1.78it/s]Extractor Estimating: 230it [02:29,  1.78it/s]Extractor Estimating: 231it [02:29,  1.81it/s]Extractor Estimating: 232it [02:30,  1.76it/s]Extractor Estimating: 233it [02:31,  1.80it/s]Extractor Estimating: 234it [02:31,  1.81it/s]Extractor Estimating: 235it [02:32,  1.80it/s]Extractor Estimating: 236it [02:32,  1.78it/s]Extractor Estimating: 237it [02:33,  1.72it/s]Extractor Estimating: 238it [02:33,  1.73it/s]Extractor Estimating: 239it [02:34,  1.82it/s]Extractor Estimating: 240it [02:34,  1.81it/s]Extractor Estimating: 241it [02:35,  1.83it/s]Extractor Estimating: 242it [02:35,  1.84it/s]Extractor Estimating: 243it [02:36,  1.77it/s]Extractor Estimating: 244it [02:37,  1.77it/s]Extractor Estimating: 245it [02:37,  1.72it/s]Extractor Estimating: 246it [02:38,  1.76it/s]Extractor Estimating: 247it [02:39,  1.62it/s]Extractor Estimating: 248it [02:39,  1.62it/s]Extractor Estimating: 249it [02:40,  1.63it/s]Extractor Estimating: 250it [02:40,  1.67it/s]Extractor Estimating: 251it [02:41,  1.62it/s]Extractor Estimating: 252it [02:42,  1.60it/s]Extractor Estimating: 253it [02:42,  1.60it/s]Extractor Estimating: 254it [02:43,  1.62it/s]Extractor Estimating: 255it [02:43,  1.63it/s]Extractor Estimating: 256it [02:44,  1.63it/s]Extractor Estimating: 257it [02:45,  1.61it/s]Extractor Estimating: 258it [02:45,  1.62it/s]Extractor Estimating: 259it [02:46,  1.53it/s]Extractor Estimating: 260it [02:47,  1.57it/s]Extractor Estimating: 261it [02:47,  1.55it/s]Extractor Estimating: 262it [02:48,  1.57it/s]Extractor Estimating: 263it [02:49,  1.56it/s]Extractor Estimating: 264it [02:49,  1.58it/s]Extractor Estimating: 265it [02:50,  1.61it/s]Extractor Estimating: 266it [02:50,  1.59it/s]Extractor Estimating: 267it [02:51,  1.59it/s]Extractor Estimating: 268it [02:52,  1.61it/s]Extractor Estimating: 269it [02:52,  1.58it/s]Extractor Estimating: 270it [02:53,  1.57it/s]Extractor Estimating: 271it [02:54,  1.59it/s]Extractor Estimating: 272it [02:54,  1.64it/s]Extractor Estimating: 273it [02:55,  1.62it/s]Extractor Estimating: 274it [02:55,  1.60it/s]Extractor Estimating: 275it [02:56,  1.61it/s]Extractor Estimating: 276it [02:57,  1.51it/s]Extractor Estimating: 277it [02:58,  1.49it/s]Extractor Estimating: 278it [02:58,  1.53it/s]Extractor Estimating: 279it [02:59,  1.55it/s]Extractor Estimating: 280it [02:59,  1.50it/s]Extractor Estimating: 281it [03:00,  1.47it/s]Extractor Estimating: 282it [03:01,  1.50it/s]Extractor Estimating: 283it [03:02,  1.49it/s]Extractor Estimating: 284it [03:02,  1.46it/s]Extractor Estimating: 285it [03:03,  1.46it/s]Extractor Estimating: 286it [03:04,  1.52it/s]Extractor Estimating: 287it [03:04,  1.46it/s]Extractor Estimating: 288it [03:05,  1.50it/s]Extractor Estimating: 289it [03:06,  1.49it/s]Extractor Estimating: 290it [03:06,  1.49it/s]Extractor Estimating: 291it [03:07,  1.48it/s]Extractor Estimating: 292it [03:08,  1.50it/s]Extractor Estimating: 293it [03:08,  1.50it/s]Extractor Estimating: 294it [03:09,  1.50it/s]Extractor Estimating: 295it [03:10,  1.50it/s]Extractor Estimating: 296it [03:10,  1.47it/s]Extractor Estimating: 297it [03:11,  1.40it/s]Extractor Estimating: 298it [03:12,  1.46it/s]Extractor Estimating: 299it [03:12,  1.45it/s]Extractor Estimating: 300it [03:13,  1.45it/s]Extractor Estimating: 301it [03:14,  1.48it/s]Extractor Estimating: 302it [03:14,  1.44it/s]Extractor Estimating: 303it [03:15,  1.46it/s]Extractor Estimating: 304it [03:16,  1.51it/s]Extractor Estimating: 305it [03:16,  1.51it/s]Extractor Estimating: 306it [03:17,  1.50it/s]Extractor Estimating: 307it [03:18,  1.55it/s]Extractor Estimating: 308it [03:18,  1.55it/s]Extractor Estimating: 309it [03:19,  1.57it/s]Extractor Estimating: 310it [03:20,  1.55it/s]Extractor Estimating: 311it [03:20,  1.52it/s]Extractor Estimating: 312it [03:21,  1.54it/s]Extractor Estimating: 313it [03:22,  1.55it/s]Extractor Estimating: 314it [03:22,  1.56it/s]Extractor Estimating: 315it [03:23,  1.52it/s]Extractor Estimating: 316it [03:24,  1.52it/s]Extractor Estimating: 317it [03:24,  1.52it/s]Extractor Estimating: 318it [03:25,  1.52it/s]Extractor Estimating: 319it [03:26,  1.46it/s]Extractor Estimating: 320it [03:26,  1.46it/s]Extractor Estimating: 321it [03:27,  1.44it/s]Extractor Estimating: 322it [03:28,  1.52it/s]Extractor Estimating: 323it [03:28,  1.50it/s]Extractor Estimating: 324it [03:29,  1.47it/s]Extractor Estimating: 325it [03:30,  1.52it/s]Extractor Estimating: 326it [03:30,  1.49it/s]Extractor Estimating: 327it [03:31,  1.50it/s]Extractor Estimating: 328it [03:32,  1.57it/s]Extractor Estimating: 329it [03:32,  1.65it/s]Extractor Estimating: 330it [03:33,  1.18it/s]Extractor Estimating: 331it [03:34,  1.27it/s]Extractor Estimating: 332it [03:35,  1.38it/s]Extractor Estimating: 333it [03:35,  1.52it/s]Extractor Estimating: 334it [03:36,  1.58it/s]Extractor Estimating: 335it [03:36,  1.63it/s]Extractor Estimating: 336it [03:37,  1.69it/s]Extractor Estimating: 337it [03:37,  1.66it/s]Extractor Estimating: 338it [03:38,  1.68it/s]Extractor Estimating: 339it [03:39,  1.69it/s]Extractor Estimating: 340it [03:39,  1.69it/s]Extractor Estimating: 341it [03:40,  1.71it/s]Extractor Estimating: 342it [03:40,  1.70it/s]Extractor Estimating: 343it [03:41,  1.72it/s]Extractor Estimating: 344it [03:42,  1.67it/s]Extractor Estimating: 345it [03:42,  1.69it/s]Extractor Estimating: 346it [03:43,  1.70it/s]Extractor Estimating: 347it [03:43,  1.71it/s]Extractor Estimating: 348it [03:44,  1.70it/s]Extractor Estimating: 349it [03:44,  1.75it/s]Extractor Estimating: 350it [03:45,  1.78it/s]Extractor Estimating: 351it [03:46,  1.58it/s]Extractor Estimating: 352it [03:46,  1.55it/s]Extractor Estimating: 353it [03:47,  1.54it/s]Extractor Estimating: 354it [03:48,  1.57it/s]Extractor Estimating: 355it [03:48,  1.56it/s]Extractor Estimating: 356it [03:49,  1.59it/s]Extractor Estimating: 357it [03:50,  1.60it/s]Extractor Estimating: 358it [03:50,  1.60it/s]Extractor Estimating: 359it [03:51,  1.55it/s]Extractor Estimating: 360it [03:52,  1.56it/s]Extractor Estimating: 361it [03:52,  1.52it/s]Extractor Estimating: 362it [03:53,  1.55it/s]Extractor Estimating: 363it [03:53,  1.57it/s]Extractor Estimating: 364it [03:54,  1.58it/s]Extractor Estimating: 365it [03:55,  1.58it/s]Extractor Estimating: 366it [03:55,  1.55it/s]Extractor Estimating: 367it [03:56,  1.56it/s]Extractor Estimating: 368it [03:57,  1.51it/s]Extractor Estimating: 369it [03:57,  1.52it/s]Extractor Estimating: 370it [03:58,  1.56it/s]Extractor Estimating: 371it [03:59,  1.59it/s]Extractor Estimating: 372it [03:59,  1.63it/s]Extractor Estimating: 373it [04:00,  1.61it/s]Extractor Estimating: 374it [04:01,  1.56it/s]Extractor Estimating: 375it [04:01,  1.56it/s]Extractor Estimating: 375it [04:01,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:58,422 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:58,428 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:58,428 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:58,428 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:46:58,428 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:46:59,050 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:46:59,051 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:46:59,651 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:47:00,706 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:47:00,707 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:03,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:03,623 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:03,623 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:03,623 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:03,623 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:47:04,348 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:47:04,349 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:47:04,948 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:47:05,103 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:47:05,103 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 03:10:50,212 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 03:10:50,249 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7462 mean pseudo reward: 0.9347304767034956
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 22832
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22932, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22932, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.098, loss:686.0388
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.093, loss:644.4431
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.096, loss:637.6153
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 89, avg_time 1.099, loss:624.6004
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 189, avg_time 1.085, loss:639.7808
>> valid entity prec:0.5143, rec:0.3965, f1:0.4478
>> valid relation prec:0.1556, rec:0.0125, f1:0.0231
>> valid relation with NER prec:0.1556, rec:0.0125, f1:0.0231
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 289, avg_time 2.614, loss:624.4297
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 78, avg_time 1.088, loss:617.4309
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 178, avg_time 1.096, loss:619.3515
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 278, avg_time 1.095, loss:663.6880
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 67, avg_time 1.091, loss:604.0215
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4709, rec:0.4678, f1:0.4694
>> valid relation prec:0.1348, rec:0.0175, f1:0.0310
>> valid relation with NER prec:0.1348, rec:0.0175, f1:0.0310
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 167, avg_time 2.629, loss:630.0593
g_step 1200, step 267, avg_time 1.087, loss:679.6741
g_step 1300, step 56, avg_time 1.100, loss:613.3364
g_step 1400, step 156, avg_time 1.079, loss:587.4496
g_step 1500, step 256, avg_time 1.097, loss:624.7505
>> valid entity prec:0.4710, rec:0.4166, f1:0.4421
>> valid relation prec:0.1790, rec:0.0212, f1:0.0379
>> valid relation with NER prec:0.1790, rec:0.0212, f1:0.0379
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 45, avg_time 2.630, loss:617.1802
g_step 1700, step 145, avg_time 1.089, loss:584.5158
g_step 1800, step 245, avg_time 1.090, loss:593.2104
g_step 1900, step 34, avg_time 1.095, loss:599.1682
g_step 2000, step 134, avg_time 1.101, loss:551.2197
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4995, rec:0.3384, f1:0.4034
>> valid relation prec:0.1835, rec:0.0231, f1:0.0410
>> valid relation with NER prec:0.1835, rec:0.0231, f1:0.0410
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 234, avg_time 2.623, loss:588.1407
g_step 2200, step 23, avg_time 1.086, loss:562.9239
g_step 2300, step 123, avg_time 1.081, loss:539.2420
g_step 2400, step 223, avg_time 1.097, loss:539.4490
g_step 2500, step 12, avg_time 1.085, loss:568.4365
>> valid entity prec:0.4903, rec:0.4614, f1:0.4754
>> valid relation prec:0.2081, rec:0.0249, f1:0.0445
>> valid relation with NER prec:0.2081, rec:0.0249, f1:0.0445
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 112, avg_time 2.619, loss:519.1429
g_step 2700, step 212, avg_time 1.089, loss:515.9283
g_step 2800, step 1, avg_time 1.100, loss:543.1230
g_step 2900, step 101, avg_time 1.069, loss:512.5692
g_step 3000, step 201, avg_time 1.109, loss:501.0309
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4966, rec:0.2916, f1:0.3675
>> valid relation prec:0.0706, rec:0.0053, f1:0.0099
>> valid relation with NER prec:0.0706, rec:0.0053, f1:0.0099
g_step 3100, step 301, avg_time 2.603, loss:522.6416
g_step 3200, step 90, avg_time 1.092, loss:477.4501
g_step 3300, step 190, avg_time 1.082, loss:485.0867
g_step 3400, step 290, avg_time 1.088, loss:502.9334
g_step 3500, step 79, avg_time 1.088, loss:465.5079
>> valid entity prec:0.4794, rec:0.3962, f1:0.4339
>> valid relation prec:0.0988, rec:0.0136, f1:0.0239
>> valid relation with NER prec:0.0988, rec:0.0136, f1:0.0239
g_step 3600, step 179, avg_time 2.616, loss:454.7497
g_step 3700, step 279, avg_time 1.108, loss:498.2628
g_step 3800, step 68, avg_time 1.092, loss:453.3450
g_step 3900, step 168, avg_time 1.089, loss:442.1602
g_step 4000, step 268, avg_time 1.090, loss:473.5564
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4757, rec:0.4824, f1:0.4791
>> valid relation prec:0.0643, rec:0.0097, f1:0.0168
>> valid relation with NER prec:0.0643, rec:0.0097, f1:0.0168
new max entity f1 on valid!
g_step 4100, step 57, avg_time 2.640, loss:447.0083
g_step 4200, step 157, avg_time 1.098, loss:428.8399
g_step 4300, step 257, avg_time 1.091, loss:445.4717
g_step 4400, step 46, avg_time 1.094, loss:436.0585
g_step 4500, step 146, avg_time 1.091, loss:420.8790
>> valid entity prec:0.5216, rec:0.3745, f1:0.4359
>> valid relation prec:0.1455, rec:0.0201, f1:0.0353
>> valid relation with NER prec:0.1455, rec:0.0201, f1:0.0353
g_step 4600, step 246, avg_time 2.608, loss:432.6875
g_step 4700, step 35, avg_time 1.092, loss:412.1383
g_step 4800, step 135, avg_time 1.087, loss:405.1485
g_step 4900, step 235, avg_time 1.088, loss:434.9595
g_step 5000, step 24, avg_time 1.085, loss:408.9347
learning rate was adjusted to 0.0008
>> valid entity prec:0.4550, rec:0.3602, f1:0.4021
>> valid relation prec:0.0915, rec:0.0129, f1:0.0226
>> valid relation with NER prec:0.0915, rec:0.0129, f1:0.0226
g_step 5100, step 124, avg_time 2.631, loss:383.9047
g_step 5200, step 224, avg_time 1.079, loss:383.2950
g_step 5300, step 13, avg_time 1.084, loss:410.7251
g_step 5400, step 113, avg_time 1.094, loss:362.2282
g_step 5500, step 213, avg_time 1.093, loss:372.7854
>> valid entity prec:0.4957, rec:0.3269, f1:0.3940
>> valid relation prec:0.0670, rec:0.0099, f1:0.0173
>> valid relation with NER prec:0.0670, rec:0.0099, f1:0.0173
g_step 5600, step 2, avg_time 2.614, loss:393.6195
g_step 5700, step 102, avg_time 1.084, loss:351.9051
g_step 5800, step 202, avg_time 1.090, loss:366.7391
g_step 5900, step 302, avg_time 1.101, loss:366.6183
g_step 6000, step 91, avg_time 1.084, loss:343.5366
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4610, rec:0.3598, f1:0.4042
>> valid relation prec:0.1042, rec:0.0150, f1:0.0262
>> valid relation with NER prec:0.1042, rec:0.0150, f1:0.0262
g_step 6100, step 191, avg_time 2.624, loss:354.8274
g_step 6200, step 291, avg_time 1.088, loss:362.0635
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 03:10:50 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 03:10:50 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_03-10-50_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 03:10:51 - WARNING - datasets.builder -   Using custom data configuration default-495f41fe5db318df
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-495f41fe5db318df/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 03:10:51,537 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:10:51,538 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:10:51,538 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:10:51,539 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:10:51,547 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:10:51,550 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:10:51,550 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:10:51,550 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:10:51,550 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:10:51,550 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:10:51,550 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 03:10:51,700 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:10:54,776 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 03:10:54,779 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-495f41fe5db318df/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.29ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.34ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.93ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.21ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.40ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.53ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.59ba/s]100%|██████████| 8/8 [00:01<00:00,  5.47ba/s]100%|██████████| 8/8 [00:01<00:00,  4.47ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.84ba/s] 40%|████      | 2/5 [00:00<00:00,  4.20ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.35ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.42ba/s]100%|██████████| 5/5 [00:01<00:00,  4.98ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.87ba/s] 25%|██▌       | 2/8 [00:00<00:00,  8.97ba/s] 50%|█████     | 4/8 [00:00<00:00,  9.75ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  9.99ba/s]100%|██████████| 8/8 [00:00<00:00, 11.19ba/s]100%|██████████| 8/8 [00:00<00:00, 10.47ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.08ba/s] 40%|████      | 2/5 [00:00<00:00,  9.10ba/s] 80%|████████  | 4/5 [00:00<00:00,  9.78ba/s]100%|██████████| 5/5 [00:00<00:00, 11.03ba/s]
[INFO|trainer.py:414] 2023-08-29 03:10:59,203 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 03:10:59,220 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 03:10:59,220 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 03:10:59,220 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 03:10:59,220 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 03:10:59,220 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 03:10:59,220 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 03:10:59,220 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:58,  3.27it/s]  0%|          | 2/585 [00:00<02:51,  3.40it/s]  1%|          | 3/585 [00:00<02:49,  3.43it/s]  1%|          | 4/585 [00:01<02:48,  3.45it/s]  1%|          | 5/585 [00:01<02:47,  3.46it/s]  1%|          | 6/585 [00:01<02:47,  3.46it/s]  1%|          | 7/585 [00:02<02:47,  3.45it/s]  1%|▏         | 8/585 [00:02<02:46,  3.46it/s]  2%|▏         | 9/585 [00:02<02:46,  3.47it/s]  2%|▏         | 10/585 [00:02<02:45,  3.47it/s]  2%|▏         | 11/585 [00:03<02:45,  3.47it/s]  2%|▏         | 12/585 [00:03<02:44,  3.47it/s]  2%|▏         | 13/585 [00:03<02:44,  3.47it/s]  2%|▏         | 14/585 [00:04<02:44,  3.48it/s]  3%|▎         | 15/585 [00:04<02:44,  3.47it/s]  3%|▎         | 16/585 [00:04<02:43,  3.47it/s]  3%|▎         | 17/585 [00:04<02:43,  3.48it/s]  3%|▎         | 18/585 [00:05<02:44,  3.45it/s]  3%|▎         | 19/585 [00:05<02:43,  3.46it/s]  3%|▎         | 20/585 [00:05<02:43,  3.46it/s]  4%|▎         | 21/585 [00:06<02:42,  3.46it/s]  4%|▍         | 22/585 [00:06<02:42,  3.46it/s]  4%|▍         | 23/585 [00:06<02:42,  3.46it/s]  4%|▍         | 24/585 [00:06<02:41,  3.47it/s]  4%|▍         | 25/585 [00:07<02:41,  3.47it/s]  4%|▍         | 26/585 [00:07<02:41,  3.47it/s]  5%|▍         | 27/585 [00:07<02:40,  3.47it/s]  5%|▍         | 28/585 [00:08<02:40,  3.47it/s]  5%|▍         | 29/585 [00:08<02:40,  3.47it/s]  5%|▌         | 30/585 [00:08<02:40,  3.47it/s]  5%|▌         | 31/585 [00:08<02:39,  3.47it/s]  5%|▌         | 32/585 [00:09<02:39,  3.46it/s]  6%|▌         | 33/585 [00:09<02:40,  3.45it/s]  6%|▌         | 34/585 [00:09<02:39,  3.45it/s]  6%|▌         | 35/585 [00:10<02:38,  3.46it/s]  6%|▌         | 36/585 [00:10<02:38,  3.46it/s]  6%|▋         | 37/585 [00:10<02:38,  3.46it/s]  6%|▋         | 38/585 [00:10<02:37,  3.46it/s]  7%|▋         | 39/585 [00:11<02:37,  3.46it/s]  7%|▋         | 40/585 [00:11<02:37,  3.46it/s]  7%|▋         | 41/585 [00:11<02:37,  3.46it/s]  7%|▋         | 42/585 [00:12<02:36,  3.47it/s]  7%|▋         | 43/585 [00:12<02:36,  3.46it/s]  8%|▊         | 44/585 [00:12<02:36,  3.47it/s]  8%|▊         | 45/585 [00:13<02:35,  3.46it/s]  8%|▊         | 46/585 [00:13<02:35,  3.47it/s]  8%|▊         | 47/585 [00:13<02:35,  3.46it/s]  8%|▊         | 48/585 [00:13<02:35,  3.46it/s]  8%|▊         | 49/585 [00:14<02:34,  3.46it/s]  9%|▊         | 50/585 [00:14<02:34,  3.46it/s]  9%|▊         | 51/585 [00:14<02:34,  3.46it/s]  9%|▉         | 52/585 [00:15<02:34,  3.46it/s]  9%|▉         | 53/585 [00:15<02:33,  3.46it/s]  9%|▉         | 54/585 [00:15<02:33,  3.46it/s]  9%|▉         | 55/585 [00:15<02:33,  3.46it/s] 10%|▉         | 56/585 [00:16<02:32,  3.46it/s] 10%|▉         | 57/585 [00:16<02:32,  3.46it/s] 10%|▉         | 58/585 [00:16<02:32,  3.46it/s] 10%|█         | 59/585 [00:17<02:31,  3.46it/s] 10%|█         | 60/585 [00:17<02:31,  3.46it/s] 10%|█         | 61/585 [00:17<02:31,  3.46it/s] 11%|█         | 62/585 [00:17<02:30,  3.46it/s] 11%|█         | 63/585 [00:18<02:30,  3.46it/s] 11%|█         | 64/585 [00:18<02:30,  3.46it/s] 11%|█         | 65/585 [00:18<02:30,  3.46it/s] 11%|█▏        | 66/585 [00:19<02:29,  3.46it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 69/585 [00:19<02:29,  3.45it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 73/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 76/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.46it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.46it/s] 14%|█▎        | 79/585 [00:22<02:26,  3.46it/s] 14%|█▎        | 80/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 83/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.45it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.45it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 90/585 [00:26<02:23,  3.46it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 93/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.46it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.46it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.46it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.46it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.46it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.45it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.45it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.45it/s] 18%|█▊        | 108/585 [00:31<02:18,  3.45it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.46it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:32<02:16,  3.46it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.45it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.46it/s] 20%|██        | 117/585 [00:33<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 03:11:33,083 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:11:33,083 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 03:11:33,083 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.93it/s][A
  2%|▏         | 12/543 [00:00<00:10, 51.02it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.18it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.51it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.07it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.71it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.42it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.05it/s][A
  9%|▉         | 48/543 [00:00<00:10, 47.03it/s][A
 10%|▉         | 53/543 [00:01<00:10, 47.12it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.14it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.18it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.20it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.27it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.23it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.03it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.91it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.90it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.93it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 47.05it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.11it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.15it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.20it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.23it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.13it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 46.98it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.97it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.89it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.95it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 47.07it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.13it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.15it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.24it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.14it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 46.95it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 47.01it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.95it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.90it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 47.04it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.11it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.16it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 47.10it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.15it/s][A
 41%|████      | 223/543 [00:04<00:06, 46.99it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 46.95it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.98it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.69it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 46.87it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 47.01it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.05it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.08it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.11it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.07it/s][A
 50%|█████     | 273/543 [00:05<00:05, 46.92it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.99it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 46.99it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 46.97it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 47.04it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.13it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.10it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.19it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.08it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.03it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 46.98it/s][A
 60%|██████    | 328/543 [00:06<00:04, 46.98it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 46.93it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 47.01it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 47.11it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 47.08it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.08it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 47.09it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 46.94it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 46.92it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 46.95it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 46.88it/s][A
 71%|███████   | 383/543 [00:08<00:03, 46.94it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 47.04it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 47.12it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.05it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.04it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.04it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 46.88it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 46.98it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 46.97it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 46.89it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 47.02it/s][A
 81%|████████  | 438/543 [00:09<00:02, 47.13it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 47.14it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 47.08it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.06it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 46.99it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 46.85it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 46.92it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.89it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 46.95it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 47.05it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 47.12it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.05it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 47.09it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 47.03it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 46.97it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 46.93it/s][A
 95%|█████████▌| 518/543 [00:10<00:00, 46.93it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 46.91it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 46.86it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 47.05it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 47.03it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.00it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:11<00:00, 47.00it/s][A 20%|██        | 117/585 [00:45<02:15,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:11:44,659 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 03:11:44,689 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:11:46,964 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:11:46,976 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:11:46,984 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:52<45:48,  5.89s/it] 20%|██        | 119/585 [00:53<32:40,  4.21s/it] 21%|██        | 120/585 [00:53<23:29,  3.03s/it] 21%|██        | 121/585 [00:53<17:04,  2.21s/it] 21%|██        | 122/585 [00:53<12:35,  1.63s/it] 21%|██        | 123/585 [00:54<09:27,  1.23s/it] 21%|██        | 124/585 [00:54<07:16,  1.06it/s] 21%|██▏       | 125/585 [00:54<05:45,  1.33it/s] 22%|██▏       | 126/585 [00:55<04:40,  1.64it/s] 22%|██▏       | 127/585 [00:55<03:55,  1.94it/s] 22%|██▏       | 128/585 [00:55<03:24,  2.24it/s] 22%|██▏       | 129/585 [00:55<03:01,  2.51it/s] 22%|██▏       | 130/585 [00:56<02:46,  2.73it/s] 22%|██▏       | 131/585 [00:56<02:35,  2.92it/s] 23%|██▎       | 132/585 [00:56<02:28,  3.06it/s] 23%|██▎       | 133/585 [00:57<02:22,  3.17it/s] 23%|██▎       | 134/585 [00:57<02:18,  3.26it/s] 23%|██▎       | 135/585 [00:57<02:15,  3.31it/s] 23%|██▎       | 136/585 [00:57<02:13,  3.36it/s] 23%|██▎       | 137/585 [00:58<02:12,  3.39it/s] 24%|██▎       | 138/585 [00:58<02:10,  3.41it/s] 24%|██▍       | 139/585 [00:58<02:10,  3.42it/s] 24%|██▍       | 140/585 [00:59<02:09,  3.44it/s] 24%|██▍       | 141/585 [00:59<02:09,  3.43it/s] 24%|██▍       | 142/585 [00:59<02:08,  3.44it/s] 24%|██▍       | 143/585 [00:59<02:08,  3.45it/s] 25%|██▍       | 144/585 [01:00<02:07,  3.45it/s] 25%|██▍       | 145/585 [01:00<02:07,  3.46it/s] 25%|██▍       | 146/585 [01:00<02:06,  3.46it/s] 25%|██▌       | 147/585 [01:01<02:06,  3.46it/s] 25%|██▌       | 148/585 [01:01<02:06,  3.46it/s] 25%|██▌       | 149/585 [01:01<02:05,  3.46it/s] 26%|██▌       | 150/585 [01:02<02:05,  3.46it/s] 26%|██▌       | 151/585 [01:02<02:05,  3.46it/s] 26%|██▌       | 152/585 [01:02<02:05,  3.45it/s] 26%|██▌       | 153/585 [01:02<02:05,  3.45it/s] 26%|██▋       | 154/585 [01:03<02:04,  3.46it/s] 26%|██▋       | 155/585 [01:03<02:04,  3.46it/s] 27%|██▋       | 156/585 [01:03<02:04,  3.46it/s] 27%|██▋       | 157/585 [01:04<02:03,  3.46it/s] 27%|██▋       | 158/585 [01:04<02:03,  3.46it/s] 27%|██▋       | 159/585 [01:04<02:02,  3.46it/s] 27%|██▋       | 160/585 [01:04<02:02,  3.46it/s] 28%|██▊       | 161/585 [01:05<02:02,  3.46it/s] 28%|██▊       | 162/585 [01:05<02:02,  3.46it/s] 28%|██▊       | 163/585 [01:05<02:02,  3.44it/s] 28%|██▊       | 164/585 [01:06<02:02,  3.44it/s] 28%|██▊       | 165/585 [01:06<02:02,  3.44it/s] 28%|██▊       | 166/585 [01:06<02:01,  3.45it/s] 29%|██▊       | 167/585 [01:06<02:01,  3.45it/s] 29%|██▊       | 168/585 [01:07<02:00,  3.45it/s] 29%|██▉       | 169/585 [01:07<02:00,  3.46it/s] 29%|██▉       | 170/585 [01:07<02:00,  3.46it/s] 29%|██▉       | 171/585 [01:08<01:59,  3.46it/s] 29%|██▉       | 172/585 [01:08<01:59,  3.46it/s] 30%|██▉       | 173/585 [01:08<01:59,  3.46it/s] 30%|██▉       | 174/585 [01:08<01:58,  3.46it/s] 30%|██▉       | 175/585 [01:09<01:58,  3.46it/s] 30%|███       | 176/585 [01:09<01:58,  3.46it/s] 30%|███       | 177/585 [01:09<01:57,  3.46it/s] 30%|███       | 178/585 [01:10<01:57,  3.46it/s] 31%|███       | 179/585 [01:10<01:57,  3.46it/s] 31%|███       | 180/585 [01:10<01:57,  3.46it/s] 31%|███       | 181/585 [01:10<01:57,  3.45it/s] 31%|███       | 182/585 [01:11<01:56,  3.45it/s] 31%|███▏      | 183/585 [01:11<01:56,  3.46it/s] 31%|███▏      | 184/585 [01:11<01:55,  3.46it/s] 32%|███▏      | 185/585 [01:12<01:55,  3.46it/s] 32%|███▏      | 186/585 [01:12<01:55,  3.46it/s] 32%|███▏      | 187/585 [01:12<01:55,  3.46it/s] 32%|███▏      | 188/585 [01:13<01:54,  3.46it/s] 32%|███▏      | 189/585 [01:13<01:54,  3.46it/s] 32%|███▏      | 190/585 [01:13<01:54,  3.46it/s] 33%|███▎      | 191/585 [01:13<01:53,  3.46it/s] 33%|███▎      | 192/585 [01:14<01:53,  3.46it/s] 33%|███▎      | 193/585 [01:14<01:53,  3.46it/s] 33%|███▎      | 194/585 [01:14<01:53,  3.46it/s] 33%|███▎      | 195/585 [01:15<01:52,  3.46it/s] 34%|███▎      | 196/585 [01:15<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:15<01:52,  3.46it/s] 34%|███▍      | 198/585 [01:15<01:51,  3.46it/s] 34%|███▍      | 199/585 [01:16<01:51,  3.46it/s] 34%|███▍      | 200/585 [01:16<01:51,  3.46it/s] 34%|███▍      | 201/585 [01:16<01:50,  3.46it/s] 35%|███▍      | 202/585 [01:17<01:50,  3.46it/s] 35%|███▍      | 203/585 [01:17<01:50,  3.45it/s] 35%|███▍      | 204/585 [01:17<01:50,  3.46it/s] 35%|███▌      | 205/585 [01:17<01:49,  3.46it/s] 35%|███▌      | 206/585 [01:18<01:49,  3.46it/s] 35%|███▌      | 207/585 [01:18<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:18<01:48,  3.46it/s] 36%|███▌      | 209/585 [01:19<01:48,  3.46it/s] 36%|███▌      | 210/585 [01:19<01:48,  3.46it/s] 36%|███▌      | 211/585 [01:19<01:48,  3.46it/s] 36%|███▌      | 212/585 [01:19<01:47,  3.46it/s] 36%|███▋      | 213/585 [01:20<01:47,  3.46it/s] 37%|███▋      | 214/585 [01:20<01:47,  3.44it/s] 37%|███▋      | 215/585 [01:20<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:21<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:21<01:46,  3.45it/s] 37%|███▋      | 218/585 [01:21<01:46,  3.46it/s] 37%|███▋      | 219/585 [01:21<01:45,  3.46it/s] 38%|███▊      | 220/585 [01:22<01:45,  3.46it/s] 38%|███▊      | 221/585 [01:22<01:45,  3.46it/s] 38%|███▊      | 222/585 [01:22<01:44,  3.46it/s] 38%|███▊      | 223/585 [01:23<01:44,  3.46it/s] 38%|███▊      | 224/585 [01:23<01:44,  3.46it/s] 38%|███▊      | 225/585 [01:23<01:44,  3.45it/s] 39%|███▊      | 226/585 [01:23<01:44,  3.45it/s] 39%|███▉      | 227/585 [01:24<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:24<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:24<01:43,  3.45it/s] 39%|███▉      | 230/585 [01:25<01:42,  3.46it/s] 39%|███▉      | 231/585 [01:25<01:42,  3.46it/s] 40%|███▉      | 232/585 [01:25<01:41,  3.46it/s] 40%|███▉      | 233/585 [01:26<01:41,  3.46it/s] 40%|████      | 234/585 [01:26<01:41,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 03:12:25,569 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:12:25,570 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 03:12:25,570 >>   Batch size = 8
{'eval_loss': 0.9832103252410889, 'eval_runtime': 11.5552, 'eval_samples_per_second': 375.761, 'eval_steps_per_second': 46.992, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.66it/s][A
  2%|▏         | 12/543 [00:00<00:10, 51.17it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.22it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.45it/s][A
  5%|▌         | 28/543 [00:00<00:10, 47.90it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.51it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.48it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.05it/s][A
  9%|▉         | 48/543 [00:00<00:10, 46.99it/s][A
 10%|▉         | 53/543 [00:01<00:10, 47.11it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.13it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.17it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.09it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.08it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.05it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.07it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.87it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.71it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.96it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 47.07it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.08it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.13it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.17it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.16it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.07it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 46.95it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.78it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.86it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.91it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 47.00it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 46.97it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.06it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.03it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 46.96it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 46.94it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.91it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.80it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.91it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 47.05it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.13it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.19it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 47.12it/s][A
 40%|████      | 218/543 [00:04<00:06, 46.92it/s][A
 41%|████      | 223/543 [00:04<00:06, 46.94it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 46.94it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.90it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.93it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 47.06it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 47.13it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.15it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.19it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.05it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 46.96it/s][A
 50%|█████     | 273/543 [00:05<00:05, 46.96it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.91it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 46.87it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 46.93it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 47.05it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.03it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.02it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 46.95it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 46.88it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 46.90it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 46.88it/s][A
 60%|██████    | 328/543 [00:06<00:04, 46.82it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 46.94it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 47.02it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 47.05it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 47.12it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.08it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 46.94it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 46.98it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 46.96it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 46.87it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 46.94it/s][A
 71%|███████   | 383/543 [00:08<00:03, 47.03it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 47.03it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 47.09it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.15it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.09it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.01it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 46.98it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 46.83it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 46.90it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 46.91it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 46.99it/s][A
 81%|████████  | 438/543 [00:09<00:02, 47.01it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 47.13it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 47.13it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.03it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 47.01it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 46.99it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 46.94it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.98it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 46.96it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 46.93it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 47.04it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.13it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 47.11it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 47.09it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 47.00it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 46.97it/s][A
 95%|█████████▌| 518/543 [00:10<00:00, 46.98it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 46.94it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 47.00it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 47.06it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 47.15it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.12it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:11<00:00, 47.12it/s][A 40%|████      | 234/585 [01:37<01:41,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:12:37,144 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 03:12:37,162 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:12:39,497 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:12:39,511 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:12:39,531 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:44<33:48,  5.79s/it] 40%|████      | 236/585 [01:45<24:06,  4.14s/it] 41%|████      | 237/585 [01:45<17:19,  2.99s/it] 41%|████      | 238/585 [01:45<12:35,  2.18s/it] 41%|████      | 239/585 [01:46<09:17,  1.61s/it] 41%|████      | 240/585 [01:46<06:58,  1.21s/it] 41%|████      | 241/585 [01:46<05:22,  1.07it/s] 41%|████▏     | 242/585 [01:46<04:14,  1.35it/s] 42%|████▏     | 243/585 [01:47<03:27,  1.65it/s] 42%|████▏     | 244/585 [01:47<02:54,  1.96it/s] 42%|████▏     | 245/585 [01:47<02:31,  2.25it/s] 42%|████▏     | 246/585 [01:48<02:14,  2.52it/s] 42%|████▏     | 247/585 [01:48<02:03,  2.73it/s] 42%|████▏     | 248/585 [01:48<01:55,  2.92it/s] 43%|████▎     | 249/585 [01:48<01:49,  3.06it/s] 43%|████▎     | 250/585 [01:49<01:45,  3.18it/s] 43%|████▎     | 251/585 [01:49<01:42,  3.26it/s] 43%|████▎     | 252/585 [01:49<01:40,  3.32it/s] 43%|████▎     | 253/585 [01:50<01:38,  3.36it/s] 43%|████▎     | 254/585 [01:50<01:37,  3.39it/s] 44%|████▎     | 255/585 [01:50<01:36,  3.41it/s] 44%|████▍     | 256/585 [01:51<01:36,  3.43it/s] 44%|████▍     | 257/585 [01:51<01:35,  3.44it/s] 44%|████▍     | 258/585 [01:51<01:35,  3.44it/s] 44%|████▍     | 259/585 [01:51<01:34,  3.45it/s] 44%|████▍     | 260/585 [01:52<01:34,  3.45it/s] 45%|████▍     | 261/585 [01:52<01:36,  3.37it/s] 45%|████▍     | 262/585 [01:52<01:35,  3.39it/s] 45%|████▍     | 263/585 [01:53<01:34,  3.41it/s] 45%|████▌     | 264/585 [01:53<01:33,  3.42it/s] 45%|████▌     | 265/585 [01:53<01:33,  3.44it/s] 45%|████▌     | 266/585 [01:53<01:32,  3.45it/s] 46%|████▌     | 267/585 [01:54<01:32,  3.45it/s] 46%|████▌     | 268/585 [01:54<01:31,  3.46it/s] 46%|████▌     | 269/585 [01:54<01:31,  3.45it/s] 46%|████▌     | 270/585 [01:55<01:31,  3.46it/s] 46%|████▋     | 271/585 [01:55<01:30,  3.46it/s] 46%|████▋     | 272/585 [01:55<01:30,  3.46it/s] 47%|████▋     | 273/585 [01:55<01:30,  3.46it/s] 47%|████▋     | 274/585 [01:56<01:29,  3.46it/s] 47%|████▋     | 275/585 [01:56<01:29,  3.46it/s] 47%|████▋     | 276/585 [01:56<01:29,  3.46it/s] 47%|████▋     | 277/585 [01:57<01:28,  3.46it/s] 48%|████▊     | 278/585 [01:57<01:28,  3.46it/s] 48%|████▊     | 279/585 [01:57<01:28,  3.46it/s] 48%|████▊     | 280/585 [01:57<01:28,  3.45it/s] 48%|████▊     | 281/585 [01:58<01:27,  3.46it/s] 48%|████▊     | 282/585 [01:58<01:27,  3.46it/s] 48%|████▊     | 283/585 [01:58<01:27,  3.46it/s] 49%|████▊     | 284/585 [01:59<01:27,  3.46it/s] 49%|████▊     | 285/585 [01:59<01:26,  3.46it/s] 49%|████▉     | 286/585 [01:59<01:26,  3.46it/s] 49%|████▉     | 287/585 [01:59<01:26,  3.46it/s] 49%|████▉     | 288/585 [02:00<01:25,  3.46it/s] 49%|████▉     | 289/585 [02:00<01:25,  3.46it/s] 50%|████▉     | 290/585 [02:00<01:25,  3.46it/s] 50%|████▉     | 291/585 [02:01<01:25,  3.45it/s] 50%|████▉     | 292/585 [02:01<01:24,  3.46it/s] 50%|█████     | 293/585 [02:01<01:24,  3.46it/s] 50%|█████     | 294/585 [02:02<01:24,  3.46it/s] 50%|█████     | 295/585 [02:02<01:23,  3.46it/s] 51%|█████     | 296/585 [02:02<01:23,  3.46it/s] 51%|█████     | 297/585 [02:02<01:23,  3.46it/s] 51%|█████     | 298/585 [02:03<01:22,  3.46it/s] 51%|█████     | 299/585 [02:03<01:22,  3.46it/s] 51%|█████▏    | 300/585 [02:03<01:22,  3.46it/s] 51%|█████▏    | 301/585 [02:04<01:22,  3.46it/s] 52%|█████▏    | 302/585 [02:04<01:21,  3.45it/s] 52%|█████▏    | 303/585 [02:04<01:21,  3.46it/s] 52%|█████▏    | 304/585 [02:04<01:21,  3.46it/s] 52%|█████▏    | 305/585 [02:05<01:20,  3.46it/s] 52%|█████▏    | 306/585 [02:05<01:20,  3.46it/s] 52%|█████▏    | 307/585 [02:05<01:20,  3.46it/s] 53%|█████▎    | 308/585 [02:06<01:19,  3.46it/s] 53%|█████▎    | 309/585 [02:06<01:19,  3.46it/s] 53%|█████▎    | 310/585 [02:06<01:19,  3.46it/s] 53%|█████▎    | 311/585 [02:06<01:19,  3.46it/s] 53%|█████▎    | 312/585 [02:07<01:18,  3.46it/s] 54%|█████▎    | 313/585 [02:07<01:18,  3.45it/s] 54%|█████▎    | 314/585 [02:07<01:18,  3.45it/s] 54%|█████▍    | 315/585 [02:08<01:18,  3.46it/s] 54%|█████▍    | 316/585 [02:08<01:17,  3.45it/s] 54%|█████▍    | 317/585 [02:08<01:17,  3.46it/s] 54%|█████▍    | 318/585 [02:08<01:17,  3.45it/s] 55%|█████▍    | 319/585 [02:09<01:16,  3.46it/s] 55%|█████▍    | 320/585 [02:09<01:16,  3.46it/s] 55%|█████▍    | 321/585 [02:09<01:16,  3.46it/s] 55%|█████▌    | 322/585 [02:10<01:15,  3.47it/s] 55%|█████▌    | 323/585 [02:10<01:15,  3.46it/s] 55%|█████▌    | 324/585 [02:10<01:15,  3.45it/s] 56%|█████▌    | 325/585 [02:10<01:15,  3.45it/s] 56%|█████▌    | 326/585 [02:11<01:14,  3.45it/s] 56%|█████▌    | 327/585 [02:11<01:14,  3.45it/s] 56%|█████▌    | 328/585 [02:11<01:14,  3.45it/s] 56%|█████▌    | 329/585 [02:12<01:14,  3.46it/s] 56%|█████▋    | 330/585 [02:12<01:13,  3.45it/s] 57%|█████▋    | 331/585 [02:12<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:13<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:13<01:12,  3.45it/s] 57%|█████▋    | 334/585 [02:13<01:12,  3.45it/s] 57%|█████▋    | 335/585 [02:13<01:12,  3.45it/s] 57%|█████▋    | 336/585 [02:14<01:12,  3.45it/s] 58%|█████▊    | 337/585 [02:14<01:11,  3.45it/s] 58%|█████▊    | 338/585 [02:14<01:11,  3.45it/s] 58%|█████▊    | 339/585 [02:15<01:11,  3.45it/s] 58%|█████▊    | 340/585 [02:15<01:10,  3.45it/s] 58%|█████▊    | 341/585 [02:15<01:10,  3.44it/s] 58%|█████▊    | 342/585 [02:15<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:16<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:16<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:16<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:17<01:09,  3.45it/s] 59%|█████▉    | 347/585 [02:17<01:08,  3.45it/s] 59%|█████▉    | 348/585 [02:17<01:08,  3.45it/s] 60%|█████▉    | 349/585 [02:17<01:08,  3.45it/s] 60%|█████▉    | 350/585 [02:18<01:08,  3.45it/s] 60%|██████    | 351/585 [02:18<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 03:13:17,787 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:13:17,787 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 03:13:17,787 >>   Batch size = 8
{'eval_loss': 0.9990856051445007, 'eval_runtime': 11.5611, 'eval_samples_per_second': 375.569, 'eval_steps_per_second': 46.968, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.53it/s][A
  2%|▏         | 12/543 [00:00<00:10, 51.09it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.18it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.56it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.15it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.89it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.62it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.13it/s][A
  9%|▉         | 48/543 [00:00<00:10, 47.11it/s][A
 10%|▉         | 53/543 [00:01<00:10, 47.01it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.18it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.17it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.17it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.19it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.23it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.13it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.87it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.92it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.96it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 46.99it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.14it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.21it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.22it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.08it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.10it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 46.94it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.89it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.91it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.90it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 47.03it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.16it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.22it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.16it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.15it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 47.06it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.95it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.89it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.87it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 46.93it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 46.75it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.28it/s][A
 39%|███▉      | 213/543 [00:04<00:06, 47.25it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.25it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.14it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 47.05it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.97it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.90it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 46.89it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 46.90it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.10it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.15it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.18it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.26it/s][A
 50%|█████     | 273/543 [00:05<00:05, 47.17it/s][A
 51%|█████     | 278/543 [00:05<00:05, 47.01it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 47.01it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 46.96it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 46.96it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.11it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.19it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.12it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.16it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.15it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 47.08it/s][A
 60%|██████    | 328/543 [00:06<00:04, 46.98it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 46.92it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 46.51it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 47.10it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 47.22it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.18it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 47.18it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.19it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 47.08it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 47.03it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 46.96it/s][A
 71%|███████   | 383/543 [00:08<00:03, 46.90it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 46.92it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 47.08it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.18it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.07it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.12it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 47.15it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 47.09it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 47.00it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 46.88it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 46.85it/s][A
 81%|████████  | 438/543 [00:09<00:02, 46.03it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 46.44it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 46.69it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 46.80it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 46.91it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 46.90it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 46.94it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.82it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 46.80it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 46.88it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 46.87it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.03it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 47.17it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 47.21it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 47.17it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 47.13it/s][A
 95%|█████████▌| 518/543 [00:10<00:00, 47.08it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 46.91it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 46.88it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 46.94it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 46.99it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.03it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:11<00:00, 47.03it/s][A 60%|██████    | 351/585 [02:30<01:07,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:13:29,360 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 03:13:29,388 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:13:31,746 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:13:31,761 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:13:31,769 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:37<22:34,  5.81s/it] 60%|██████    | 353/585 [02:37<16:04,  4.16s/it] 61%|██████    | 354/585 [02:37<11:32,  3.00s/it] 61%|██████    | 355/585 [02:38<08:22,  2.18s/it] 61%|██████    | 356/585 [02:38<06:09,  1.62s/it] 61%|██████    | 357/585 [02:38<04:37,  1.22s/it] 61%|██████    | 358/585 [02:38<03:33,  1.07it/s] 61%|██████▏   | 359/585 [02:39<02:48,  1.34it/s] 62%|██████▏   | 360/585 [02:39<02:16,  1.65it/s] 62%|██████▏   | 361/585 [02:39<01:54,  1.96it/s] 62%|██████▏   | 362/585 [02:40<01:39,  2.25it/s] 62%|██████▏   | 363/585 [02:40<01:28,  2.52it/s] 62%|██████▏   | 364/585 [02:40<01:20,  2.74it/s] 62%|██████▏   | 365/585 [02:40<01:15,  2.92it/s] 63%|██████▎   | 366/585 [02:41<01:11,  3.07it/s] 63%|██████▎   | 367/585 [02:41<01:08,  3.18it/s] 63%|██████▎   | 368/585 [02:41<01:06,  3.26it/s] 63%|██████▎   | 369/585 [02:42<01:04,  3.32it/s] 63%|██████▎   | 370/585 [02:42<01:03,  3.36it/s] 63%|██████▎   | 371/585 [02:42<01:03,  3.39it/s] 64%|██████▎   | 372/585 [02:42<01:02,  3.42it/s] 64%|██████▍   | 373/585 [02:43<01:01,  3.43it/s] 64%|██████▍   | 374/585 [02:43<01:01,  3.45it/s] 64%|██████▍   | 375/585 [02:43<01:00,  3.44it/s] 64%|██████▍   | 376/585 [02:44<01:00,  3.45it/s] 64%|██████▍   | 377/585 [02:44<01:00,  3.45it/s] 65%|██████▍   | 378/585 [02:44<00:59,  3.46it/s] 65%|██████▍   | 379/585 [02:45<00:59,  3.46it/s] 65%|██████▍   | 380/585 [02:45<00:59,  3.46it/s] 65%|██████▌   | 381/585 [02:45<00:58,  3.46it/s] 65%|██████▌   | 382/585 [02:45<00:58,  3.46it/s] 65%|██████▌   | 383/585 [02:46<00:58,  3.47it/s] 66%|██████▌   | 384/585 [02:46<00:58,  3.46it/s] 66%|██████▌   | 385/585 [02:46<00:57,  3.46it/s] 66%|██████▌   | 386/585 [02:47<00:57,  3.46it/s] 66%|██████▌   | 387/585 [02:47<00:57,  3.46it/s] 66%|██████▋   | 388/585 [02:47<00:56,  3.47it/s] 66%|██████▋   | 389/585 [02:47<00:56,  3.46it/s] 67%|██████▋   | 390/585 [02:48<00:56,  3.46it/s] 67%|██████▋   | 391/585 [02:48<00:55,  3.47it/s] 67%|██████▋   | 392/585 [02:48<00:55,  3.47it/s] 67%|██████▋   | 393/585 [02:49<00:55,  3.47it/s] 67%|██████▋   | 394/585 [02:49<00:55,  3.47it/s] 68%|██████▊   | 395/585 [02:49<00:54,  3.47it/s] 68%|██████▊   | 396/585 [02:49<00:54,  3.47it/s] 68%|██████▊   | 397/585 [02:50<00:54,  3.45it/s] 68%|██████▊   | 398/585 [02:50<00:54,  3.46it/s] 68%|██████▊   | 399/585 [02:50<00:53,  3.47it/s] 68%|██████▊   | 400/585 [02:51<00:53,  3.46it/s] 69%|██████▊   | 401/585 [02:51<00:53,  3.47it/s] 69%|██████▊   | 402/585 [02:51<00:52,  3.47it/s] 69%|██████▉   | 403/585 [02:51<00:52,  3.47it/s] 69%|██████▉   | 404/585 [02:52<00:52,  3.47it/s] 69%|██████▉   | 405/585 [02:52<00:51,  3.47it/s] 69%|██████▉   | 406/585 [02:52<00:51,  3.47it/s] 70%|██████▉   | 407/585 [02:53<00:51,  3.47it/s] 70%|██████▉   | 408/585 [02:53<00:51,  3.46it/s] 70%|██████▉   | 409/585 [02:53<00:50,  3.46it/s] 70%|███████   | 410/585 [02:53<00:50,  3.46it/s] 70%|███████   | 411/585 [02:54<00:50,  3.47it/s] 70%|███████   | 412/585 [02:54<00:49,  3.46it/s] 71%|███████   | 413/585 [02:54<00:49,  3.47it/s] 71%|███████   | 414/585 [02:55<00:49,  3.46it/s] 71%|███████   | 415/585 [02:55<00:49,  3.46it/s] 71%|███████   | 416/585 [02:55<00:48,  3.47it/s] 71%|███████▏  | 417/585 [02:55<00:48,  3.47it/s] 71%|███████▏  | 418/585 [02:56<00:48,  3.47it/s] 72%|███████▏  | 419/585 [02:56<00:48,  3.45it/s] 72%|███████▏  | 420/585 [02:56<00:47,  3.46it/s] 72%|███████▏  | 421/585 [02:57<00:47,  3.46it/s] 72%|███████▏  | 422/585 [02:57<00:47,  3.46it/s] 72%|███████▏  | 423/585 [02:57<00:46,  3.46it/s] 72%|███████▏  | 424/585 [02:58<00:46,  3.46it/s] 73%|███████▎  | 425/585 [02:58<00:46,  3.47it/s] 73%|███████▎  | 426/585 [02:58<00:45,  3.47it/s] 73%|███████▎  | 427/585 [02:58<00:45,  3.47it/s] 73%|███████▎  | 428/585 [02:59<00:45,  3.47it/s] 73%|███████▎  | 429/585 [02:59<00:45,  3.47it/s] 74%|███████▎  | 430/585 [02:59<00:44,  3.45it/s] 74%|███████▎  | 431/585 [03:00<00:44,  3.46it/s] 74%|███████▍  | 432/585 [03:00<00:44,  3.46it/s] 74%|███████▍  | 433/585 [03:00<00:43,  3.46it/s] 74%|███████▍  | 434/585 [03:00<00:43,  3.46it/s] 74%|███████▍  | 435/585 [03:01<00:43,  3.46it/s] 75%|███████▍  | 436/585 [03:01<00:42,  3.47it/s] 75%|███████▍  | 437/585 [03:01<00:42,  3.46it/s] 75%|███████▍  | 438/585 [03:02<00:42,  3.46it/s] 75%|███████▌  | 439/585 [03:02<00:42,  3.47it/s] 75%|███████▌  | 440/585 [03:02<00:41,  3.47it/s] 75%|███████▌  | 441/585 [03:02<00:41,  3.47it/s] 76%|███████▌  | 442/585 [03:03<00:41,  3.47it/s] 76%|███████▌  | 443/585 [03:03<00:41,  3.46it/s] 76%|███████▌  | 444/585 [03:03<00:40,  3.46it/s] 76%|███████▌  | 445/585 [03:04<00:40,  3.46it/s] 76%|███████▌  | 446/585 [03:04<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:04<00:39,  3.46it/s] 77%|███████▋  | 448/585 [03:04<00:39,  3.46it/s] 77%|███████▋  | 449/585 [03:05<00:39,  3.46it/s] 77%|███████▋  | 450/585 [03:05<00:39,  3.46it/s] 77%|███████▋  | 451/585 [03:05<00:38,  3.46it/s] 77%|███████▋  | 452/585 [03:06<00:38,  3.46it/s] 77%|███████▋  | 453/585 [03:06<00:38,  3.46it/s] 78%|███████▊  | 454/585 [03:06<00:37,  3.46it/s] 78%|███████▊  | 455/585 [03:06<00:38,  3.35it/s] 78%|███████▊  | 456/585 [03:07<00:38,  3.38it/s] 78%|███████▊  | 457/585 [03:07<00:37,  3.41it/s] 78%|███████▊  | 458/585 [03:07<00:37,  3.42it/s] 78%|███████▊  | 459/585 [03:08<00:36,  3.43it/s] 79%|███████▊  | 460/585 [03:08<00:36,  3.43it/s] 79%|███████▉  | 461/585 [03:08<00:36,  3.44it/s] 79%|███████▉  | 462/585 [03:09<00:35,  3.44it/s] 79%|███████▉  | 463/585 [03:09<00:35,  3.45it/s] 79%|███████▉  | 464/585 [03:09<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:09<00:34,  3.46it/s] 80%|███████▉  | 466/585 [03:10<00:35,  3.33it/s] 80%|███████▉  | 467/585 [03:10<00:34,  3.37it/s] 80%|████████  | 468/585 [03:10<00:34,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 03:14:10,047 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:14:10,047 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 03:14:10,047 >>   Batch size = 8
{'eval_loss': 1.0063879489898682, 'eval_runtime': 11.5566, 'eval_samples_per_second': 375.715, 'eval_steps_per_second': 46.986, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.78it/s][A
  2%|▏         | 12/543 [00:00<00:10, 50.93it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.25it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.48it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.02it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.71it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.49it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.12it/s][A
  9%|▉         | 48/543 [00:00<00:10, 47.15it/s][A
 10%|▉         | 53/543 [00:01<00:10, 47.21it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.10it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.20it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.25it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.28it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.23it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.15it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.91it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.89it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 47.03it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 47.00it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.03it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.16it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.22it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.17it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.20it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 47.07it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.99it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 47.05it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.93it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 47.00it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.09it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.18it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.18it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.11it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 47.10it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 47.03it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.97it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 47.09it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 47.04it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.08it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.14it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 47.06it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.06it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.07it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 47.05it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.98it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 47.02it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 47.13it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 47.08it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.05it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.10it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.09it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.07it/s][A
 50%|█████     | 273/543 [00:05<00:05, 47.07it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.93it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 46.94it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 47.06it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 47.05it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.05it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.08it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.08it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.00it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.00it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 46.97it/s][A
 60%|██████    | 328/543 [00:06<00:04, 46.93it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 47.03it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 47.02it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 47.02it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 47.02it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.09it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 47.09it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.06it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 47.06it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 46.98it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 46.88it/s][A
 71%|███████   | 383/543 [00:08<00:03, 46.98it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 47.06it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 47.08it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.12it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.10it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 46.96it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 47.05it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 47.00it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 46.93it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 46.92it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 47.05it/s][A
 81%|████████  | 438/543 [00:09<00:02, 47.08it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 47.08it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 47.09it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.02it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 47.01it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 47.05it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 46.99it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.88it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 47.03it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 47.05it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 47.04it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.09it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 47.07it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 46.97it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 47.03it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 47.01it/s][A
 95%|█████████▌| 518/543 [00:10<00:00, 47.03it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 47.00it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 47.06it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 47.03it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 47.01it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.01it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:11<00:00, 47.01it/s][A 80%|████████  | 468/585 [03:22<00:34,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:14:21,616 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 03:14:21,636 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:14:24,186 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:14:24,211 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:14:24,227 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:30<11:48,  6.11s/it] 80%|████████  | 470/585 [03:30<08:22,  4.37s/it] 81%|████████  | 471/585 [03:31<05:58,  3.14s/it] 81%|████████  | 472/585 [03:31<04:18,  2.29s/it] 81%|████████  | 473/585 [03:31<03:09,  1.69s/it] 81%|████████  | 474/585 [03:31<02:20,  1.27s/it] 81%|████████  | 475/585 [03:32<01:47,  1.03it/s] 81%|████████▏ | 476/585 [03:32<01:23,  1.30it/s] 82%|████████▏ | 477/585 [03:32<01:07,  1.60it/s] 82%|████████▏ | 478/585 [03:33<00:56,  1.91it/s] 82%|████████▏ | 479/585 [03:33<00:48,  2.21it/s] 82%|████████▏ | 480/585 [03:33<00:42,  2.48it/s] 82%|████████▏ | 481/585 [03:33<00:38,  2.71it/s] 82%|████████▏ | 482/585 [03:34<00:35,  2.90it/s] 83%|████████▎ | 483/585 [03:34<00:33,  3.03it/s] 83%|████████▎ | 484/585 [03:34<00:32,  3.15it/s] 83%|████████▎ | 485/585 [03:35<00:30,  3.24it/s] 83%|████████▎ | 486/585 [03:35<00:29,  3.31it/s] 83%|████████▎ | 487/585 [03:35<00:29,  3.35it/s] 83%|████████▎ | 488/585 [03:35<00:28,  3.39it/s] 84%|████████▎ | 489/585 [03:36<00:28,  3.41it/s] 84%|████████▍ | 490/585 [03:36<00:27,  3.43it/s] 84%|████████▍ | 491/585 [03:36<00:27,  3.44it/s] 84%|████████▍ | 492/585 [03:37<00:26,  3.45it/s] 84%|████████▍ | 493/585 [03:37<00:26,  3.46it/s] 84%|████████▍ | 494/585 [03:37<00:26,  3.45it/s] 85%|████████▍ | 495/585 [03:37<00:26,  3.46it/s] 85%|████████▍ | 496/585 [03:38<00:25,  3.46it/s] 85%|████████▍ | 497/585 [03:38<00:25,  3.46it/s] 85%|████████▌ | 498/585 [03:38<00:25,  3.47it/s] 85%|████████▌ | 499/585 [03:39<00:24,  3.46it/s] 85%|████████▌ | 500/585 [03:39<00:24,  3.47it/s]                                                  85%|████████▌ | 500/585 [03:39<00:24,  3.47it/s] 86%|████████▌ | 501/585 [03:39<00:24,  3.47it/s] 86%|████████▌ | 502/585 [03:40<00:23,  3.47it/s] 86%|████████▌ | 503/585 [03:40<00:23,  3.46it/s] 86%|████████▌ | 504/585 [03:40<00:23,  3.46it/s] 86%|████████▋ | 505/585 [03:40<00:23,  3.45it/s] 86%|████████▋ | 506/585 [03:41<00:22,  3.46it/s] 87%|████████▋ | 507/585 [03:41<00:22,  3.46it/s] 87%|████████▋ | 508/585 [03:41<00:22,  3.46it/s] 87%|████████▋ | 509/585 [03:42<00:21,  3.46it/s] 87%|████████▋ | 510/585 [03:42<00:21,  3.46it/s] 87%|████████▋ | 511/585 [03:42<00:21,  3.47it/s] 88%|████████▊ | 512/585 [03:42<00:21,  3.46it/s] 88%|████████▊ | 513/585 [03:43<00:20,  3.46it/s] 88%|████████▊ | 514/585 [03:43<00:20,  3.47it/s] 88%|████████▊ | 515/585 [03:43<00:20,  3.47it/s] 88%|████████▊ | 516/585 [03:44<00:20,  3.44it/s] 88%|████████▊ | 517/585 [03:44<00:19,  3.45it/s] 89%|████████▊ | 518/585 [03:44<00:19,  3.45it/s] 89%|████████▊ | 519/585 [03:44<00:19,  3.46it/s] 89%|████████▉ | 520/585 [03:45<00:18,  3.46it/s] 89%|████████▉ | 521/585 [03:45<00:18,  3.46it/s] 89%|████████▉ | 522/585 [03:45<00:18,  3.46it/s] 89%|████████▉ | 523/585 [03:46<00:17,  3.46it/s] 90%|████████▉ | 524/585 [03:46<00:17,  3.46it/s] 90%|████████▉ | 525/585 [03:46<00:17,  3.46it/s] 90%|████████▉ | 526/585 [03:46<00:17,  3.46it/s] 90%|█████████ | 527/585 [03:47<00:16,  3.45it/s] 90%|█████████ | 528/585 [03:47<00:16,  3.45it/s] 90%|█████████ | 529/585 [03:47<00:16,  3.46it/s] 91%|█████████ | 530/585 [03:48<00:15,  3.46it/s] 91%|█████████ | 531/585 [03:48<00:15,  3.47it/s] 91%|█████████ | 532/585 [03:48<00:15,  3.47it/s] 91%|█████████ | 533/585 [03:48<00:14,  3.47it/s] 91%|█████████▏| 534/585 [03:49<00:14,  3.46it/s] 91%|█████████▏| 535/585 [03:49<00:14,  3.47it/s] 92%|█████████▏| 536/585 [03:49<00:14,  3.47it/s] 92%|█████████▏| 537/585 [03:50<00:13,  3.47it/s] 92%|█████████▏| 538/585 [03:50<00:13,  3.46it/s] 92%|█████████▏| 539/585 [03:50<00:13,  3.46it/s] 92%|█████████▏| 540/585 [03:50<00:12,  3.46it/s] 92%|█████████▏| 541/585 [03:51<00:12,  3.47it/s] 93%|█████████▎| 542/585 [03:51<00:12,  3.46it/s] 93%|█████████▎| 543/585 [03:51<00:12,  3.47it/s] 93%|█████████▎| 544/585 [03:52<00:11,  3.47it/s] 93%|█████████▎| 545/585 [03:52<00:11,  3.47it/s] 93%|█████████▎| 546/585 [03:52<00:11,  3.47it/s] 94%|█████████▎| 547/585 [03:53<00:10,  3.47it/s] 94%|█████████▎| 548/585 [03:53<00:10,  3.47it/s] 94%|█████████▍| 549/585 [03:53<00:10,  3.45it/s] 94%|█████████▍| 550/585 [03:53<00:10,  3.46it/s] 94%|█████████▍| 551/585 [03:54<00:09,  3.46it/s] 94%|█████████▍| 552/585 [03:54<00:09,  3.47it/s] 95%|█████████▍| 553/585 [03:54<00:09,  3.46it/s] 95%|█████████▍| 554/585 [03:55<00:08,  3.47it/s] 95%|█████████▍| 555/585 [03:55<00:08,  3.47it/s] 95%|█████████▌| 556/585 [03:55<00:08,  3.47it/s] 95%|█████████▌| 557/585 [03:55<00:08,  3.47it/s] 95%|█████████▌| 558/585 [03:56<00:07,  3.47it/s] 96%|█████████▌| 559/585 [03:56<00:07,  3.47it/s] 96%|█████████▌| 560/585 [03:56<00:07,  3.46it/s] 96%|█████████▌| 561/585 [03:57<00:06,  3.46it/s] 96%|█████████▌| 562/585 [03:57<00:06,  3.46it/s] 96%|█████████▌| 563/585 [03:57<00:06,  3.46it/s] 96%|█████████▋| 564/585 [03:57<00:06,  3.46it/s] 97%|█████████▋| 565/585 [03:58<00:05,  3.47it/s] 97%|█████████▋| 566/585 [03:58<00:05,  3.47it/s] 97%|█████████▋| 567/585 [03:58<00:05,  3.46it/s] 97%|█████████▋| 568/585 [03:59<00:04,  3.47it/s] 97%|█████████▋| 569/585 [03:59<00:04,  3.47it/s] 97%|█████████▋| 570/585 [03:59<00:04,  3.47it/s] 98%|█████████▊| 571/585 [03:59<00:04,  3.45it/s] 98%|█████████▊| 572/585 [04:00<00:03,  3.45it/s] 98%|█████████▊| 573/585 [04:00<00:03,  3.45it/s] 98%|█████████▊| 574/585 [04:00<00:03,  3.46it/s] 98%|█████████▊| 575/585 [04:01<00:02,  3.46it/s] 98%|█████████▊| 576/585 [04:01<00:02,  3.46it/s] 99%|█████████▊| 577/585 [04:01<00:02,  3.47it/s] 99%|█████████▉| 578/585 [04:01<00:02,  3.46it/s] 99%|█████████▉| 579/585 [04:02<00:01,  3.46it/s] 99%|█████████▉| 580/585 [04:02<00:01,  3.45it/s] 99%|█████████▉| 581/585 [04:02<00:01,  3.45it/s] 99%|█████████▉| 582/585 [04:03<00:00,  3.45it/s]100%|█████████▉| 583/585 [04:03<00:00,  3.46it/s]100%|█████████▉| 584/585 [04:03<00:00,  3.46it/s]100%|██████████| 585/585 [04:03<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 03:15:03,210 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:15:03,210 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 03:15:03,210 >>   Batch size = 8
{'eval_loss': 1.0169041156768799, 'eval_runtime': 11.5499, 'eval_samples_per_second': 375.935, 'eval_steps_per_second': 47.014, 'epoch': 4.0}
{'loss': 0.6233, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.68it/s][A
  2%|▏         | 12/543 [00:00<00:10, 51.17it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.33it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.61it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.15it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.83it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.49it/s][A
  8%|▊         | 43/543 [00:00<00:11, 42.39it/s][A
  9%|▉         | 48/543 [00:01<00:11, 43.74it/s][A
 10%|▉         | 53/543 [00:01<00:10, 44.83it/s][A
 11%|█         | 58/543 [00:01<00:10, 45.55it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 46.12it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 46.48it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 46.78it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 46.85it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 46.26it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.26it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.56it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.80it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 46.94it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.08it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.15it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.14it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.18it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.02it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 46.87it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.91it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 47.00it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 47.05it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 47.07it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.18it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.19it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.11it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.06it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 46.86it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.86it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 46.94it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 47.08it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 47.07it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.15it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.14it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 47.04it/s][A
 40%|████      | 218/543 [00:04<00:06, 46.98it/s][A
 41%|████      | 223/543 [00:04<00:06, 46.95it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 46.91it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.99it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.99it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 47.02it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 47.11it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.18it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.10it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.01it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 46.99it/s][A
 50%|█████     | 273/543 [00:05<00:05, 46.93it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.95it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 47.08it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 47.09it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 47.00it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.14it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.08it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.06it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.05it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 46.97it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 46.90it/s][A
 60%|██████    | 328/543 [00:06<00:04, 46.99it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 47.10it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 47.10it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 47.06it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 47.14it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.07it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 47.01it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 46.86it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 46.87it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 46.88it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 47.04it/s][A
 71%|███████   | 383/543 [00:08<00:03, 47.11it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 47.12it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 47.12it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.13it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.00it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.01it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 47.01it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 46.95it/s][A
 78%|███████▊  | 423/543 [00:09<00:02, 47.03it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 46.99it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 47.15it/s][A
 81%|████████  | 438/543 [00:09<00:02, 47.12it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 47.14it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 47.05it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 46.96it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 46.98it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 46.99it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 46.80it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 47.13it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 47.13it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 47.12it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 47.15it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.07it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 46.88it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 46.96it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 46.94it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 46.89it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 47.01it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 47.08it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 47.13it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 47.15it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 47.19it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.02it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:11<00:00, 47.02it/s][A100%|██████████| 585/585 [04:15<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:15:14,803 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 03:15:14,823 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:15:17,091 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:15:17,114 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:15:17,127 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 03:15:21,571 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 03:15:21,574 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117 (score: 0.9832103252410889).
                                                 100%|██████████| 585/585 [04:24<00:00,  3.46it/s]100%|██████████| 585/585 [04:24<00:00,  2.21it/s]
[INFO|trainer.py:1894] 2023-08-29 03:15:23,366 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 03:15:23,385 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:15:25,817 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:15:25,831 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:15:25,849 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:15:26,075 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:26,075 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:26,075 >>   train_loss               =     0.6193
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:26,075 >>   train_runtime            = 0:04:24.13
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:26,075 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:26,075 >>   train_samples_per_second =     141.97
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:26,075 >>   train_steps_per_second   =      2.215
{'eval_loss': 1.0205129384994507, 'eval_runtime': 11.5779, 'eval_samples_per_second': 375.025, 'eval_steps_per_second': 46.9, 'epoch': 5.0}
{'train_runtime': 264.1394, 'train_samples_per_second': 141.97, 'train_steps_per_second': 2.215, 'train_loss': 0.6192987882173978, 'epoch': 5.0}
08/29/2023 03:15:26 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 03:15:26,117 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:15:26,117 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 03:15:26,117 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 59.04it/s]  2%|▏         | 12/543 [00:00<00:10, 51.88it/s]  3%|▎         | 18/543 [00:00<00:10, 49.99it/s]  4%|▍         | 24/543 [00:00<00:10, 49.14it/s]  5%|▌         | 29/543 [00:00<00:10, 48.72it/s]  6%|▋         | 34/543 [00:00<00:10, 48.46it/s]  7%|▋         | 39/543 [00:00<00:10, 48.29it/s]  8%|▊         | 44/543 [00:00<00:10, 47.82it/s]  9%|▉         | 49/543 [00:01<00:10, 47.28it/s] 10%|▉         | 54/543 [00:01<00:10, 47.27it/s] 11%|█         | 59/543 [00:01<00:10, 47.36it/s] 12%|█▏        | 64/543 [00:01<00:10, 47.38it/s] 13%|█▎        | 69/543 [00:01<00:09, 47.57it/s] 14%|█▎        | 74/543 [00:01<00:09, 47.60it/s] 15%|█▍        | 79/543 [00:01<00:09, 47.61it/s] 15%|█▌        | 84/543 [00:01<00:09, 47.54it/s] 16%|█▋        | 89/543 [00:01<00:09, 47.44it/s] 17%|█▋        | 94/543 [00:01<00:09, 47.10it/s] 18%|█▊        | 99/543 [00:02<00:09, 47.03it/s] 19%|█▉        | 104/543 [00:02<00:09, 47.19it/s] 20%|██        | 109/543 [00:02<00:09, 47.34it/s] 21%|██        | 114/543 [00:02<00:09, 47.43it/s] 22%|██▏       | 119/543 [00:02<00:08, 47.47it/s] 23%|██▎       | 124/543 [00:02<00:08, 47.53it/s] 24%|██▍       | 129/543 [00:02<00:08, 47.50it/s] 25%|██▍       | 134/543 [00:02<00:08, 47.22it/s] 26%|██▌       | 139/543 [00:02<00:08, 47.06it/s] 27%|██▋       | 144/543 [00:03<00:08, 47.09it/s] 27%|██▋       | 149/543 [00:03<00:08, 47.11it/s] 28%|██▊       | 154/543 [00:03<00:08, 47.24it/s] 29%|██▉       | 159/543 [00:03<00:08, 47.38it/s] 30%|███       | 164/543 [00:03<00:07, 47.42it/s] 31%|███       | 169/543 [00:03<00:07, 47.42it/s] 32%|███▏      | 174/543 [00:03<00:07, 47.41it/s] 33%|███▎      | 179/543 [00:03<00:07, 47.36it/s] 34%|███▍      | 184/543 [00:03<00:07, 47.20it/s] 35%|███▍      | 189/543 [00:03<00:07, 47.15it/s] 36%|███▌      | 194/543 [00:04<00:07, 47.16it/s] 37%|███▋      | 199/543 [00:04<00:07, 47.20it/s] 38%|███▊      | 204/543 [00:04<00:07, 47.26it/s] 38%|███▊      | 209/543 [00:04<00:07, 47.35it/s] 39%|███▉      | 214/543 [00:04<00:06, 47.43it/s] 40%|████      | 219/543 [00:04<00:06, 47.42it/s] 41%|████▏     | 224/543 [00:04<00:06, 47.36it/s] 42%|████▏     | 229/543 [00:04<00:06, 47.29it/s] 43%|████▎     | 234/543 [00:04<00:06, 47.20it/s] 44%|████▍     | 239/543 [00:05<00:06, 47.17it/s] 45%|████▍     | 244/543 [00:05<00:06, 47.25it/s] 46%|████▌     | 249/543 [00:05<00:06, 47.23it/s] 47%|████▋     | 254/543 [00:05<00:06, 47.32it/s] 48%|████▊     | 259/543 [00:05<00:06, 47.29it/s] 49%|████▊     | 264/543 [00:05<00:05, 47.27it/s] 50%|████▉     | 269/543 [00:05<00:05, 47.17it/s] 50%|█████     | 274/543 [00:05<00:05, 47.19it/s] 51%|█████▏    | 279/543 [00:05<00:05, 47.21it/s] 52%|█████▏    | 284/543 [00:05<00:05, 47.15it/s] 53%|█████▎    | 289/543 [00:06<00:05, 47.11it/s] 54%|█████▍    | 294/543 [00:06<00:05, 47.30it/s] 55%|█████▌    | 299/543 [00:06<00:05, 47.35it/s] 56%|█████▌    | 304/543 [00:06<00:05, 47.29it/s] 57%|█████▋    | 309/543 [00:06<00:04, 47.17it/s] 58%|█████▊    | 314/543 [00:06<00:04, 47.27it/s] 59%|█████▊    | 319/543 [00:06<00:04, 47.25it/s] 60%|█████▉    | 324/543 [00:06<00:04, 47.17it/s] 61%|██████    | 329/543 [00:06<00:04, 47.19it/s] 62%|██████▏   | 334/543 [00:07<00:04, 47.08it/s] 62%|██████▏   | 339/543 [00:07<00:04, 47.19it/s] 63%|██████▎   | 344/543 [00:07<00:04, 47.28it/s] 64%|██████▍   | 349/543 [00:07<00:04, 47.39it/s] 65%|██████▌   | 354/543 [00:07<00:03, 47.37it/s] 66%|██████▌   | 359/543 [00:07<00:03, 47.33it/s] 67%|██████▋   | 364/543 [00:07<00:03, 47.35it/s] 68%|██████▊   | 369/543 [00:07<00:03, 47.13it/s] 69%|██████▉   | 374/543 [00:07<00:03, 47.18it/s] 70%|██████▉   | 379/543 [00:07<00:03, 47.18it/s] 71%|███████   | 384/543 [00:08<00:03, 47.16it/s] 72%|███████▏  | 389/543 [00:08<00:03, 47.24it/s] 73%|███████▎  | 394/543 [00:08<00:03, 47.09it/s] 73%|███████▎  | 399/543 [00:08<00:03, 47.19it/s] 74%|███████▍  | 404/543 [00:08<00:02, 47.23it/s] 75%|███████▌  | 409/543 [00:08<00:02, 47.22it/s] 76%|███████▌  | 414/543 [00:08<00:02, 47.25it/s] 77%|███████▋  | 419/543 [00:08<00:02, 47.23it/s] 78%|███████▊  | 424/543 [00:08<00:02, 47.19it/s] 79%|███████▉  | 429/543 [00:09<00:02, 47.27it/s] 80%|███████▉  | 434/543 [00:09<00:02, 47.23it/s] 81%|████████  | 439/543 [00:09<00:02, 47.11it/s] 82%|████████▏ | 444/543 [00:09<00:02, 47.17it/s] 83%|████████▎ | 449/543 [00:09<00:01, 47.27it/s] 84%|████████▎ | 454/543 [00:09<00:01, 47.23it/s] 85%|████████▍ | 459/543 [00:09<00:01, 47.18it/s] 85%|████████▌ | 464/543 [00:09<00:01, 47.20it/s] 86%|████████▋ | 469/543 [00:09<00:01, 47.15it/s] 87%|████████▋ | 474/543 [00:09<00:01, 47.17it/s] 88%|████████▊ | 479/543 [00:10<00:01, 47.29it/s] 89%|████████▉ | 484/543 [00:10<00:01, 47.24it/s] 90%|█████████ | 489/543 [00:10<00:01, 47.18it/s] 91%|█████████ | 494/543 [00:10<00:01, 47.25it/s] 92%|█████████▏| 499/543 [00:10<00:00, 47.23it/s] 93%|█████████▎| 504/543 [00:10<00:00, 47.26it/s] 94%|█████████▎| 509/543 [00:10<00:00, 47.20it/s] 95%|█████████▍| 514/543 [00:10<00:00, 47.19it/s] 96%|█████████▌| 519/543 [00:10<00:00, 47.17it/s] 97%|█████████▋| 524/543 [00:11<00:00, 47.16it/s] 97%|█████████▋| 529/543 [00:11<00:00, 47.14it/s] 98%|█████████▊| 534/543 [00:11<00:00, 47.19it/s] 99%|█████████▉| 539/543 [00:11<00:00, 47.18it/s]100%|██████████| 543/543 [00:11<00:00, 47.36it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:15:37,605 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:37,605 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:37,605 >>   eval_loss               =     0.9832
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:37,605 >>   eval_runtime            = 0:00:11.48
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:37,605 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:37,605 >>   eval_samples_per_second =     377.96
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:37,605 >>   eval_steps_per_second   =     47.267
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:15:37,605 >>   perplexity              =      2.673
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:15:43,686 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:15:43,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:15:43,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:15:43,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:15:43,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:15:43,990 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:15:43,991 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:15:44,255 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:15:45,298 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:15:45,299 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:15:47,038 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:15:47,043 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:15:47,043 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:15:47,043 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:15:47,043 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:15:47,361 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:15:47,362 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:15:47,621 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:15:47,785 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:15:47,785 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:06,  1.69it/s]Extractor Predicting: 11it [00:06,  1.71it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.61it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:09,  1.54it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:11,  1.47it/s]Extractor Predicting: 20it [00:12,  1.45it/s]Extractor Predicting: 21it [00:13,  1.47it/s]Extractor Predicting: 22it [00:13,  1.50it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:15,  1.51it/s]Extractor Predicting: 26it [00:16,  1.50it/s]Extractor Predicting: 27it [00:17,  1.49it/s]Extractor Predicting: 28it [00:17,  1.51it/s]Extractor Predicting: 29it [00:18,  1.49it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:19,  1.50it/s]Extractor Predicting: 32it [00:20,  1.51it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:21,  1.50it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:23,  1.46it/s]Extractor Predicting: 37it [00:24,  1.37it/s]Extractor Predicting: 38it [00:24,  1.39it/s]Extractor Predicting: 39it [00:25,  1.43it/s]Extractor Predicting: 40it [00:26,  1.46it/s]Extractor Predicting: 41it [00:26,  1.46it/s]Extractor Predicting: 42it [00:27,  1.45it/s]Extractor Predicting: 43it [00:28,  1.49it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:29,  1.51it/s]Extractor Predicting: 46it [00:30,  1.52it/s]Extractor Predicting: 47it [00:30,  1.52it/s]Extractor Predicting: 48it [00:31,  1.51it/s]Extractor Predicting: 49it [00:32,  1.50it/s]Extractor Predicting: 50it [00:32,  1.51it/s]Extractor Predicting: 51it [00:33,  1.52it/s]Extractor Predicting: 52it [00:34,  1.50it/s]Extractor Predicting: 53it [00:34,  1.48it/s]Extractor Predicting: 54it [00:35,  1.51it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:37,  1.49it/s]Extractor Predicting: 58it [00:38,  1.53it/s]Extractor Predicting: 59it [00:38,  1.49it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:40,  1.52it/s]Extractor Predicting: 62it [00:40,  1.53it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 64it [00:42,  1.53it/s]Extractor Predicting: 65it [00:42,  1.57it/s]Extractor Predicting: 66it [00:43,  1.57it/s]Extractor Predicting: 67it [00:43,  1.56it/s]Extractor Predicting: 68it [00:44,  1.53it/s]Extractor Predicting: 69it [00:45,  1.54it/s]Extractor Predicting: 70it [00:45,  1.53it/s]Extractor Predicting: 71it [00:46,  1.53it/s]Extractor Predicting: 72it [00:47,  1.53it/s]Extractor Predicting: 73it [00:47,  1.51it/s]Extractor Predicting: 74it [00:48,  1.49it/s]Extractor Predicting: 75it [00:49,  1.46it/s]Extractor Predicting: 76it [00:49,  1.48it/s]Extractor Predicting: 77it [00:50,  1.51it/s]Extractor Predicting: 78it [00:51,  1.50it/s]Extractor Predicting: 79it [00:51,  1.48it/s]Extractor Predicting: 80it [00:52,  1.51it/s]Extractor Predicting: 81it [00:53,  1.50it/s]Extractor Predicting: 82it [00:53,  1.51it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:55,  1.53it/s]Extractor Predicting: 85it [00:55,  1.54it/s]Extractor Predicting: 86it [00:56,  1.51it/s]Extractor Predicting: 87it [00:57,  1.51it/s]Extractor Predicting: 88it [00:57,  1.53it/s]Extractor Predicting: 89it [00:58,  1.54it/s]Extractor Predicting: 90it [00:59,  1.51it/s]Extractor Predicting: 91it [00:59,  1.48it/s]Extractor Predicting: 92it [01:00,  1.47it/s]Extractor Predicting: 93it [01:01,  1.39it/s]Extractor Predicting: 94it [01:02,  1.43it/s]Extractor Predicting: 95it [01:02,  1.45it/s]Extractor Predicting: 96it [01:03,  1.48it/s]Extractor Predicting: 97it [01:03,  1.51it/s]Extractor Predicting: 98it [01:04,  1.50it/s]Extractor Predicting: 99it [01:05,  1.48it/s]Extractor Predicting: 100it [01:06,  1.49it/s]Extractor Predicting: 101it [01:06,  1.51it/s]Extractor Predicting: 102it [01:07,  1.50it/s]Extractor Predicting: 103it [01:07,  1.51it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.49it/s]Extractor Predicting: 106it [01:09,  1.51it/s]Extractor Predicting: 107it [01:10,  1.51it/s]Extractor Predicting: 108it [01:11,  1.51it/s]Extractor Predicting: 109it [01:11,  1.51it/s]Extractor Predicting: 110it [01:12,  1.50it/s]Extractor Predicting: 111it [01:13,  1.51it/s]Extractor Predicting: 112it [01:13,  1.54it/s]Extractor Predicting: 113it [01:14,  1.54it/s]Extractor Predicting: 114it [01:15,  1.53it/s]Extractor Predicting: 115it [01:15,  1.52it/s]Extractor Predicting: 116it [01:16,  1.51it/s]Extractor Predicting: 117it [01:17,  1.53it/s]Extractor Predicting: 118it [01:17,  1.49it/s]Extractor Predicting: 119it [01:18,  1.50it/s]Extractor Predicting: 120it [01:19,  1.49it/s]Extractor Predicting: 121it [01:19,  1.47it/s]Extractor Predicting: 122it [01:20,  1.47it/s]Extractor Predicting: 123it [01:21,  1.51it/s]Extractor Predicting: 124it [01:21,  1.52it/s]Extractor Predicting: 125it [01:22,  1.54it/s]Extractor Predicting: 126it [01:23,  1.49it/s]Extractor Predicting: 127it [01:23,  1.48it/s]Extractor Predicting: 128it [01:24,  1.50it/s]Extractor Predicting: 129it [01:25,  1.52it/s]Extractor Predicting: 130it [01:25,  1.56it/s]Extractor Predicting: 131it [01:26,  1.52it/s]Extractor Predicting: 132it [01:27,  1.51it/s]Extractor Predicting: 133it [01:27,  1.52it/s]Extractor Predicting: 134it [01:28,  1.52it/s]Extractor Predicting: 135it [01:29,  1.54it/s]Extractor Predicting: 136it [01:29,  1.53it/s]Extractor Predicting: 137it [01:30,  1.55it/s]Extractor Predicting: 138it [01:31,  1.52it/s]Extractor Predicting: 139it [01:31,  1.50it/s]Extractor Predicting: 140it [01:32,  1.53it/s]Extractor Predicting: 141it [01:33,  1.54it/s]Extractor Predicting: 142it [01:33,  1.57it/s]Extractor Predicting: 143it [01:34,  1.57it/s]Extractor Predicting: 144it [01:34,  1.57it/s]Extractor Predicting: 145it [01:35,  1.53it/s]Extractor Predicting: 146it [01:36,  1.55it/s]Extractor Predicting: 147it [01:36,  1.55it/s]Extractor Predicting: 148it [01:37,  1.56it/s]Extractor Predicting: 149it [01:38,  1.57it/s]Extractor Predicting: 150it [01:38,  1.52it/s]Extractor Predicting: 151it [01:39,  1.52it/s]Extractor Predicting: 152it [01:40,  1.52it/s]Extractor Predicting: 153it [01:40,  1.50it/s]Extractor Predicting: 154it [01:41,  1.52it/s]Extractor Predicting: 155it [01:42,  1.49it/s]Extractor Predicting: 156it [01:42,  1.48it/s]Extractor Predicting: 157it [01:43,  1.49it/s]Extractor Predicting: 158it [01:44,  1.50it/s]Extractor Predicting: 159it [01:44,  1.52it/s]Extractor Predicting: 160it [01:45,  1.54it/s]Extractor Predicting: 161it [01:46,  1.55it/s]Extractor Predicting: 162it [01:46,  1.56it/s]Extractor Predicting: 163it [01:47,  1.47it/s]Extractor Predicting: 163it [01:47,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:17:43,389 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:17:43,414 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:17:43,415 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:17:43,415 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:17:43,415 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:17:44,248 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:17:44,249 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:17:44,933 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:17:46,047 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:17:46,047 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:17:48,755 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:17:48,759 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:17:48,759 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:17:48,759 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:17:48,759 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:17:49,520 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:17:49,521 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:17:49,793 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:17:49,961 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:17:49,961 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3005050505050505,
  "recall": 0.02740672501151543,
  "score": 0.050232165470662726,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.59it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:18,  1.59it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:20,  1.58it/s]Extractor Predicting: 34it [00:21,  1.57it/s]Extractor Predicting: 35it [00:22,  1.60it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:27,  1.51it/s]Extractor Predicting: 44it [00:27,  1.51it/s]Extractor Predicting: 45it [00:28,  1.49it/s]Extractor Predicting: 46it [00:29,  1.47it/s]Extractor Predicting: 47it [00:30,  1.45it/s]Extractor Predicting: 48it [00:30,  1.48it/s]Extractor Predicting: 49it [00:31,  1.47it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:32,  1.48it/s]Extractor Predicting: 52it [00:33,  1.47it/s]Extractor Predicting: 53it [00:34,  1.46it/s]Extractor Predicting: 54it [00:34,  1.41it/s]Extractor Predicting: 55it [00:35,  1.46it/s]Extractor Predicting: 56it [00:36,  1.50it/s]Extractor Predicting: 57it [00:36,  1.48it/s]Extractor Predicting: 58it [00:37,  1.48it/s]Extractor Predicting: 59it [00:38,  1.45it/s]Extractor Predicting: 60it [00:38,  1.46it/s]Extractor Predicting: 61it [00:39,  1.49it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.44it/s]Extractor Predicting: 64it [00:41,  1.45it/s]Extractor Predicting: 65it [00:42,  1.46it/s]Extractor Predicting: 66it [00:43,  1.44it/s]Extractor Predicting: 67it [00:43,  1.45it/s]Extractor Predicting: 68it [00:44,  1.44it/s]Extractor Predicting: 69it [00:45,  1.45it/s]Extractor Predicting: 70it [00:46,  1.33it/s]Extractor Predicting: 71it [00:46,  1.37it/s]Extractor Predicting: 72it [00:47,  1.39it/s]Extractor Predicting: 73it [00:48,  1.40it/s]Extractor Predicting: 74it [00:48,  1.44it/s]Extractor Predicting: 75it [00:49,  1.45it/s]Extractor Predicting: 76it [00:50,  1.47it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.49it/s]Extractor Predicting: 79it [00:52,  1.46it/s]Extractor Predicting: 80it [00:52,  1.49it/s]Extractor Predicting: 81it [00:53,  1.38it/s]Extractor Predicting: 82it [00:54,  1.42it/s]Extractor Predicting: 83it [00:54,  1.46it/s]Extractor Predicting: 84it [00:55,  1.43it/s]Extractor Predicting: 85it [00:56,  1.44it/s]Extractor Predicting: 86it [00:56,  1.47it/s]Extractor Predicting: 87it [00:57,  1.49it/s]Extractor Predicting: 88it [00:58,  1.47it/s]Extractor Predicting: 89it [00:58,  1.51it/s]Extractor Predicting: 90it [00:59,  1.52it/s]Extractor Predicting: 91it [01:00,  1.54it/s]Extractor Predicting: 92it [01:00,  1.54it/s]Extractor Predicting: 93it [01:01,  1.54it/s]Extractor Predicting: 94it [01:02,  1.52it/s]Extractor Predicting: 95it [01:02,  1.54it/s]Extractor Predicting: 96it [01:03,  1.56it/s]Extractor Predicting: 97it [01:04,  1.55it/s]Extractor Predicting: 98it [01:04,  1.49it/s]Extractor Predicting: 99it [01:05,  1.50it/s]Extractor Predicting: 100it [01:06,  1.49it/s]Extractor Predicting: 101it [01:06,  1.49it/s]Extractor Predicting: 102it [01:07,  1.51it/s]Extractor Predicting: 103it [01:08,  1.49it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.50it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:10,  1.45it/s]Extractor Predicting: 108it [01:11,  1.48it/s]Extractor Predicting: 109it [01:12,  1.43it/s]Extractor Predicting: 110it [01:12,  1.45it/s]Extractor Predicting: 111it [01:13,  1.44it/s]Extractor Predicting: 112it [01:14,  1.46it/s]Extractor Predicting: 113it [01:15,  1.46it/s]Extractor Predicting: 114it [01:15,  1.46it/s]Extractor Predicting: 115it [01:16,  1.46it/s]Extractor Predicting: 116it [01:17,  1.44it/s]Extractor Predicting: 117it [01:17,  1.45it/s]Extractor Predicting: 118it [01:18,  1.50it/s]Extractor Predicting: 119it [01:19,  1.49it/s]Extractor Predicting: 120it [01:19,  1.53it/s]Extractor Predicting: 121it [01:20,  1.52it/s]Extractor Predicting: 122it [01:21,  1.54it/s]Extractor Predicting: 123it [01:21,  1.56it/s]Extractor Predicting: 124it [01:22,  1.58it/s]Extractor Predicting: 125it [01:22,  1.56it/s]Extractor Predicting: 126it [01:23,  1.53it/s]Extractor Predicting: 127it [01:24,  1.52it/s]Extractor Predicting: 128it [01:24,  1.51it/s]Extractor Predicting: 129it [01:25,  1.51it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:26,  1.56it/s]Extractor Predicting: 132it [01:27,  1.54it/s]Extractor Predicting: 133it [01:28,  1.54it/s]Extractor Predicting: 134it [01:28,  1.56it/s]Extractor Predicting: 135it [01:29,  1.56it/s]Extractor Predicting: 136it [01:30,  1.55it/s]Extractor Predicting: 137it [01:30,  1.59it/s]Extractor Predicting: 138it [01:31,  1.57it/s]Extractor Predicting: 139it [01:31,  1.57it/s]Extractor Predicting: 140it [01:32,  1.55it/s]Extractor Predicting: 141it [01:33,  1.56it/s]Extractor Predicting: 142it [01:33,  1.54it/s]Extractor Predicting: 143it [01:34,  1.52it/s]Extractor Predicting: 144it [01:35,  1.52it/s]Extractor Predicting: 145it [01:35,  1.48it/s]Extractor Predicting: 146it [01:36,  1.44it/s]Extractor Predicting: 147it [01:37,  1.46it/s]Extractor Predicting: 148it [01:38,  1.46it/s]Extractor Predicting: 149it [01:38,  1.48it/s]Extractor Predicting: 150it [01:39,  1.49it/s]Extractor Predicting: 151it [01:40,  1.35it/s]Extractor Predicting: 152it [01:40,  1.39it/s]Extractor Predicting: 153it [01:41,  1.44it/s]Extractor Predicting: 154it [01:42,  1.48it/s]Extractor Predicting: 155it [01:42,  1.52it/s]Extractor Predicting: 156it [01:43,  1.49it/s]Extractor Predicting: 157it [01:44,  1.50it/s]Extractor Predicting: 158it [01:44,  1.50it/s]Extractor Predicting: 159it [01:45,  1.50it/s]Extractor Predicting: 160it [01:46,  1.53it/s]Extractor Predicting: 161it [01:46,  1.46it/s]Extractor Predicting: 162it [01:47,  1.51it/s]Extractor Predicting: 163it [01:48,  1.54it/s]Extractor Predicting: 164it [01:48,  1.51it/s]Extractor Predicting: 165it [01:49,  1.56it/s]Extractor Predicting: 166it [01:50,  1.56it/s]Extractor Predicting: 167it [01:50,  1.55it/s]Extractor Predicting: 168it [01:51,  1.60it/s]Extractor Predicting: 169it [01:51,  1.61it/s]Extractor Predicting: 170it [01:52,  1.57it/s]Extractor Predicting: 171it [01:53,  1.56it/s]Extractor Predicting: 172it [01:53,  1.56it/s]Extractor Predicting: 173it [01:54,  1.53it/s]Extractor Predicting: 174it [01:55,  1.57it/s]Extractor Predicting: 175it [01:55,  1.56it/s]Extractor Predicting: 176it [01:56,  1.54it/s]Extractor Predicting: 177it [01:57,  1.52it/s]Extractor Predicting: 178it [01:57,  1.52it/s]Extractor Predicting: 179it [01:58,  1.60it/s]Extractor Predicting: 180it [01:59,  1.57it/s]Extractor Predicting: 181it [01:59,  1.57it/s]Extractor Predicting: 182it [02:00,  1.55it/s]Extractor Predicting: 183it [02:00,  1.54it/s]Extractor Predicting: 184it [02:01,  1.53it/s]Extractor Predicting: 185it [02:02,  1.56it/s]Extractor Predicting: 186it [02:02,  1.54it/s]Extractor Predicting: 187it [02:03,  1.53it/s]Extractor Predicting: 188it [02:04,  1.52it/s]Extractor Predicting: 189it [02:04,  1.50it/s]Extractor Predicting: 190it [02:05,  1.48it/s]Extractor Predicting: 191it [02:06,  1.48it/s]Extractor Predicting: 192it [02:06,  1.50it/s]Extractor Predicting: 193it [02:07,  1.53it/s]Extractor Predicting: 194it [02:08,  1.55it/s]Extractor Predicting: 195it [02:08,  1.51it/s]Extractor Predicting: 196it [02:09,  1.53it/s]Extractor Predicting: 197it [02:10,  1.53it/s]Extractor Predicting: 198it [02:10,  1.53it/s]Extractor Predicting: 199it [02:11,  1.52it/s]Extractor Predicting: 200it [02:12,  1.52it/s]Extractor Predicting: 201it [02:12,  1.53it/s]Extractor Predicting: 202it [02:13,  1.53it/s]Extractor Predicting: 203it [02:14,  1.53it/s]Extractor Predicting: 204it [02:14,  1.54it/s]Extractor Predicting: 205it [02:15,  1.53it/s]Extractor Predicting: 206it [02:16,  1.52it/s]Extractor Predicting: 207it [02:16,  1.53it/s]Extractor Predicting: 208it [02:17,  1.54it/s]Extractor Predicting: 209it [02:18,  1.54it/s]Extractor Predicting: 210it [02:18,  1.53it/s]Extractor Predicting: 211it [02:19,  1.52it/s]Extractor Predicting: 212it [02:20,  1.53it/s]Extractor Predicting: 213it [02:20,  1.52it/s]Extractor Predicting: 214it [02:21,  1.53it/s]Extractor Predicting: 215it [02:22,  1.47it/s]Extractor Predicting: 216it [02:22,  1.50it/s]Extractor Predicting: 217it [02:23,  1.52it/s]Extractor Predicting: 218it [02:23,  1.53it/s]Extractor Predicting: 219it [02:24,  1.51it/s]Extractor Predicting: 220it [02:25,  1.52it/s]Extractor Predicting: 221it [02:25,  1.51it/s]Extractor Predicting: 222it [02:26,  1.52it/s]Extractor Predicting: 223it [02:27,  1.46it/s]Extractor Predicting: 224it [02:28,  1.47it/s]Extractor Predicting: 225it [02:28,  1.45it/s]Extractor Predicting: 226it [02:29,  1.46it/s]Extractor Predicting: 227it [02:30,  1.43it/s]Extractor Predicting: 228it [02:30,  1.38it/s]Extractor Predicting: 229it [02:31,  1.41it/s]Extractor Predicting: 230it [02:32,  1.43it/s]Extractor Predicting: 231it [02:32,  1.43it/s]Extractor Predicting: 232it [02:33,  1.44it/s]Extractor Predicting: 233it [02:34,  1.45it/s]Extractor Predicting: 234it [02:35,  1.46it/s]Extractor Predicting: 235it [02:35,  1.47it/s]Extractor Predicting: 236it [02:36,  1.48it/s]Extractor Predicting: 237it [02:37,  1.48it/s]Extractor Predicting: 238it [02:37,  1.53it/s]Extractor Predicting: 239it [02:38,  1.55it/s]Extractor Predicting: 240it [02:38,  1.60it/s]Extractor Predicting: 241it [02:39,  1.59it/s]Extractor Predicting: 242it [02:40,  1.59it/s]Extractor Predicting: 243it [02:40,  1.55it/s]Extractor Predicting: 244it [02:41,  1.55it/s]Extractor Predicting: 245it [02:42,  1.58it/s]Extractor Predicting: 246it [02:42,  1.56it/s]Extractor Predicting: 247it [02:43,  1.57it/s]Extractor Predicting: 248it [02:43,  1.57it/s]Extractor Predicting: 249it [02:44,  1.62it/s]Extractor Predicting: 250it [02:45,  1.63it/s]Extractor Predicting: 251it [02:45,  1.68it/s]Extractor Predicting: 252it [02:46,  1.70it/s]Extractor Predicting: 253it [02:46,  1.67it/s]Extractor Predicting: 254it [02:47,  1.68it/s]Extractor Predicting: 255it [02:48,  1.63it/s]Extractor Predicting: 256it [02:48,  1.67it/s]Extractor Predicting: 257it [02:49,  1.63it/s]Extractor Predicting: 258it [02:50,  1.42it/s]Extractor Predicting: 259it [02:50,  1.42it/s]Extractor Predicting: 260it [02:51,  1.50it/s]Extractor Predicting: 261it [02:52,  1.49it/s]Extractor Predicting: 262it [02:52,  1.52it/s]Extractor Predicting: 263it [02:53,  1.57it/s]Extractor Predicting: 264it [02:54,  1.59it/s]Extractor Predicting: 265it [02:54,  1.59it/s]Extractor Predicting: 266it [02:55,  1.62it/s]Extractor Predicting: 267it [02:55,  1.64it/s]Extractor Predicting: 268it [02:56,  1.63it/s]Extractor Predicting: 269it [02:57,  1.59it/s]Extractor Predicting: 270it [02:57,  1.61it/s]Extractor Predicting: 271it [02:58,  1.60it/s]Extractor Predicting: 272it [02:59,  1.57it/s]Extractor Predicting: 273it [02:59,  1.62it/s]Extractor Predicting: 274it [03:00,  1.63it/s]Extractor Predicting: 275it [03:00,  1.62it/s]Extractor Predicting: 276it [03:01,  1.60it/s]Extractor Predicting: 277it [03:02,  1.60it/s]Extractor Predicting: 278it [03:02,  1.62it/s]Extractor Predicting: 279it [03:03,  1.58it/s]Extractor Predicting: 280it [03:03,  1.63it/s]Extractor Predicting: 281it [03:04,  1.64it/s]Extractor Predicting: 282it [03:05,  1.66it/s]Extractor Predicting: 283it [03:05,  1.66it/s]Extractor Predicting: 284it [03:06,  1.65it/s]Extractor Predicting: 285it [03:06,  1.66it/s]Extractor Predicting: 286it [03:07,  1.65it/s]Extractor Predicting: 287it [03:08,  1.65it/s]Extractor Predicting: 288it [03:08,  1.61it/s]Extractor Predicting: 289it [03:09,  1.61it/s]Extractor Predicting: 290it [03:10,  1.54it/s]Extractor Predicting: 291it [03:10,  1.56it/s]Extractor Predicting: 292it [03:11,  1.59it/s]Extractor Predicting: 293it [03:12,  1.61it/s]Extractor Predicting: 294it [03:12,  1.61it/s]Extractor Predicting: 295it [03:13,  1.62it/s]Extractor Predicting: 296it [03:13,  1.53it/s]Extractor Predicting: 297it [03:14,  1.52it/s]Extractor Predicting: 298it [03:15,  1.47it/s]Extractor Predicting: 299it [03:16,  1.48it/s]Extractor Predicting: 300it [03:16,  1.50it/s]Extractor Predicting: 301it [03:17,  1.51it/s]Extractor Predicting: 302it [03:17,  1.52it/s]Extractor Predicting: 303it [03:18,  1.48it/s]Extractor Predicting: 304it [03:19,  1.48it/s]Extractor Predicting: 305it [03:20,  1.52it/s]Extractor Predicting: 306it [03:20,  1.48it/s]Extractor Predicting: 307it [03:21,  1.45it/s]Extractor Predicting: 308it [03:22,  1.45it/s]Extractor Predicting: 309it [03:22,  1.48it/s]Extractor Predicting: 310it [03:23,  1.51it/s]Extractor Predicting: 311it [03:24,  1.53it/s]Extractor Predicting: 312it [03:24,  1.51it/s]Extractor Predicting: 313it [03:25,  1.48it/s]Extractor Predicting: 314it [03:26,  1.46it/s]Extractor Predicting: 315it [03:26,  1.46it/s]Extractor Predicting: 316it [03:27,  1.46it/s]Extractor Predicting: 317it [03:28,  1.46it/s]Extractor Predicting: 318it [03:28,  1.47it/s]Extractor Predicting: 319it [03:29,  1.48it/s]Extractor Predicting: 320it [03:30,  1.49it/s]Extractor Predicting: 321it [03:30,  1.54it/s]Extractor Predicting: 322it [03:31,  1.53it/s]Extractor Predicting: 323it [03:32,  1.51it/s]Extractor Predicting: 324it [03:32,  1.50it/s]Extractor Predicting: 325it [03:33,  1.49it/s]Extractor Predicting: 326it [03:34,  1.48it/s]Extractor Predicting: 327it [03:34,  1.46it/s]Extractor Predicting: 328it [03:35,  1.47it/s]Extractor Predicting: 329it [03:36,  1.51it/s]Extractor Predicting: 330it [03:36,  1.53it/s]Extractor Predicting: 331it [03:37,  1.69it/s]Extractor Predicting: 331it [03:37,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:21:36,156 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:21:36,160 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:21:36,160 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:21:36,160 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:21:36,160 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:21:36,785 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:21:36,786 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:21:37,352 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:21:38,370 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:21:38,371 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:21:41,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:21:41,228 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:21:41,228 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:21:41,228 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:21:41,228 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:21:41,871 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:21:41,872 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:21:42,435 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:21:42,591 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:21:42,592 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4938704028021016,
  "recall": 0.07111335266675073,
  "score": 0.12432492009258238,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.44it/s]Extractor Predicting: 8it [00:05,  1.39it/s]Extractor Predicting: 9it [00:06,  1.40it/s]Extractor Predicting: 10it [00:06,  1.41it/s]Extractor Predicting: 11it [00:07,  1.42it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:09,  1.43it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.42it/s]Extractor Predicting: 16it [00:11,  1.44it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:13,  1.44it/s]Extractor Predicting: 20it [00:13,  1.42it/s]Extractor Predicting: 21it [00:14,  1.41it/s]Extractor Predicting: 22it [00:15,  1.40it/s]Extractor Predicting: 23it [00:16,  1.38it/s]Extractor Predicting: 24it [00:16,  1.39it/s]Extractor Predicting: 25it [00:17,  1.40it/s]Extractor Predicting: 26it [00:18,  1.43it/s]Extractor Predicting: 27it [00:18,  1.46it/s]Extractor Predicting: 28it [00:19,  1.44it/s]Extractor Predicting: 29it [00:20,  1.44it/s]Extractor Predicting: 30it [00:20,  1.44it/s]Extractor Predicting: 31it [00:21,  1.43it/s]Extractor Predicting: 32it [00:22,  1.46it/s]Extractor Predicting: 33it [00:22,  1.47it/s]Extractor Predicting: 34it [00:23,  1.46it/s]Extractor Predicting: 35it [00:24,  1.48it/s]Extractor Predicting: 36it [00:25,  1.48it/s]Extractor Predicting: 37it [00:25,  1.48it/s]Extractor Predicting: 38it [00:26,  1.49it/s]Extractor Predicting: 39it [00:27,  1.43it/s]Extractor Predicting: 40it [00:27,  1.44it/s]Extractor Predicting: 41it [00:28,  1.46it/s]Extractor Predicting: 42it [00:29,  1.48it/s]Extractor Predicting: 43it [00:29,  1.47it/s]Extractor Predicting: 44it [00:30,  1.48it/s]Extractor Predicting: 45it [00:31,  1.47it/s]Extractor Predicting: 46it [00:31,  1.48it/s]Extractor Predicting: 47it [00:32,  1.46it/s]Extractor Predicting: 48it [00:33,  1.49it/s]Extractor Predicting: 49it [00:33,  1.51it/s]Extractor Predicting: 50it [00:34,  1.53it/s]Extractor Predicting: 51it [00:35,  1.53it/s]Extractor Predicting: 52it [00:35,  1.52it/s]Extractor Predicting: 53it [00:36,  1.44it/s]Extractor Predicting: 54it [00:37,  1.47it/s]Extractor Predicting: 55it [00:37,  1.49it/s]Extractor Predicting: 56it [00:38,  1.55it/s]Extractor Predicting: 57it [00:39,  1.59it/s]Extractor Predicting: 58it [00:39,  1.65it/s]Extractor Predicting: 59it [00:40,  1.72it/s]Extractor Predicting: 60it [00:40,  1.79it/s]Extractor Predicting: 61it [00:41,  1.83it/s]Extractor Predicting: 62it [00:41,  1.82it/s]Extractor Predicting: 63it [00:42,  1.83it/s]Extractor Predicting: 64it [00:42,  1.82it/s]Extractor Predicting: 65it [00:43,  1.81it/s]Extractor Predicting: 66it [00:43,  1.81it/s]Extractor Predicting: 67it [00:44,  1.80it/s]Extractor Predicting: 68it [00:44,  1.82it/s]Extractor Predicting: 69it [00:45,  1.84it/s]Extractor Predicting: 70it [00:46,  1.81it/s]Extractor Predicting: 71it [00:46,  1.81it/s]Extractor Predicting: 72it [00:47,  1.83it/s]Extractor Predicting: 73it [00:47,  1.86it/s]Extractor Predicting: 74it [00:48,  1.88it/s]Extractor Predicting: 75it [00:48,  1.86it/s]Extractor Predicting: 76it [00:49,  1.84it/s]Extractor Predicting: 77it [00:49,  1.90it/s]Extractor Predicting: 78it [00:50,  1.83it/s]Extractor Predicting: 79it [00:50,  1.83it/s]Extractor Predicting: 80it [00:51,  1.82it/s]Extractor Predicting: 81it [00:52,  1.81it/s]Extractor Predicting: 82it [00:52,  1.83it/s]Extractor Predicting: 83it [00:53,  1.83it/s]Extractor Predicting: 84it [00:53,  1.84it/s]Extractor Predicting: 85it [00:54,  1.85it/s]Extractor Predicting: 86it [00:54,  1.72it/s]Extractor Predicting: 87it [00:55,  1.67it/s]Extractor Predicting: 88it [00:56,  1.64it/s]Extractor Predicting: 89it [00:56,  1.62it/s]Extractor Predicting: 90it [00:57,  1.62it/s]Extractor Predicting: 91it [00:58,  1.59it/s]Extractor Predicting: 92it [00:58,  1.58it/s]Extractor Predicting: 93it [00:59,  1.59it/s]Extractor Predicting: 94it [00:59,  1.58it/s]Extractor Predicting: 95it [01:00,  1.61it/s]Extractor Predicting: 96it [01:01,  1.61it/s]Extractor Predicting: 97it [01:01,  1.62it/s]Extractor Predicting: 98it [01:02,  1.62it/s]Extractor Predicting: 99it [01:03,  1.59it/s]Extractor Predicting: 100it [01:03,  1.54it/s]Extractor Predicting: 101it [01:04,  1.57it/s]Extractor Predicting: 102it [01:05,  1.56it/s]Extractor Predicting: 103it [01:05,  1.52it/s]Extractor Predicting: 104it [01:06,  1.50it/s]Extractor Predicting: 105it [01:07,  1.49it/s]Extractor Predicting: 106it [01:07,  1.48it/s]Extractor Predicting: 107it [01:08,  1.46it/s]Extractor Predicting: 108it [01:09,  1.41it/s]Extractor Predicting: 108it [01:09,  1.56it/s]
[INFO|configuration_utils.py:515] 2023-08-29 03:22:53,349 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:22:53,351 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:22:53,356 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:22:53,357 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 03:22:53,359 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:22:56,365 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 03:22:56,367 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 03:22:56,379 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:22:56,380 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:22:56,385 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:22:56,390 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:22:56,390 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:22:56,390 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:22:56,390 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:22:56,390 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:22:56,390 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7988748241912799,
  "recall": 0.09098189972769502,
  "score": 0.1633592177164222,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 03:22:56,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:22:57,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:22:58,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:22:58,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:22:59,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:00,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:00,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:01,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:02,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:02,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:03,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:04,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:05,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:06,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:06,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:07,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:08,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:08,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:09,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:10,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:10,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:29, 14.98s/it][WARNING|generation_utils.py:914] 2023-08-29 03:23:11,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:12,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:13,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:13,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:14,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:15,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:15,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:16,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:17,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:17,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:18,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:19,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:19,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:20,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:21,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:22,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:22,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:23,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:24,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:25,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:29<03:07, 14.46s/it][WARNING|generation_utils.py:914] 2023-08-29 03:23:25,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:26,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:27,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:28,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:28,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:29,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:30,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:31,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:32,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:32,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:33,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:34,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:34,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:35,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:36,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:37,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:37,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:38,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:39,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:40,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:44<02:57, 14.79s/it][WARNING|generation_utils.py:914] 2023-08-29 03:23:40,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:41,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:42,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:43,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:43,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:45,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:45,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:46,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:47,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:48,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:48,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:49,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:50,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:50,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:51,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:52,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:53,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:54,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:54,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:55,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:56,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:00<02:48, 15.35s/it][WARNING|generation_utils.py:914] 2023-08-29 03:23:57,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:57,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:58,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:59,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:23:59,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:00,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:01,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:02,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:03,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:03,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:04,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:05,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:05,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:06,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:07,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:07,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:08,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:09,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:10,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:10,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:11,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:12,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:16<02:35, 15.57s/it][WARNING|generation_utils.py:914] 2023-08-29 03:24:13,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:13,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:14,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:15,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:15,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:16,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:17,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:17,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:18,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:19,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:19,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:20,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:21,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:21,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:22,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:23,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:23,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:24,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:25,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:26,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:26,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:27,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:27,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:32<02:20, 15.59s/it][WARNING|generation_utils.py:914] 2023-08-29 03:24:28,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:29,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:30,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:31,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:31,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:32,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:33,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:34,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:34,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:35,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:36,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:37,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:37,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:38,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:39,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:39,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:40,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:41,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:41,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:42,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:43,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:44,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:48<02:06, 15.79s/it][WARNING|generation_utils.py:914] 2023-08-29 03:24:44,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:45,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:46,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:46,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:47,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:48,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:48,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:49,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:50,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:51,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:52,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:53,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:53,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:54,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:55,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:55,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:56,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:57,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:57,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:58,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:24:59,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:03<01:48, 15.55s/it][WARNING|generation_utils.py:914] 2023-08-29 03:24:59,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:00,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:01,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:01,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:02,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:03,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:04,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:04,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:05,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:06,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:06,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:07,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:07,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:08,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:09,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:09,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:10,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:10,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:11,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:12,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:16<01:28, 14.72s/it][WARNING|generation_utils.py:914] 2023-08-29 03:25:12,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:13,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:14,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:14,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:15,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:16,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:16,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:17,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:18,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:18,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:19,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:20,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:21,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:21,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:22,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:23,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:23,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:24,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:25,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:25,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:29<01:11, 14.33s/it][WARNING|generation_utils.py:914] 2023-08-29 03:25:26,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:27,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:27,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:28,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:29,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:29,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:30,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:31,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:31,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:32,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:33,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:33,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:34,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:35,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:35,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:36,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:36,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:37,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:38,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:39,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:39,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:40,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:44<00:57, 14.44s/it][WARNING|generation_utils.py:914] 2023-08-29 03:25:41,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:42,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:42,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:43,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:44,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:45,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:45,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:46,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:47,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:48,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:49,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:49,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:50,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:51,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:51,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:52,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:53,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:54,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:54,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:55,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:56,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:57,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:57,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:01<00:46, 15.38s/it][WARNING|generation_utils.py:914] 2023-08-29 03:25:58,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:25:59,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:00,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:00,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:01,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:02,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:03,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:03,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:04,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:05,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:06,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:07,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:07,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:08,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:09,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:09,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:10,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:11,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:11,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:12,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:13,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:14,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:14,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:18<00:31, 15.89s/it][WARNING|generation_utils.py:914] 2023-08-29 03:26:15,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:16,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:17,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:17,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:18,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:19,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:19,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:20,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:20,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:21,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:21,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:22,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:23,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:23,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:24,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:25,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:25,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:26,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:27,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:27,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:28,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:29,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:33<00:15, 15.39s/it][WARNING|generation_utils.py:914] 2023-08-29 03:26:29,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:30,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:31,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:32,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:33,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:33,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:34,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:35,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:35,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:36,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:37,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:38,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:39,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:39,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:40,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:41,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:41,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:42,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:43,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:44,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:44,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:26:45,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:50<00:00, 15.84s/it]Generating: 100%|██████████| 15/15 [03:50<00:00, 15.34s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:53,230 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:53,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:53,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:53,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:53,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:26:53,849 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:26:53,850 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:26:54,432 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:26:55,483 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:26:55,483 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:58,353 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:58,355 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:58,355 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:58,355 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:58,356 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:26:59,073 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:26:59,074 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:26:59,634 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:26:59,791 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:26:59,791 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : characters . Context : Later in Life , he played the title character , a young son of a Scottish novelist . Head Entity : son of a Scottish novelist , Tail Entity : Harry .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : made from material .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : cast member .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : follows .', 'success_rate': 0.8821022727272727, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : league .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.94375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major in the Commons . Head Entity : John Major , Tail Entity : Liberal Party .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8778409090909091, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : mother .', 'success_rate': 0.8328804347826086, 'errors': {''}}
['Relation : residence . Context : Later in life he studied at the Conservatory of Fine Arts at Oxford University and at the London School of Economics . Head Entity : William Poulton , Tail Entity : Oxford University .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8478260869565217, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : shares border with . Context : Later in 2003 , the country became a part of NATO and joined Canada as part of the region . Head Entity : Canada , Tail Entity : United States .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8863636363636364, 'errors': {''}}
['Relation : twinned administrative body . Context : Later in 1453 the county became a part of a county council of the Legislative Assembly of the state of New York , headed by William P. Chase , and a Democratic constituency , headed by William R. Williams . Head Entity : Legislative Assembly , Tail Entity : Legislative Assembly of New York .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8863636363636364, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 10202
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10302, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.52it/s]Extractor Estimating: 2it [00:01,  1.57it/s]Extractor Estimating: 3it [00:01,  1.59it/s]Extractor Estimating: 4it [00:02,  1.58it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:03,  1.55it/s]Extractor Estimating: 7it [00:04,  1.56it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.60it/s]Extractor Estimating: 11it [00:06,  1.59it/s]Extractor Estimating: 12it [00:07,  1.39it/s]Extractor Estimating: 13it [00:08,  1.45it/s]Extractor Estimating: 14it [00:09,  1.40it/s]Extractor Estimating: 15it [00:09,  1.47it/s]Extractor Estimating: 16it [00:10,  1.51it/s]Extractor Estimating: 17it [00:11,  1.42it/s]Extractor Estimating: 18it [00:11,  1.46it/s]Extractor Estimating: 19it [00:12,  1.54it/s]Extractor Estimating: 20it [00:13,  1.59it/s]Extractor Estimating: 21it [00:13,  1.57it/s]Extractor Estimating: 22it [00:14,  1.55it/s]Extractor Estimating: 23it [00:15,  1.55it/s]Extractor Estimating: 24it [00:15,  1.60it/s]Extractor Estimating: 25it [00:16,  1.57it/s]Extractor Estimating: 26it [00:16,  1.57it/s]Extractor Estimating: 27it [00:17,  1.58it/s]Extractor Estimating: 28it [00:18,  1.58it/s]Extractor Estimating: 29it [00:18,  1.61it/s]Extractor Estimating: 30it [00:19,  1.61it/s]Extractor Estimating: 31it [00:20,  1.55it/s]Extractor Estimating: 32it [00:20,  1.52it/s]Extractor Estimating: 33it [00:21,  1.46it/s]Extractor Estimating: 34it [00:22,  1.51it/s]Extractor Estimating: 35it [00:22,  1.53it/s]Extractor Estimating: 36it [00:23,  1.54it/s]Extractor Estimating: 37it [00:24,  1.52it/s]Extractor Estimating: 38it [00:24,  1.58it/s]Extractor Estimating: 39it [00:25,  1.59it/s]Extractor Estimating: 40it [00:25,  1.59it/s]Extractor Estimating: 41it [00:26,  1.59it/s]Extractor Estimating: 42it [00:27,  1.53it/s]Extractor Estimating: 43it [00:27,  1.61it/s]Extractor Estimating: 44it [00:28,  1.61it/s]Extractor Estimating: 45it [00:29,  1.56it/s]Extractor Estimating: 46it [00:29,  1.54it/s]Extractor Estimating: 47it [00:30,  1.52it/s]Extractor Estimating: 48it [00:31,  1.53it/s]Extractor Estimating: 49it [00:31,  1.54it/s]Extractor Estimating: 50it [00:32,  1.56it/s]Extractor Estimating: 51it [00:33,  1.55it/s]Extractor Estimating: 52it [00:33,  1.50it/s]Extractor Estimating: 53it [00:34,  1.55it/s]Extractor Estimating: 54it [00:35,  1.49it/s]Extractor Estimating: 55it [00:35,  1.52it/s]Extractor Estimating: 56it [00:36,  1.53it/s]Extractor Estimating: 57it [00:36,  1.54it/s]Extractor Estimating: 58it [00:37,  1.48it/s]Extractor Estimating: 59it [00:38,  1.42it/s]Extractor Estimating: 60it [00:39,  1.48it/s]Extractor Estimating: 61it [00:39,  1.48it/s]Extractor Estimating: 62it [00:40,  1.47it/s]Extractor Estimating: 63it [00:41,  1.54it/s]Extractor Estimating: 64it [00:41,  1.56it/s]Extractor Estimating: 65it [00:42,  1.51it/s]Extractor Estimating: 66it [00:43,  1.49it/s]Extractor Estimating: 67it [00:43,  1.49it/s]Extractor Estimating: 68it [00:44,  1.49it/s]Extractor Estimating: 69it [00:45,  1.54it/s]Extractor Estimating: 70it [00:45,  1.57it/s]Extractor Estimating: 71it [00:46,  1.57it/s]Extractor Estimating: 72it [00:46,  1.58it/s]Extractor Estimating: 73it [00:47,  1.54it/s]Extractor Estimating: 74it [00:48,  1.55it/s]Extractor Estimating: 75it [00:48,  1.50it/s]Extractor Estimating: 76it [00:49,  1.54it/s]Extractor Estimating: 77it [00:50,  1.45it/s]Extractor Estimating: 78it [00:50,  1.49it/s]Extractor Estimating: 79it [00:51,  1.53it/s]Extractor Estimating: 80it [00:52,  1.54it/s]Extractor Estimating: 81it [00:52,  1.55it/s]Extractor Estimating: 82it [00:53,  1.53it/s]Extractor Estimating: 83it [00:54,  1.56it/s]Extractor Estimating: 84it [00:54,  1.61it/s]Extractor Estimating: 85it [00:55,  1.60it/s]Extractor Estimating: 86it [00:56,  1.54it/s]Extractor Estimating: 87it [00:56,  1.57it/s]Extractor Estimating: 88it [00:57,  1.59it/s]Extractor Estimating: 89it [00:57,  1.55it/s]Extractor Estimating: 90it [00:58,  1.43it/s]Extractor Estimating: 91it [00:59,  1.49it/s]Extractor Estimating: 92it [01:00,  1.44it/s]Extractor Estimating: 93it [01:00,  1.46it/s]Extractor Estimating: 94it [01:01,  1.52it/s]Extractor Estimating: 95it [01:02,  1.49it/s]Extractor Estimating: 96it [01:02,  1.45it/s]Extractor Estimating: 97it [01:03,  1.52it/s]Extractor Estimating: 98it [01:04,  1.53it/s]Extractor Estimating: 99it [01:04,  1.52it/s]Extractor Estimating: 100it [01:05,  1.59it/s]Extractor Estimating: 101it [01:05,  1.54it/s]Extractor Estimating: 102it [01:06,  1.54it/s]Extractor Estimating: 103it [01:07,  1.52it/s]Extractor Estimating: 104it [01:07,  1.53it/s]Extractor Estimating: 105it [01:08,  1.53it/s]Extractor Estimating: 106it [01:09,  1.49it/s]Extractor Estimating: 107it [01:09,  1.54it/s]Extractor Estimating: 108it [01:10,  1.49it/s]Extractor Estimating: 109it [01:11,  1.53it/s]Extractor Estimating: 110it [01:11,  1.54it/s]Extractor Estimating: 111it [01:12,  1.56it/s]Extractor Estimating: 112it [01:13,  1.57it/s]Extractor Estimating: 113it [01:13,  1.53it/s]Extractor Estimating: 114it [01:14,  1.52it/s]Extractor Estimating: 115it [01:15,  1.56it/s]Extractor Estimating: 116it [01:15,  1.54it/s]Extractor Estimating: 117it [01:16,  1.56it/s]Extractor Estimating: 118it [01:17,  1.56it/s]Extractor Estimating: 119it [01:17,  1.53it/s]Extractor Estimating: 120it [01:18,  1.54it/s]Extractor Estimating: 121it [01:18,  1.53it/s]Extractor Estimating: 122it [01:19,  1.57it/s]Extractor Estimating: 123it [01:20,  1.58it/s]Extractor Estimating: 124it [01:20,  1.61it/s]Extractor Estimating: 125it [01:21,  1.64it/s]Extractor Estimating: 126it [01:21,  1.64it/s]Extractor Estimating: 127it [01:22,  1.65it/s]Extractor Estimating: 128it [01:23,  1.62it/s]Extractor Estimating: 129it [01:23,  1.60it/s]Extractor Estimating: 130it [01:24,  1.59it/s]Extractor Estimating: 131it [01:25,  1.64it/s]Extractor Estimating: 132it [01:25,  1.62it/s]Extractor Estimating: 133it [01:26,  1.61it/s]Extractor Estimating: 134it [01:26,  1.62it/s]Extractor Estimating: 135it [01:27,  1.62it/s]Extractor Estimating: 136it [01:28,  1.61it/s]Extractor Estimating: 137it [01:28,  1.61it/s]Extractor Estimating: 138it [01:29,  1.62it/s]Extractor Estimating: 139it [01:30,  1.63it/s]Extractor Estimating: 140it [01:30,  1.62it/s]Extractor Estimating: 141it [01:31,  1.65it/s]Extractor Estimating: 142it [01:31,  1.66it/s]Extractor Estimating: 143it [01:32,  1.67it/s]Extractor Estimating: 144it [01:33,  1.63it/s]Extractor Estimating: 145it [01:33,  1.68it/s]Extractor Estimating: 146it [01:34,  1.65it/s]Extractor Estimating: 147it [01:34,  1.64it/s]Extractor Estimating: 148it [01:35,  1.66it/s]Extractor Estimating: 149it [01:36,  1.65it/s]Extractor Estimating: 150it [01:36,  1.61it/s]Extractor Estimating: 151it [01:37,  1.58it/s]Extractor Estimating: 152it [01:38,  1.49it/s]Extractor Estimating: 153it [01:38,  1.53it/s]Extractor Estimating: 154it [01:39,  1.55it/s]Extractor Estimating: 155it [01:39,  1.58it/s]Extractor Estimating: 156it [01:40,  1.55it/s]Extractor Estimating: 157it [01:41,  1.59it/s]Extractor Estimating: 158it [01:42,  1.48it/s]Extractor Estimating: 159it [01:42,  1.47it/s]Extractor Estimating: 160it [01:43,  1.48it/s]Extractor Estimating: 161it [01:44,  1.49it/s]Extractor Estimating: 162it [01:44,  1.47it/s]Extractor Estimating: 163it [01:45,  1.43it/s]Extractor Estimating: 164it [01:46,  1.48it/s]Extractor Estimating: 165it [01:46,  1.52it/s]Extractor Estimating: 166it [01:47,  1.59it/s]Extractor Estimating: 167it [01:47,  1.55it/s]Extractor Estimating: 168it [01:48,  1.53it/s]Extractor Estimating: 169it [01:49,  1.51it/s]Extractor Estimating: 170it [01:50,  1.48it/s]Extractor Estimating: 171it [01:50,  1.54it/s]Extractor Estimating: 172it [01:51,  1.48it/s]Extractor Estimating: 173it [01:52,  1.48it/s]Extractor Estimating: 174it [01:52,  1.48it/s]Extractor Estimating: 175it [01:53,  1.46it/s]Extractor Estimating: 176it [01:54,  1.52it/s]Extractor Estimating: 177it [01:54,  1.55it/s]Extractor Estimating: 178it [01:55,  1.56it/s]Extractor Estimating: 179it [01:55,  1.56it/s]Extractor Estimating: 180it [01:56,  1.56it/s]Extractor Estimating: 181it [01:57,  1.57it/s]Extractor Estimating: 182it [01:57,  1.57it/s]Extractor Estimating: 183it [01:58,  1.54it/s]Extractor Estimating: 184it [01:59,  1.51it/s]Extractor Estimating: 185it [01:59,  1.44it/s]Extractor Estimating: 186it [02:00,  1.47it/s]Extractor Estimating: 187it [02:01,  1.45it/s]Extractor Estimating: 188it [02:01,  1.50it/s]Extractor Estimating: 189it [02:02,  1.52it/s]Extractor Estimating: 190it [02:03,  1.53it/s]Extractor Estimating: 191it [02:03,  1.53it/s]Extractor Estimating: 192it [02:04,  1.51it/s]Extractor Estimating: 193it [02:05,  1.52it/s]Extractor Estimating: 194it [02:05,  1.48it/s]Extractor Estimating: 195it [02:06,  1.55it/s]Extractor Estimating: 196it [02:07,  1.54it/s]Extractor Estimating: 197it [02:07,  1.56it/s]Extractor Estimating: 198it [02:08,  1.57it/s]Extractor Estimating: 199it [02:09,  1.50it/s]Extractor Estimating: 200it [02:09,  1.48it/s]Extractor Estimating: 201it [02:10,  1.55it/s]Extractor Estimating: 202it [02:10,  1.62it/s]Extractor Estimating: 203it [02:11,  1.69it/s]Extractor Estimating: 204it [02:12,  1.71it/s]Extractor Estimating: 205it [02:12,  1.61it/s]Extractor Estimating: 206it [02:13,  1.70it/s]Extractor Estimating: 207it [02:13,  1.63it/s]Extractor Estimating: 208it [02:14,  1.65it/s]Extractor Estimating: 209it [02:15,  1.67it/s]Extractor Estimating: 210it [02:15,  1.63it/s]Extractor Estimating: 211it [02:16,  1.62it/s]Extractor Estimating: 212it [02:16,  1.66it/s]Extractor Estimating: 213it [02:17,  1.73it/s]Extractor Estimating: 214it [02:18,  1.72it/s]Extractor Estimating: 215it [02:18,  1.73it/s]Extractor Estimating: 216it [02:19,  1.77it/s]Extractor Estimating: 217it [02:19,  1.78it/s]Extractor Estimating: 218it [02:20,  1.74it/s]Extractor Estimating: 219it [02:20,  1.78it/s]Extractor Estimating: 220it [02:21,  1.80it/s]Extractor Estimating: 221it [02:21,  1.82it/s]Extractor Estimating: 222it [02:22,  1.81it/s]Extractor Estimating: 223it [02:23,  1.76it/s]Extractor Estimating: 224it [02:23,  1.81it/s]Extractor Estimating: 225it [02:24,  1.76it/s]Extractor Estimating: 226it [02:24,  1.76it/s]Extractor Estimating: 227it [02:25,  1.75it/s]Extractor Estimating: 228it [02:25,  1.72it/s]Extractor Estimating: 229it [02:26,  1.74it/s]Extractor Estimating: 230it [02:27,  1.71it/s]Extractor Estimating: 231it [02:27,  1.73it/s]Extractor Estimating: 232it [02:28,  1.71it/s]Extractor Estimating: 233it [02:28,  1.73it/s]Extractor Estimating: 234it [02:29,  1.73it/s]Extractor Estimating: 235it [02:30,  1.69it/s]Extractor Estimating: 236it [02:30,  1.72it/s]Extractor Estimating: 237it [02:31,  1.69it/s]Extractor Estimating: 238it [02:31,  1.56it/s]Extractor Estimating: 239it [02:32,  1.57it/s]Extractor Estimating: 240it [02:33,  1.65it/s]Extractor Estimating: 241it [02:33,  1.66it/s]Extractor Estimating: 242it [02:34,  1.70it/s]Extractor Estimating: 243it [02:34,  1.67it/s]Extractor Estimating: 244it [02:35,  1.66it/s]Extractor Estimating: 245it [02:36,  1.75it/s]Extractor Estimating: 246it [02:36,  1.82it/s]Extractor Estimating: 247it [02:37,  1.78it/s]Extractor Estimating: 248it [02:37,  1.81it/s]Extractor Estimating: 249it [02:38,  1.80it/s]Extractor Estimating: 250it [02:38,  1.82it/s]Extractor Estimating: 251it [02:39,  1.74it/s]Extractor Estimating: 252it [02:39,  1.73it/s]Extractor Estimating: 253it [02:40,  1.67it/s]Extractor Estimating: 254it [02:41,  1.62it/s]Extractor Estimating: 255it [02:41,  1.63it/s]Extractor Estimating: 256it [02:42,  1.58it/s]Extractor Estimating: 257it [02:43,  1.56it/s]Extractor Estimating: 258it [02:43,  1.63it/s]Extractor Estimating: 259it [02:44,  1.61it/s]Extractor Estimating: 260it [02:45,  1.62it/s]Extractor Estimating: 261it [02:45,  1.63it/s]Extractor Estimating: 262it [02:46,  1.63it/s]Extractor Estimating: 263it [02:46,  1.64it/s]Extractor Estimating: 264it [02:47,  1.60it/s]Extractor Estimating: 265it [02:48,  1.64it/s]Extractor Estimating: 266it [02:48,  1.63it/s]Extractor Estimating: 267it [02:49,  1.61it/s]Extractor Estimating: 268it [02:49,  1.63it/s]Extractor Estimating: 269it [02:50,  1.58it/s]Extractor Estimating: 270it [02:51,  1.57it/s]Extractor Estimating: 271it [02:51,  1.58it/s]Extractor Estimating: 272it [02:52,  1.55it/s]Extractor Estimating: 273it [02:53,  1.62it/s]Extractor Estimating: 274it [02:53,  1.61it/s]Extractor Estimating: 275it [02:54,  1.51it/s]Extractor Estimating: 276it [02:55,  1.55it/s]Extractor Estimating: 277it [02:55,  1.49it/s]Extractor Estimating: 278it [02:56,  1.52it/s]Extractor Estimating: 279it [02:57,  1.52it/s]Extractor Estimating: 280it [02:57,  1.61it/s]Extractor Estimating: 281it [02:58,  1.60it/s]Extractor Estimating: 282it [02:58,  1.61it/s]Extractor Estimating: 283it [02:59,  1.53it/s]Extractor Estimating: 284it [03:00,  1.50it/s]Extractor Estimating: 285it [03:01,  1.47it/s]Extractor Estimating: 286it [03:01,  1.50it/s]Extractor Estimating: 287it [03:02,  1.52it/s]Extractor Estimating: 288it [03:03,  1.48it/s]Extractor Estimating: 289it [03:03,  1.48it/s]Extractor Estimating: 290it [03:04,  1.52it/s]Extractor Estimating: 291it [03:05,  1.50it/s]Extractor Estimating: 292it [03:05,  1.56it/s]Extractor Estimating: 293it [03:06,  1.53it/s]Extractor Estimating: 294it [03:07,  1.47it/s]Extractor Estimating: 295it [03:07,  1.47it/s]Extractor Estimating: 296it [03:08,  1.46it/s]Extractor Estimating: 297it [03:09,  1.49it/s]Extractor Estimating: 298it [03:09,  1.50it/s]Extractor Estimating: 299it [03:10,  1.53it/s]Extractor Estimating: 300it [03:10,  1.53it/s]Extractor Estimating: 301it [03:11,  1.57it/s]Extractor Estimating: 302it [03:12,  1.52it/s]Extractor Estimating: 303it [03:12,  1.52it/s]Extractor Estimating: 304it [03:13,  1.51it/s]Extractor Estimating: 305it [03:14,  1.51it/s]Extractor Estimating: 306it [03:14,  1.52it/s]Extractor Estimating: 307it [03:15,  1.51it/s]Extractor Estimating: 308it [03:16,  1.53it/s]Extractor Estimating: 309it [03:16,  1.57it/s]Extractor Estimating: 310it [03:17,  1.54it/s]Extractor Estimating: 311it [03:18,  1.53it/s]Extractor Estimating: 312it [03:18,  1.58it/s]Extractor Estimating: 313it [03:19,  1.59it/s]Extractor Estimating: 314it [03:19,  1.60it/s]Extractor Estimating: 315it [03:20,  1.57it/s]Extractor Estimating: 316it [03:21,  1.59it/s]Extractor Estimating: 317it [03:21,  1.61it/s]Extractor Estimating: 318it [03:22,  1.59it/s]Extractor Estimating: 319it [03:23,  1.59it/s]Extractor Estimating: 320it [03:23,  1.55it/s]Extractor Estimating: 321it [03:24,  1.55it/s]Extractor Estimating: 322it [03:25,  1.57it/s]Extractor Estimating: 323it [03:25,  1.52it/s]Extractor Estimating: 324it [03:26,  1.57it/s]Extractor Estimating: 325it [03:27,  1.54it/s]Extractor Estimating: 326it [03:27,  1.61it/s]Extractor Estimating: 327it [03:28,  1.69it/s]Extractor Estimating: 328it [03:28,  1.71it/s]Extractor Estimating: 329it [03:29,  1.69it/s]Extractor Estimating: 330it [03:30,  1.59it/s]Extractor Estimating: 331it [03:30,  1.69it/s]Extractor Estimating: 332it [03:31,  1.77it/s]Extractor Estimating: 333it [03:31,  1.82it/s]Extractor Estimating: 334it [03:32,  1.85it/s]Extractor Estimating: 335it [03:32,  1.88it/s]Extractor Estimating: 336it [03:33,  1.92it/s]Extractor Estimating: 337it [03:33,  1.87it/s]Extractor Estimating: 338it [03:34,  1.87it/s]Extractor Estimating: 339it [03:34,  1.79it/s]Extractor Estimating: 340it [03:35,  1.75it/s]Extractor Estimating: 341it [03:35,  1.80it/s]Extractor Estimating: 342it [03:36,  1.81it/s]Extractor Estimating: 343it [03:36,  1.85it/s]Extractor Estimating: 344it [03:37,  1.79it/s]Extractor Estimating: 345it [03:38,  1.79it/s]Extractor Estimating: 346it [03:38,  1.82it/s]Extractor Estimating: 347it [03:39,  1.81it/s]Extractor Estimating: 348it [03:39,  1.84it/s]Extractor Estimating: 349it [03:40,  1.86it/s]Extractor Estimating: 350it [03:40,  1.83it/s]Extractor Estimating: 351it [03:41,  1.71it/s]Extractor Estimating: 352it [03:42,  1.66it/s]Extractor Estimating: 353it [03:42,  1.62it/s]Extractor Estimating: 354it [03:43,  1.61it/s]Extractor Estimating: 355it [03:44,  1.60it/s]Extractor Estimating: 356it [03:44,  1.66it/s]Extractor Estimating: 357it [03:45,  1.61it/s]Extractor Estimating: 358it [03:45,  1.62it/s]Extractor Estimating: 359it [03:46,  1.58it/s]Extractor Estimating: 360it [03:47,  1.58it/s]Extractor Estimating: 361it [03:47,  1.61it/s]Extractor Estimating: 362it [03:48,  1.58it/s]Extractor Estimating: 363it [03:49,  1.61it/s]Extractor Estimating: 364it [03:49,  1.53it/s]Extractor Estimating: 365it [03:50,  1.53it/s]Extractor Estimating: 366it [03:51,  1.57it/s]Extractor Estimating: 367it [03:51,  1.60it/s]Extractor Estimating: 368it [03:52,  1.60it/s]Extractor Estimating: 369it [03:52,  1.58it/s]Extractor Estimating: 370it [03:53,  1.58it/s]Extractor Estimating: 371it [03:54,  1.54it/s]Extractor Estimating: 372it [03:54,  1.55it/s]Extractor Estimating: 373it [03:55,  1.55it/s]Extractor Estimating: 374it [03:56,  1.57it/s]Extractor Estimating: 375it [03:56,  1.63it/s]Extractor Estimating: 375it [03:56,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:12,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:12,409 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:12,409 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:12,409 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:12,409 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:31:12,714 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:31:12,716 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:31:13,422 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:31:14,481 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:31:14,481 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:16,591 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:16,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:16,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:16,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:16,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:31:16,914 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:31:16,915 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:31:17,181 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:31:17,338 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:31:17,338 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 05:56:02,084 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 05:56:02,109 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7491 mean pseudo reward: 0.9460050620143183
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 20012
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20112, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20112, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.092, loss:540.0094
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.087, loss:477.1166
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.094, loss:487.9965
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.108, loss:465.0669
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.087, loss:474.6035
>> valid entity prec:0.4616, rec:0.4165, f1:0.4379
>> valid relation prec:0.1109, rec:0.0115, f1:0.0209
>> valid relation with NER prec:0.1109, rec:0.0115, f1:0.0209
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.646, loss:474.6609
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.084, loss:457.3342
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.117, loss:457.4549
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.083, loss:498.3343
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.089, loss:428.3880
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4633, rec:0.4718, f1:0.4675
>> valid relation prec:0.1484, rec:0.0221, f1:0.0385
>> valid relation with NER prec:0.1484, rec:0.0221, f1:0.0385
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.648, loss:468.8758
g_step 1200, step 261, avg_time 1.094, loss:487.6814
g_step 1300, step 48, avg_time 1.083, loss:465.6724
g_step 1400, step 148, avg_time 1.091, loss:437.1396
g_step 1500, step 248, avg_time 1.098, loss:466.1404
>> valid entity prec:0.4445, rec:0.4378, f1:0.4411
>> valid relation prec:0.0894, rec:0.0090, f1:0.0163
>> valid relation with NER prec:0.0894, rec:0.0090, f1:0.0163
g_step 1600, step 35, avg_time 2.624, loss:444.6973
g_step 1700, step 135, avg_time 1.095, loss:423.0152
g_step 1800, step 235, avg_time 1.094, loss:433.2549
g_step 1900, step 22, avg_time 1.084, loss:433.0140
g_step 2000, step 122, avg_time 1.082, loss:413.8002
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4629, rec:0.4206, f1:0.4407
>> valid relation prec:0.1438, rec:0.0198, f1:0.0349
>> valid relation with NER prec:0.1438, rec:0.0198, f1:0.0349
g_step 2100, step 222, avg_time 2.646, loss:416.5506
g_step 2200, step 9, avg_time 1.092, loss:419.9091
g_step 2300, step 109, avg_time 1.094, loss:378.3485
g_step 2400, step 209, avg_time 1.106, loss:394.1692
g_step 2500, step 309, avg_time 1.084, loss:412.8052
>> valid entity prec:0.4919, rec:0.3130, f1:0.3825
>> valid relation prec:0.1075, rec:0.0113, f1:0.0204
>> valid relation with NER prec:0.1075, rec:0.0113, f1:0.0204
g_step 2600, step 96, avg_time 2.597, loss:370.7260
g_step 2700, step 196, avg_time 1.100, loss:383.0016
g_step 2800, step 296, avg_time 1.103, loss:390.9466
g_step 2900, step 83, avg_time 1.081, loss:352.5778
g_step 3000, step 183, avg_time 1.107, loss:362.5642
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4822, rec:0.4242, f1:0.4514
>> valid relation prec:0.0707, rec:0.0078, f1:0.0141
>> valid relation with NER prec:0.0707, rec:0.0078, f1:0.0141
g_step 3100, step 283, avg_time 2.627, loss:382.8993
g_step 3200, step 70, avg_time 1.087, loss:360.9143
g_step 3300, step 170, avg_time 1.099, loss:346.0400
g_step 3400, step 270, avg_time 1.092, loss:365.6060
g_step 3500, step 57, avg_time 1.092, loss:336.1620
>> valid entity prec:0.4568, rec:0.5059, f1:0.4801
>> valid relation prec:0.0521, rec:0.0092, f1:0.0157
>> valid relation with NER prec:0.0521, rec:0.0092, f1:0.0157
new max entity f1 on valid!
g_step 3600, step 157, avg_time 2.639, loss:331.4227
g_step 3700, step 257, avg_time 1.091, loss:341.1379
g_step 3800, step 44, avg_time 1.084, loss:336.0520
g_step 3900, step 144, avg_time 1.084, loss:314.5424
g_step 4000, step 244, avg_time 1.102, loss:348.4499
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4817, rec:0.4130, f1:0.4447
>> valid relation prec:0.1458, rec:0.0161, f1:0.0291
>> valid relation with NER prec:0.1458, rec:0.0161, f1:0.0291
g_step 4100, step 31, avg_time 2.614, loss:327.5750
g_step 4200, step 131, avg_time 1.101, loss:305.6548
g_step 4300, step 231, avg_time 1.090, loss:311.5137
g_step 4400, step 18, avg_time 1.074, loss:322.6247
g_step 4500, step 118, avg_time 1.107, loss:287.7807
>> valid entity prec:0.4793, rec:0.4266, f1:0.4514
>> valid relation prec:0.0972, rec:0.0159, f1:0.0273
>> valid relation with NER prec:0.0972, rec:0.0159, f1:0.0273
g_step 4600, step 218, avg_time 2.630, loss:305.2663
g_step 4700, step 5, avg_time 1.074, loss:309.6082
g_step 4800, step 105, avg_time 1.098, loss:267.6955
g_step 4900, step 205, avg_time 1.089, loss:298.2490
g_step 5000, step 305, avg_time 1.100, loss:312.7063
learning rate was adjusted to 0.0008
>> valid entity prec:0.4467, rec:0.3536, f1:0.3948
>> valid relation prec:0.0306, rec:0.0051, f1:0.0087
>> valid relation with NER prec:0.0306, rec:0.0051, f1:0.0087
g_step 5100, step 92, avg_time 2.624, loss:278.3510
g_step 5200, step 192, avg_time 1.086, loss:287.7905
g_step 5300, step 292, avg_time 1.097, loss:293.0714
g_step 5400, step 79, avg_time 1.092, loss:265.8958
g_step 5500, step 179, avg_time 1.102, loss:271.0686
>> valid entity prec:0.4880, rec:0.3627, f1:0.4161
>> valid relation prec:0.0822, rec:0.0157, f1:0.0263
>> valid relation with NER prec:0.0822, rec:0.0157, f1:0.0263
g_step 5600, step 279, avg_time 2.607, loss:288.4858
g_step 5700, step 66, avg_time 1.089, loss:266.0521
g_step 5800, step 166, avg_time 1.087, loss:269.3358
g_step 5900, step 266, avg_time 1.104, loss:260.3004
g_step 6000, step 53, avg_time 1.097, loss:256.4838
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4598, rec:0.3574, f1:0.4022
>> valid relation prec:0.0614, rec:0.0095, f1:0.0164
>> valid relation with NER prec:0.0614, rec:0.0095, f1:0.0164
g_step 6100, step 153, avg_time 2.629, loss:252.8746
g_step 6200, step 253, avg_time 1.090, loss:267.0919
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 05:56:02 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 05:56:02 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_05-56-02_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 05:56:03 - WARNING - datasets.builder -   Using custom data configuration default-c06a8caf88d94191
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c06a8caf88d94191/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 05:56:03,331 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:56:03,332 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:56:03,332 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:56:03,333 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:56:03,342 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:56:03,347 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:56:03,347 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:56:03,348 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:56:03,348 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:56:03,348 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:56:03,348 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 05:56:03,503 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:56:06,648 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 05:56:06,651 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c06a8caf88d94191/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.13ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.02ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.41ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.63ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.74ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.80ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.85ba/s]100%|██████████| 8/8 [00:01<00:00,  5.77ba/s]100%|██████████| 8/8 [00:01<00:00,  4.91ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.07ba/s] 40%|████      | 2/5 [00:00<00:00,  4.33ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.44ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.48ba/s]100%|██████████| 5/5 [00:00<00:00,  5.09ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.36ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.84ba/s] 50%|█████     | 4/8 [00:00<00:00,  9.87ba/s] 75%|███████▌  | 6/8 [00:00<00:00, 10.17ba/s]100%|██████████| 8/8 [00:00<00:00, 11.39ba/s]100%|██████████| 8/8 [00:00<00:00, 10.70ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  7.79ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.46ba/s]100%|██████████| 5/5 [00:00<00:00, 11.87ba/s]100%|██████████| 5/5 [00:00<00:00, 11.06ba/s]
[INFO|trainer.py:414] 2023-08-29 05:56:10,857 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 05:56:10,870 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 05:56:10,870 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-29 05:56:10,870 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 05:56:10,870 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 05:56:10,870 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 05:56:10,870 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 05:56:10,870 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.34it/s]  0%|          | 2/585 [00:00<02:50,  3.42it/s]  1%|          | 3/585 [00:00<02:48,  3.45it/s]  1%|          | 4/585 [00:01<02:47,  3.47it/s]  1%|          | 5/585 [00:01<02:46,  3.47it/s]  1%|          | 6/585 [00:01<02:46,  3.47it/s]  1%|          | 7/585 [00:02<02:46,  3.48it/s]  1%|▏         | 8/585 [00:02<02:46,  3.47it/s]  2%|▏         | 9/585 [00:02<02:45,  3.47it/s]  2%|▏         | 10/585 [00:02<02:45,  3.48it/s]  2%|▏         | 11/585 [00:03<02:44,  3.48it/s]  2%|▏         | 12/585 [00:03<02:44,  3.48it/s]  2%|▏         | 13/585 [00:03<02:44,  3.48it/s]  2%|▏         | 14/585 [00:04<02:44,  3.48it/s]  3%|▎         | 15/585 [00:04<02:43,  3.48it/s]  3%|▎         | 16/585 [00:04<02:43,  3.48it/s]  3%|▎         | 17/585 [00:04<02:43,  3.48it/s]  3%|▎         | 18/585 [00:05<02:42,  3.49it/s]  3%|▎         | 19/585 [00:05<02:42,  3.48it/s]  3%|▎         | 20/585 [00:05<02:42,  3.48it/s]  4%|▎         | 21/585 [00:06<02:41,  3.48it/s]  4%|▍         | 22/585 [00:06<02:41,  3.48it/s]  4%|▍         | 23/585 [00:06<02:41,  3.48it/s]  4%|▍         | 24/585 [00:06<02:40,  3.49it/s]  4%|▍         | 25/585 [00:07<02:40,  3.49it/s]  4%|▍         | 26/585 [00:07<02:40,  3.49it/s]  5%|▍         | 27/585 [00:07<02:40,  3.49it/s]  5%|▍         | 28/585 [00:08<02:39,  3.48it/s]  5%|▍         | 29/585 [00:08<02:39,  3.48it/s]  5%|▌         | 30/585 [00:08<02:39,  3.48it/s]  5%|▌         | 31/585 [00:08<02:39,  3.48it/s]  5%|▌         | 32/585 [00:09<02:38,  3.48it/s]  6%|▌         | 33/585 [00:09<02:38,  3.48it/s]  6%|▌         | 34/585 [00:09<02:38,  3.48it/s]  6%|▌         | 35/585 [00:10<02:37,  3.48it/s]  6%|▌         | 36/585 [00:10<02:37,  3.48it/s]  6%|▋         | 37/585 [00:10<02:37,  3.48it/s]  6%|▋         | 38/585 [00:10<02:36,  3.48it/s]  7%|▋         | 39/585 [00:11<02:36,  3.48it/s]  7%|▋         | 40/585 [00:11<02:36,  3.48it/s]  7%|▋         | 41/585 [00:11<02:36,  3.48it/s]  7%|▋         | 42/585 [00:12<02:36,  3.48it/s]  7%|▋         | 43/585 [00:12<02:35,  3.48it/s]  8%|▊         | 44/585 [00:12<02:35,  3.48it/s]  8%|▊         | 45/585 [00:12<02:35,  3.48it/s]  8%|▊         | 46/585 [00:13<02:34,  3.48it/s]  8%|▊         | 47/585 [00:13<02:34,  3.48it/s]  8%|▊         | 48/585 [00:13<02:34,  3.48it/s]  8%|▊         | 49/585 [00:14<02:33,  3.48it/s]  9%|▊         | 50/585 [00:14<02:33,  3.48it/s]  9%|▊         | 51/585 [00:14<02:33,  3.48it/s]  9%|▉         | 52/585 [00:14<02:33,  3.47it/s]  9%|▉         | 53/585 [00:15<02:33,  3.48it/s]  9%|▉         | 54/585 [00:15<02:32,  3.48it/s]  9%|▉         | 55/585 [00:15<02:32,  3.48it/s] 10%|▉         | 56/585 [00:16<02:31,  3.48it/s] 10%|▉         | 57/585 [00:16<02:31,  3.48it/s] 10%|▉         | 58/585 [00:16<02:31,  3.48it/s] 10%|█         | 59/585 [00:16<02:31,  3.48it/s] 10%|█         | 60/585 [00:17<02:30,  3.48it/s] 10%|█         | 61/585 [00:17<02:30,  3.48it/s] 11%|█         | 62/585 [00:17<02:30,  3.48it/s] 11%|█         | 63/585 [00:18<02:30,  3.47it/s] 11%|█         | 64/585 [00:18<02:30,  3.47it/s] 11%|█         | 65/585 [00:18<02:29,  3.48it/s] 11%|█▏        | 66/585 [00:18<02:29,  3.47it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.47it/s] 12%|█▏        | 68/585 [00:19<02:28,  3.48it/s] 12%|█▏        | 69/585 [00:19<02:28,  3.48it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.48it/s] 12%|█▏        | 71/585 [00:20<02:27,  3.48it/s] 12%|█▏        | 72/585 [00:20<02:27,  3.48it/s] 12%|█▏        | 73/585 [00:20<02:27,  3.48it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 76/585 [00:21<02:26,  3.47it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.47it/s] 13%|█▎        | 78/585 [00:22<02:25,  3.47it/s] 14%|█▎        | 79/585 [00:22<02:25,  3.48it/s] 14%|█▎        | 80/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 82/585 [00:23<02:24,  3.48it/s] 14%|█▍        | 83/585 [00:23<02:24,  3.48it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.48it/s] 15%|█▍        | 85/585 [00:24<02:23,  3.48it/s] 15%|█▍        | 86/585 [00:24<02:23,  3.47it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.48it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 89/585 [00:25<02:22,  3.48it/s] 15%|█▌        | 90/585 [00:25<02:22,  3.48it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.48it/s] 16%|█▌        | 92/585 [00:26<02:21,  3.47it/s] 16%|█▌        | 93/585 [00:26<02:21,  3.47it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.47it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.47it/s] 16%|█▋        | 96/585 [00:27<02:20,  3.47it/s] 17%|█▋        | 97/585 [00:27<02:20,  3.47it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 99/585 [00:28<02:19,  3.47it/s] 17%|█▋        | 100/585 [00:28<02:19,  3.47it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.47it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.47it/s] 18%|█▊        | 103/585 [00:29<02:18,  3.47it/s] 18%|█▊        | 104/585 [00:29<02:18,  3.47it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.46it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.47it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.47it/s] 19%|█▉        | 111/585 [00:31<02:16,  3.47it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.47it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.47it/s] 19%|█▉        | 114/585 [00:32<02:15,  3.47it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.47it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.47it/s] 20%|██        | 117/585 [00:33<02:14,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 05:56:44,586 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:56:44,586 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 05:56:44,586 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 58.36it/s][A
  2%|▏         | 12/543 [00:00<00:10, 51.45it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.54it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.69it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.19it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.80it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.57it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.16it/s][A
  9%|▉         | 48/543 [00:00<00:10, 47.18it/s][A
 10%|▉         | 53/543 [00:01<00:10, 47.21it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.30it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.39it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.39it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.28it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.23it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.20it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 47.00it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.96it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 47.02it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 47.09it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.24it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.38it/s][A
 22%|██▏       | 118/543 [00:02<00:08, 47.33it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.32it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.20it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 47.01it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 47.02it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 47.01it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 47.03it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 47.11it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.23it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.32it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.33it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.32it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 47.10it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 47.02it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.99it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.99it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 47.03it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.09it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.17it/s][A
 39%|███▉      | 213/543 [00:04<00:06, 47.25it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.21it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.25it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 47.16it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 47.03it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 47.04it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 47.10it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 47.08it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.13it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.17it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.19it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.22it/s][A
 50%|█████     | 273/543 [00:05<00:05, 47.18it/s][A
 51%|█████     | 278/543 [00:05<00:05, 47.10it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 47.03it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 47.08it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 47.11it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.12it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.09it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.19it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.19it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.17it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 47.15it/s][A
 60%|██████    | 328/543 [00:06<00:04, 47.04it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 46.97it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 47.10it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 47.13it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 47.03it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.14it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 47.22it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.15it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 47.10it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 47.03it/s][A
 70%|██████▉   | 378/543 [00:07<00:03, 46.94it/s][A
 71%|███████   | 383/543 [00:08<00:03, 46.99it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 47.12it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 47.11it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.10it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.14it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.22it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 46.88it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 47.09it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 47.02it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 46.94it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 47.08it/s][A
 81%|████████  | 438/543 [00:09<00:02, 47.05it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 47.09it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 47.15it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.22it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 47.06it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 47.06it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 47.06it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.98it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 47.04it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 47.13it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 47.12it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.13it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 47.20it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 47.16it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 46.95it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 47.06it/s][A
 95%|█████████▌| 518/543 [00:10<00:00, 47.03it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 46.95it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 47.08it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 47.07it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 47.07it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.08it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:11<00:00, 47.08it/s][A 20%|██        | 117/585 [00:45<02:14,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:56:56,134 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 05:56:56,156 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:56:58,513 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:56:58,528 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:56:58,541 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:52<45:10,  5.80s/it] 20%|██        | 119/585 [00:52<32:13,  4.15s/it] 21%|██        | 120/585 [00:52<23:10,  2.99s/it] 21%|██        | 121/585 [00:53<16:51,  2.18s/it] 21%|██        | 122/585 [00:53<12:26,  1.61s/it] 21%|██        | 123/585 [00:53<09:21,  1.21s/it] 21%|██        | 124/585 [00:54<07:11,  1.07it/s] 21%|██▏       | 125/585 [00:54<05:41,  1.35it/s] 22%|██▏       | 126/585 [00:54<04:37,  1.65it/s] 22%|██▏       | 127/585 [00:54<03:53,  1.96it/s] 22%|██▏       | 128/585 [00:55<03:22,  2.26it/s] 22%|██▏       | 129/585 [00:55<03:00,  2.52it/s] 22%|██▏       | 130/585 [00:55<02:45,  2.74it/s] 22%|██▏       | 131/585 [00:56<02:35,  2.93it/s] 23%|██▎       | 132/585 [00:56<02:27,  3.07it/s] 23%|██▎       | 133/585 [00:56<02:21,  3.19it/s] 23%|██▎       | 134/585 [00:56<02:18,  3.27it/s] 23%|██▎       | 135/585 [00:57<02:15,  3.33it/s] 23%|██▎       | 136/585 [00:57<02:13,  3.37it/s] 23%|██▎       | 137/585 [00:57<02:11,  3.40it/s] 24%|██▎       | 138/585 [00:58<02:10,  3.42it/s] 24%|██▍       | 139/585 [00:58<02:09,  3.45it/s] 24%|██▍       | 140/585 [00:58<02:08,  3.45it/s] 24%|██▍       | 141/585 [00:58<02:09,  3.43it/s] 24%|██▍       | 142/585 [00:59<02:08,  3.45it/s] 24%|██▍       | 143/585 [00:59<02:07,  3.46it/s] 25%|██▍       | 144/585 [00:59<02:07,  3.46it/s] 25%|██▍       | 145/585 [01:00<02:06,  3.47it/s] 25%|██▍       | 146/585 [01:00<02:06,  3.47it/s] 25%|██▌       | 147/585 [01:00<02:06,  3.47it/s] 25%|██▌       | 148/585 [01:00<02:05,  3.47it/s] 25%|██▌       | 149/585 [01:01<02:05,  3.47it/s] 26%|██▌       | 150/585 [01:01<02:05,  3.47it/s] 26%|██▌       | 151/585 [01:01<02:04,  3.48it/s] 26%|██▌       | 152/585 [01:02<02:04,  3.47it/s] 26%|██▌       | 153/585 [01:02<02:04,  3.47it/s] 26%|██▋       | 154/585 [01:02<02:04,  3.47it/s] 26%|██▋       | 155/585 [01:02<02:03,  3.47it/s] 27%|██▋       | 156/585 [01:03<02:03,  3.47it/s] 27%|██▋       | 157/585 [01:03<02:03,  3.47it/s] 27%|██▋       | 158/585 [01:03<02:02,  3.47it/s] 27%|██▋       | 159/585 [01:04<02:02,  3.47it/s] 27%|██▋       | 160/585 [01:04<02:02,  3.47it/s] 28%|██▊       | 161/585 [01:04<02:02,  3.47it/s] 28%|██▊       | 162/585 [01:05<02:01,  3.47it/s] 28%|██▊       | 163/585 [01:05<02:02,  3.45it/s] 28%|██▊       | 164/585 [01:05<02:01,  3.46it/s] 28%|██▊       | 165/585 [01:05<02:01,  3.46it/s] 28%|██▊       | 166/585 [01:06<02:00,  3.47it/s] 29%|██▊       | 167/585 [01:06<02:00,  3.47it/s] 29%|██▊       | 168/585 [01:06<02:00,  3.46it/s] 29%|██▉       | 169/585 [01:07<02:00,  3.47it/s] 29%|██▉       | 170/585 [01:07<01:59,  3.47it/s] 29%|██▉       | 171/585 [01:07<01:59,  3.47it/s] 29%|██▉       | 172/585 [01:07<01:59,  3.47it/s] 30%|██▉       | 173/585 [01:08<01:58,  3.47it/s] 30%|██▉       | 174/585 [01:08<01:58,  3.47it/s] 30%|██▉       | 175/585 [01:08<01:58,  3.47it/s] 30%|███       | 176/585 [01:09<01:57,  3.47it/s] 30%|███       | 177/585 [01:09<01:57,  3.47it/s] 30%|███       | 178/585 [01:09<01:57,  3.47it/s] 31%|███       | 179/585 [01:09<01:57,  3.47it/s] 31%|███       | 180/585 [01:10<01:56,  3.47it/s] 31%|███       | 181/585 [01:10<01:56,  3.47it/s] 31%|███       | 182/585 [01:10<01:56,  3.47it/s] 31%|███▏      | 183/585 [01:11<01:55,  3.47it/s] 31%|███▏      | 184/585 [01:11<01:55,  3.47it/s] 32%|███▏      | 185/585 [01:11<01:55,  3.47it/s] 32%|███▏      | 186/585 [01:11<01:54,  3.47it/s] 32%|███▏      | 187/585 [01:12<01:54,  3.47it/s] 32%|███▏      | 188/585 [01:12<01:54,  3.47it/s] 32%|███▏      | 189/585 [01:12<01:54,  3.47it/s] 32%|███▏      | 190/585 [01:13<01:53,  3.47it/s] 33%|███▎      | 191/585 [01:13<01:53,  3.47it/s] 33%|███▎      | 192/585 [01:13<01:53,  3.47it/s] 33%|███▎      | 193/585 [01:13<01:52,  3.47it/s] 33%|███▎      | 194/585 [01:14<01:52,  3.47it/s] 33%|███▎      | 195/585 [01:14<01:52,  3.46it/s] 34%|███▎      | 196/585 [01:14<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:15<01:51,  3.47it/s] 34%|███▍      | 198/585 [01:15<01:51,  3.47it/s] 34%|███▍      | 199/585 [01:15<01:51,  3.46it/s] 34%|███▍      | 200/585 [01:15<01:51,  3.47it/s] 34%|███▍      | 201/585 [01:16<01:50,  3.47it/s] 35%|███▍      | 202/585 [01:16<01:50,  3.47it/s] 35%|███▍      | 203/585 [01:16<01:50,  3.47it/s] 35%|███▍      | 204/585 [01:17<01:49,  3.47it/s] 35%|███▌      | 205/585 [01:17<01:49,  3.47it/s] 35%|███▌      | 206/585 [01:17<01:50,  3.44it/s] 35%|███▌      | 207/585 [01:17<01:49,  3.45it/s] 36%|███▌      | 208/585 [01:18<01:49,  3.46it/s] 36%|███▌      | 209/585 [01:18<01:48,  3.46it/s] 36%|███▌      | 210/585 [01:18<01:48,  3.46it/s] 36%|███▌      | 211/585 [01:19<01:47,  3.46it/s] 36%|███▌      | 212/585 [01:19<01:47,  3.47it/s] 36%|███▋      | 213/585 [01:19<01:47,  3.47it/s] 37%|███▋      | 214/585 [01:20<01:47,  3.47it/s] 37%|███▋      | 215/585 [01:20<01:46,  3.47it/s] 37%|███▋      | 216/585 [01:20<01:46,  3.46it/s] 37%|███▋      | 217/585 [01:20<01:46,  3.46it/s] 37%|███▋      | 218/585 [01:21<01:46,  3.46it/s] 37%|███▋      | 219/585 [01:21<01:45,  3.46it/s] 38%|███▊      | 220/585 [01:21<01:45,  3.46it/s] 38%|███▊      | 221/585 [01:22<01:44,  3.47it/s] 38%|███▊      | 222/585 [01:22<01:44,  3.47it/s] 38%|███▊      | 223/585 [01:22<01:44,  3.47it/s] 38%|███▊      | 224/585 [01:22<01:44,  3.47it/s] 38%|███▊      | 225/585 [01:23<01:43,  3.47it/s] 39%|███▊      | 226/585 [01:23<01:43,  3.47it/s] 39%|███▉      | 227/585 [01:23<01:43,  3.47it/s] 39%|███▉      | 228/585 [01:24<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:24<01:43,  3.46it/s] 39%|███▉      | 230/585 [01:24<01:42,  3.46it/s] 39%|███▉      | 231/585 [01:24<01:42,  3.46it/s] 40%|███▉      | 232/585 [01:25<01:41,  3.46it/s] 40%|███▉      | 233/585 [01:25<01:41,  3.47it/s] 40%|████      | 234/585 [01:25<01:41,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 05:57:36,700 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:57:36,701 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 05:57:36,701 >>   Batch size = 8
{'eval_loss': 1.0140827894210815, 'eval_runtime': 11.5296, 'eval_samples_per_second': 376.597, 'eval_steps_per_second': 47.096, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.67it/s][A
  2%|▏         | 12/543 [00:00<00:10, 50.84it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.29it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.54it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.13it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.85it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.60it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.17it/s][A
  9%|▉         | 48/543 [00:01<00:10, 47.10it/s][A
 10%|▉         | 53/543 [00:01<00:10, 47.02it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.06it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.12it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.20it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.15it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.18it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.19it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 47.00it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 47.00it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 47.02it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 47.02it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.07it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.15it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.18it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.15it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.12it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 47.03it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.89it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.99it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.95it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 46.95it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.12it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.17it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.14it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.14it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 47.04it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.96it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.97it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.95it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 46.81it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.01it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.12it/s][A
 39%|███▉      | 213/543 [00:04<00:06, 47.17it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.12it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.13it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 46.84it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.84it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.93it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 46.92it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 47.02it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.09it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.15it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.16it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.13it/s][A
 50%|█████     | 273/543 [00:05<00:05, 47.04it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.96it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 47.00it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 47.03it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 47.04it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.09it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.16it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.22it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.15it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.11it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 46.95it/s][A
 60%|██████    | 328/543 [00:06<00:04, 46.99it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 47.02it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 47.06it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 47.08it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 47.10it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.17it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 47.15it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.09it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 47.02it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 46.98it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 47.06it/s][A
 71%|███████   | 383/543 [00:08<00:03, 46.97it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 47.05it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 47.03it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.14it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.14it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.02it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 47.00it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 46.94it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 46.96it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 47.04it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 47.08it/s][A
 81%|████████  | 438/543 [00:09<00:02, 47.01it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 47.03it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 47.13it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.09it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 47.05it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 47.01it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 46.95it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.96it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 47.10it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 47.09it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 47.11it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.14it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 47.10it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 46.96it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 47.04it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 46.96it/s][A
 95%|█████████▌| 518/543 [00:10<00:00, 46.96it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 47.04it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 47.06it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 47.09it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 47.07it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.07it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:37<01:41,  3.47it/s]
100%|██████████| 543/543 [00:11<00:00, 47.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:57:48,269 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 05:57:48,286 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:57:50,479 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:57:50,497 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:57:50,511 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:44<34:14,  5.87s/it] 40%|████      | 236/585 [01:44<24:24,  4.20s/it] 41%|████      | 237/585 [01:45<17:32,  3.02s/it] 41%|████      | 238/585 [01:45<12:44,  2.20s/it] 41%|████      | 239/585 [01:45<09:23,  1.63s/it] 41%|████      | 240/585 [01:46<07:03,  1.23s/it] 41%|████      | 241/585 [01:46<05:25,  1.06it/s] 41%|████▏     | 242/585 [01:46<04:16,  1.34it/s] 42%|████▏     | 243/585 [01:46<03:28,  1.64it/s] 42%|████▏     | 244/585 [01:47<02:54,  1.95it/s] 42%|████▏     | 245/585 [01:47<02:31,  2.25it/s] 42%|████▏     | 246/585 [01:47<02:14,  2.51it/s] 42%|████▏     | 247/585 [01:48<02:03,  2.73it/s] 42%|████▏     | 248/585 [01:48<01:55,  2.92it/s] 43%|████▎     | 249/585 [01:48<01:49,  3.07it/s] 43%|████▎     | 250/585 [01:49<01:45,  3.18it/s] 43%|████▎     | 251/585 [01:49<01:42,  3.26it/s] 43%|████▎     | 252/585 [01:49<01:40,  3.33it/s] 43%|████▎     | 253/585 [01:49<01:38,  3.37it/s] 43%|████▎     | 254/585 [01:50<01:37,  3.40it/s] 44%|████▎     | 255/585 [01:50<01:36,  3.42it/s] 44%|████▍     | 256/585 [01:50<01:35,  3.43it/s] 44%|████▍     | 257/585 [01:51<01:35,  3.45it/s] 44%|████▍     | 258/585 [01:51<01:34,  3.45it/s] 44%|████▍     | 259/585 [01:51<01:34,  3.46it/s] 44%|████▍     | 260/585 [01:51<01:33,  3.46it/s] 45%|████▍     | 261/585 [01:52<01:35,  3.38it/s] 45%|████▍     | 262/585 [01:52<01:34,  3.40it/s] 45%|████▍     | 263/585 [01:52<01:34,  3.42it/s] 45%|████▌     | 264/585 [01:53<01:33,  3.44it/s] 45%|████▌     | 265/585 [01:53<01:32,  3.45it/s] 45%|████▌     | 266/585 [01:53<01:32,  3.46it/s] 46%|████▌     | 267/585 [01:53<01:31,  3.46it/s] 46%|████▌     | 268/585 [01:54<01:31,  3.46it/s] 46%|████▌     | 269/585 [01:54<01:31,  3.46it/s] 46%|████▌     | 270/585 [01:54<01:30,  3.46it/s] 46%|████▋     | 271/585 [01:55<01:30,  3.47it/s] 46%|████▋     | 272/585 [01:55<01:30,  3.47it/s] 47%|████▋     | 273/585 [01:55<01:29,  3.47it/s] 47%|████▋     | 274/585 [01:55<01:29,  3.47it/s] 47%|████▋     | 275/585 [01:56<01:29,  3.47it/s] 47%|████▋     | 276/585 [01:56<01:28,  3.47it/s] 47%|████▋     | 277/585 [01:56<01:28,  3.47it/s] 48%|████▊     | 278/585 [01:57<01:28,  3.47it/s] 48%|████▊     | 279/585 [01:57<01:28,  3.47it/s] 48%|████▊     | 280/585 [01:57<01:28,  3.46it/s] 48%|████▊     | 281/585 [01:57<01:27,  3.47it/s] 48%|████▊     | 282/585 [01:58<01:27,  3.47it/s] 48%|████▊     | 283/585 [01:58<01:27,  3.47it/s] 49%|████▊     | 284/585 [01:58<01:26,  3.47it/s] 49%|████▊     | 285/585 [01:59<01:26,  3.47it/s] 49%|████▉     | 286/585 [01:59<01:26,  3.47it/s] 49%|████▉     | 287/585 [01:59<01:25,  3.47it/s] 49%|████▉     | 288/585 [01:59<01:25,  3.47it/s] 49%|████▉     | 289/585 [02:00<01:25,  3.47it/s] 50%|████▉     | 290/585 [02:00<01:24,  3.47it/s] 50%|████▉     | 291/585 [02:00<01:24,  3.46it/s] 50%|████▉     | 292/585 [02:01<01:24,  3.47it/s] 50%|█████     | 293/585 [02:01<01:24,  3.47it/s] 50%|█████     | 294/585 [02:01<01:23,  3.47it/s] 50%|█████     | 295/585 [02:01<01:23,  3.47it/s] 51%|█████     | 296/585 [02:02<01:23,  3.47it/s] 51%|█████     | 297/585 [02:02<01:23,  3.47it/s] 51%|█████     | 298/585 [02:02<01:22,  3.47it/s] 51%|█████     | 299/585 [02:03<01:22,  3.47it/s] 51%|█████▏    | 300/585 [02:03<01:22,  3.47it/s] 51%|█████▏    | 301/585 [02:03<01:21,  3.47it/s] 52%|█████▏    | 302/585 [02:04<01:21,  3.46it/s] 52%|█████▏    | 303/585 [02:04<01:21,  3.46it/s] 52%|█████▏    | 304/585 [02:04<01:21,  3.46it/s] 52%|█████▏    | 305/585 [02:04<01:20,  3.47it/s] 52%|█████▏    | 306/585 [02:05<01:20,  3.47it/s] 52%|█████▏    | 307/585 [02:05<01:20,  3.47it/s] 53%|█████▎    | 308/585 [02:05<01:19,  3.47it/s] 53%|█████▎    | 309/585 [02:06<01:19,  3.47it/s] 53%|█████▎    | 310/585 [02:06<01:19,  3.47it/s] 53%|█████▎    | 311/585 [02:06<01:18,  3.47it/s] 53%|█████▎    | 312/585 [02:06<01:18,  3.47it/s] 54%|█████▎    | 313/585 [02:07<01:18,  3.47it/s] 54%|█████▎    | 314/585 [02:07<01:18,  3.47it/s] 54%|█████▍    | 315/585 [02:07<01:17,  3.47it/s] 54%|█████▍    | 316/585 [02:08<01:17,  3.46it/s] 54%|█████▍    | 317/585 [02:08<01:17,  3.47it/s] 54%|█████▍    | 318/585 [02:08<01:16,  3.47it/s] 55%|█████▍    | 319/585 [02:08<01:16,  3.47it/s] 55%|█████▍    | 320/585 [02:09<01:16,  3.47it/s] 55%|█████▍    | 321/585 [02:09<01:16,  3.47it/s] 55%|█████▌    | 322/585 [02:09<01:15,  3.47it/s] 55%|█████▌    | 323/585 [02:10<01:15,  3.47it/s] 55%|█████▌    | 324/585 [02:10<01:15,  3.47it/s] 56%|█████▌    | 325/585 [02:10<01:15,  3.43it/s] 56%|█████▌    | 326/585 [02:10<01:15,  3.45it/s] 56%|█████▌    | 327/585 [02:11<01:14,  3.45it/s] 56%|█████▌    | 328/585 [02:11<01:14,  3.46it/s] 56%|█████▌    | 329/585 [02:11<01:13,  3.46it/s] 56%|█████▋    | 330/585 [02:12<01:13,  3.47it/s] 57%|█████▋    | 331/585 [02:12<01:13,  3.47it/s] 57%|█████▋    | 332/585 [02:12<01:12,  3.47it/s] 57%|█████▋    | 333/585 [02:12<01:12,  3.47it/s] 57%|█████▋    | 334/585 [02:13<01:12,  3.47it/s] 57%|█████▋    | 335/585 [02:13<01:12,  3.47it/s] 57%|█████▋    | 336/585 [02:13<01:11,  3.47it/s] 58%|█████▊    | 337/585 [02:14<01:11,  3.47it/s] 58%|█████▊    | 338/585 [02:14<01:11,  3.47it/s] 58%|█████▊    | 339/585 [02:14<01:11,  3.46it/s] 58%|█████▊    | 340/585 [02:14<01:10,  3.47it/s] 58%|█████▊    | 341/585 [02:15<01:10,  3.47it/s] 58%|█████▊    | 342/585 [02:15<01:10,  3.47it/s] 59%|█████▊    | 343/585 [02:15<01:09,  3.47it/s] 59%|█████▉    | 344/585 [02:16<01:09,  3.47it/s] 59%|█████▉    | 345/585 [02:16<01:09,  3.47it/s] 59%|█████▉    | 346/585 [02:16<01:08,  3.47it/s] 59%|█████▉    | 347/585 [02:17<01:08,  3.46it/s] 59%|█████▉    | 348/585 [02:17<01:08,  3.46it/s] 60%|█████▉    | 349/585 [02:17<01:08,  3.47it/s] 60%|█████▉    | 350/585 [02:17<01:07,  3.47it/s] 60%|██████    | 351/585 [02:18<01:07,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 05:58:29,071 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:58:29,072 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 05:58:29,072 >>   Batch size = 8
{'eval_loss': 1.0221312046051025, 'eval_runtime': 11.5473, 'eval_samples_per_second': 376.02, 'eval_steps_per_second': 47.024, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.81it/s][A
  2%|▏         | 12/543 [00:00<00:10, 51.13it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.38it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.57it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.08it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.74it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.39it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.00it/s][A
  9%|▉         | 48/543 [00:00<00:10, 46.98it/s][A
 10%|▉         | 53/543 [00:01<00:10, 47.02it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.03it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.18it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.18it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.25it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.17it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.09it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.87it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.87it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.90it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 46.98it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.11it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.18it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.20it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.14it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.04it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 46.90it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.89it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.86it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.96it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 47.03it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.17it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.17it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.12it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.12it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 46.93it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.85it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.84it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.92it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 47.00it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.12it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.14it/s][A
 39%|███▉      | 213/543 [00:04<00:06, 47.16it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.15it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.08it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 46.94it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.88it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.88it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 46.92it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 47.01it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.12it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.17it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.14it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.13it/s][A
 50%|█████     | 273/543 [00:05<00:05, 47.02it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.95it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 46.99it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 46.79it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 46.94it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.07it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.08it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.08it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.12it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.08it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 46.94it/s][A
 60%|██████    | 328/543 [00:06<00:04, 47.00it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 47.01it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 46.90it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 46.93it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 47.05it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.06it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 47.10it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.06it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 46.89it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 46.85it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 46.94it/s][A
 71%|███████   | 383/543 [00:08<00:03, 46.92it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 47.01it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 46.99it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.05it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.06it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.08it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 46.97it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 46.94it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 46.96it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 46.86it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 46.97it/s][A
 81%|████████  | 438/543 [00:09<00:02, 47.08it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 47.09it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 46.94it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.09it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 47.04it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 46.97it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 46.87it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.88it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 46.88it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 47.04it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 47.08it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 46.94it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 46.94it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 47.00it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 46.89it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 46.95it/s][A
 95%|█████████▌| 518/543 [00:10<00:00, 46.91it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 46.86it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 47.01it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 47.11it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 47.11it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.08it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:29<01:07,  3.47it/s]
100%|██████████| 543/543 [00:11<00:00, 47.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:58:40,638 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 05:58:40,661 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:58:42,805 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:58:42,819 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:58:42,829 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:36<22:24,  5.77s/it] 60%|██████    | 353/585 [02:37<15:57,  4.13s/it] 61%|██████    | 354/585 [02:37<11:27,  2.97s/it] 61%|██████    | 355/585 [02:37<08:18,  2.17s/it] 61%|██████    | 356/585 [02:37<06:07,  1.60s/it] 61%|██████    | 357/585 [02:38<04:35,  1.21s/it] 61%|██████    | 358/585 [02:38<03:32,  1.07it/s] 61%|██████▏   | 359/585 [02:38<02:47,  1.35it/s] 62%|██████▏   | 360/585 [02:39<02:16,  1.65it/s] 62%|██████▏   | 361/585 [02:39<01:54,  1.96it/s] 62%|██████▏   | 362/585 [02:39<01:38,  2.26it/s] 62%|██████▏   | 363/585 [02:39<01:28,  2.52it/s] 62%|██████▏   | 364/585 [02:40<01:20,  2.75it/s] 62%|██████▏   | 365/585 [02:40<01:15,  2.93it/s] 63%|██████▎   | 366/585 [02:40<01:11,  3.08it/s] 63%|██████▎   | 367/585 [02:41<01:08,  3.18it/s] 63%|██████▎   | 368/585 [02:41<01:06,  3.27it/s] 63%|██████▎   | 369/585 [02:41<01:05,  3.32it/s] 63%|██████▎   | 370/585 [02:41<01:04,  3.36it/s] 63%|██████▎   | 371/585 [02:42<01:03,  3.40it/s] 64%|██████▎   | 372/585 [02:42<01:02,  3.42it/s] 64%|██████▍   | 373/585 [02:42<01:01,  3.44it/s] 64%|██████▍   | 374/585 [02:43<01:01,  3.44it/s] 64%|██████▍   | 375/585 [02:43<01:00,  3.45it/s] 64%|██████▍   | 376/585 [02:43<01:00,  3.46it/s] 64%|██████▍   | 377/585 [02:43<01:00,  3.46it/s] 65%|██████▍   | 378/585 [02:44<00:59,  3.46it/s] 65%|██████▍   | 379/585 [02:44<00:59,  3.47it/s] 65%|██████▍   | 380/585 [02:44<00:59,  3.46it/s] 65%|██████▌   | 381/585 [02:45<00:58,  3.46it/s] 65%|██████▌   | 382/585 [02:45<00:58,  3.46it/s] 65%|██████▌   | 383/585 [02:45<00:58,  3.47it/s] 66%|██████▌   | 384/585 [02:45<00:57,  3.47it/s] 66%|██████▌   | 385/585 [02:46<00:57,  3.46it/s] 66%|██████▌   | 386/585 [02:46<00:57,  3.47it/s] 66%|██████▌   | 387/585 [02:46<00:57,  3.47it/s] 66%|██████▋   | 388/585 [02:47<00:56,  3.47it/s] 66%|██████▋   | 389/585 [02:47<00:56,  3.47it/s] 67%|██████▋   | 390/585 [02:47<00:56,  3.47it/s] 67%|██████▋   | 391/585 [02:47<00:56,  3.46it/s] 67%|██████▋   | 392/585 [02:48<00:55,  3.47it/s] 67%|██████▋   | 393/585 [02:48<00:55,  3.47it/s] 67%|██████▋   | 394/585 [02:48<00:55,  3.47it/s] 68%|██████▊   | 395/585 [02:49<00:54,  3.47it/s] 68%|██████▊   | 396/585 [02:49<00:54,  3.47it/s] 68%|██████▊   | 397/585 [02:49<00:54,  3.47it/s] 68%|██████▊   | 398/585 [02:49<00:53,  3.47it/s] 68%|██████▊   | 399/585 [02:50<00:53,  3.47it/s] 68%|██████▊   | 400/585 [02:50<00:53,  3.47it/s] 69%|██████▊   | 401/585 [02:50<00:53,  3.47it/s] 69%|██████▊   | 402/585 [02:51<00:52,  3.46it/s] 69%|██████▉   | 403/585 [02:51<00:52,  3.46it/s] 69%|██████▉   | 404/585 [02:51<00:52,  3.47it/s] 69%|██████▉   | 405/585 [02:52<00:51,  3.46it/s] 69%|██████▉   | 406/585 [02:52<00:52,  3.40it/s] 70%|██████▉   | 407/585 [02:52<00:52,  3.42it/s] 70%|██████▉   | 408/585 [02:52<00:51,  3.43it/s] 70%|██████▉   | 409/585 [02:53<00:51,  3.44it/s] 70%|███████   | 410/585 [02:53<00:50,  3.45it/s] 70%|███████   | 411/585 [02:53<00:50,  3.46it/s] 70%|███████   | 412/585 [02:54<00:49,  3.46it/s] 71%|███████   | 413/585 [02:54<00:49,  3.45it/s] 71%|███████   | 414/585 [02:54<00:49,  3.46it/s] 71%|███████   | 415/585 [02:54<00:49,  3.46it/s] 71%|███████   | 416/585 [02:55<00:48,  3.46it/s] 71%|███████▏  | 417/585 [02:55<00:48,  3.46it/s] 71%|███████▏  | 418/585 [02:55<00:48,  3.46it/s] 72%|███████▏  | 419/585 [02:56<00:47,  3.47it/s] 72%|███████▏  | 420/585 [02:56<00:47,  3.47it/s] 72%|███████▏  | 421/585 [02:56<00:47,  3.47it/s] 72%|███████▏  | 422/585 [02:56<00:47,  3.47it/s] 72%|███████▏  | 423/585 [02:57<00:46,  3.47it/s] 72%|███████▏  | 424/585 [02:57<00:46,  3.46it/s] 73%|███████▎  | 425/585 [02:57<00:46,  3.46it/s] 73%|███████▎  | 426/585 [02:58<00:45,  3.47it/s] 73%|███████▎  | 427/585 [02:58<00:45,  3.47it/s] 73%|███████▎  | 428/585 [02:58<00:45,  3.47it/s] 73%|███████▎  | 429/585 [02:58<00:44,  3.47it/s] 74%|███████▎  | 430/585 [02:59<00:44,  3.47it/s] 74%|███████▎  | 431/585 [02:59<00:44,  3.47it/s] 74%|███████▍  | 432/585 [02:59<00:44,  3.47it/s] 74%|███████▍  | 433/585 [03:00<00:43,  3.47it/s] 74%|███████▍  | 434/585 [03:00<00:43,  3.47it/s] 74%|███████▍  | 435/585 [03:00<00:43,  3.46it/s] 75%|███████▍  | 436/585 [03:00<00:43,  3.46it/s] 75%|███████▍  | 437/585 [03:01<00:42,  3.46it/s] 75%|███████▍  | 438/585 [03:01<00:42,  3.46it/s] 75%|███████▌  | 439/585 [03:01<00:42,  3.47it/s] 75%|███████▌  | 440/585 [03:02<00:41,  3.46it/s] 75%|███████▌  | 441/585 [03:02<00:41,  3.47it/s] 76%|███████▌  | 442/585 [03:02<00:41,  3.47it/s] 76%|███████▌  | 443/585 [03:02<00:40,  3.47it/s] 76%|███████▌  | 444/585 [03:03<00:40,  3.47it/s] 76%|███████▌  | 445/585 [03:03<00:40,  3.47it/s] 76%|███████▌  | 446/585 [03:03<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:04<00:39,  3.46it/s] 77%|███████▋  | 448/585 [03:04<00:39,  3.46it/s] 77%|███████▋  | 449/585 [03:04<00:39,  3.46it/s] 77%|███████▋  | 450/585 [03:05<00:38,  3.46it/s] 77%|███████▋  | 451/585 [03:05<00:38,  3.47it/s] 77%|███████▋  | 452/585 [03:05<00:38,  3.47it/s] 77%|███████▋  | 453/585 [03:05<00:38,  3.47it/s] 78%|███████▊  | 454/585 [03:06<00:37,  3.47it/s] 78%|███████▊  | 455/585 [03:06<00:37,  3.47it/s] 78%|███████▊  | 456/585 [03:06<00:37,  3.46it/s] 78%|███████▊  | 457/585 [03:07<00:36,  3.47it/s] 78%|███████▊  | 458/585 [03:07<00:36,  3.46it/s] 78%|███████▊  | 459/585 [03:07<00:36,  3.47it/s] 79%|███████▊  | 460/585 [03:07<00:36,  3.47it/s] 79%|███████▉  | 461/585 [03:08<00:35,  3.47it/s] 79%|███████▉  | 462/585 [03:08<00:35,  3.47it/s] 79%|███████▉  | 463/585 [03:08<00:35,  3.47it/s] 79%|███████▉  | 464/585 [03:09<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:09<00:34,  3.46it/s] 80%|███████▉  | 466/585 [03:09<00:34,  3.46it/s] 80%|███████▉  | 467/585 [03:09<00:34,  3.47it/s] 80%|████████  | 468/585 [03:10<00:33,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 05:59:21,114 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:59:21,114 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 05:59:21,114 >>   Batch size = 8
{'eval_loss': 1.039098858833313, 'eval_runtime': 11.5542, 'eval_samples_per_second': 375.794, 'eval_steps_per_second': 46.996, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.62it/s][A
  2%|▏         | 12/543 [00:00<00:10, 51.02it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.32it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.57it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.12it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.89it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.37it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.10it/s][A
  9%|▉         | 48/543 [00:00<00:10, 47.03it/s][A
 10%|▉         | 53/543 [00:01<00:10, 47.05it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.12it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.17it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.23it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.25it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.25it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.23it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.98it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 47.04it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.95it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 47.02it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.02it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.15it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.20it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.23it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.20it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 47.09it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 47.02it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 47.04it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 47.06it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 46.90it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.10it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.15it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.21it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.04it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 47.13it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 47.02it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 47.08it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 47.09it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 47.07it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.00it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.10it/s][A
 39%|███▉      | 213/543 [00:04<00:06, 47.19it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.17it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.19it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 47.09it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 47.03it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 47.06it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 47.11it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 47.01it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.05it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.05it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.15it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.15it/s][A
 50%|█████     | 273/543 [00:05<00:05, 47.13it/s][A
 51%|█████     | 278/543 [00:05<00:05, 47.12it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 47.08it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 47.13it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 47.06it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.03it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 46.98it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.10it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.07it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.13it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 47.11it/s][A
 60%|██████    | 328/543 [00:06<00:04, 47.01it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 47.07it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 47.08it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 47.02it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 47.04it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.07it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 47.11it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.01it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 47.11it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 47.03it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 46.96it/s][A
 71%|███████   | 383/543 [00:08<00:03, 47.06it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 47.03it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 46.98it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.11it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.15it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.07it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 47.12it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 47.07it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 46.88it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 47.02it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 47.01it/s][A
 81%|████████  | 438/543 [00:09<00:02, 46.96it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 47.02it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 47.15it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.16it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 46.97it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 47.10it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 47.05it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.98it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 47.04it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 47.04it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 46.98it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.03it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 47.15it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 47.13it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 47.07it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 47.05it/s][A
 95%|█████████▌| 518/543 [00:10<00:00, 46.97it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 47.04it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 47.01it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 46.96it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 46.98it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.07it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:11<00:00, 47.07it/s][A 80%|████████  | 468/585 [03:21<00:33,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:59:32,667 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 05:59:32,684 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:59:34,863 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:59:34,878 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:59:34,886 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:28<11:02,  5.71s/it] 80%|████████  | 470/585 [03:28<07:49,  4.09s/it] 81%|████████  | 471/585 [03:29<05:35,  2.95s/it] 81%|████████  | 472/585 [03:29<04:02,  2.15s/it] 81%|████████  | 473/585 [03:29<02:58,  1.59s/it] 81%|████████  | 474/585 [03:30<02:13,  1.20s/it] 81%|████████  | 475/585 [03:30<01:41,  1.08it/s] 81%|████████▏ | 476/585 [03:30<01:20,  1.36it/s] 82%|████████▏ | 477/585 [03:30<01:04,  1.67it/s] 82%|████████▏ | 478/585 [03:31<00:54,  1.97it/s] 82%|████████▏ | 479/585 [03:31<00:46,  2.27it/s] 82%|████████▏ | 480/585 [03:31<00:41,  2.53it/s] 82%|████████▏ | 481/585 [03:32<00:37,  2.76it/s] 82%|████████▏ | 482/585 [03:32<00:35,  2.94it/s] 83%|████████▎ | 483/585 [03:32<00:33,  3.08it/s] 83%|████████▎ | 484/585 [03:32<00:31,  3.19it/s] 83%|████████▎ | 485/585 [03:33<00:30,  3.27it/s] 83%|████████▎ | 486/585 [03:33<00:29,  3.33it/s] 83%|████████▎ | 487/585 [03:33<00:29,  3.37it/s] 83%|████████▎ | 488/585 [03:34<00:28,  3.40it/s] 84%|████████▎ | 489/585 [03:34<00:28,  3.42it/s] 84%|████████▍ | 490/585 [03:34<00:27,  3.44it/s] 84%|████████▍ | 491/585 [03:34<00:27,  3.45it/s] 84%|████████▍ | 492/585 [03:35<00:27,  3.44it/s] 84%|████████▍ | 493/585 [03:35<00:26,  3.46it/s] 84%|████████▍ | 494/585 [03:35<00:26,  3.46it/s] 85%|████████▍ | 495/585 [03:36<00:25,  3.47it/s] 85%|████████▍ | 496/585 [03:36<00:25,  3.47it/s] 85%|████████▍ | 497/585 [03:36<00:25,  3.47it/s] 85%|████████▌ | 498/585 [03:36<00:25,  3.47it/s] 85%|████████▌ | 499/585 [03:37<00:24,  3.47it/s] 85%|████████▌ | 500/585 [03:37<00:24,  3.47it/s]                                                  85%|████████▌ | 500/585 [03:37<00:24,  3.47it/s] 86%|████████▌ | 501/585 [03:37<00:24,  3.47it/s] 86%|████████▌ | 502/585 [03:38<00:23,  3.47it/s] 86%|████████▌ | 503/585 [03:38<00:23,  3.47it/s] 86%|████████▌ | 504/585 [03:38<00:23,  3.47it/s] 86%|████████▋ | 505/585 [03:38<00:23,  3.47it/s] 86%|████████▋ | 506/585 [03:39<00:22,  3.47it/s] 87%|████████▋ | 507/585 [03:39<00:22,  3.47it/s] 87%|████████▋ | 508/585 [03:39<00:22,  3.47it/s] 87%|████████▋ | 509/585 [03:40<00:21,  3.46it/s] 87%|████████▋ | 510/585 [03:40<00:21,  3.46it/s] 87%|████████▋ | 511/585 [03:40<00:21,  3.47it/s] 88%|████████▊ | 512/585 [03:40<00:21,  3.47it/s] 88%|████████▊ | 513/585 [03:41<00:20,  3.47it/s] 88%|████████▊ | 514/585 [03:41<00:20,  3.47it/s] 88%|████████▊ | 515/585 [03:41<00:20,  3.47it/s] 88%|████████▊ | 516/585 [03:42<00:19,  3.47it/s] 88%|████████▊ | 517/585 [03:42<00:19,  3.47it/s] 89%|████████▊ | 518/585 [03:42<00:19,  3.47it/s] 89%|████████▊ | 519/585 [03:42<00:19,  3.46it/s] 89%|████████▉ | 520/585 [03:43<00:18,  3.46it/s] 89%|████████▉ | 521/585 [03:43<00:18,  3.47it/s] 89%|████████▉ | 522/585 [03:43<00:18,  3.47it/s] 89%|████████▉ | 523/585 [03:44<00:17,  3.47it/s] 90%|████████▉ | 524/585 [03:44<00:17,  3.47it/s] 90%|████████▉ | 525/585 [03:44<00:17,  3.47it/s] 90%|████████▉ | 526/585 [03:44<00:17,  3.47it/s] 90%|█████████ | 527/585 [03:45<00:16,  3.47it/s] 90%|█████████ | 528/585 [03:45<00:16,  3.47it/s] 90%|█████████ | 529/585 [03:45<00:16,  3.47it/s] 91%|█████████ | 530/585 [03:46<00:15,  3.47it/s] 91%|█████████ | 531/585 [03:46<00:15,  3.47it/s] 91%|█████████ | 532/585 [03:46<00:15,  3.47it/s] 91%|█████████ | 533/585 [03:47<00:15,  3.47it/s] 91%|█████████▏| 534/585 [03:47<00:14,  3.47it/s] 91%|█████████▏| 535/585 [03:47<00:14,  3.47it/s] 92%|█████████▏| 536/585 [03:47<00:14,  3.47it/s] 92%|█████████▏| 537/585 [03:48<00:13,  3.47it/s] 92%|█████████▏| 538/585 [03:48<00:13,  3.47it/s] 92%|█████████▏| 539/585 [03:48<00:13,  3.47it/s] 92%|█████████▏| 540/585 [03:49<00:12,  3.47it/s] 92%|█████████▏| 541/585 [03:49<00:12,  3.46it/s] 93%|█████████▎| 542/585 [03:49<00:12,  3.46it/s] 93%|█████████▎| 543/585 [03:49<00:12,  3.47it/s] 93%|█████████▎| 544/585 [03:50<00:11,  3.47it/s] 93%|█████████▎| 545/585 [03:50<00:11,  3.47it/s] 93%|█████████▎| 546/585 [03:50<00:11,  3.46it/s] 94%|█████████▎| 547/585 [03:51<00:10,  3.46it/s] 94%|█████████▎| 548/585 [03:51<00:10,  3.46it/s] 94%|█████████▍| 549/585 [03:51<00:10,  3.47it/s] 94%|█████████▍| 550/585 [03:51<00:10,  3.47it/s] 94%|█████████▍| 551/585 [03:52<00:09,  3.46it/s] 94%|█████████▍| 552/585 [03:52<00:09,  3.38it/s] 95%|█████████▍| 553/585 [03:52<00:09,  3.40it/s] 95%|█████████▍| 554/585 [03:53<00:09,  3.42it/s] 95%|█████████▍| 555/585 [03:53<00:08,  3.44it/s] 95%|█████████▌| 556/585 [03:53<00:08,  3.45it/s] 95%|█████████▌| 557/585 [03:53<00:08,  3.46it/s] 95%|█████████▌| 558/585 [03:54<00:07,  3.46it/s] 96%|█████████▌| 559/585 [03:54<00:07,  3.46it/s] 96%|█████████▌| 560/585 [03:54<00:07,  3.46it/s] 96%|█████████▌| 561/585 [03:55<00:06,  3.47it/s] 96%|█████████▌| 562/585 [03:55<00:06,  3.47it/s] 96%|█████████▌| 563/585 [03:55<00:06,  3.46it/s] 96%|█████████▋| 564/585 [03:55<00:06,  3.46it/s] 97%|█████████▋| 565/585 [03:56<00:05,  3.46it/s] 97%|█████████▋| 566/585 [03:56<00:05,  3.47it/s] 97%|█████████▋| 567/585 [03:56<00:05,  3.47it/s] 97%|█████████▋| 568/585 [03:57<00:04,  3.47it/s] 97%|█████████▋| 569/585 [03:57<00:04,  3.47it/s] 97%|█████████▋| 570/585 [03:57<00:04,  3.47it/s] 98%|█████████▊| 571/585 [03:57<00:04,  3.47it/s] 98%|█████████▊| 572/585 [03:58<00:03,  3.47it/s] 98%|█████████▊| 573/585 [03:58<00:03,  3.47it/s] 98%|█████████▊| 574/585 [03:58<00:03,  3.46it/s] 98%|█████████▊| 575/585 [03:59<00:02,  3.46it/s] 98%|█████████▊| 576/585 [03:59<00:02,  3.47it/s] 99%|█████████▊| 577/585 [03:59<00:02,  3.47it/s] 99%|█████████▉| 578/585 [04:00<00:02,  3.46it/s] 99%|█████████▉| 579/585 [04:00<00:01,  3.46it/s] 99%|█████████▉| 580/585 [04:00<00:01,  3.46it/s] 99%|█████████▉| 581/585 [04:00<00:01,  3.46it/s] 99%|█████████▉| 582/585 [04:01<00:00,  3.47it/s]100%|█████████▉| 583/585 [04:01<00:00,  3.47it/s]100%|█████████▉| 584/585 [04:01<00:00,  3.47it/s]100%|██████████| 585/585 [04:02<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 06:00:12,909 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:00:12,909 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 06:00:12,909 >>   Batch size = 8
{'eval_loss': 1.0492087602615356, 'eval_runtime': 11.5418, 'eval_samples_per_second': 376.196, 'eval_steps_per_second': 47.046, 'epoch': 4.0}
{'loss': 0.5262, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.77it/s][A
  2%|▏         | 12/543 [00:00<00:10, 51.12it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.40it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.62it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.21it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.85it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.50it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.23it/s][A
  9%|▉         | 48/543 [00:00<00:10, 47.04it/s][A
 10%|▉         | 53/543 [00:01<00:10, 47.02it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.10it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.18it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.21it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.27it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.28it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.22it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 47.00it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 47.02it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.92it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 46.90it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.10it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.14it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.21it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.20it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.21it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 47.07it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.95it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.98it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.94it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 46.97it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.13it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.18it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.17it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.21it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 47.09it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.90it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.91it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.99it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 46.88it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.04it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.13it/s][A
 39%|███▉      | 213/543 [00:04<00:06, 47.19it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.20it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.22it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 47.08it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.98it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.64it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 46.77it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 46.75it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 46.98it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.10it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.14it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.21it/s][A
 50%|█████     | 273/543 [00:05<00:05, 47.04it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.98it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 47.00it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 47.00it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 46.93it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 46.95it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.10it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.17it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.21it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.20it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 47.07it/s][A
 60%|██████    | 328/543 [00:06<00:04, 47.03it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 47.09it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 46.91it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 46.94it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 46.97it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.10it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 47.15it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.17it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 47.12it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 47.01it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 46.66it/s][A
 71%|███████   | 383/543 [00:08<00:03, 46.99it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 46.97it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 46.97it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.11it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.12it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.19it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 47.18it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 47.09it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 46.99it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 47.00it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 47.03it/s][A
 81%|████████  | 438/543 [00:09<00:02, 46.98it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 47.11it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 47.19it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.06it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 47.02it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 47.11it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 47.03it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 47.05it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 47.05it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 46.99it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 47.05it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.16it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 47.18it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 47.08it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 47.08it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 47.04it/s][A
 95%|█████████▌| 518/543 [00:10<00:00, 46.92it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 47.00it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 46.99it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 46.94it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 47.06it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.13it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:11<00:00, 47.13it/s][A100%|██████████| 585/585 [04:13<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:00:24,461 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 06:00:24,477 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:00:26,731 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:00:26,747 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:00:26,761 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 06:00:31,194 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 06:00:31,196 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117 (score: 1.0140827894210815).
                                                 100%|██████████| 585/585 [04:21<00:00,  3.46it/s]100%|██████████| 585/585 [04:21<00:00,  2.23it/s]
[INFO|trainer.py:1894] 2023-08-29 06:00:32,726 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 06:00:32,743 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:00:34,952 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:00:34,969 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:00:34,977 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 06:00:35,159 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:35,159 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:35,160 >>   train_loss               =     0.5225
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:35,160 >>   train_runtime            = 0:04:21.84
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:35,160 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:35,160 >>   train_samples_per_second =    143.194
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:35,160 >>   train_steps_per_second   =      2.234
{'eval_loss': 1.0522184371948242, 'eval_runtime': 11.5354, 'eval_samples_per_second': 376.407, 'eval_steps_per_second': 47.073, 'epoch': 5.0}
{'train_runtime': 261.8469, 'train_samples_per_second': 143.194, 'train_steps_per_second': 2.234, 'train_loss': 0.5225261003543169, 'epoch': 5.0}
08/29/2023 06:00:35 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 06:00:35,199 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:00:35,200 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 06:00:35,200 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 58.98it/s]  2%|▏         | 12/543 [00:00<00:10, 51.90it/s]  3%|▎         | 18/543 [00:00<00:10, 50.02it/s]  4%|▍         | 24/543 [00:00<00:10, 49.09it/s]  5%|▌         | 29/543 [00:00<00:10, 48.69it/s]  6%|▋         | 34/543 [00:00<00:10, 48.47it/s]  7%|▋         | 39/543 [00:00<00:10, 48.28it/s]  8%|▊         | 44/543 [00:00<00:10, 47.85it/s]  9%|▉         | 49/543 [00:01<00:10, 47.36it/s] 10%|▉         | 54/543 [00:01<00:10, 47.27it/s] 11%|█         | 59/543 [00:01<00:10, 47.34it/s] 12%|█▏        | 64/543 [00:01<00:10, 47.44it/s] 13%|█▎        | 69/543 [00:01<00:09, 47.60it/s] 14%|█▎        | 74/543 [00:01<00:09, 47.58it/s] 15%|█▍        | 79/543 [00:01<00:09, 47.64it/s] 15%|█▌        | 84/543 [00:01<00:09, 47.62it/s] 16%|█▋        | 89/543 [00:01<00:09, 47.49it/s] 17%|█▋        | 94/543 [00:01<00:09, 47.19it/s] 18%|█▊        | 99/543 [00:02<00:09, 47.11it/s] 19%|█▉        | 104/543 [00:02<00:09, 47.24it/s] 20%|██        | 109/543 [00:02<00:09, 47.39it/s] 21%|██        | 114/543 [00:02<00:09, 47.43it/s] 22%|██▏       | 119/543 [00:02<00:08, 47.36it/s] 23%|██▎       | 124/543 [00:02<00:08, 47.55it/s] 24%|██▍       | 129/543 [00:02<00:08, 47.55it/s] 25%|██▍       | 134/543 [00:02<00:08, 47.40it/s] 26%|██▌       | 139/543 [00:02<00:08, 47.19it/s] 27%|██▋       | 144/543 [00:03<00:08, 47.09it/s] 27%|██▋       | 149/543 [00:03<00:08, 47.10it/s] 28%|██▊       | 154/543 [00:03<00:08, 47.31it/s] 29%|██▉       | 159/543 [00:03<00:08, 47.46it/s] 30%|███       | 164/543 [00:03<00:07, 47.51it/s] 31%|███       | 169/543 [00:03<00:07, 47.55it/s] 32%|███▏      | 174/543 [00:03<00:07, 47.43it/s] 33%|███▎      | 179/543 [00:03<00:07, 47.52it/s] 34%|███▍      | 184/543 [00:03<00:07, 47.36it/s] 35%|███▍      | 189/543 [00:03<00:07, 47.17it/s] 36%|███▌      | 194/543 [00:04<00:07, 47.25it/s] 37%|███▋      | 199/543 [00:04<00:07, 47.30it/s] 38%|███▊      | 204/543 [00:04<00:07, 47.38it/s] 38%|███▊      | 209/543 [00:04<00:07, 47.44it/s] 39%|███▉      | 214/543 [00:04<00:06, 47.52it/s] 40%|████      | 219/543 [00:04<00:06, 47.58it/s] 41%|████▏     | 224/543 [00:04<00:06, 47.54it/s] 42%|████▏     | 229/543 [00:04<00:06, 47.30it/s] 43%|████▎     | 234/543 [00:04<00:06, 47.19it/s] 44%|████▍     | 239/543 [00:05<00:06, 47.16it/s] 45%|████▍     | 244/543 [00:05<00:06, 46.98it/s] 46%|████▌     | 249/543 [00:05<00:06, 47.20it/s] 47%|████▋     | 254/543 [00:05<00:06, 47.27it/s] 48%|████▊     | 259/543 [00:05<00:06, 47.32it/s] 49%|████▊     | 264/543 [00:05<00:05, 47.40it/s] 50%|████▉     | 269/543 [00:05<00:05, 47.49it/s] 50%|█████     | 274/543 [00:05<00:05, 47.40it/s] 51%|█████▏    | 279/543 [00:05<00:05, 47.26it/s] 52%|█████▏    | 284/543 [00:05<00:05, 47.24it/s] 53%|█████▎    | 289/543 [00:06<00:05, 47.19it/s] 54%|█████▍    | 294/543 [00:06<00:05, 47.15it/s] 55%|█████▌    | 299/543 [00:06<00:05, 47.29it/s] 56%|█████▌    | 304/543 [00:06<00:05, 47.32it/s] 57%|█████▋    | 309/543 [00:06<00:04, 47.33it/s] 58%|█████▊    | 314/543 [00:06<00:04, 47.39it/s] 59%|█████▊    | 319/543 [00:06<00:04, 47.44it/s] 60%|█████▉    | 324/543 [00:06<00:04, 47.36it/s] 61%|██████    | 329/543 [00:06<00:04, 47.25it/s] 62%|██████▏   | 334/543 [00:07<00:04, 47.13it/s] 62%|██████▏   | 339/543 [00:07<00:04, 47.23it/s] 63%|██████▎   | 344/543 [00:07<00:04, 47.27it/s] 64%|██████▍   | 349/543 [00:07<00:04, 47.33it/s] 65%|██████▌   | 354/543 [00:07<00:03, 47.34it/s] 66%|██████▌   | 359/543 [00:07<00:03, 47.35it/s] 67%|██████▋   | 364/543 [00:07<00:03, 47.35it/s] 68%|██████▊   | 369/543 [00:07<00:03, 47.41it/s] 69%|██████▉   | 374/543 [00:07<00:03, 47.35it/s] 70%|██████▉   | 379/543 [00:07<00:03, 47.25it/s] 71%|███████   | 384/543 [00:08<00:03, 47.18it/s] 72%|███████▏  | 389/543 [00:08<00:03, 47.26it/s] 73%|███████▎  | 394/543 [00:08<00:03, 47.30it/s] 73%|███████▎  | 399/543 [00:08<00:03, 47.30it/s] 74%|███████▍  | 404/543 [00:08<00:02, 47.38it/s] 75%|███████▌  | 409/543 [00:08<00:02, 47.29it/s] 76%|███████▌  | 414/543 [00:08<00:02, 47.35it/s] 77%|███████▋  | 419/543 [00:08<00:02, 47.33it/s] 78%|███████▊  | 424/543 [00:08<00:02, 47.34it/s] 79%|███████▉  | 429/543 [00:09<00:02, 47.28it/s] 80%|███████▉  | 434/543 [00:09<00:02, 47.17it/s] 81%|████████  | 439/543 [00:09<00:02, 47.30it/s] 82%|████████▏ | 444/543 [00:09<00:02, 47.36it/s] 83%|████████▎ | 449/543 [00:09<00:01, 47.35it/s] 84%|████████▎ | 454/543 [00:09<00:01, 47.35it/s] 85%|████████▍ | 459/543 [00:09<00:01, 47.37it/s] 85%|████████▌ | 464/543 [00:09<00:01, 47.29it/s] 86%|████████▋ | 469/543 [00:09<00:01, 47.19it/s] 87%|████████▋ | 474/543 [00:09<00:01, 47.25it/s] 88%|████████▊ | 479/543 [00:10<00:01, 47.20it/s] 89%|████████▉ | 484/543 [00:10<00:01, 47.12it/s] 90%|█████████ | 489/543 [00:10<00:01, 47.22it/s] 91%|█████████ | 494/543 [00:10<00:01, 47.28it/s] 92%|█████████▏| 499/543 [00:10<00:00, 47.28it/s] 93%|█████████▎| 504/543 [00:10<00:00, 47.30it/s] 94%|█████████▎| 509/543 [00:10<00:00, 47.30it/s] 95%|█████████▍| 514/543 [00:10<00:00, 47.30it/s] 96%|█████████▌| 519/543 [00:10<00:00, 47.24it/s] 97%|█████████▋| 524/543 [00:11<00:00, 47.23it/s] 97%|█████████▋| 529/543 [00:11<00:00, 47.18it/s] 98%|█████████▊| 534/543 [00:11<00:00, 47.17it/s] 99%|█████████▉| 539/543 [00:11<00:00, 47.20it/s]100%|██████████| 543/543 [00:11<00:00, 47.42it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 06:00:46,672 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:46,672 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:46,672 >>   eval_loss               =     1.0141
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:46,672 >>   eval_runtime            = 0:00:11.47
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:46,673 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:46,673 >>   eval_samples_per_second =    378.464
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:46,673 >>   eval_steps_per_second   =      47.33
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:00:46,673 >>   perplexity              =     2.7568
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:53,388 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:53,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:53,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:53,392 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:53,392 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:00:54,023 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:00:54,024 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:00:54,586 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:00:55,603 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:00:55,603 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:58,434 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:58,439 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:58,439 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:58,439 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:58,439 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:00:59,293 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:00:59,294 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:00:59,873 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:01:00,015 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:01:00,015 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.70it/s]Extractor Predicting: 10it [00:05,  1.70it/s]Extractor Predicting: 11it [00:06,  1.71it/s]Extractor Predicting: 12it [00:07,  1.72it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:09,  1.55it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:11,  1.47it/s]Extractor Predicting: 20it [00:12,  1.45it/s]Extractor Predicting: 21it [00:13,  1.47it/s]Extractor Predicting: 22it [00:13,  1.50it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:15,  1.50it/s]Extractor Predicting: 25it [00:15,  1.50it/s]Extractor Predicting: 26it [00:16,  1.49it/s]Extractor Predicting: 27it [00:17,  1.48it/s]Extractor Predicting: 28it [00:17,  1.50it/s]Extractor Predicting: 29it [00:18,  1.47it/s]Extractor Predicting: 30it [00:19,  1.48it/s]Extractor Predicting: 31it [00:19,  1.49it/s]Extractor Predicting: 32it [00:20,  1.49it/s]Extractor Predicting: 33it [00:21,  1.49it/s]Extractor Predicting: 34it [00:21,  1.48it/s]Extractor Predicting: 35it [00:22,  1.47it/s]Extractor Predicting: 36it [00:23,  1.45it/s]Extractor Predicting: 37it [00:24,  1.43it/s]Extractor Predicting: 38it [00:24,  1.44it/s]Extractor Predicting: 39it [00:25,  1.47it/s]Extractor Predicting: 40it [00:26,  1.41it/s]Extractor Predicting: 41it [00:26,  1.42it/s]Extractor Predicting: 42it [00:27,  1.42it/s]Extractor Predicting: 43it [00:28,  1.46it/s]Extractor Predicting: 44it [00:28,  1.49it/s]Extractor Predicting: 45it [00:29,  1.49it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:30,  1.51it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:32,  1.49it/s]Extractor Predicting: 50it [00:32,  1.50it/s]Extractor Predicting: 51it [00:33,  1.50it/s]Extractor Predicting: 52it [00:34,  1.49it/s]Extractor Predicting: 53it [00:34,  1.46it/s]Extractor Predicting: 54it [00:35,  1.49it/s]Extractor Predicting: 55it [00:36,  1.52it/s]Extractor Predicting: 56it [00:36,  1.49it/s]Extractor Predicting: 57it [00:37,  1.47it/s]Extractor Predicting: 58it [00:38,  1.51it/s]Extractor Predicting: 59it [00:38,  1.47it/s]Extractor Predicting: 60it [00:39,  1.48it/s]Extractor Predicting: 61it [00:40,  1.50it/s]Extractor Predicting: 62it [00:40,  1.51it/s]Extractor Predicting: 63it [00:41,  1.50it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:42,  1.54it/s]Extractor Predicting: 66it [00:43,  1.54it/s]Extractor Predicting: 67it [00:44,  1.54it/s]Extractor Predicting: 68it [00:44,  1.51it/s]Extractor Predicting: 69it [00:45,  1.52it/s]Extractor Predicting: 70it [00:46,  1.51it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:47,  1.52it/s]Extractor Predicting: 73it [00:48,  1.50it/s]Extractor Predicting: 74it [00:48,  1.48it/s]Extractor Predicting: 75it [00:49,  1.45it/s]Extractor Predicting: 76it [00:50,  1.47it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:52,  1.48it/s]Extractor Predicting: 80it [00:52,  1.50it/s]Extractor Predicting: 81it [00:53,  1.50it/s]Extractor Predicting: 82it [00:54,  1.50it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:55,  1.53it/s]Extractor Predicting: 85it [00:56,  1.53it/s]Extractor Predicting: 86it [00:56,  1.51it/s]Extractor Predicting: 87it [00:57,  1.51it/s]Extractor Predicting: 88it [00:58,  1.52it/s]Extractor Predicting: 89it [00:58,  1.54it/s]Extractor Predicting: 90it [00:59,  1.51it/s]Extractor Predicting: 91it [01:00,  1.49it/s]Extractor Predicting: 92it [01:00,  1.47it/s]Extractor Predicting: 93it [01:01,  1.52it/s]Extractor Predicting: 94it [01:02,  1.53it/s]Extractor Predicting: 95it [01:02,  1.52it/s]Extractor Predicting: 96it [01:03,  1.54it/s]Extractor Predicting: 97it [01:04,  1.55it/s]Extractor Predicting: 98it [01:04,  1.42it/s]Extractor Predicting: 99it [01:05,  1.42it/s]Extractor Predicting: 100it [01:06,  1.45it/s]Extractor Predicting: 101it [01:06,  1.47it/s]Extractor Predicting: 102it [01:07,  1.48it/s]Extractor Predicting: 103it [01:08,  1.49it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.49it/s]Extractor Predicting: 106it [01:10,  1.51it/s]Extractor Predicting: 107it [01:10,  1.51it/s]Extractor Predicting: 108it [01:11,  1.50it/s]Extractor Predicting: 109it [01:12,  1.50it/s]Extractor Predicting: 110it [01:12,  1.49it/s]Extractor Predicting: 111it [01:13,  1.50it/s]Extractor Predicting: 112it [01:14,  1.53it/s]Extractor Predicting: 113it [01:14,  1.53it/s]Extractor Predicting: 114it [01:15,  1.51it/s]Extractor Predicting: 115it [01:16,  1.50it/s]Extractor Predicting: 116it [01:16,  1.50it/s]Extractor Predicting: 117it [01:17,  1.52it/s]Extractor Predicting: 118it [01:18,  1.47it/s]Extractor Predicting: 119it [01:18,  1.48it/s]Extractor Predicting: 120it [01:19,  1.48it/s]Extractor Predicting: 121it [01:20,  1.46it/s]Extractor Predicting: 122it [01:20,  1.46it/s]Extractor Predicting: 123it [01:21,  1.49it/s]Extractor Predicting: 124it [01:22,  1.50it/s]Extractor Predicting: 125it [01:22,  1.52it/s]Extractor Predicting: 126it [01:23,  1.48it/s]Extractor Predicting: 127it [01:24,  1.48it/s]Extractor Predicting: 128it [01:24,  1.50it/s]Extractor Predicting: 129it [01:25,  1.52it/s]Extractor Predicting: 130it [01:26,  1.56it/s]Extractor Predicting: 131it [01:26,  1.52it/s]Extractor Predicting: 132it [01:27,  1.51it/s]Extractor Predicting: 133it [01:28,  1.51it/s]Extractor Predicting: 134it [01:28,  1.52it/s]Extractor Predicting: 135it [01:29,  1.54it/s]Extractor Predicting: 136it [01:30,  1.53it/s]Extractor Predicting: 137it [01:30,  1.54it/s]Extractor Predicting: 138it [01:31,  1.52it/s]Extractor Predicting: 139it [01:32,  1.50it/s]Extractor Predicting: 140it [01:32,  1.53it/s]Extractor Predicting: 141it [01:33,  1.54it/s]Extractor Predicting: 142it [01:34,  1.56it/s]Extractor Predicting: 143it [01:34,  1.57it/s]Extractor Predicting: 144it [01:35,  1.56it/s]Extractor Predicting: 145it [01:36,  1.54it/s]Extractor Predicting: 146it [01:36,  1.55it/s]Extractor Predicting: 147it [01:37,  1.55it/s]Extractor Predicting: 148it [01:37,  1.56it/s]Extractor Predicting: 149it [01:38,  1.57it/s]Extractor Predicting: 150it [01:39,  1.52it/s]Extractor Predicting: 151it [01:39,  1.53it/s]Extractor Predicting: 152it [01:40,  1.53it/s]Extractor Predicting: 153it [01:41,  1.51it/s]Extractor Predicting: 154it [01:41,  1.52it/s]Extractor Predicting: 155it [01:42,  1.50it/s]Extractor Predicting: 156it [01:43,  1.48it/s]Extractor Predicting: 157it [01:43,  1.49it/s]Extractor Predicting: 158it [01:44,  1.51it/s]Extractor Predicting: 159it [01:45,  1.53it/s]Extractor Predicting: 160it [01:45,  1.55it/s]Extractor Predicting: 161it [01:46,  1.56it/s]Extractor Predicting: 162it [01:47,  1.57it/s]Extractor Predicting: 163it [01:47,  1.47it/s]Extractor Predicting: 163it [01:47,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:56,649 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:56,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:56,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:56,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:56,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:02:57,274 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:02:57,275 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:02:57,848 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:02:58,844 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:02:58,844 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:03:01,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:03:01,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:03:01,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:03:01,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:03:01,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:03:02,428 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:03:02,430 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:03:03,009 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:03:03,155 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:03:03,155 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2425629290617849,
  "recall": 0.024412713035467527,
  "score": 0.04436074492571667,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:17,  1.57it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.57it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:21,  1.57it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.53it/s]Extractor Predicting: 37it [00:23,  1.54it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:24,  1.59it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:27,  1.50it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:28,  1.49it/s]Extractor Predicting: 46it [00:29,  1.47it/s]Extractor Predicting: 47it [00:30,  1.45it/s]Extractor Predicting: 48it [00:30,  1.47it/s]Extractor Predicting: 49it [00:31,  1.46it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:32,  1.47it/s]Extractor Predicting: 52it [00:33,  1.47it/s]Extractor Predicting: 53it [00:34,  1.46it/s]Extractor Predicting: 54it [00:35,  1.42it/s]Extractor Predicting: 55it [00:35,  1.46it/s]Extractor Predicting: 56it [00:36,  1.50it/s]Extractor Predicting: 57it [00:37,  1.48it/s]Extractor Predicting: 58it [00:37,  1.48it/s]Extractor Predicting: 59it [00:38,  1.45it/s]Extractor Predicting: 60it [00:39,  1.46it/s]Extractor Predicting: 61it [00:39,  1.49it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.44it/s]Extractor Predicting: 64it [00:41,  1.45it/s]Extractor Predicting: 65it [00:42,  1.46it/s]Extractor Predicting: 66it [00:43,  1.44it/s]Extractor Predicting: 67it [00:43,  1.45it/s]Extractor Predicting: 68it [00:44,  1.44it/s]Extractor Predicting: 69it [00:45,  1.45it/s]Extractor Predicting: 70it [00:46,  1.33it/s]Extractor Predicting: 71it [00:46,  1.38it/s]Extractor Predicting: 72it [00:47,  1.40it/s]Extractor Predicting: 73it [00:48,  1.41it/s]Extractor Predicting: 74it [00:48,  1.45it/s]Extractor Predicting: 75it [00:49,  1.46it/s]Extractor Predicting: 76it [00:50,  1.48it/s]Extractor Predicting: 77it [00:50,  1.51it/s]Extractor Predicting: 78it [00:51,  1.50it/s]Extractor Predicting: 79it [00:52,  1.47it/s]Extractor Predicting: 80it [00:52,  1.50it/s]Extractor Predicting: 81it [00:53,  1.39it/s]Extractor Predicting: 82it [00:54,  1.43it/s]Extractor Predicting: 83it [00:54,  1.47it/s]Extractor Predicting: 84it [00:55,  1.44it/s]Extractor Predicting: 85it [00:56,  1.45it/s]Extractor Predicting: 86it [00:57,  1.47it/s]Extractor Predicting: 87it [00:57,  1.49it/s]Extractor Predicting: 88it [00:58,  1.47it/s]Extractor Predicting: 89it [00:59,  1.51it/s]Extractor Predicting: 90it [00:59,  1.51it/s]Extractor Predicting: 91it [01:00,  1.53it/s]Extractor Predicting: 92it [01:00,  1.53it/s]Extractor Predicting: 93it [01:01,  1.53it/s]Extractor Predicting: 94it [01:02,  1.51it/s]Extractor Predicting: 95it [01:02,  1.53it/s]Extractor Predicting: 96it [01:03,  1.55it/s]Extractor Predicting: 97it [01:04,  1.55it/s]Extractor Predicting: 98it [01:04,  1.49it/s]Extractor Predicting: 99it [01:05,  1.50it/s]Extractor Predicting: 100it [01:06,  1.49it/s]Extractor Predicting: 101it [01:06,  1.49it/s]Extractor Predicting: 102it [01:07,  1.51it/s]Extractor Predicting: 103it [01:08,  1.49it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.50it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:11,  1.47it/s]Extractor Predicting: 109it [01:12,  1.42it/s]Extractor Predicting: 110it [01:13,  1.43it/s]Extractor Predicting: 111it [01:13,  1.42it/s]Extractor Predicting: 112it [01:14,  1.45it/s]Extractor Predicting: 113it [01:15,  1.45it/s]Extractor Predicting: 114it [01:15,  1.46it/s]Extractor Predicting: 115it [01:16,  1.44it/s]Extractor Predicting: 116it [01:17,  1.42it/s]Extractor Predicting: 117it [01:17,  1.44it/s]Extractor Predicting: 118it [01:18,  1.48it/s]Extractor Predicting: 119it [01:19,  1.49it/s]Extractor Predicting: 120it [01:19,  1.52it/s]Extractor Predicting: 121it [01:20,  1.51it/s]Extractor Predicting: 122it [01:21,  1.54it/s]Extractor Predicting: 123it [01:21,  1.56it/s]Extractor Predicting: 124it [01:22,  1.58it/s]Extractor Predicting: 125it [01:23,  1.55it/s]Extractor Predicting: 126it [01:23,  1.52it/s]Extractor Predicting: 127it [01:24,  1.52it/s]Extractor Predicting: 128it [01:25,  1.51it/s]Extractor Predicting: 129it [01:25,  1.51it/s]Extractor Predicting: 130it [01:26,  1.50it/s]Extractor Predicting: 131it [01:27,  1.56it/s]Extractor Predicting: 132it [01:27,  1.53it/s]Extractor Predicting: 133it [01:28,  1.54it/s]Extractor Predicting: 134it [01:28,  1.56it/s]Extractor Predicting: 135it [01:29,  1.55it/s]Extractor Predicting: 136it [01:30,  1.56it/s]Extractor Predicting: 137it [01:30,  1.60it/s]Extractor Predicting: 138it [01:31,  1.58it/s]Extractor Predicting: 139it [01:32,  1.57it/s]Extractor Predicting: 140it [01:32,  1.56it/s]Extractor Predicting: 141it [01:33,  1.57it/s]Extractor Predicting: 142it [01:34,  1.55it/s]Extractor Predicting: 143it [01:34,  1.53it/s]Extractor Predicting: 144it [01:35,  1.53it/s]Extractor Predicting: 145it [01:36,  1.49it/s]Extractor Predicting: 146it [01:36,  1.45it/s]Extractor Predicting: 147it [01:37,  1.48it/s]Extractor Predicting: 148it [01:38,  1.47it/s]Extractor Predicting: 149it [01:38,  1.50it/s]Extractor Predicting: 150it [01:39,  1.50it/s]Extractor Predicting: 151it [01:40,  1.35it/s]Extractor Predicting: 152it [01:41,  1.39it/s]Extractor Predicting: 153it [01:41,  1.45it/s]Extractor Predicting: 154it [01:42,  1.48it/s]Extractor Predicting: 155it [01:42,  1.52it/s]Extractor Predicting: 156it [01:43,  1.49it/s]Extractor Predicting: 157it [01:44,  1.50it/s]Extractor Predicting: 158it [01:44,  1.51it/s]Extractor Predicting: 159it [01:45,  1.51it/s]Extractor Predicting: 160it [01:46,  1.54it/s]Extractor Predicting: 161it [01:47,  1.47it/s]Extractor Predicting: 162it [01:47,  1.52it/s]Extractor Predicting: 163it [01:48,  1.54it/s]Extractor Predicting: 164it [01:48,  1.52it/s]Extractor Predicting: 165it [01:49,  1.58it/s]Extractor Predicting: 166it [01:50,  1.58it/s]Extractor Predicting: 167it [01:50,  1.57it/s]Extractor Predicting: 168it [01:51,  1.61it/s]Extractor Predicting: 169it [01:51,  1.62it/s]Extractor Predicting: 170it [01:52,  1.58it/s]Extractor Predicting: 171it [01:53,  1.56it/s]Extractor Predicting: 172it [01:53,  1.56it/s]Extractor Predicting: 173it [01:54,  1.53it/s]Extractor Predicting: 174it [01:55,  1.57it/s]Extractor Predicting: 175it [01:55,  1.56it/s]Extractor Predicting: 176it [01:56,  1.54it/s]Extractor Predicting: 177it [01:57,  1.52it/s]Extractor Predicting: 178it [01:57,  1.52it/s]Extractor Predicting: 179it [01:58,  1.59it/s]Extractor Predicting: 180it [01:59,  1.57it/s]Extractor Predicting: 181it [01:59,  1.56it/s]Extractor Predicting: 182it [02:00,  1.54it/s]Extractor Predicting: 183it [02:01,  1.54it/s]Extractor Predicting: 184it [02:01,  1.53it/s]Extractor Predicting: 185it [02:02,  1.56it/s]Extractor Predicting: 186it [02:03,  1.54it/s]Extractor Predicting: 187it [02:03,  1.53it/s]Extractor Predicting: 188it [02:04,  1.51it/s]Extractor Predicting: 189it [02:05,  1.49it/s]Extractor Predicting: 190it [02:05,  1.46it/s]Extractor Predicting: 191it [02:06,  1.47it/s]Extractor Predicting: 192it [02:07,  1.49it/s]Extractor Predicting: 193it [02:07,  1.52it/s]Extractor Predicting: 194it [02:08,  1.53it/s]Extractor Predicting: 195it [02:09,  1.51it/s]Extractor Predicting: 196it [02:09,  1.53it/s]Extractor Predicting: 197it [02:10,  1.51it/s]Extractor Predicting: 198it [02:10,  1.53it/s]Extractor Predicting: 199it [02:11,  1.52it/s]Extractor Predicting: 200it [02:12,  1.52it/s]Extractor Predicting: 201it [02:12,  1.52it/s]Extractor Predicting: 202it [02:13,  1.53it/s]Extractor Predicting: 203it [02:14,  1.53it/s]Extractor Predicting: 204it [02:14,  1.53it/s]Extractor Predicting: 205it [02:15,  1.53it/s]Extractor Predicting: 206it [02:16,  1.52it/s]Extractor Predicting: 207it [02:16,  1.53it/s]Extractor Predicting: 208it [02:17,  1.54it/s]Extractor Predicting: 209it [02:18,  1.54it/s]Extractor Predicting: 210it [02:18,  1.53it/s]Extractor Predicting: 211it [02:19,  1.52it/s]Extractor Predicting: 212it [02:20,  1.54it/s]Extractor Predicting: 213it [02:20,  1.53it/s]Extractor Predicting: 214it [02:21,  1.53it/s]Extractor Predicting: 215it [02:22,  1.49it/s]Extractor Predicting: 216it [02:22,  1.52it/s]Extractor Predicting: 217it [02:23,  1.54it/s]Extractor Predicting: 218it [02:24,  1.54it/s]Extractor Predicting: 219it [02:24,  1.52it/s]Extractor Predicting: 220it [02:25,  1.53it/s]Extractor Predicting: 221it [02:26,  1.51it/s]Extractor Predicting: 222it [02:26,  1.53it/s]Extractor Predicting: 223it [02:27,  1.47it/s]Extractor Predicting: 224it [02:28,  1.48it/s]Extractor Predicting: 225it [02:28,  1.46it/s]Extractor Predicting: 226it [02:29,  1.48it/s]Extractor Predicting: 227it [02:30,  1.44it/s]Extractor Predicting: 228it [02:30,  1.39it/s]Extractor Predicting: 229it [02:31,  1.41it/s]Extractor Predicting: 230it [02:32,  1.43it/s]Extractor Predicting: 231it [02:33,  1.44it/s]Extractor Predicting: 232it [02:33,  1.45it/s]Extractor Predicting: 233it [02:34,  1.45it/s]Extractor Predicting: 234it [02:35,  1.47it/s]Extractor Predicting: 235it [02:35,  1.47it/s]Extractor Predicting: 236it [02:36,  1.49it/s]Extractor Predicting: 237it [02:37,  1.48it/s]Extractor Predicting: 238it [02:37,  1.54it/s]Extractor Predicting: 239it [02:38,  1.56it/s]Extractor Predicting: 240it [02:38,  1.61it/s]Extractor Predicting: 241it [02:39,  1.60it/s]Extractor Predicting: 242it [02:40,  1.59it/s]Extractor Predicting: 243it [02:40,  1.55it/s]Extractor Predicting: 244it [02:41,  1.56it/s]Extractor Predicting: 245it [02:42,  1.59it/s]Extractor Predicting: 246it [02:42,  1.56it/s]Extractor Predicting: 247it [02:43,  1.57it/s]Extractor Predicting: 248it [02:43,  1.59it/s]Extractor Predicting: 249it [02:44,  1.63it/s]Extractor Predicting: 250it [02:45,  1.64it/s]Extractor Predicting: 251it [02:45,  1.69it/s]Extractor Predicting: 252it [02:46,  1.70it/s]Extractor Predicting: 253it [02:46,  1.67it/s]Extractor Predicting: 254it [02:47,  1.68it/s]Extractor Predicting: 255it [02:48,  1.63it/s]Extractor Predicting: 256it [02:48,  1.67it/s]Extractor Predicting: 257it [02:49,  1.62it/s]Extractor Predicting: 258it [02:50,  1.42it/s]Extractor Predicting: 259it [02:50,  1.42it/s]Extractor Predicting: 260it [02:51,  1.50it/s]Extractor Predicting: 261it [02:52,  1.49it/s]Extractor Predicting: 262it [02:52,  1.52it/s]Extractor Predicting: 263it [02:53,  1.57it/s]Extractor Predicting: 264it [02:54,  1.59it/s]Extractor Predicting: 265it [02:54,  1.59it/s]Extractor Predicting: 266it [02:55,  1.62it/s]Extractor Predicting: 267it [02:55,  1.65it/s]Extractor Predicting: 268it [02:56,  1.64it/s]Extractor Predicting: 269it [02:57,  1.60it/s]Extractor Predicting: 270it [02:57,  1.62it/s]Extractor Predicting: 271it [02:58,  1.60it/s]Extractor Predicting: 272it [02:59,  1.58it/s]Extractor Predicting: 273it [02:59,  1.63it/s]Extractor Predicting: 274it [03:00,  1.62it/s]Extractor Predicting: 275it [03:00,  1.60it/s]Extractor Predicting: 276it [03:01,  1.58it/s]Extractor Predicting: 277it [03:02,  1.59it/s]Extractor Predicting: 278it [03:02,  1.61it/s]Extractor Predicting: 279it [03:03,  1.56it/s]Extractor Predicting: 280it [03:03,  1.62it/s]Extractor Predicting: 281it [03:04,  1.63it/s]Extractor Predicting: 282it [03:05,  1.65it/s]Extractor Predicting: 283it [03:05,  1.65it/s]Extractor Predicting: 284it [03:06,  1.64it/s]Extractor Predicting: 285it [03:07,  1.65it/s]Extractor Predicting: 286it [03:07,  1.65it/s]Extractor Predicting: 287it [03:08,  1.64it/s]Extractor Predicting: 288it [03:08,  1.60it/s]Extractor Predicting: 289it [03:09,  1.60it/s]Extractor Predicting: 290it [03:10,  1.53it/s]Extractor Predicting: 291it [03:10,  1.56it/s]Extractor Predicting: 292it [03:11,  1.60it/s]Extractor Predicting: 293it [03:12,  1.61it/s]Extractor Predicting: 294it [03:12,  1.61it/s]Extractor Predicting: 295it [03:13,  1.62it/s]Extractor Predicting: 296it [03:14,  1.54it/s]Extractor Predicting: 297it [03:14,  1.52it/s]Extractor Predicting: 298it [03:15,  1.47it/s]Extractor Predicting: 299it [03:16,  1.48it/s]Extractor Predicting: 300it [03:16,  1.50it/s]Extractor Predicting: 301it [03:17,  1.51it/s]Extractor Predicting: 302it [03:18,  1.52it/s]Extractor Predicting: 303it [03:18,  1.48it/s]Extractor Predicting: 304it [03:19,  1.49it/s]Extractor Predicting: 305it [03:20,  1.52it/s]Extractor Predicting: 306it [03:20,  1.49it/s]Extractor Predicting: 307it [03:21,  1.46it/s]Extractor Predicting: 308it [03:22,  1.46it/s]Extractor Predicting: 309it [03:22,  1.49it/s]Extractor Predicting: 310it [03:23,  1.51it/s]Extractor Predicting: 311it [03:24,  1.53it/s]Extractor Predicting: 312it [03:24,  1.51it/s]Extractor Predicting: 313it [03:25,  1.48it/s]Extractor Predicting: 314it [03:26,  1.46it/s]Extractor Predicting: 315it [03:26,  1.46it/s]Extractor Predicting: 316it [03:27,  1.46it/s]Extractor Predicting: 317it [03:28,  1.46it/s]Extractor Predicting: 318it [03:28,  1.47it/s]Extractor Predicting: 319it [03:29,  1.48it/s]Extractor Predicting: 320it [03:30,  1.49it/s]Extractor Predicting: 321it [03:30,  1.54it/s]Extractor Predicting: 322it [03:31,  1.53it/s]Extractor Predicting: 323it [03:32,  1.51it/s]Extractor Predicting: 324it [03:32,  1.50it/s]Extractor Predicting: 325it [03:33,  1.50it/s]Extractor Predicting: 326it [03:34,  1.48it/s]Extractor Predicting: 327it [03:34,  1.46it/s]Extractor Predicting: 328it [03:35,  1.47it/s]Extractor Predicting: 329it [03:36,  1.51it/s]Extractor Predicting: 330it [03:36,  1.54it/s]Extractor Predicting: 331it [03:37,  1.69it/s]Extractor Predicting: 331it [03:37,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:06:48,422 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:06:48,427 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:06:48,427 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:06:48,427 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:06:48,427 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:06:48,745 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:06:48,746 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:06:49,315 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:06:50,325 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:06:50,325 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:06:53,209 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:06:53,213 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:06:53,213 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:06:53,213 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:06:53,214 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:06:53,851 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:06:53,852 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:06:54,441 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:06:54,593 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:06:54,593 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5377932232841007,
  "recall": 0.0780481654268062,
  "score": 0.13631358731556928,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:04,  1.42it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.43it/s]Extractor Predicting: 10it [00:06,  1.43it/s]Extractor Predicting: 11it [00:07,  1.42it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:09,  1.43it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:10,  1.41it/s]Extractor Predicting: 16it [00:11,  1.43it/s]Extractor Predicting: 17it [00:11,  1.36it/s]Extractor Predicting: 18it [00:12,  1.40it/s]Extractor Predicting: 19it [00:13,  1.40it/s]Extractor Predicting: 20it [00:14,  1.39it/s]Extractor Predicting: 21it [00:14,  1.38it/s]Extractor Predicting: 22it [00:15,  1.38it/s]Extractor Predicting: 23it [00:16,  1.37it/s]Extractor Predicting: 24it [00:16,  1.38it/s]Extractor Predicting: 25it [00:17,  1.39it/s]Extractor Predicting: 26it [00:18,  1.43it/s]Extractor Predicting: 27it [00:18,  1.45it/s]Extractor Predicting: 28it [00:19,  1.43it/s]Extractor Predicting: 29it [00:20,  1.44it/s]Extractor Predicting: 30it [00:21,  1.45it/s]Extractor Predicting: 31it [00:21,  1.44it/s]Extractor Predicting: 32it [00:22,  1.47it/s]Extractor Predicting: 33it [00:23,  1.48it/s]Extractor Predicting: 34it [00:23,  1.47it/s]Extractor Predicting: 35it [00:24,  1.48it/s]Extractor Predicting: 36it [00:25,  1.48it/s]Extractor Predicting: 37it [00:25,  1.48it/s]Extractor Predicting: 38it [00:26,  1.49it/s]Extractor Predicting: 39it [00:27,  1.44it/s]Extractor Predicting: 40it [00:27,  1.44it/s]Extractor Predicting: 41it [00:28,  1.46it/s]Extractor Predicting: 42it [00:29,  1.48it/s]Extractor Predicting: 43it [00:29,  1.47it/s]Extractor Predicting: 44it [00:30,  1.48it/s]Extractor Predicting: 45it [00:31,  1.47it/s]Extractor Predicting: 46it [00:31,  1.48it/s]Extractor Predicting: 47it [00:32,  1.47it/s]Extractor Predicting: 48it [00:33,  1.50it/s]Extractor Predicting: 49it [00:33,  1.50it/s]Extractor Predicting: 50it [00:34,  1.53it/s]Extractor Predicting: 51it [00:35,  1.53it/s]Extractor Predicting: 52it [00:35,  1.52it/s]Extractor Predicting: 53it [00:36,  1.45it/s]Extractor Predicting: 54it [00:37,  1.48it/s]Extractor Predicting: 55it [00:37,  1.50it/s]Extractor Predicting: 56it [00:38,  1.56it/s]Extractor Predicting: 57it [00:39,  1.60it/s]Extractor Predicting: 58it [00:39,  1.64it/s]Extractor Predicting: 59it [00:40,  1.72it/s]Extractor Predicting: 60it [00:40,  1.79it/s]Extractor Predicting: 61it [00:41,  1.83it/s]Extractor Predicting: 62it [00:41,  1.82it/s]Extractor Predicting: 63it [00:42,  1.85it/s]Extractor Predicting: 64it [00:42,  1.83it/s]Extractor Predicting: 65it [00:43,  1.82it/s]Extractor Predicting: 66it [00:43,  1.82it/s]Extractor Predicting: 67it [00:44,  1.81it/s]Extractor Predicting: 68it [00:45,  1.83it/s]Extractor Predicting: 69it [00:45,  1.86it/s]Extractor Predicting: 70it [00:46,  1.82it/s]Extractor Predicting: 71it [00:46,  1.82it/s]Extractor Predicting: 72it [00:47,  1.84it/s]Extractor Predicting: 73it [00:47,  1.87it/s]Extractor Predicting: 74it [00:48,  1.88it/s]Extractor Predicting: 75it [00:48,  1.87it/s]Extractor Predicting: 76it [00:49,  1.84it/s]Extractor Predicting: 77it [00:49,  1.90it/s]Extractor Predicting: 78it [00:50,  1.84it/s]Extractor Predicting: 79it [00:50,  1.84it/s]Extractor Predicting: 80it [00:51,  1.82it/s]Extractor Predicting: 81it [00:52,  1.81it/s]Extractor Predicting: 82it [00:52,  1.83it/s]Extractor Predicting: 83it [00:53,  1.84it/s]Extractor Predicting: 84it [00:53,  1.84it/s]Extractor Predicting: 85it [00:54,  1.85it/s]Extractor Predicting: 86it [00:54,  1.72it/s]Extractor Predicting: 87it [00:55,  1.66it/s]Extractor Predicting: 88it [00:56,  1.63it/s]Extractor Predicting: 89it [00:56,  1.61it/s]Extractor Predicting: 90it [00:57,  1.63it/s]Extractor Predicting: 91it [00:58,  1.60it/s]Extractor Predicting: 92it [00:58,  1.58it/s]Extractor Predicting: 93it [00:59,  1.59it/s]Extractor Predicting: 94it [01:00,  1.58it/s]Extractor Predicting: 95it [01:00,  1.61it/s]Extractor Predicting: 96it [01:01,  1.60it/s]Extractor Predicting: 97it [01:01,  1.61it/s]Extractor Predicting: 98it [01:02,  1.61it/s]Extractor Predicting: 99it [01:03,  1.59it/s]Extractor Predicting: 100it [01:03,  1.53it/s]Extractor Predicting: 101it [01:04,  1.56it/s]Extractor Predicting: 102it [01:05,  1.55it/s]Extractor Predicting: 103it [01:05,  1.50it/s]Extractor Predicting: 104it [01:06,  1.49it/s]Extractor Predicting: 105it [01:07,  1.48it/s]Extractor Predicting: 106it [01:07,  1.47it/s]Extractor Predicting: 107it [01:08,  1.33it/s]Extractor Predicting: 108it [01:09,  1.32it/s]Extractor Predicting: 108it [01:09,  1.55it/s]
[INFO|configuration_utils.py:515] 2023-08-29 06:08:05,464 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:08:05,465 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 06:08:05,470 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:08:05,471 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 06:08:05,474 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 06:08:08,639 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 06:08:08,641 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 06:08:08,652 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:08:08,652 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 06:08:08,660 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:08:08,663 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:08:08,663 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:08:08,663 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:08:08,663 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:08:08,663 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:08:08,663 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.8497790868924889,
  "recall": 0.09242351433605639,
  "score": 0.166714822305692,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 06:08:08,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:09,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:10,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:10,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:11,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:12,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:13,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:13,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:14,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:15,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:15,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:16,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:17,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:17,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:18,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:19,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:19,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:20,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:21,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:21,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:22,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:18, 14.21s/it][WARNING|generation_utils.py:914] 2023-08-29 06:08:23,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:23,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:24,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:25,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:25,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:26,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:27,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:28,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:28,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:29,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:29,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:30,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:31,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:32,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:32,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:33,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:34,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:34,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:35,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:36,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:28<03:02, 14.02s/it][WARNING|generation_utils.py:914] 2023-08-29 06:08:36,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:37,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:38,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:39,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:39,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:40,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:41,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:42,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:43,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:43,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:44,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:45,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:45,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:46,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:47,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:48,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:48,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:49,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:50,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:50,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:51, 14.32s/it][WARNING|generation_utils.py:914] 2023-08-29 06:08:51,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:52,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:53,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:53,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:54,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:55,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:55,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:56,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:57,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:57,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:58,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:59,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:00,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:01,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:01,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:02,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:03,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:04,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:04,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:05,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:06,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:57<02:41, 14.67s/it][WARNING|generation_utils.py:914] 2023-08-29 06:09:06,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:07,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:08,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:08,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:09,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:10,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:11,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:12,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:12,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:13,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:14,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:14,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:15,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:16,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:17,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:17,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:18,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:19,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:19,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:20,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:21,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:12<02:27, 14.77s/it][WARNING|generation_utils.py:914] 2023-08-29 06:09:21,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:22,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:23,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:23,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:24,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:25,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:26,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:26,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:27,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:28,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:28,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:29,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:30,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:31,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:31,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:32,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:32,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:33,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:34,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:34,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:35,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:36,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:27<02:13, 14.86s/it][WARNING|generation_utils.py:914] 2023-08-29 06:09:36,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:37,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:38,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:38,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:39,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:40,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:40,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:41,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:42,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:42,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:44,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:44,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:45,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:46,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:47,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:47,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:48,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:49,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:50,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:50,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:51,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:43<02:00, 15.01s/it][WARNING|generation_utils.py:914] 2023-08-29 06:09:52,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:52,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:53,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:54,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:55,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:55,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:56,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:57,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:57,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:09:58,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:00,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:00,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:01,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:02,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:02,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:03,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:04,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:05,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:05,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:06,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:07,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:59<01:46, 15.25s/it][WARNING|generation_utils.py:914] 2023-08-29 06:10:07,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:08,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:09,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:09,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:10,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:11,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:12,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:12,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:13,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:14,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:14,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:15,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:15,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:16,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:17,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:17,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:18,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:19,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:19,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:20,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:12<01:28, 14.67s/it][WARNING|generation_utils.py:914] 2023-08-29 06:10:21,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:21,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:23,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:23,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:24,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:25,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:26,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:26,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:27,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:28,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:28,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:29,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:30,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:30,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:31,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:32,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:33,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:33,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:34,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:35,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:26<01:13, 14.62s/it][WARNING|generation_utils.py:914] 2023-08-29 06:10:35,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:36,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:37,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:37,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:38,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:39,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:39,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:40,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:41,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:41,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:42,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:43,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:44,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:44,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:45,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:45,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:46,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:47,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:47,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:48,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:49,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:49,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:41<00:58, 14.69s/it][WARNING|generation_utils.py:914] 2023-08-29 06:10:50,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:51,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:52,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:52,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:53,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:54,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:55,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:55,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:56,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:57,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:57,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:58,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:10:59,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:00,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:00,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:01,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:02,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:03,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:04,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:05,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:05,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:57<00:45, 15.04s/it][WARNING|generation_utils.py:914] 2023-08-29 06:11:06,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:07,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:08,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:08,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:09,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:10,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:11,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:11,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:12,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:13,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:13,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:14,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:15,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:16,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:16,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:17,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:18,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:19,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:19,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:20,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:21,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:13<00:30, 15.21s/it][WARNING|generation_utils.py:914] 2023-08-29 06:11:22,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:22,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:23,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:23,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:24,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:25,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:25,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:26,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:27,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:27,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:28,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:28,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:29,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:30,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:30,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:31,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:32,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:32,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:33,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:34,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:34,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:26<00:14, 14.58s/it][WARNING|generation_utils.py:914] 2023-08-29 06:11:35,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:36,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:37,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:37,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:38,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:39,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:40,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:40,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:41,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:42,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:43,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:43,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:44,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:45,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:46,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:46,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:47,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:48,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:49,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:49,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:50,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:11:51,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:43<00:00, 15.24s/it]Generating: 100%|██████████| 15/15 [03:43<00:00, 14.88s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:58,532 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:58,538 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:58,539 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:58,539 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:58,539 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:11:59,159 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:11:59,160 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:11:59,781 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:12:00,829 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:12:00,829 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:12:03,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:12:03,686 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:12:03,686 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:12:03,686 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:12:03,686 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:12:04,318 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:12:04,319 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:12:04,894 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:12:05,051 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:12:05,051 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : made from material .', 'success_rate': 0.90625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : cast member .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9360119047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 630, 'raw': 672}
{'prompt': 'Relation : league .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9546875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.959375, 'errors': {''}}
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major in the Parliament of England . Head Entity : John Major , Tail Entity : Parliamentary Labour Party .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.9017857142857143, 'errors': {''}}
['Relation : twinned administrative body . Context : Later in 1492 the county became a part of a county administrative unit , the Township of the City of New York , which included the township itself . Head Entity : Township of the City of New York , Tail Entity : Township of New York .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 8625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.60it/s]Extractor Estimating: 2it [00:01,  1.42it/s]Extractor Estimating: 3it [00:01,  1.53it/s]Extractor Estimating: 4it [00:02,  1.57it/s]Extractor Estimating: 5it [00:03,  1.56it/s]Extractor Estimating: 6it [00:03,  1.57it/s]Extractor Estimating: 7it [00:04,  1.64it/s]Extractor Estimating: 8it [00:04,  1.70it/s]Extractor Estimating: 9it [00:05,  1.75it/s]Extractor Estimating: 10it [00:06,  1.76it/s]Extractor Estimating: 11it [00:06,  1.68it/s]Extractor Estimating: 12it [00:07,  1.69it/s]Extractor Estimating: 13it [00:07,  1.67it/s]Extractor Estimating: 14it [00:08,  1.70it/s]Extractor Estimating: 15it [00:09,  1.65it/s]Extractor Estimating: 16it [00:09,  1.61it/s]Extractor Estimating: 17it [00:10,  1.63it/s]Extractor Estimating: 18it [00:10,  1.65it/s]Extractor Estimating: 19it [00:11,  1.52it/s]Extractor Estimating: 20it [00:12,  1.57it/s]Extractor Estimating: 21it [00:12,  1.59it/s]Extractor Estimating: 22it [00:13,  1.57it/s]Extractor Estimating: 23it [00:14,  1.60it/s]Extractor Estimating: 24it [00:14,  1.61it/s]Extractor Estimating: 25it [00:15,  1.62it/s]Extractor Estimating: 26it [00:15,  1.66it/s]Extractor Estimating: 27it [00:16,  1.65it/s]Extractor Estimating: 28it [00:17,  1.66it/s]Extractor Estimating: 29it [00:17,  1.64it/s]Extractor Estimating: 30it [00:18,  1.48it/s]Extractor Estimating: 31it [00:19,  1.47it/s]Extractor Estimating: 32it [00:19,  1.55it/s]Extractor Estimating: 33it [00:20,  1.54it/s]Extractor Estimating: 34it [00:21,  1.57it/s]Extractor Estimating: 35it [00:21,  1.57it/s]Extractor Estimating: 36it [00:22,  1.61it/s]Extractor Estimating: 37it [00:22,  1.66it/s]Extractor Estimating: 38it [00:23,  1.70it/s]Extractor Estimating: 39it [00:24,  1.65it/s]Extractor Estimating: 40it [00:24,  1.66it/s]Extractor Estimating: 41it [00:25,  1.45it/s]Extractor Estimating: 42it [00:26,  1.50it/s]Extractor Estimating: 43it [00:26,  1.55it/s]Extractor Estimating: 44it [00:27,  1.59it/s]Extractor Estimating: 45it [00:28,  1.59it/s]Extractor Estimating: 46it [00:28,  1.66it/s]Extractor Estimating: 47it [00:29,  1.63it/s]Extractor Estimating: 48it [00:29,  1.62it/s]Extractor Estimating: 49it [00:30,  1.63it/s]Extractor Estimating: 50it [00:31,  1.61it/s]Extractor Estimating: 51it [00:31,  1.60it/s]Extractor Estimating: 52it [00:32,  1.55it/s]Extractor Estimating: 53it [00:33,  1.57it/s]Extractor Estimating: 54it [00:33,  1.52it/s]Extractor Estimating: 55it [00:34,  1.50it/s]Extractor Estimating: 56it [00:35,  1.47it/s]Extractor Estimating: 57it [00:35,  1.48it/s]Extractor Estimating: 58it [00:36,  1.49it/s]Extractor Estimating: 59it [00:37,  1.49it/s]Extractor Estimating: 60it [00:37,  1.48it/s]Extractor Estimating: 61it [00:38,  1.51it/s]Extractor Estimating: 62it [00:39,  1.51it/s]Extractor Estimating: 63it [00:39,  1.55it/s]Extractor Estimating: 64it [00:40,  1.50it/s]Extractor Estimating: 65it [00:41,  1.51it/s]Extractor Estimating: 66it [00:41,  1.51it/s]Extractor Estimating: 67it [00:42,  1.56it/s]Extractor Estimating: 68it [00:43,  1.54it/s]Extractor Estimating: 69it [00:43,  1.54it/s]Extractor Estimating: 70it [00:44,  1.55it/s]Extractor Estimating: 71it [00:44,  1.56it/s]Extractor Estimating: 72it [00:45,  1.54it/s]Extractor Estimating: 73it [00:46,  1.56it/s]Extractor Estimating: 74it [00:46,  1.58it/s]Extractor Estimating: 75it [00:47,  1.52it/s]Extractor Estimating: 76it [00:48,  1.54it/s]Extractor Estimating: 77it [00:48,  1.56it/s]Extractor Estimating: 78it [00:49,  1.58it/s]Extractor Estimating: 79it [00:50,  1.56it/s]Extractor Estimating: 80it [00:50,  1.54it/s]Extractor Estimating: 81it [00:51,  1.55it/s]Extractor Estimating: 82it [00:52,  1.55it/s]Extractor Estimating: 83it [00:52,  1.60it/s]Extractor Estimating: 84it [00:53,  1.61it/s]Extractor Estimating: 85it [00:53,  1.65it/s]Extractor Estimating: 86it [00:54,  1.63it/s]Extractor Estimating: 87it [00:55,  1.60it/s]Extractor Estimating: 88it [00:55,  1.48it/s]Extractor Estimating: 89it [00:56,  1.49it/s]Extractor Estimating: 90it [00:57,  1.46it/s]Extractor Estimating: 91it [00:57,  1.47it/s]Extractor Estimating: 92it [00:58,  1.55it/s]Extractor Estimating: 93it [00:59,  1.62it/s]Extractor Estimating: 94it [00:59,  1.60it/s]Extractor Estimating: 95it [01:00,  1.57it/s]Extractor Estimating: 96it [01:00,  1.59it/s]Extractor Estimating: 97it [01:01,  1.59it/s]Extractor Estimating: 98it [01:02,  1.60it/s]Extractor Estimating: 99it [01:02,  1.62it/s]Extractor Estimating: 100it [01:03,  1.66it/s]Extractor Estimating: 101it [01:03,  1.65it/s]Extractor Estimating: 102it [01:04,  1.60it/s]Extractor Estimating: 103it [01:05,  1.58it/s]Extractor Estimating: 104it [01:05,  1.58it/s]Extractor Estimating: 105it [01:06,  1.61it/s]Extractor Estimating: 106it [01:07,  1.64it/s]Extractor Estimating: 107it [01:07,  1.62it/s]Extractor Estimating: 108it [01:08,  1.57it/s]Extractor Estimating: 109it [01:09,  1.60it/s]Extractor Estimating: 110it [01:09,  1.56it/s]Extractor Estimating: 111it [01:10,  1.49it/s]Extractor Estimating: 112it [01:11,  1.50it/s]Extractor Estimating: 113it [01:11,  1.57it/s]Extractor Estimating: 114it [01:12,  1.60it/s]Extractor Estimating: 115it [01:12,  1.61it/s]Extractor Estimating: 116it [01:13,  1.60it/s]Extractor Estimating: 117it [01:14,  1.60it/s]Extractor Estimating: 118it [01:14,  1.56it/s]Extractor Estimating: 119it [01:15,  1.58it/s]Extractor Estimating: 120it [01:16,  1.55it/s]Extractor Estimating: 121it [01:16,  1.58it/s]Extractor Estimating: 122it [01:17,  1.64it/s]Extractor Estimating: 123it [01:17,  1.63it/s]Extractor Estimating: 124it [01:18,  1.63it/s]Extractor Estimating: 125it [01:19,  1.65it/s]Extractor Estimating: 126it [01:19,  1.64it/s]Extractor Estimating: 127it [01:20,  1.68it/s]Extractor Estimating: 128it [01:20,  1.67it/s]Extractor Estimating: 129it [01:21,  1.69it/s]Extractor Estimating: 130it [01:22,  1.67it/s]Extractor Estimating: 131it [01:22,  1.66it/s]Extractor Estimating: 132it [01:23,  1.66it/s]Extractor Estimating: 133it [01:23,  1.70it/s]Extractor Estimating: 134it [01:24,  1.67it/s]Extractor Estimating: 135it [01:25,  1.60it/s]Extractor Estimating: 136it [01:25,  1.61it/s]Extractor Estimating: 137it [01:26,  1.59it/s]Extractor Estimating: 138it [01:27,  1.62it/s]Extractor Estimating: 139it [01:27,  1.63it/s]Extractor Estimating: 140it [01:28,  1.61it/s]Extractor Estimating: 141it [01:28,  1.63it/s]Extractor Estimating: 142it [01:29,  1.69it/s]Extractor Estimating: 143it [01:29,  1.70it/s]Extractor Estimating: 144it [01:30,  1.65it/s]Extractor Estimating: 145it [01:31,  1.64it/s]Extractor Estimating: 146it [01:31,  1.63it/s]Extractor Estimating: 147it [01:32,  1.63it/s]Extractor Estimating: 148it [01:33,  1.65it/s]Extractor Estimating: 149it [01:33,  1.66it/s]Extractor Estimating: 150it [01:34,  1.66it/s]Extractor Estimating: 151it [01:34,  1.59it/s]Extractor Estimating: 152it [01:35,  1.59it/s]Extractor Estimating: 153it [01:36,  1.64it/s]Extractor Estimating: 154it [01:36,  1.63it/s]Extractor Estimating: 155it [01:37,  1.61it/s]Extractor Estimating: 156it [01:38,  1.61it/s]Extractor Estimating: 157it [01:38,  1.60it/s]Extractor Estimating: 158it [01:39,  1.57it/s]Extractor Estimating: 159it [01:39,  1.57it/s]Extractor Estimating: 160it [01:40,  1.57it/s]Extractor Estimating: 161it [01:41,  1.57it/s]Extractor Estimating: 162it [01:42,  1.35it/s]Extractor Estimating: 163it [01:42,  1.36it/s]Extractor Estimating: 164it [01:43,  1.36it/s]Extractor Estimating: 165it [01:44,  1.42it/s]Extractor Estimating: 166it [01:44,  1.45it/s]Extractor Estimating: 167it [01:45,  1.49it/s]Extractor Estimating: 168it [01:46,  1.53it/s]Extractor Estimating: 169it [01:46,  1.48it/s]Extractor Estimating: 170it [01:47,  1.36it/s]Extractor Estimating: 171it [01:48,  1.40it/s]Extractor Estimating: 172it [01:49,  1.42it/s]Extractor Estimating: 173it [01:49,  1.49it/s]Extractor Estimating: 174it [01:50,  1.52it/s]Extractor Estimating: 175it [01:51,  1.47it/s]Extractor Estimating: 176it [01:51,  1.46it/s]Extractor Estimating: 177it [01:52,  1.42it/s]Extractor Estimating: 178it [01:53,  1.46it/s]Extractor Estimating: 179it [01:53,  1.48it/s]Extractor Estimating: 180it [01:54,  1.45it/s]Extractor Estimating: 181it [01:55,  1.45it/s]Extractor Estimating: 182it [01:55,  1.49it/s]Extractor Estimating: 183it [01:56,  1.49it/s]Extractor Estimating: 184it [01:57,  1.46it/s]Extractor Estimating: 185it [01:58,  1.38it/s]Extractor Estimating: 186it [01:58,  1.41it/s]Extractor Estimating: 187it [01:59,  1.39it/s]Extractor Estimating: 188it [02:00,  1.34it/s]Extractor Estimating: 189it [02:00,  1.38it/s]Extractor Estimating: 190it [02:01,  1.41it/s]Extractor Estimating: 191it [02:02,  1.47it/s]Extractor Estimating: 192it [02:02,  1.46it/s]Extractor Estimating: 193it [02:03,  1.48it/s]Extractor Estimating: 194it [02:04,  1.45it/s]Extractor Estimating: 195it [02:05,  1.42it/s]Extractor Estimating: 196it [02:05,  1.40it/s]Extractor Estimating: 197it [02:06,  1.41it/s]Extractor Estimating: 198it [02:07,  1.47it/s]Extractor Estimating: 199it [02:07,  1.43it/s]Extractor Estimating: 200it [02:08,  1.48it/s]Extractor Estimating: 201it [02:09,  1.55it/s]Extractor Estimating: 202it [02:09,  1.61it/s]Extractor Estimating: 203it [02:10,  1.68it/s]Extractor Estimating: 204it [02:10,  1.63it/s]Extractor Estimating: 205it [02:11,  1.59it/s]Extractor Estimating: 206it [02:12,  1.62it/s]Extractor Estimating: 207it [02:12,  1.75it/s]Extractor Estimating: 208it [02:13,  1.76it/s]Extractor Estimating: 209it [02:13,  1.75it/s]Extractor Estimating: 210it [02:14,  1.77it/s]Extractor Estimating: 211it [02:14,  1.78it/s]Extractor Estimating: 212it [02:15,  1.80it/s]Extractor Estimating: 213it [02:15,  1.79it/s]Extractor Estimating: 214it [02:16,  1.82it/s]Extractor Estimating: 215it [02:16,  1.80it/s]Extractor Estimating: 216it [02:17,  1.81it/s]Extractor Estimating: 217it [02:18,  1.84it/s]Extractor Estimating: 218it [02:18,  1.82it/s]Extractor Estimating: 219it [02:19,  1.79it/s]Extractor Estimating: 220it [02:19,  1.86it/s]Extractor Estimating: 221it [02:20,  1.75it/s]Extractor Estimating: 222it [02:20,  1.77it/s]Extractor Estimating: 223it [02:21,  1.78it/s]Extractor Estimating: 224it [02:21,  1.82it/s]Extractor Estimating: 225it [02:22,  1.80it/s]Extractor Estimating: 226it [02:23,  1.76it/s]Extractor Estimating: 227it [02:23,  1.77it/s]Extractor Estimating: 228it [02:24,  1.73it/s]Extractor Estimating: 229it [02:24,  1.78it/s]Extractor Estimating: 230it [02:25,  1.68it/s]Extractor Estimating: 231it [02:26,  1.72it/s]Extractor Estimating: 232it [02:26,  1.76it/s]Extractor Estimating: 233it [02:27,  1.78it/s]Extractor Estimating: 234it [02:27,  1.75it/s]Extractor Estimating: 235it [02:28,  1.81it/s]Extractor Estimating: 236it [02:28,  1.74it/s]Extractor Estimating: 237it [02:29,  1.70it/s]Extractor Estimating: 238it [02:30,  1.69it/s]Extractor Estimating: 239it [02:30,  1.70it/s]Extractor Estimating: 240it [02:31,  1.68it/s]Extractor Estimating: 241it [02:31,  1.76it/s]Extractor Estimating: 242it [02:32,  1.78it/s]Extractor Estimating: 243it [02:32,  1.72it/s]Extractor Estimating: 244it [02:33,  1.70it/s]Extractor Estimating: 245it [02:34,  1.77it/s]Extractor Estimating: 246it [02:34,  1.76it/s]Extractor Estimating: 247it [02:35,  1.75it/s]Extractor Estimating: 248it [02:35,  1.67it/s]Extractor Estimating: 249it [02:36,  1.55it/s]Extractor Estimating: 250it [02:37,  1.61it/s]Extractor Estimating: 251it [02:37,  1.59it/s]Extractor Estimating: 252it [02:38,  1.57it/s]Extractor Estimating: 253it [02:39,  1.61it/s]Extractor Estimating: 254it [02:39,  1.62it/s]Extractor Estimating: 255it [02:40,  1.62it/s]Extractor Estimating: 256it [02:40,  1.59it/s]Extractor Estimating: 257it [02:41,  1.57it/s]Extractor Estimating: 258it [02:42,  1.60it/s]Extractor Estimating: 259it [02:42,  1.56it/s]Extractor Estimating: 260it [02:43,  1.57it/s]Extractor Estimating: 261it [02:44,  1.54it/s]Extractor Estimating: 262it [02:44,  1.52it/s]Extractor Estimating: 263it [02:45,  1.56it/s]Extractor Estimating: 264it [02:46,  1.54it/s]Extractor Estimating: 265it [02:46,  1.56it/s]Extractor Estimating: 266it [02:47,  1.56it/s]Extractor Estimating: 267it [02:47,  1.61it/s]Extractor Estimating: 268it [02:48,  1.65it/s]Extractor Estimating: 269it [02:49,  1.63it/s]Extractor Estimating: 270it [02:49,  1.62it/s]Extractor Estimating: 271it [02:50,  1.63it/s]Extractor Estimating: 272it [02:50,  1.65it/s]Extractor Estimating: 273it [02:51,  1.66it/s]Extractor Estimating: 274it [02:52,  1.66it/s]Extractor Estimating: 275it [02:52,  1.61it/s]Extractor Estimating: 276it [02:53,  1.58it/s]Extractor Estimating: 277it [02:54,  1.54it/s]Extractor Estimating: 278it [02:54,  1.58it/s]Extractor Estimating: 279it [02:55,  1.58it/s]Extractor Estimating: 280it [02:56,  1.61it/s]Extractor Estimating: 281it [02:56,  1.66it/s]Extractor Estimating: 282it [02:57,  1.67it/s]Extractor Estimating: 283it [02:57,  1.63it/s]Extractor Estimating: 284it [02:58,  1.65it/s]Extractor Estimating: 285it [02:59,  1.63it/s]Extractor Estimating: 286it [02:59,  1.63it/s]Extractor Estimating: 287it [03:00,  1.68it/s]Extractor Estimating: 288it [03:00,  1.65it/s]Extractor Estimating: 289it [03:01,  1.69it/s]Extractor Estimating: 290it [03:02,  1.63it/s]Extractor Estimating: 291it [03:02,  1.55it/s]Extractor Estimating: 292it [03:03,  1.58it/s]Extractor Estimating: 293it [03:03,  1.61it/s]Extractor Estimating: 294it [03:04,  1.67it/s]Extractor Estimating: 295it [03:05,  1.66it/s]Extractor Estimating: 296it [03:05,  1.66it/s]Extractor Estimating: 297it [03:06,  1.55it/s]Extractor Estimating: 298it [03:07,  1.57it/s]Extractor Estimating: 299it [03:07,  1.56it/s]Extractor Estimating: 300it [03:08,  1.55it/s]Extractor Estimating: 301it [03:09,  1.51it/s]Extractor Estimating: 302it [03:09,  1.51it/s]Extractor Estimating: 303it [03:10,  1.52it/s]Extractor Estimating: 304it [03:11,  1.54it/s]Extractor Estimating: 305it [03:11,  1.55it/s]Extractor Estimating: 306it [03:12,  1.57it/s]Extractor Estimating: 307it [03:12,  1.59it/s]Extractor Estimating: 308it [03:13,  1.57it/s]Extractor Estimating: 309it [03:14,  1.61it/s]Extractor Estimating: 310it [03:14,  1.61it/s]Extractor Estimating: 311it [03:15,  1.57it/s]Extractor Estimating: 312it [03:15,  1.63it/s]Extractor Estimating: 313it [03:16,  1.61it/s]Extractor Estimating: 314it [03:17,  1.62it/s]Extractor Estimating: 315it [03:17,  1.62it/s]Extractor Estimating: 316it [03:18,  1.57it/s]Extractor Estimating: 317it [03:19,  1.58it/s]Extractor Estimating: 318it [03:19,  1.52it/s]Extractor Estimating: 319it [03:20,  1.53it/s]Extractor Estimating: 320it [03:21,  1.54it/s]Extractor Estimating: 321it [03:21,  1.54it/s]Extractor Estimating: 322it [03:22,  1.53it/s]Extractor Estimating: 323it [03:23,  1.60it/s]Extractor Estimating: 324it [03:23,  1.61it/s]Extractor Estimating: 325it [03:24,  1.62it/s]Extractor Estimating: 326it [03:24,  1.76it/s]Extractor Estimating: 327it [03:25,  1.83it/s]Extractor Estimating: 328it [03:25,  1.90it/s]Extractor Estimating: 329it [03:26,  1.96it/s]Extractor Estimating: 330it [03:26,  1.99it/s]Extractor Estimating: 331it [03:27,  1.99it/s]Extractor Estimating: 332it [03:27,  1.97it/s]Extractor Estimating: 333it [03:28,  1.68it/s]Extractor Estimating: 334it [03:28,  1.79it/s]Extractor Estimating: 335it [03:29,  1.83it/s]Extractor Estimating: 336it [03:29,  1.89it/s]Extractor Estimating: 337it [03:30,  1.96it/s]Extractor Estimating: 338it [03:30,  1.96it/s]Extractor Estimating: 339it [03:31,  1.93it/s]Extractor Estimating: 340it [03:31,  1.93it/s]Extractor Estimating: 341it [03:32,  1.99it/s]Extractor Estimating: 342it [03:33,  1.90it/s]Extractor Estimating: 343it [03:33,  1.90it/s]Extractor Estimating: 344it [03:34,  1.96it/s]Extractor Estimating: 345it [03:34,  2.02it/s]Extractor Estimating: 346it [03:35,  1.99it/s]Extractor Estimating: 347it [03:35,  1.95it/s]Extractor Estimating: 348it [03:35,  2.03it/s]Extractor Estimating: 349it [03:36,  2.13it/s]Extractor Estimating: 350it [03:36,  1.99it/s]Extractor Estimating: 351it [03:37,  1.85it/s]Extractor Estimating: 352it [03:38,  1.68it/s]Extractor Estimating: 353it [03:38,  1.64it/s]Extractor Estimating: 354it [03:39,  1.62it/s]Extractor Estimating: 355it [03:40,  1.61it/s]Extractor Estimating: 356it [03:40,  1.61it/s]Extractor Estimating: 357it [03:41,  1.57it/s]Extractor Estimating: 358it [03:42,  1.56it/s]Extractor Estimating: 359it [03:42,  1.56it/s]Extractor Estimating: 360it [03:43,  1.57it/s]Extractor Estimating: 361it [03:44,  1.56it/s]Extractor Estimating: 362it [03:44,  1.60it/s]Extractor Estimating: 363it [03:45,  1.54it/s]Extractor Estimating: 364it [03:46,  1.57it/s]Extractor Estimating: 365it [03:46,  1.56it/s]Extractor Estimating: 366it [03:47,  1.60it/s]Extractor Estimating: 367it [03:47,  1.60it/s]Extractor Estimating: 368it [03:48,  1.54it/s]Extractor Estimating: 369it [03:49,  1.50it/s]Extractor Estimating: 370it [03:49,  1.53it/s]Extractor Estimating: 371it [03:50,  1.54it/s]Extractor Estimating: 372it [03:51,  1.58it/s]Extractor Estimating: 373it [03:51,  1.57it/s]Extractor Estimating: 374it [03:52,  1.59it/s]Extractor Estimating: 375it [03:52,  1.63it/s]Extractor Estimating: 375it [03:52,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:14,541 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:14,547 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:14,547 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:14,547 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:14,547 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:16:14,844 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:16:14,845 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:16:15,524 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:16:16,570 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:16:16,571 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:18,369 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:18,373 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:18,373 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:18,373 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:18,373 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:16:18,687 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:16:18,688 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:16:19,371 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:16:19,530 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:16:19,530 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 08:42:33,974 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 08:42:33,998 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7496 mean pseudo reward: 0.9300479551984082
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 16716
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16816, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16816, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.148, loss:470.8454
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.097, loss:450.7836
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.095, loss:426.4452
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.098, loss:408.0156
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.112, loss:410.1595
>> valid entity prec:0.4985, rec:0.4111, f1:0.4506
>> valid relation prec:0.1229, rec:0.0099, f1:0.0183
>> valid relation with NER prec:0.1229, rec:0.0099, f1:0.0183
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.640, loss:426.5172
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.090, loss:395.5834
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.108, loss:408.8763
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.127, loss:411.8711
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.110, loss:388.9762
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5222, rec:0.3924, f1:0.4481
>> valid relation prec:0.1236, rec:0.0099, f1:0.0184
>> valid relation with NER prec:0.1236, rec:0.0099, f1:0.0184
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.623, loss:397.9660
g_step 1200, step 261, avg_time 1.095, loss:397.9338
g_step 1300, step 48, avg_time 1.111, loss:382.5495
g_step 1400, step 148, avg_time 1.110, loss:380.2228
g_step 1500, step 248, avg_time 1.108, loss:375.5028
>> valid entity prec:0.5103, rec:0.4621, f1:0.4850
>> valid relation prec:0.1068, rec:0.0095, f1:0.0174
>> valid relation with NER prec:0.1068, rec:0.0095, f1:0.0174
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.624, loss:382.0644
g_step 1700, step 135, avg_time 1.113, loss:367.8327
g_step 1800, step 235, avg_time 1.107, loss:363.6313
g_step 1900, step 22, avg_time 1.098, loss:352.4800
g_step 2000, step 122, avg_time 1.116, loss:325.2943
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5065, rec:0.4699, f1:0.4875
>> valid relation prec:0.1146, rec:0.0150, f1:0.0265
>> valid relation with NER prec:0.1146, rec:0.0150, f1:0.0265
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 222, avg_time 2.639, loss:346.9145
g_step 2200, step 9, avg_time 1.101, loss:358.4022
g_step 2300, step 109, avg_time 1.084, loss:330.5204
g_step 2400, step 209, avg_time 1.119, loss:335.1464
g_step 2500, step 309, avg_time 1.113, loss:345.1713
>> valid entity prec:0.4392, rec:0.3294, f1:0.3765
>> valid relation prec:0.0786, rec:0.0076, f1:0.0139
>> valid relation with NER prec:0.0786, rec:0.0076, f1:0.0139
g_step 2600, step 96, avg_time 2.640, loss:318.2960
g_step 2700, step 196, avg_time 1.101, loss:323.9337
g_step 2800, step 296, avg_time 1.101, loss:330.8491
g_step 2900, step 83, avg_time 1.123, loss:316.2049
g_step 3000, step 183, avg_time 1.100, loss:307.1893
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5020, rec:0.4036, f1:0.4475
>> valid relation prec:0.1201, rec:0.0164, f1:0.0288
>> valid relation with NER prec:0.1201, rec:0.0164, f1:0.0288
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 283, avg_time 2.626, loss:320.2904
g_step 3200, step 70, avg_time 1.132, loss:295.3611
g_step 3300, step 170, avg_time 1.104, loss:298.4755
g_step 3400, step 270, avg_time 1.116, loss:306.7760
g_step 3500, step 57, avg_time 1.092, loss:287.8311
>> valid entity prec:0.4972, rec:0.3701, f1:0.4243
>> valid relation prec:0.1419, rec:0.0187, f1:0.0330
>> valid relation with NER prec:0.1419, rec:0.0187, f1:0.0330
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 157, avg_time 2.640, loss:278.5702
g_step 3700, step 257, avg_time 1.121, loss:303.2196
g_step 3800, step 44, avg_time 1.090, loss:286.0620
g_step 3900, step 144, avg_time 1.101, loss:281.5651
g_step 4000, step 244, avg_time 1.104, loss:284.2708
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4588, rec:0.4381, f1:0.4482
>> valid relation prec:0.1230, rec:0.0217, f1:0.0369
>> valid relation with NER prec:0.1230, rec:0.0217, f1:0.0369
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 31, avg_time 2.650, loss:286.6732
g_step 4200, step 131, avg_time 1.101, loss:264.3716
g_step 4300, step 231, avg_time 1.116, loss:273.0469
g_step 4400, step 18, avg_time 1.103, loss:282.6468
g_step 4500, step 118, avg_time 1.107, loss:252.5174
>> valid entity prec:0.4836, rec:0.3760, f1:0.4231
>> valid relation prec:0.1011, rec:0.0152, f1:0.0265
>> valid relation with NER prec:0.1011, rec:0.0152, f1:0.0265
g_step 4600, step 218, avg_time 2.648, loss:269.6686
g_step 4700, step 5, avg_time 1.107, loss:261.4630
g_step 4800, step 105, avg_time 1.114, loss:236.6654
g_step 4900, step 205, avg_time 1.097, loss:255.3356
g_step 5000, step 305, avg_time 1.120, loss:268.6962
learning rate was adjusted to 0.0008
>> valid entity prec:0.4951, rec:0.4109, f1:0.4491
>> valid relation prec:0.1271, rec:0.0249, f1:0.0416
>> valid relation with NER prec:0.1271, rec:0.0249, f1:0.0416
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 92, avg_time 2.634, loss:236.1901
g_step 5200, step 192, avg_time 1.105, loss:248.5198
g_step 5300, step 292, avg_time 1.126, loss:250.1566
g_step 5400, step 79, avg_time 1.113, loss:233.1998
g_step 5500, step 179, avg_time 1.119, loss:234.3677
>> valid entity prec:0.4855, rec:0.4500, f1:0.4671
>> valid relation prec:0.1084, rec:0.0297, f1:0.0467
>> valid relation with NER prec:0.1084, rec:0.0297, f1:0.0467
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5600, step 279, avg_time 2.632, loss:256.6806
g_step 5700, step 66, avg_time 1.096, loss:221.3713
g_step 5800, step 166, avg_time 1.109, loss:238.4485
g_step 5900, step 266, avg_time 1.128, loss:235.9068
g_step 6000, step 53, avg_time 1.093, loss:226.0332
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4856, rec:0.3650, f1:0.4168
>> valid relation prec:0.0725, rec:0.0152, f1:0.0252
>> valid relation with NER prec:0.0725, rec:0.0152, f1:0.0252
g_step 6100, step 153, avg_time 2.662, loss:223.8841
g_step 6200, step 253, avg_time 1.092, loss:232.3122
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 08:42:33 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 08:42:34 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_08-42-33_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 08:42:34 - WARNING - datasets.builder -   Using custom data configuration default-26d35dd69ee2dab8
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-26d35dd69ee2dab8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 08:42:35,222 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:42:35,223 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:42:35,223 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:42:35,224 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:42:35,234 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:42:35,238 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:42:35,239 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:42:35,239 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:42:35,239 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:42:35,239 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:42:35,239 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 08:42:35,365 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:42:38,468 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 08:42:38,475 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-26d35dd69ee2dab8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.14ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.01ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.43ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.68ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.80ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.89ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.95ba/s]100%|██████████| 8/8 [00:01<00:00,  4.98ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.29ba/s] 40%|████      | 2/5 [00:00<00:00,  4.52ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.57ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.72ba/s]100%|██████████| 5/5 [00:01<00:00,  4.61ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.39ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.75ba/s] 50%|█████     | 4/8 [00:00<00:00,  9.57ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  9.95ba/s]100%|██████████| 8/8 [00:00<00:00, 11.22ba/s]100%|██████████| 8/8 [00:00<00:00, 10.53ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.34ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.61ba/s]100%|██████████| 5/5 [00:00<00:00, 11.91ba/s]100%|██████████| 5/5 [00:00<00:00, 11.18ba/s]
[INFO|trainer.py:414] 2023-08-29 08:42:42,750 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 08:42:42,761 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 08:42:42,761 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 08:42:42,761 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 08:42:42,761 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 08:42:42,761 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 08:42:42,761 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 08:42:42,761 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:56,  3.30it/s]  0%|          | 2/585 [00:00<02:51,  3.40it/s]  1%|          | 3/585 [00:00<02:49,  3.43it/s]  1%|          | 4/585 [00:01<02:48,  3.46it/s]  1%|          | 5/585 [00:01<02:47,  3.47it/s]  1%|          | 6/585 [00:01<02:47,  3.47it/s]  1%|          | 7/585 [00:02<02:46,  3.48it/s]  1%|▏         | 8/585 [00:02<02:46,  3.47it/s]  2%|▏         | 9/585 [00:02<02:45,  3.48it/s]  2%|▏         | 10/585 [00:02<02:45,  3.47it/s]  2%|▏         | 11/585 [00:03<02:45,  3.47it/s]  2%|▏         | 12/585 [00:03<02:44,  3.47it/s]  2%|▏         | 13/585 [00:03<02:44,  3.47it/s]  2%|▏         | 14/585 [00:04<02:44,  3.47it/s]  3%|▎         | 15/585 [00:04<02:44,  3.47it/s]  3%|▎         | 16/585 [00:04<02:43,  3.47it/s]  3%|▎         | 17/585 [00:04<02:43,  3.47it/s]  3%|▎         | 18/585 [00:05<02:43,  3.47it/s]  3%|▎         | 19/585 [00:05<02:43,  3.47it/s]  3%|▎         | 20/585 [00:05<02:43,  3.46it/s]  4%|▎         | 21/585 [00:06<02:42,  3.47it/s]  4%|▍         | 22/585 [00:06<02:42,  3.47it/s]  4%|▍         | 23/585 [00:06<02:42,  3.47it/s]  4%|▍         | 24/585 [00:06<02:41,  3.47it/s]  4%|▍         | 25/585 [00:07<02:41,  3.47it/s]  4%|▍         | 26/585 [00:07<02:41,  3.47it/s]  5%|▍         | 27/585 [00:07<02:40,  3.47it/s]  5%|▍         | 28/585 [00:08<02:40,  3.47it/s]  5%|▍         | 29/585 [00:08<02:40,  3.47it/s]  5%|▌         | 30/585 [00:08<02:40,  3.46it/s]  5%|▌         | 31/585 [00:08<02:40,  3.46it/s]  5%|▌         | 32/585 [00:09<02:39,  3.47it/s]  6%|▌         | 33/585 [00:09<02:39,  3.47it/s]  6%|▌         | 34/585 [00:09<02:38,  3.47it/s]  6%|▌         | 35/585 [00:10<02:38,  3.47it/s]  6%|▌         | 36/585 [00:10<02:38,  3.47it/s]  6%|▋         | 37/585 [00:10<02:38,  3.47it/s]  6%|▋         | 38/585 [00:10<02:37,  3.47it/s]  7%|▋         | 39/585 [00:11<02:37,  3.47it/s]  7%|▋         | 40/585 [00:11<02:37,  3.47it/s]  7%|▋         | 41/585 [00:11<02:37,  3.46it/s]  7%|▋         | 42/585 [00:12<02:36,  3.47it/s]  7%|▋         | 43/585 [00:12<02:36,  3.47it/s]  8%|▊         | 44/585 [00:12<02:35,  3.47it/s]  8%|▊         | 45/585 [00:12<02:35,  3.47it/s]  8%|▊         | 46/585 [00:13<02:35,  3.47it/s]  8%|▊         | 47/585 [00:13<02:35,  3.47it/s]  8%|▊         | 48/585 [00:13<02:34,  3.47it/s]  8%|▊         | 49/585 [00:14<02:34,  3.47it/s]  9%|▊         | 50/585 [00:14<02:34,  3.47it/s]  9%|▊         | 51/585 [00:14<02:33,  3.47it/s]  9%|▉         | 52/585 [00:15<02:34,  3.46it/s]  9%|▉         | 53/585 [00:15<02:33,  3.46it/s]  9%|▉         | 54/585 [00:15<02:33,  3.46it/s]  9%|▉         | 55/585 [00:15<02:33,  3.46it/s] 10%|▉         | 56/585 [00:16<02:32,  3.46it/s] 10%|▉         | 57/585 [00:16<02:32,  3.46it/s] 10%|▉         | 58/585 [00:16<02:32,  3.46it/s] 10%|█         | 59/585 [00:17<02:32,  3.46it/s] 10%|█         | 60/585 [00:17<02:31,  3.46it/s] 10%|█         | 61/585 [00:17<02:31,  3.46it/s] 11%|█         | 62/585 [00:17<02:31,  3.46it/s] 11%|█         | 63/585 [00:18<02:31,  3.44it/s] 11%|█         | 64/585 [00:18<02:31,  3.44it/s] 11%|█         | 65/585 [00:18<02:30,  3.44it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.45it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 69/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:20<02:27,  3.47it/s] 12%|█▏        | 73/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.45it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 76/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.46it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.46it/s] 14%|█▎        | 79/585 [00:22<02:25,  3.47it/s] 14%|█▎        | 80/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 83/585 [00:23<02:24,  3.47it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.47it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.47it/s] 15%|█▍        | 86/585 [00:24<02:23,  3.47it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 90/585 [00:25<02:22,  3.47it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.47it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.47it/s] 16%|█▌        | 93/585 [00:26<02:21,  3.47it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.47it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.47it/s] 16%|█▋        | 96/585 [00:27<02:20,  3.47it/s] 17%|█▋        | 97/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 100/585 [00:28<02:19,  3.47it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.47it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.47it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.47it/s] 18%|█▊        | 104/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.46it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.46it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:32<02:16,  3.46it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.46it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.46it/s] 20%|██        | 117/585 [00:33<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 08:43:16,585 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:43:16,585 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 08:43:16,585 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 58.11it/s][A
  2%|▏         | 12/543 [00:00<00:10, 50.98it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.15it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.44it/s][A
  5%|▌         | 28/543 [00:00<00:10, 47.89it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.57it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.32it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.12it/s][A
  9%|▉         | 48/543 [00:00<00:10, 47.17it/s][A
 10%|▉         | 53/543 [00:01<00:10, 47.19it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.22it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.13it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.28it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.20it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.07it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.07it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.94it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.92it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 47.02it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 47.05it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.14it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.17it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.22it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.12it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.08it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 47.00it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.81it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.96it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 47.05it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 47.01it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.01it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.12it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.16it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 46.98it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 46.99it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.90it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.84it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 47.02it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 47.11it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.09it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.03it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 47.12it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.01it/s][A
 41%|████      | 223/543 [00:04<00:06, 46.97it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 46.91it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.88it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.94it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 47.02it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 47.09it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.12it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.16it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.09it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 46.98it/s][A
 50%|█████     | 273/543 [00:05<00:05, 46.95it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.91it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 46.52it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 47.09it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 47.11it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.13it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.16it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.13it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.02it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 46.98it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 46.81it/s][A
 60%|██████    | 328/543 [00:06<00:04, 46.85it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 46.99it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 47.10it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 47.10it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 47.08it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.12it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 46.94it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 46.97it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 46.93it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 46.88it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 46.93it/s][A
 71%|███████   | 383/543 [00:08<00:03, 47.04it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 47.12it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 47.11it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.07it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 46.99it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 46.94it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 46.97it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 46.75it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 46.96it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 47.07it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 47.11it/s][A
 81%|████████  | 438/543 [00:09<00:02, 46.95it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 47.07it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 47.02it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 46.91it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 46.96it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 46.96it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 46.91it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.92it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 47.06it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 47.02it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 47.09it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.05it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 46.97it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 46.89it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 46.93it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 46.85it/s][A
 95%|█████████▌| 518/543 [00:10<00:00, 46.95it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 47.05it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 47.07it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 47.06it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 47.07it/s][A
100%|██████████| 543/543 [00:11<00:00, 46.96it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:45<02:15,  3.46it/s]
100%|██████████| 543/543 [00:11<00:00, 46.96it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:43:28,161 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 08:43:28,179 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:43:30,398 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:43:30,414 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:43:30,424 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:52<44:46,  5.75s/it] 20%|██        | 119/585 [00:52<31:57,  4.11s/it] 21%|██        | 120/585 [00:52<22:59,  2.97s/it] 21%|██        | 121/585 [00:53<16:43,  2.16s/it] 21%|██        | 122/585 [00:53<12:21,  1.60s/it] 21%|██        | 123/585 [00:53<09:17,  1.21s/it] 21%|██        | 124/585 [00:54<07:09,  1.07it/s] 21%|██▏       | 125/585 [00:54<05:39,  1.35it/s] 22%|██▏       | 126/585 [00:54<04:37,  1.66it/s] 22%|██▏       | 127/585 [00:54<03:53,  1.97it/s] 22%|██▏       | 128/585 [00:55<03:22,  2.26it/s] 22%|██▏       | 129/585 [00:55<03:00,  2.52it/s] 22%|██▏       | 130/585 [00:55<02:45,  2.74it/s] 22%|██▏       | 131/585 [00:56<02:35,  2.92it/s] 23%|██▎       | 132/585 [00:56<02:27,  3.07it/s] 23%|██▎       | 133/585 [00:56<02:22,  3.18it/s] 23%|██▎       | 134/585 [00:56<02:18,  3.26it/s] 23%|██▎       | 135/585 [00:57<02:15,  3.32it/s] 23%|██▎       | 136/585 [00:57<02:13,  3.36it/s] 23%|██▎       | 137/585 [00:57<02:12,  3.39it/s] 24%|██▎       | 138/585 [00:58<02:11,  3.41it/s] 24%|██▍       | 139/585 [00:58<02:10,  3.43it/s] 24%|██▍       | 140/585 [00:58<02:09,  3.44it/s] 24%|██▍       | 141/585 [00:58<02:09,  3.44it/s] 24%|██▍       | 142/585 [00:59<02:08,  3.45it/s] 24%|██▍       | 143/585 [00:59<02:07,  3.45it/s] 25%|██▍       | 144/585 [00:59<02:07,  3.46it/s] 25%|██▍       | 145/585 [01:00<02:07,  3.46it/s] 25%|██▍       | 146/585 [01:00<02:06,  3.46it/s] 25%|██▌       | 147/585 [01:00<02:06,  3.45it/s] 25%|██▌       | 148/585 [01:00<02:06,  3.46it/s] 25%|██▌       | 149/585 [01:01<02:05,  3.46it/s] 26%|██▌       | 150/585 [01:01<02:05,  3.46it/s] 26%|██▌       | 151/585 [01:01<02:05,  3.46it/s] 26%|██▌       | 152/585 [01:02<02:05,  3.45it/s] 26%|██▌       | 153/585 [01:02<02:05,  3.45it/s] 26%|██▋       | 154/585 [01:02<02:04,  3.46it/s] 26%|██▋       | 155/585 [01:02<02:04,  3.46it/s] 27%|██▋       | 156/585 [01:03<02:03,  3.46it/s] 27%|██▋       | 157/585 [01:03<02:03,  3.46it/s] 27%|██▋       | 158/585 [01:03<02:03,  3.46it/s] 27%|██▋       | 159/585 [01:04<02:03,  3.46it/s] 27%|██▋       | 160/585 [01:04<02:02,  3.46it/s] 28%|██▊       | 161/585 [01:04<02:02,  3.46it/s] 28%|██▊       | 162/585 [01:04<02:02,  3.46it/s] 28%|██▊       | 163/585 [01:05<02:02,  3.46it/s] 28%|██▊       | 164/585 [01:05<02:01,  3.46it/s] 28%|██▊       | 165/585 [01:05<02:01,  3.46it/s] 28%|██▊       | 166/585 [01:06<02:01,  3.46it/s] 29%|██▊       | 167/585 [01:06<02:00,  3.47it/s] 29%|██▊       | 168/585 [01:06<02:00,  3.46it/s] 29%|██▉       | 169/585 [01:07<02:00,  3.46it/s] 29%|██▉       | 170/585 [01:07<01:59,  3.46it/s] 29%|██▉       | 171/585 [01:07<01:59,  3.46it/s] 29%|██▉       | 172/585 [01:07<01:59,  3.46it/s] 30%|██▉       | 173/585 [01:08<01:58,  3.46it/s] 30%|██▉       | 174/585 [01:08<01:59,  3.45it/s] 30%|██▉       | 175/585 [01:08<01:58,  3.45it/s] 30%|███       | 176/585 [01:09<01:58,  3.45it/s] 30%|███       | 177/585 [01:09<01:58,  3.45it/s] 30%|███       | 178/585 [01:09<01:57,  3.45it/s] 31%|███       | 179/585 [01:09<01:57,  3.46it/s] 31%|███       | 180/585 [01:10<01:57,  3.46it/s] 31%|███       | 181/585 [01:10<01:56,  3.46it/s] 31%|███       | 182/585 [01:10<01:56,  3.46it/s] 31%|███▏      | 183/585 [01:11<01:56,  3.46it/s] 31%|███▏      | 184/585 [01:11<01:55,  3.46it/s] 32%|███▏      | 185/585 [01:11<01:55,  3.45it/s] 32%|███▏      | 186/585 [01:11<01:55,  3.46it/s] 32%|███▏      | 187/585 [01:12<01:55,  3.46it/s] 32%|███▏      | 188/585 [01:12<01:55,  3.45it/s] 32%|███▏      | 189/585 [01:12<01:54,  3.45it/s] 32%|███▏      | 190/585 [01:13<01:54,  3.45it/s] 33%|███▎      | 191/585 [01:13<01:54,  3.45it/s] 33%|███▎      | 192/585 [01:13<01:53,  3.46it/s] 33%|███▎      | 193/585 [01:13<01:53,  3.46it/s] 33%|███▎      | 194/585 [01:14<01:53,  3.46it/s] 33%|███▎      | 195/585 [01:14<01:52,  3.46it/s] 34%|███▎      | 196/585 [01:14<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:15<01:52,  3.46it/s] 34%|███▍      | 198/585 [01:15<01:51,  3.46it/s] 34%|███▍      | 199/585 [01:15<01:51,  3.46it/s] 34%|███▍      | 200/585 [01:15<01:51,  3.46it/s] 34%|███▍      | 201/585 [01:16<01:51,  3.46it/s] 35%|███▍      | 202/585 [01:16<01:51,  3.44it/s] 35%|███▍      | 203/585 [01:16<01:50,  3.45it/s] 35%|███▍      | 204/585 [01:17<01:50,  3.45it/s] 35%|███▌      | 205/585 [01:17<01:50,  3.45it/s] 35%|███▌      | 206/585 [01:17<01:49,  3.46it/s] 35%|███▌      | 207/585 [01:18<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:18<01:49,  3.46it/s] 36%|███▌      | 209/585 [01:18<01:48,  3.46it/s] 36%|███▌      | 210/585 [01:18<01:48,  3.46it/s] 36%|███▌      | 211/585 [01:19<01:48,  3.46it/s] 36%|███▌      | 212/585 [01:19<01:47,  3.46it/s] 36%|███▋      | 213/585 [01:19<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:20<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:20<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:20<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:20<01:46,  3.46it/s] 37%|███▋      | 218/585 [01:21<01:46,  3.46it/s] 37%|███▋      | 219/585 [01:21<01:45,  3.46it/s] 38%|███▊      | 220/585 [01:21<01:45,  3.46it/s] 38%|███▊      | 221/585 [01:22<01:45,  3.46it/s] 38%|███▊      | 222/585 [01:22<01:44,  3.46it/s] 38%|███▊      | 223/585 [01:22<01:44,  3.46it/s] 38%|███▊      | 224/585 [01:22<01:44,  3.45it/s] 38%|███▊      | 225/585 [01:23<01:44,  3.45it/s] 39%|███▊      | 226/585 [01:23<01:43,  3.46it/s] 39%|███▉      | 227/585 [01:23<01:44,  3.42it/s] 39%|███▉      | 228/585 [01:24<01:43,  3.43it/s] 39%|███▉      | 229/585 [01:24<01:43,  3.44it/s] 39%|███▉      | 230/585 [01:24<01:43,  3.44it/s] 39%|███▉      | 231/585 [01:24<01:42,  3.45it/s] 40%|███▉      | 232/585 [01:25<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:25<01:41,  3.46it/s] 40%|████      | 234/585 [01:25<01:41,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 08:44:08,638 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:44:08,638 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 08:44:08,638 >>   Batch size = 8
{'eval_loss': 1.0670812129974365, 'eval_runtime': 11.5592, 'eval_samples_per_second': 375.633, 'eval_steps_per_second': 46.976, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.53it/s][A
  2%|▏         | 12/543 [00:00<00:10, 50.81it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.17it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.51it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.17it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.88it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.15it/s][A
  8%|▊         | 43/543 [00:00<00:10, 46.75it/s][A
  9%|▉         | 48/543 [00:01<00:10, 46.79it/s][A
 10%|▉         | 53/543 [00:01<00:10, 46.85it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.04it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.08it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.19it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.18it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.26it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 47.19it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 47.01it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.89it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.91it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 46.97it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.11it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.15it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.20it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.06it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.20it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 47.06it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 47.05it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.96it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.91it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 47.01it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.13it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.07it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.16it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.17it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 47.09it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 47.00it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.94it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.92it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 46.95it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 46.97it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.07it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 47.13it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.12it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.21it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 47.13it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 47.12it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 47.00it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 46.93it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 47.04it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.05it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.05it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.12it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.18it/s][A
 50%|█████     | 273/543 [00:05<00:05, 47.18it/s][A
 51%|█████     | 278/543 [00:05<00:05, 47.02it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 47.04it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 46.95it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 47.02it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.09it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.12it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.02it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.08it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.12it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 47.03it/s][A
 60%|██████    | 328/543 [00:06<00:04, 47.03it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 46.99it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 46.93it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 47.05it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 47.10it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.06it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 46.97it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.10it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 47.05it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 47.02it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 47.03it/s][A
 71%|███████   | 383/543 [00:08<00:03, 46.95it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 46.99it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 47.10it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.10it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.09it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.09it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 47.11it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 47.03it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 47.04it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 47.05it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 46.95it/s][A
 81%|████████  | 438/543 [00:09<00:02, 47.00it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 47.08it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 47.08it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.05it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 47.05it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 47.03it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 46.88it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.98it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 46.98it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 46.96it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 47.07it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.13it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 47.11it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 47.10it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 47.01it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 46.98it/s][A
 95%|█████████▌| 518/543 [00:10<00:00, 47.02it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 47.03it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 46.96it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 47.00it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 47.12it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.10it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:37<01:41,  3.45it/s]
100%|██████████| 543/543 [00:11<00:00, 47.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:44:20,208 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 08:44:20,222 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:44:22,451 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:44:22,463 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:44:22,472 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:44<33:36,  5.76s/it] 40%|████      | 236/585 [01:44<23:58,  4.12s/it] 41%|████      | 237/585 [01:44<17:13,  2.97s/it] 41%|████      | 238/585 [01:45<12:31,  2.17s/it] 41%|████      | 239/585 [01:45<09:14,  1.60s/it] 41%|████      | 240/585 [01:45<06:57,  1.21s/it] 41%|████      | 241/585 [01:46<05:20,  1.07it/s] 41%|████▏     | 242/585 [01:46<04:13,  1.35it/s] 42%|████▏     | 243/585 [01:46<03:26,  1.66it/s] 42%|████▏     | 244/585 [01:46<02:53,  1.96it/s] 42%|████▏     | 245/585 [01:47<02:30,  2.26it/s] 42%|████▏     | 246/585 [01:47<02:14,  2.52it/s] 42%|████▏     | 247/585 [01:47<02:03,  2.74it/s] 42%|████▏     | 248/585 [01:48<01:55,  2.93it/s] 43%|████▎     | 249/585 [01:48<01:49,  3.07it/s] 43%|████▎     | 250/585 [01:48<01:45,  3.18it/s] 43%|████▎     | 251/585 [01:48<01:42,  3.26it/s] 43%|████▎     | 252/585 [01:49<01:40,  3.33it/s] 43%|████▎     | 253/585 [01:49<01:38,  3.37it/s] 43%|████▎     | 254/585 [01:49<01:37,  3.40it/s] 44%|████▎     | 255/585 [01:50<01:36,  3.42it/s] 44%|████▍     | 256/585 [01:50<01:35,  3.44it/s] 44%|████▍     | 257/585 [01:50<01:35,  3.45it/s] 44%|████▍     | 258/585 [01:51<01:35,  3.43it/s] 44%|████▍     | 259/585 [01:51<01:34,  3.45it/s] 44%|████▍     | 260/585 [01:51<01:34,  3.45it/s] 45%|████▍     | 261/585 [01:51<01:33,  3.46it/s] 45%|████▍     | 262/585 [01:52<01:36,  3.33it/s] 45%|████▍     | 263/585 [01:52<01:35,  3.36it/s] 45%|████▌     | 264/585 [01:52<01:34,  3.39it/s] 45%|████▌     | 265/585 [01:53<01:33,  3.41it/s] 45%|████▌     | 266/585 [01:53<01:33,  3.43it/s] 46%|████▌     | 267/585 [01:53<01:32,  3.44it/s] 46%|████▌     | 268/585 [01:53<01:31,  3.45it/s] 46%|████▌     | 269/585 [01:54<01:31,  3.44it/s] 46%|████▌     | 270/585 [01:54<01:31,  3.45it/s] 46%|████▋     | 271/585 [01:54<01:30,  3.46it/s] 46%|████▋     | 272/585 [01:55<01:30,  3.46it/s] 47%|████▋     | 273/585 [01:55<01:30,  3.47it/s] 47%|████▋     | 274/585 [01:55<01:29,  3.47it/s] 47%|████▋     | 275/585 [01:55<01:29,  3.46it/s] 47%|████▋     | 276/585 [01:56<01:29,  3.47it/s] 47%|████▋     | 277/585 [01:56<01:28,  3.47it/s] 48%|████▊     | 278/585 [01:56<01:28,  3.47it/s] 48%|████▊     | 279/585 [01:57<01:28,  3.47it/s] 48%|████▊     | 280/585 [01:57<01:28,  3.46it/s] 48%|████▊     | 281/585 [01:57<01:27,  3.46it/s] 48%|████▊     | 282/585 [01:57<01:27,  3.46it/s] 48%|████▊     | 283/585 [01:58<01:27,  3.46it/s] 49%|████▊     | 284/585 [01:58<01:26,  3.47it/s] 49%|████▊     | 285/585 [01:58<01:26,  3.47it/s] 49%|████▉     | 286/585 [01:59<01:26,  3.47it/s] 49%|████▉     | 287/585 [01:59<01:25,  3.47it/s] 49%|████▉     | 288/585 [01:59<01:25,  3.47it/s] 49%|████▉     | 289/585 [01:59<01:25,  3.47it/s] 50%|████▉     | 290/585 [02:00<01:25,  3.47it/s] 50%|████▉     | 291/585 [02:00<01:24,  3.47it/s] 50%|████▉     | 292/585 [02:00<01:24,  3.47it/s] 50%|█████     | 293/585 [02:01<01:24,  3.47it/s] 50%|█████     | 294/585 [02:01<01:23,  3.47it/s] 50%|█████     | 295/585 [02:01<01:23,  3.46it/s] 51%|█████     | 296/585 [02:02<01:23,  3.47it/s] 51%|█████     | 297/585 [02:02<01:23,  3.46it/s] 51%|█████     | 298/585 [02:02<01:22,  3.47it/s] 51%|█████     | 299/585 [02:02<01:22,  3.46it/s] 51%|█████▏    | 300/585 [02:03<01:22,  3.46it/s] 51%|█████▏    | 301/585 [02:03<01:22,  3.46it/s] 52%|█████▏    | 302/585 [02:03<01:21,  3.46it/s] 52%|█████▏    | 303/585 [02:04<01:21,  3.46it/s] 52%|█████▏    | 304/585 [02:04<01:21,  3.46it/s] 52%|█████▏    | 305/585 [02:04<01:20,  3.46it/s] 52%|█████▏    | 306/585 [02:04<01:20,  3.47it/s] 52%|█████▏    | 307/585 [02:05<01:20,  3.46it/s] 53%|█████▎    | 308/585 [02:05<01:20,  3.46it/s] 53%|█████▎    | 309/585 [02:05<01:19,  3.46it/s] 53%|█████▎    | 310/585 [02:06<01:19,  3.46it/s] 53%|█████▎    | 311/585 [02:06<01:19,  3.46it/s] 53%|█████▎    | 312/585 [02:06<01:18,  3.46it/s] 54%|█████▎    | 313/585 [02:06<01:18,  3.46it/s] 54%|█████▎    | 314/585 [02:07<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:07<01:17,  3.46it/s] 54%|█████▍    | 316/585 [02:07<01:17,  3.46it/s] 54%|█████▍    | 317/585 [02:08<01:17,  3.46it/s] 54%|█████▍    | 318/585 [02:08<01:17,  3.46it/s] 55%|█████▍    | 319/585 [02:08<01:16,  3.46it/s] 55%|█████▍    | 320/585 [02:08<01:16,  3.46it/s] 55%|█████▍    | 321/585 [02:09<01:16,  3.45it/s] 55%|█████▌    | 322/585 [02:09<01:16,  3.46it/s] 55%|█████▌    | 323/585 [02:09<01:15,  3.46it/s] 55%|█████▌    | 324/585 [02:10<01:15,  3.46it/s] 56%|█████▌    | 325/585 [02:10<01:15,  3.46it/s] 56%|█████▌    | 326/585 [02:10<01:14,  3.46it/s] 56%|█████▌    | 327/585 [02:10<01:14,  3.46it/s] 56%|█████▌    | 328/585 [02:11<01:14,  3.46it/s] 56%|█████▌    | 329/585 [02:11<01:13,  3.46it/s] 56%|█████▋    | 330/585 [02:11<01:13,  3.46it/s] 57%|█████▋    | 331/585 [02:12<01:13,  3.46it/s] 57%|█████▋    | 332/585 [02:12<01:13,  3.46it/s] 57%|█████▋    | 333/585 [02:12<01:12,  3.46it/s] 57%|█████▋    | 334/585 [02:12<01:12,  3.46it/s] 57%|█████▋    | 335/585 [02:13<01:12,  3.46it/s] 57%|█████▋    | 336/585 [02:13<01:12,  3.46it/s] 58%|█████▊    | 337/585 [02:13<01:11,  3.46it/s] 58%|█████▊    | 338/585 [02:14<01:11,  3.46it/s] 58%|█████▊    | 339/585 [02:14<01:11,  3.46it/s] 58%|█████▊    | 340/585 [02:14<01:10,  3.46it/s] 58%|█████▊    | 341/585 [02:15<01:10,  3.46it/s] 58%|█████▊    | 342/585 [02:15<01:10,  3.46it/s] 59%|█████▊    | 343/585 [02:15<01:10,  3.46it/s] 59%|█████▉    | 344/585 [02:15<01:09,  3.46it/s] 59%|█████▉    | 345/585 [02:16<01:09,  3.46it/s] 59%|█████▉    | 346/585 [02:16<01:09,  3.46it/s] 59%|█████▉    | 347/585 [02:16<01:08,  3.45it/s] 59%|█████▉    | 348/585 [02:17<01:08,  3.46it/s] 60%|█████▉    | 349/585 [02:17<01:08,  3.46it/s] 60%|█████▉    | 350/585 [02:17<01:07,  3.46it/s] 60%|██████    | 351/585 [02:17<01:07,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 08:45:00,708 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:45:00,708 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 08:45:00,708 >>   Batch size = 8
{'eval_loss': 1.08614981174469, 'eval_runtime': 11.5516, 'eval_samples_per_second': 375.878, 'eval_steps_per_second': 47.006, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.78it/s][A
  2%|▏         | 12/543 [00:00<00:10, 50.92it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.22it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.47it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.02it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.80it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.53it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.07it/s][A
  9%|▉         | 48/543 [00:00<00:10, 46.93it/s][A
 10%|▉         | 53/543 [00:01<00:10, 46.99it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.12it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.11it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.16it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.01it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.05it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 46.88it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.90it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.78it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.87it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 46.97it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.09it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.02it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.13it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.03it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.08it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 46.87it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.89it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.84it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.98it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 47.08it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.07it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.02it/s][A
 31%|███       | 168/543 [00:03<00:08, 45.98it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 46.36it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 46.63it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.75it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.72it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.86it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 46.80it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 46.94it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 46.90it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 46.63it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.04it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.13it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 47.10it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 47.01it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.98it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 47.01it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 46.96it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 46.96it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 46.90it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 46.93it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.06it/s][A
 50%|█████     | 273/543 [00:05<00:05, 47.10it/s][A
 51%|█████     | 278/543 [00:05<00:05, 47.01it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 47.04it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 47.09it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 47.00it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.00it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 46.95it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 46.86it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 46.89it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.07it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 47.01it/s][A
 60%|██████    | 328/543 [00:06<00:04, 47.03it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 47.04it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 47.00it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 46.96it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 46.92it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 46.79it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 46.88it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 46.99it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 47.05it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 47.02it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 47.11it/s][A
 71%|███████   | 383/543 [00:08<00:03, 47.03it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 46.91it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 46.88it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 46.85it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 46.85it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 46.89it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 46.92it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 46.99it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 47.09it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 47.03it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 47.04it/s][A
 81%|████████  | 438/543 [00:09<00:02, 46.98it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 46.95it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 46.87it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 46.94it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 46.88it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 46.83it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 46.95it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 47.07it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 47.07it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 47.07it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 46.99it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 46.90it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 46.96it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 46.88it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 46.89it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 46.94it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 47.03it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 47.04it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 47.05it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 47.01it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 46.94it/s][A
100%|██████████| 543/543 [00:11<00:00, 46.83it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:29<01:07,  3.46it/s]
100%|██████████| 543/543 [00:11<00:00, 46.83it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:45:12,304 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 08:45:12,322 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:45:14,390 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:45:14,409 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:45:14,417 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:36<22:27,  5.78s/it] 60%|██████    | 353/585 [02:36<15:59,  4.14s/it] 61%|██████    | 354/585 [02:37<11:28,  2.98s/it] 61%|██████    | 355/585 [02:37<08:19,  2.17s/it] 61%|██████    | 356/585 [02:37<06:08,  1.61s/it] 61%|██████    | 357/585 [02:37<04:36,  1.21s/it] 61%|██████    | 358/585 [02:38<03:32,  1.07it/s] 61%|██████▏   | 359/585 [02:38<02:47,  1.35it/s] 62%|██████▏   | 360/585 [02:38<02:16,  1.65it/s] 62%|██████▏   | 361/585 [02:39<01:54,  1.96it/s] 62%|██████▏   | 362/585 [02:39<01:38,  2.26it/s] 62%|██████▏   | 363/585 [02:39<01:28,  2.52it/s] 62%|██████▏   | 364/585 [02:39<01:20,  2.74it/s] 62%|██████▏   | 365/585 [02:40<01:15,  2.93it/s] 63%|██████▎   | 366/585 [02:40<01:11,  3.07it/s] 63%|██████▎   | 367/585 [02:40<01:08,  3.18it/s] 63%|██████▎   | 368/585 [02:41<01:06,  3.26it/s] 63%|██████▎   | 369/585 [02:41<01:04,  3.33it/s] 63%|██████▎   | 370/585 [02:41<01:03,  3.37it/s] 63%|██████▎   | 371/585 [02:41<01:02,  3.40it/s] 64%|██████▎   | 372/585 [02:42<01:02,  3.42it/s] 64%|██████▍   | 373/585 [02:42<01:01,  3.44it/s] 64%|██████▍   | 374/585 [02:42<01:01,  3.45it/s] 64%|██████▍   | 375/585 [02:43<01:00,  3.45it/s] 64%|██████▍   | 376/585 [02:43<01:00,  3.46it/s] 64%|██████▍   | 377/585 [02:43<01:00,  3.46it/s] 65%|██████▍   | 378/585 [02:43<00:59,  3.46it/s] 65%|██████▍   | 379/585 [02:44<00:59,  3.46it/s] 65%|██████▍   | 380/585 [02:44<00:59,  3.47it/s] 65%|██████▌   | 381/585 [02:44<00:58,  3.47it/s] 65%|██████▌   | 382/585 [02:45<00:58,  3.47it/s] 65%|██████▌   | 383/585 [02:45<00:58,  3.47it/s] 66%|██████▌   | 384/585 [02:45<00:57,  3.47it/s] 66%|██████▌   | 385/585 [02:46<00:57,  3.47it/s] 66%|██████▌   | 386/585 [02:46<00:57,  3.46it/s] 66%|██████▌   | 387/585 [02:46<00:57,  3.46it/s] 66%|██████▋   | 388/585 [02:46<00:56,  3.46it/s] 66%|██████▋   | 389/585 [02:47<00:56,  3.46it/s] 67%|██████▋   | 390/585 [02:47<00:56,  3.46it/s] 67%|██████▋   | 391/585 [02:47<00:56,  3.46it/s] 67%|██████▋   | 392/585 [02:48<00:55,  3.46it/s] 67%|██████▋   | 393/585 [02:48<00:55,  3.47it/s] 67%|██████▋   | 394/585 [02:48<00:55,  3.47it/s] 68%|██████▊   | 395/585 [02:48<00:54,  3.47it/s] 68%|██████▊   | 396/585 [02:49<00:54,  3.47it/s] 68%|██████▊   | 397/585 [02:49<00:54,  3.46it/s] 68%|██████▊   | 398/585 [02:49<00:54,  3.46it/s] 68%|██████▊   | 399/585 [02:50<00:53,  3.47it/s] 68%|██████▊   | 400/585 [02:50<00:53,  3.47it/s] 69%|██████▊   | 401/585 [02:50<00:53,  3.47it/s] 69%|██████▊   | 402/585 [02:50<00:52,  3.47it/s] 69%|██████▉   | 403/585 [02:51<00:52,  3.47it/s] 69%|██████▉   | 404/585 [02:51<00:52,  3.47it/s] 69%|██████▉   | 405/585 [02:51<00:51,  3.47it/s] 69%|██████▉   | 406/585 [02:52<00:51,  3.47it/s] 70%|██████▉   | 407/585 [02:52<00:51,  3.45it/s] 70%|██████▉   | 408/585 [02:52<00:51,  3.45it/s] 70%|██████▉   | 409/585 [02:52<00:50,  3.46it/s] 70%|███████   | 410/585 [02:53<00:50,  3.46it/s] 70%|███████   | 411/585 [02:53<00:50,  3.47it/s] 70%|███████   | 412/585 [02:53<00:49,  3.46it/s] 71%|███████   | 413/585 [02:54<00:49,  3.47it/s] 71%|███████   | 414/585 [02:54<00:49,  3.46it/s] 71%|███████   | 415/585 [02:54<00:49,  3.46it/s] 71%|███████   | 416/585 [02:54<00:48,  3.47it/s] 71%|███████▏  | 417/585 [02:55<00:48,  3.47it/s] 71%|███████▏  | 418/585 [02:55<00:48,  3.46it/s] 72%|███████▏  | 419/585 [02:55<00:47,  3.46it/s] 72%|███████▏  | 420/585 [02:56<00:47,  3.46it/s] 72%|███████▏  | 421/585 [02:56<00:47,  3.47it/s] 72%|███████▏  | 422/585 [02:56<00:46,  3.47it/s] 72%|███████▏  | 423/585 [02:56<00:46,  3.47it/s] 72%|███████▏  | 424/585 [02:57<00:46,  3.47it/s] 73%|███████▎  | 425/585 [02:57<00:46,  3.47it/s] 73%|███████▎  | 426/585 [02:57<00:45,  3.47it/s] 73%|███████▎  | 427/585 [02:58<00:45,  3.47it/s] 73%|███████▎  | 428/585 [02:58<00:45,  3.47it/s] 73%|███████▎  | 429/585 [02:58<00:45,  3.47it/s] 74%|███████▎  | 430/585 [02:59<00:44,  3.46it/s] 74%|███████▎  | 431/585 [02:59<00:44,  3.45it/s] 74%|███████▍  | 432/585 [02:59<00:44,  3.45it/s] 74%|███████▍  | 433/585 [02:59<00:44,  3.45it/s] 74%|███████▍  | 434/585 [03:00<00:43,  3.45it/s] 74%|███████▍  | 435/585 [03:00<00:43,  3.46it/s] 75%|███████▍  | 436/585 [03:00<00:43,  3.46it/s] 75%|███████▍  | 437/585 [03:01<00:42,  3.46it/s] 75%|███████▍  | 438/585 [03:01<00:42,  3.46it/s] 75%|███████▌  | 439/585 [03:01<00:42,  3.46it/s] 75%|███████▌  | 440/585 [03:01<00:41,  3.46it/s] 75%|███████▌  | 441/585 [03:02<00:41,  3.46it/s] 76%|███████▌  | 442/585 [03:02<00:41,  3.46it/s] 76%|███████▌  | 443/585 [03:02<00:41,  3.46it/s] 76%|███████▌  | 444/585 [03:03<00:40,  3.46it/s] 76%|███████▌  | 445/585 [03:03<00:40,  3.46it/s] 76%|███████▌  | 446/585 [03:03<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:03<00:39,  3.46it/s] 77%|███████▋  | 448/585 [03:04<00:39,  3.45it/s] 77%|███████▋  | 449/585 [03:04<00:39,  3.45it/s] 77%|███████▋  | 450/585 [03:04<00:39,  3.45it/s] 77%|███████▋  | 451/585 [03:05<00:38,  3.45it/s] 77%|███████▋  | 452/585 [03:05<00:38,  3.45it/s] 77%|███████▋  | 453/585 [03:05<00:38,  3.46it/s] 78%|███████▊  | 454/585 [03:05<00:37,  3.45it/s] 78%|███████▊  | 455/585 [03:06<00:37,  3.45it/s] 78%|███████▊  | 456/585 [03:06<00:37,  3.46it/s] 78%|███████▊  | 457/585 [03:06<00:37,  3.45it/s] 78%|███████▊  | 458/585 [03:07<00:36,  3.46it/s] 78%|███████▊  | 459/585 [03:07<00:36,  3.45it/s] 79%|███████▊  | 460/585 [03:07<00:36,  3.45it/s] 79%|███████▉  | 461/585 [03:07<00:35,  3.46it/s] 79%|███████▉  | 462/585 [03:08<00:35,  3.46it/s] 79%|███████▉  | 463/585 [03:08<00:35,  3.45it/s] 79%|███████▉  | 464/585 [03:08<00:35,  3.46it/s] 79%|███████▉  | 465/585 [03:09<00:34,  3.46it/s] 80%|███████▉  | 466/585 [03:09<00:34,  3.46it/s] 80%|███████▉  | 467/585 [03:09<00:34,  3.46it/s] 80%|████████  | 468/585 [03:09<00:33,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 08:45:52,807 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:45:52,807 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 08:45:52,807 >>   Batch size = 8
{'eval_loss': 1.102057695388794, 'eval_runtime': 11.5742, 'eval_samples_per_second': 375.143, 'eval_steps_per_second': 46.914, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 58.02it/s][A
  2%|▏         | 12/543 [00:00<00:10, 51.10it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.30it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.47it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.03it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.68it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.42it/s][A
  8%|▊         | 43/543 [00:00<00:10, 47.00it/s][A
  9%|▉         | 48/543 [00:00<00:10, 47.07it/s][A
 10%|▉         | 53/543 [00:01<00:10, 47.02it/s][A
 11%|█         | 58/543 [00:01<00:10, 47.14it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 47.08it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.27it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.16it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.17it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 46.90it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.81it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.87it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.98it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 46.91it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 47.06it/s][A
 21%|██        | 113/543 [00:02<00:09, 47.02it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 47.14it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.13it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.16it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 47.00it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.87it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.84it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.93it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 46.97it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.02it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.09it/s][A
 31%|███       | 168/543 [00:03<00:07, 46.98it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.04it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 47.02it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.94it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.96it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 47.02it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 46.88it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 47.06it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.12it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 46.97it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.07it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.08it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 46.83it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.94it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.94it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 46.89it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 46.99it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 47.09it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 46.98it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.07it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.05it/s][A
 50%|█████     | 273/543 [00:05<00:05, 46.95it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.92it/s][A
 52%|█████▏    | 283/543 [00:05<00:05, 46.82it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 46.75it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 46.89it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 46.98it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.04it/s][A
 57%|█████▋    | 308/543 [00:06<00:04, 47.10it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.03it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.10it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 47.08it/s][A
 60%|██████    | 328/543 [00:06<00:04, 46.93it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 46.87it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 46.93it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 46.94it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 46.96it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.00it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 47.09it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.14it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 47.00it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 46.89it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 46.73it/s][A
 71%|███████   | 383/543 [00:08<00:03, 46.82it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 46.93it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 46.97it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 46.98it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.06it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.03it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 47.01it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 47.04it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 46.94it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 46.87it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 46.93it/s][A
 81%|████████  | 438/543 [00:09<00:02, 46.92it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 46.90it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 47.03it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.12it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 46.93it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 47.01it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 46.83it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.89it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 46.93it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 46.92it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 46.97it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 47.02it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 46.80it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 47.00it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 46.98it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 46.98it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 46.93it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 46.92it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 46.82it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 46.94it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 46.91it/s][A
100%|██████████| 543/543 [00:11<00:00, 47.01it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:21<00:33,  3.46it/s]
100%|██████████| 543/543 [00:11<00:00, 47.01it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:46:04,387 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 08:46:04,406 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:46:06,776 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:46:06,792 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:46:06,801 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:28<11:18,  5.85s/it] 80%|████████  | 470/585 [03:29<08:00,  4.18s/it] 81%|████████  | 471/585 [03:29<05:43,  3.01s/it] 81%|████████  | 472/585 [03:29<04:08,  2.20s/it] 81%|████████  | 473/585 [03:29<03:01,  1.62s/it] 81%|████████  | 474/585 [03:30<02:15,  1.22s/it] 81%|████████  | 475/585 [03:30<01:43,  1.06it/s] 81%|████████▏ | 476/585 [03:30<01:21,  1.34it/s] 82%|████████▏ | 477/585 [03:31<01:05,  1.64it/s] 82%|████████▏ | 478/585 [03:31<00:54,  1.95it/s] 82%|████████▏ | 479/585 [03:31<00:47,  2.24it/s] 82%|████████▏ | 480/585 [03:31<00:41,  2.51it/s] 82%|████████▏ | 481/585 [03:32<00:38,  2.73it/s] 82%|████████▏ | 482/585 [03:32<00:35,  2.92it/s] 83%|████████▎ | 483/585 [03:32<00:33,  3.06it/s] 83%|████████▎ | 484/585 [03:33<00:31,  3.17it/s] 83%|████████▎ | 485/585 [03:33<00:30,  3.25it/s] 83%|████████▎ | 486/585 [03:33<00:29,  3.31it/s] 83%|████████▎ | 487/585 [03:34<00:29,  3.35it/s] 83%|████████▎ | 488/585 [03:34<00:28,  3.39it/s] 84%|████████▎ | 489/585 [03:34<00:28,  3.41it/s] 84%|████████▍ | 490/585 [03:34<00:27,  3.42it/s] 84%|████████▍ | 491/585 [03:35<00:27,  3.43it/s] 84%|████████▍ | 492/585 [03:35<00:27,  3.44it/s] 84%|████████▍ | 493/585 [03:35<00:26,  3.45it/s] 84%|████████▍ | 494/585 [03:36<00:26,  3.45it/s] 85%|████████▍ | 495/585 [03:36<00:26,  3.45it/s] 85%|████████▍ | 496/585 [03:36<00:25,  3.45it/s] 85%|████████▍ | 497/585 [03:36<00:25,  3.45it/s] 85%|████████▌ | 498/585 [03:37<00:25,  3.45it/s] 85%|████████▌ | 499/585 [03:37<00:24,  3.46it/s] 85%|████████▌ | 500/585 [03:37<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [03:37<00:24,  3.46it/s] 86%|████████▌ | 501/585 [03:38<00:24,  3.45it/s] 86%|████████▌ | 502/585 [03:38<00:24,  3.45it/s] 86%|████████▌ | 503/585 [03:38<00:23,  3.46it/s] 86%|████████▌ | 504/585 [03:38<00:23,  3.45it/s] 86%|████████▋ | 505/585 [03:39<00:23,  3.45it/s] 86%|████████▋ | 506/585 [03:39<00:22,  3.45it/s] 87%|████████▋ | 507/585 [03:39<00:22,  3.46it/s] 87%|████████▋ | 508/585 [03:40<00:22,  3.45it/s] 87%|████████▋ | 509/585 [03:40<00:21,  3.46it/s] 87%|████████▋ | 510/585 [03:40<00:21,  3.46it/s] 87%|████████▋ | 511/585 [03:40<00:21,  3.45it/s] 88%|████████▊ | 512/585 [03:41<00:21,  3.44it/s] 88%|████████▊ | 513/585 [03:41<00:20,  3.45it/s] 88%|████████▊ | 514/585 [03:41<00:20,  3.45it/s] 88%|████████▊ | 515/585 [03:42<00:20,  3.46it/s] 88%|████████▊ | 516/585 [03:42<00:19,  3.45it/s] 88%|████████▊ | 517/585 [03:42<00:19,  3.46it/s] 89%|████████▊ | 518/585 [03:42<00:19,  3.46it/s] 89%|████████▊ | 519/585 [03:43<00:19,  3.45it/s] 89%|████████▉ | 520/585 [03:43<00:18,  3.46it/s] 89%|████████▉ | 521/585 [03:43<00:18,  3.46it/s] 89%|████████▉ | 522/585 [03:44<00:18,  3.45it/s] 89%|████████▉ | 523/585 [03:44<00:18,  3.44it/s] 90%|████████▉ | 524/585 [03:44<00:17,  3.44it/s] 90%|████████▉ | 525/585 [03:45<00:17,  3.45it/s] 90%|████████▉ | 526/585 [03:45<00:17,  3.45it/s] 90%|█████████ | 527/585 [03:45<00:16,  3.45it/s] 90%|█████████ | 528/585 [03:45<00:16,  3.45it/s] 90%|█████████ | 529/585 [03:46<00:16,  3.45it/s] 91%|█████████ | 530/585 [03:46<00:15,  3.45it/s] 91%|█████████ | 531/585 [03:46<00:15,  3.45it/s] 91%|█████████ | 532/585 [03:47<00:15,  3.45it/s] 91%|█████████ | 533/585 [03:47<00:15,  3.45it/s] 91%|█████████▏| 534/585 [03:47<00:14,  3.44it/s] 91%|█████████▏| 535/585 [03:47<00:14,  3.45it/s] 92%|█████████▏| 536/585 [03:48<00:14,  3.45it/s] 92%|█████████▏| 537/585 [03:48<00:13,  3.45it/s] 92%|█████████▏| 538/585 [03:48<00:13,  3.45it/s] 92%|█████████▏| 539/585 [03:49<00:13,  3.45it/s] 92%|█████████▏| 540/585 [03:49<00:13,  3.45it/s] 92%|█████████▏| 541/585 [03:49<00:12,  3.45it/s] 93%|█████████▎| 542/585 [03:49<00:12,  3.45it/s] 93%|█████████▎| 543/585 [03:50<00:12,  3.45it/s] 93%|█████████▎| 544/585 [03:50<00:11,  3.45it/s] 93%|█████████▎| 545/585 [03:50<00:11,  3.44it/s] 93%|█████████▎| 546/585 [03:51<00:11,  3.44it/s] 94%|█████████▎| 547/585 [03:51<00:11,  3.45it/s] 94%|█████████▎| 548/585 [03:51<00:10,  3.45it/s] 94%|█████████▍| 549/585 [03:51<00:10,  3.45it/s] 94%|█████████▍| 550/585 [03:52<00:10,  3.45it/s] 94%|█████████▍| 551/585 [03:52<00:10,  3.39it/s] 94%|█████████▍| 552/585 [03:52<00:09,  3.40it/s] 95%|█████████▍| 553/585 [03:53<00:09,  3.42it/s] 95%|█████████▍| 554/585 [03:53<00:09,  3.43it/s] 95%|█████████▍| 555/585 [03:53<00:08,  3.43it/s] 95%|█████████▌| 556/585 [03:54<00:08,  3.44it/s] 95%|█████████▌| 557/585 [03:54<00:08,  3.44it/s] 95%|█████████▌| 558/585 [03:54<00:07,  3.45it/s] 96%|█████████▌| 559/585 [03:54<00:07,  3.45it/s] 96%|█████████▌| 560/585 [03:55<00:07,  3.45it/s] 96%|█████████▌| 561/585 [03:55<00:06,  3.45it/s] 96%|█████████▌| 562/585 [03:55<00:06,  3.45it/s] 96%|█████████▌| 563/585 [03:56<00:06,  3.45it/s] 96%|█████████▋| 564/585 [03:56<00:06,  3.45it/s] 97%|█████████▋| 565/585 [03:56<00:05,  3.45it/s] 97%|█████████▋| 566/585 [03:56<00:05,  3.45it/s] 97%|█████████▋| 567/585 [03:57<00:05,  3.44it/s] 97%|█████████▋| 568/585 [03:57<00:04,  3.45it/s] 97%|█████████▋| 569/585 [03:57<00:04,  3.45it/s] 97%|█████████▋| 570/585 [03:58<00:04,  3.45it/s] 98%|█████████▊| 571/585 [03:58<00:04,  3.45it/s] 98%|█████████▊| 572/585 [03:58<00:03,  3.45it/s] 98%|█████████▊| 573/585 [03:58<00:03,  3.45it/s] 98%|█████████▊| 574/585 [03:59<00:03,  3.45it/s] 98%|█████████▊| 575/585 [03:59<00:02,  3.45it/s] 98%|█████████▊| 576/585 [03:59<00:02,  3.45it/s] 99%|█████████▊| 577/585 [04:00<00:02,  3.45it/s] 99%|█████████▉| 578/585 [04:00<00:02,  3.44it/s] 99%|█████████▉| 579/585 [04:00<00:01,  3.45it/s] 99%|█████████▉| 580/585 [04:00<00:01,  3.45it/s] 99%|█████████▉| 581/585 [04:01<00:01,  3.45it/s] 99%|█████████▉| 582/585 [04:01<00:00,  3.45it/s]100%|█████████▉| 583/585 [04:01<00:00,  3.45it/s]100%|█████████▉| 584/585 [04:02<00:00,  3.45it/s]100%|██████████| 585/585 [04:02<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 08:46:45,199 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:46:45,199 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 08:46:45,199 >>   Batch size = 8
{'eval_loss': 1.1101621389389038, 'eval_runtime': 11.5657, 'eval_samples_per_second': 375.419, 'eval_steps_per_second': 46.949, 'epoch': 4.0}
{'loss': 0.4088, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.98it/s][A
  2%|▏         | 12/543 [00:00<00:10, 50.73it/s][A
  3%|▎         | 18/543 [00:00<00:10, 49.18it/s][A
  4%|▍         | 23/543 [00:00<00:10, 48.46it/s][A
  5%|▌         | 28/543 [00:00<00:10, 48.03it/s][A
  6%|▌         | 33/543 [00:00<00:10, 47.63it/s][A
  7%|▋         | 38/543 [00:00<00:10, 47.32it/s][A
  8%|▊         | 43/543 [00:00<00:10, 46.97it/s][A
  9%|▉         | 48/543 [00:00<00:10, 46.89it/s][A
 10%|▉         | 53/543 [00:01<00:10, 46.90it/s][A
 11%|█         | 58/543 [00:01<00:10, 46.99it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 46.95it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 47.10it/s][A
 13%|█▎        | 73/543 [00:01<00:09, 47.15it/s][A
 14%|█▍        | 78/543 [00:01<00:09, 47.11it/s][A
 15%|█▌        | 83/543 [00:01<00:09, 46.88it/s][A
 16%|█▌        | 88/543 [00:01<00:09, 46.90it/s][A
 17%|█▋        | 93/543 [00:01<00:09, 46.74it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 46.86it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 46.91it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 46.99it/s][A
 21%|██        | 113/543 [00:02<00:09, 46.97it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 46.94it/s][A
 23%|██▎       | 123/543 [00:02<00:08, 47.10it/s][A
 24%|██▎       | 128/543 [00:02<00:08, 47.12it/s][A
 24%|██▍       | 133/543 [00:02<00:08, 47.01it/s][A
 25%|██▌       | 138/543 [00:02<00:08, 46.98it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 46.85it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 46.90it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 46.98it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 47.05it/s][A
 30%|███       | 163/543 [00:03<00:08, 47.11it/s][A
 31%|███       | 168/543 [00:03<00:07, 47.12it/s][A
 32%|███▏      | 173/543 [00:03<00:07, 47.07it/s][A
 33%|███▎      | 178/543 [00:03<00:07, 47.02it/s][A
 34%|███▎      | 183/543 [00:03<00:07, 46.91it/s][A
 35%|███▍      | 188/543 [00:03<00:07, 46.91it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 46.88it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 46.80it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 46.86it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 47.05it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 47.08it/s][A
 40%|████      | 218/543 [00:04<00:06, 47.12it/s][A
 41%|████      | 223/543 [00:04<00:06, 47.09it/s][A
 42%|████▏     | 228/543 [00:04<00:06, 46.94it/s][A
 43%|████▎     | 233/543 [00:04<00:06, 46.93it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 46.89it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 46.86it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 46.95it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 46.94it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 47.10it/s][A
 48%|████▊     | 263/543 [00:05<00:05, 47.14it/s][A
 49%|████▉     | 268/543 [00:05<00:05, 47.20it/s][A
 50%|█████     | 273/543 [00:05<00:05, 47.17it/s][A
 51%|█████     | 278/543 [00:05<00:05, 46.65it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 46.98it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 46.93it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 46.94it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 47.07it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 47.09it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 46.95it/s][A
 58%|█████▊    | 313/543 [00:06<00:04, 47.10it/s][A
 59%|█████▊    | 318/543 [00:06<00:04, 47.11it/s][A
 59%|█████▉    | 323/543 [00:06<00:04, 47.01it/s][A
 60%|██████    | 328/543 [00:06<00:04, 47.06it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 46.95it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 46.85it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 47.01it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 47.08it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 47.03it/s][A
 66%|██████▌   | 358/543 [00:07<00:03, 47.05it/s][A
 67%|██████▋   | 363/543 [00:07<00:03, 47.02it/s][A
 68%|██████▊   | 368/543 [00:07<00:03, 46.99it/s][A
 69%|██████▊   | 373/543 [00:07<00:03, 47.01it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 47.01it/s][A
 71%|███████   | 383/543 [00:08<00:03, 46.91it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 46.99it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 47.06it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 47.04it/s][A
 74%|███████▍  | 403/543 [00:08<00:02, 47.08it/s][A
 75%|███████▌  | 408/543 [00:08<00:02, 47.12it/s][A
 76%|███████▌  | 413/543 [00:08<00:02, 47.03it/s][A
 77%|███████▋  | 418/543 [00:08<00:02, 46.90it/s][A
 78%|███████▊  | 423/543 [00:08<00:02, 46.98it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 46.89it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 46.93it/s][A
 81%|████████  | 438/543 [00:09<00:02, 47.01it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 46.95it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 46.90it/s][A
 83%|████████▎ | 453/543 [00:09<00:01, 47.05it/s][A
 84%|████████▍ | 458/543 [00:09<00:01, 47.07it/s][A
 85%|████████▌ | 463/543 [00:09<00:01, 47.08it/s][A
 86%|████████▌ | 468/543 [00:09<00:01, 47.07it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 46.56it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 46.84it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 46.97it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 46.96it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 46.98it/s][A
 92%|█████████▏| 498/543 [00:10<00:00, 47.08it/s][A
 93%|█████████▎| 503/543 [00:10<00:00, 46.98it/s][A
 94%|█████████▎| 508/543 [00:10<00:00, 46.96it/s][A
 94%|█████████▍| 513/543 [00:10<00:00, 47.03it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 46.93it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 46.90it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 46.78it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 46.87it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 46.92it/s][A
100%|██████████| 543/543 [00:11<00:00, 46.95it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:13<00:00,  3.45it/s]
100%|██████████| 543/543 [00:11<00:00, 46.95it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:46:56,766 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 08:46:56,783 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:46:58,856 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:46:58,874 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:46:58,884 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 08:47:03,279 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 08:47:03,282 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117 (score: 1.0670812129974365).
                                                 100%|██████████| 585/585 [04:22<00:00,  3.45it/s]100%|██████████| 585/585 [04:22<00:00,  2.23it/s]
[INFO|trainer.py:1894] 2023-08-29 08:47:04,964 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 08:47:04,982 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:47:07,283 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:47:07,299 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:47:07,309 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:47:07,523 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:07,523 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:07,523 >>   train_loss               =     0.4059
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:07,524 >>   train_runtime            = 0:04:22.19
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:07,524 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:07,524 >>   train_samples_per_second =    143.023
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:07,524 >>   train_steps_per_second   =      2.231
{'eval_loss': 1.1160199642181396, 'eval_runtime': 11.5551, 'eval_samples_per_second': 375.764, 'eval_steps_per_second': 46.992, 'epoch': 5.0}
{'train_runtime': 262.1961, 'train_samples_per_second': 143.023, 'train_steps_per_second': 2.231, 'train_loss': 0.40592211209810697, 'epoch': 5.0}
08/29/2023 08:47:07 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 08:47:07,567 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:47:07,567 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 08:47:07,568 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 59.50it/s]  2%|▏         | 12/543 [00:00<00:10, 52.03it/s]  3%|▎         | 18/543 [00:00<00:10, 49.83it/s]  4%|▍         | 24/543 [00:00<00:10, 48.99it/s]  5%|▌         | 29/543 [00:00<00:10, 48.59it/s]  6%|▋         | 34/543 [00:00<00:10, 48.21it/s]  7%|▋         | 39/543 [00:00<00:10, 48.12it/s]  8%|▊         | 44/543 [00:00<00:10, 47.75it/s]  9%|▉         | 49/543 [00:01<00:10, 47.32it/s] 10%|▉         | 54/543 [00:01<00:10, 47.35it/s] 11%|█         | 59/543 [00:01<00:10, 47.40it/s] 12%|█▏        | 64/543 [00:01<00:10, 47.39it/s] 13%|█▎        | 69/543 [00:01<00:09, 47.42it/s] 14%|█▎        | 74/543 [00:01<00:09, 47.40it/s] 15%|█▍        | 79/543 [00:01<00:09, 47.50it/s] 15%|█▌        | 84/543 [00:01<00:09, 47.54it/s] 16%|█▋        | 89/543 [00:01<00:09, 47.37it/s] 17%|█▋        | 94/543 [00:01<00:09, 47.24it/s] 18%|█▊        | 99/543 [00:02<00:09, 47.16it/s] 19%|█▉        | 104/543 [00:02<00:09, 47.17it/s] 20%|██        | 109/543 [00:02<00:09, 47.22it/s] 21%|██        | 114/543 [00:02<00:09, 47.30it/s] 22%|██▏       | 119/543 [00:02<00:08, 47.33it/s] 23%|██▎       | 124/543 [00:02<00:08, 47.36it/s] 24%|██▍       | 129/543 [00:02<00:08, 47.43it/s] 25%|██▍       | 134/543 [00:02<00:08, 47.32it/s] 26%|██▌       | 139/543 [00:02<00:08, 47.18it/s] 27%|██▋       | 144/543 [00:03<00:08, 47.09it/s] 27%|██▋       | 149/543 [00:03<00:08, 47.06it/s] 28%|██▊       | 154/543 [00:03<00:08, 47.17it/s] 29%|██▉       | 159/543 [00:03<00:08, 47.12it/s] 30%|███       | 164/543 [00:03<00:08, 47.18it/s] 31%|███       | 169/543 [00:03<00:07, 47.09it/s] 32%|███▏      | 174/543 [00:03<00:07, 47.30it/s] 33%|███▎      | 179/543 [00:03<00:07, 47.31it/s] 34%|███▍      | 184/543 [00:03<00:07, 47.22it/s] 35%|███▍      | 189/543 [00:03<00:07, 47.12it/s] 36%|███▌      | 194/543 [00:04<00:07, 47.13it/s] 37%|███▋      | 199/543 [00:04<00:07, 46.97it/s] 38%|███▊      | 204/543 [00:04<00:07, 47.12it/s] 38%|███▊      | 209/543 [00:04<00:07, 47.21it/s] 39%|███▉      | 214/543 [00:04<00:06, 47.25it/s] 40%|████      | 219/543 [00:04<00:06, 47.29it/s] 41%|████▏     | 224/543 [00:04<00:06, 47.37it/s] 42%|████▏     | 229/543 [00:04<00:06, 47.23it/s] 43%|████▎     | 234/543 [00:04<00:06, 47.15it/s] 44%|████▍     | 239/543 [00:05<00:06, 47.14it/s] 45%|████▍     | 244/543 [00:05<00:06, 47.10it/s] 46%|████▌     | 249/543 [00:05<00:06, 47.10it/s] 47%|████▋     | 254/543 [00:05<00:06, 47.13it/s] 48%|████▊     | 259/543 [00:05<00:06, 47.19it/s] 49%|████▊     | 264/543 [00:05<00:05, 47.23it/s] 50%|████▉     | 269/543 [00:05<00:05, 47.22it/s] 50%|█████     | 274/543 [00:05<00:05, 47.28it/s] 51%|█████▏    | 279/543 [00:05<00:05, 47.28it/s] 52%|█████▏    | 284/543 [00:05<00:05, 47.15it/s] 53%|█████▎    | 289/543 [00:06<00:05, 46.96it/s] 54%|█████▍    | 294/543 [00:06<00:05, 47.08it/s] 55%|█████▌    | 299/543 [00:06<00:05, 47.12it/s] 56%|█████▌    | 304/543 [00:06<00:05, 47.16it/s] 57%|█████▋    | 309/543 [00:06<00:04, 47.30it/s] 58%|█████▊    | 314/543 [00:06<00:04, 47.27it/s] 59%|█████▊    | 319/543 [00:06<00:04, 47.17it/s] 60%|█████▉    | 324/543 [00:06<00:04, 47.17it/s] 61%|██████    | 329/543 [00:06<00:04, 47.11it/s] 62%|██████▏   | 334/543 [00:07<00:04, 47.10it/s] 62%|██████▏   | 339/543 [00:07<00:04, 47.09it/s] 63%|██████▎   | 344/543 [00:07<00:04, 47.13it/s] 64%|██████▍   | 349/543 [00:07<00:04, 47.15it/s] 65%|██████▌   | 354/543 [00:07<00:04, 47.12it/s] 66%|██████▌   | 359/543 [00:07<00:03, 47.22it/s] 67%|██████▋   | 364/543 [00:07<00:03, 47.30it/s] 68%|██████▊   | 369/543 [00:07<00:03, 47.22it/s] 69%|██████▉   | 374/543 [00:07<00:03, 47.16it/s] 70%|██████▉   | 379/543 [00:07<00:03, 47.12it/s] 71%|███████   | 384/543 [00:08<00:03, 46.95it/s] 72%|███████▏  | 389/543 [00:08<00:03, 46.99it/s] 73%|███████▎  | 394/543 [00:08<00:03, 47.11it/s] 73%|███████▎  | 399/543 [00:08<00:03, 47.10it/s] 74%|███████▍  | 404/543 [00:08<00:02, 47.16it/s] 75%|███████▌  | 409/543 [00:08<00:02, 47.20it/s] 76%|███████▌  | 414/543 [00:08<00:02, 47.09it/s] 77%|███████▋  | 419/543 [00:08<00:02, 47.05it/s] 78%|███████▊  | 424/543 [00:08<00:02, 47.07it/s] 79%|███████▉  | 429/543 [00:09<00:02, 47.05it/s] 80%|███████▉  | 434/543 [00:09<00:02, 47.06it/s] 81%|████████  | 439/543 [00:09<00:02, 47.14it/s] 82%|████████▏ | 444/543 [00:09<00:02, 47.02it/s] 83%|████████▎ | 449/543 [00:09<00:01, 47.03it/s] 84%|████████▎ | 454/543 [00:09<00:01, 47.21it/s] 85%|████████▍ | 459/543 [00:09<00:01, 47.23it/s] 85%|████████▌ | 464/543 [00:09<00:01, 47.14it/s] 86%|████████▋ | 469/543 [00:09<00:01, 47.18it/s] 87%|████████▋ | 474/543 [00:10<00:01, 47.08it/s] 88%|████████▊ | 479/543 [00:10<00:01, 47.04it/s] 89%|████████▉ | 484/543 [00:10<00:01, 47.13it/s] 90%|█████████ | 489/543 [00:10<00:01, 47.16it/s] 91%|█████████ | 494/543 [00:10<00:01, 47.15it/s] 92%|█████████▏| 499/543 [00:10<00:00, 47.11it/s] 93%|█████████▎| 504/543 [00:10<00:00, 47.20it/s] 94%|█████████▎| 509/543 [00:10<00:00, 47.04it/s] 95%|█████████▍| 514/543 [00:10<00:00, 47.06it/s] 96%|█████████▌| 519/543 [00:10<00:00, 47.19it/s] 97%|█████████▋| 524/543 [00:11<00:00, 47.23it/s] 97%|█████████▋| 529/543 [00:11<00:00, 47.13it/s] 98%|█████████▊| 534/543 [00:11<00:00, 47.15it/s] 99%|█████████▉| 539/543 [00:11<00:00, 47.07it/s]100%|██████████| 543/543 [00:11<00:00, 47.28it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:47:19,076 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:19,076 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:19,076 >>   eval_loss               =     1.0671
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:19,076 >>   eval_runtime            = 0:00:11.50
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:19,076 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:19,076 >>   eval_samples_per_second =     377.29
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:19,076 >>   eval_steps_per_second   =     47.183
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:47:19,076 >>   perplexity              =     2.9069
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:25,773 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:25,779 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:25,779 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:25,779 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:25,779 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:47:26,374 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:47:26,375 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:47:26,950 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:47:27,962 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:47:27,962 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:30,765 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:30,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:30,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:30,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:30,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:47:31,409 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:47:31,411 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:47:31,964 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:47:32,115 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:47:32,115 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:06,  1.70it/s]Extractor Predicting: 11it [00:06,  1.71it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:09,  1.55it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:11,  1.47it/s]Extractor Predicting: 20it [00:12,  1.45it/s]Extractor Predicting: 21it [00:13,  1.48it/s]Extractor Predicting: 22it [00:13,  1.51it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:15,  1.51it/s]Extractor Predicting: 26it [00:16,  1.50it/s]Extractor Predicting: 27it [00:17,  1.49it/s]Extractor Predicting: 28it [00:17,  1.51it/s]Extractor Predicting: 29it [00:18,  1.49it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:19,  1.50it/s]Extractor Predicting: 32it [00:20,  1.51it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:22,  1.42it/s]Extractor Predicting: 35it [00:22,  1.43it/s]Extractor Predicting: 36it [00:23,  1.42it/s]Extractor Predicting: 37it [00:24,  1.43it/s]Extractor Predicting: 38it [00:24,  1.43it/s]Extractor Predicting: 39it [00:25,  1.46it/s]Extractor Predicting: 40it [00:26,  1.47it/s]Extractor Predicting: 41it [00:26,  1.46it/s]Extractor Predicting: 42it [00:27,  1.45it/s]Extractor Predicting: 43it [00:28,  1.48it/s]Extractor Predicting: 44it [00:28,  1.50it/s]Extractor Predicting: 45it [00:29,  1.50it/s]Extractor Predicting: 46it [00:30,  1.52it/s]Extractor Predicting: 47it [00:30,  1.51it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:32,  1.49it/s]Extractor Predicting: 50it [00:32,  1.50it/s]Extractor Predicting: 51it [00:33,  1.51it/s]Extractor Predicting: 52it [00:34,  1.49it/s]Extractor Predicting: 53it [00:34,  1.47it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:36,  1.53it/s]Extractor Predicting: 56it [00:36,  1.49it/s]Extractor Predicting: 57it [00:37,  1.48it/s]Extractor Predicting: 58it [00:38,  1.51it/s]Extractor Predicting: 59it [00:38,  1.48it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.50it/s]Extractor Predicting: 62it [00:40,  1.51it/s]Extractor Predicting: 63it [00:41,  1.50it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:42,  1.54it/s]Extractor Predicting: 66it [00:43,  1.55it/s]Extractor Predicting: 67it [00:44,  1.54it/s]Extractor Predicting: 68it [00:44,  1.51it/s]Extractor Predicting: 69it [00:45,  1.52it/s]Extractor Predicting: 70it [00:46,  1.51it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:47,  1.51it/s]Extractor Predicting: 73it [00:48,  1.50it/s]Extractor Predicting: 74it [00:48,  1.48it/s]Extractor Predicting: 75it [00:49,  1.45it/s]Extractor Predicting: 76it [00:50,  1.47it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.49it/s]Extractor Predicting: 79it [00:52,  1.48it/s]Extractor Predicting: 80it [00:52,  1.50it/s]Extractor Predicting: 81it [00:53,  1.50it/s]Extractor Predicting: 82it [00:54,  1.51it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:55,  1.53it/s]Extractor Predicting: 85it [00:56,  1.54it/s]Extractor Predicting: 86it [00:56,  1.51it/s]Extractor Predicting: 87it [00:57,  1.51it/s]Extractor Predicting: 88it [00:58,  1.53it/s]Extractor Predicting: 89it [00:58,  1.54it/s]Extractor Predicting: 90it [00:59,  1.52it/s]Extractor Predicting: 91it [01:00,  1.49it/s]Extractor Predicting: 92it [01:00,  1.47it/s]Extractor Predicting: 93it [01:01,  1.52it/s]Extractor Predicting: 94it [01:02,  1.53it/s]Extractor Predicting: 95it [01:02,  1.52it/s]Extractor Predicting: 96it [01:03,  1.53it/s]Extractor Predicting: 97it [01:03,  1.55it/s]Extractor Predicting: 98it [01:04,  1.53it/s]Extractor Predicting: 99it [01:05,  1.52it/s]Extractor Predicting: 100it [01:05,  1.52it/s]Extractor Predicting: 101it [01:06,  1.53it/s]Extractor Predicting: 102it [01:07,  1.40it/s]Extractor Predicting: 103it [01:08,  1.43it/s]Extractor Predicting: 104it [01:08,  1.46it/s]Extractor Predicting: 105it [01:09,  1.45it/s]Extractor Predicting: 106it [01:10,  1.49it/s]Extractor Predicting: 107it [01:10,  1.49it/s]Extractor Predicting: 108it [01:11,  1.49it/s]Extractor Predicting: 109it [01:12,  1.49it/s]Extractor Predicting: 110it [01:12,  1.48it/s]Extractor Predicting: 111it [01:13,  1.50it/s]Extractor Predicting: 112it [01:14,  1.53it/s]Extractor Predicting: 113it [01:14,  1.53it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:16,  1.50it/s]Extractor Predicting: 116it [01:16,  1.50it/s]Extractor Predicting: 117it [01:17,  1.52it/s]Extractor Predicting: 118it [01:18,  1.48it/s]Extractor Predicting: 119it [01:18,  1.49it/s]Extractor Predicting: 120it [01:19,  1.48it/s]Extractor Predicting: 121it [01:20,  1.46it/s]Extractor Predicting: 122it [01:20,  1.46it/s]Extractor Predicting: 123it [01:21,  1.49it/s]Extractor Predicting: 124it [01:22,  1.51it/s]Extractor Predicting: 125it [01:22,  1.52it/s]Extractor Predicting: 126it [01:23,  1.48it/s]Extractor Predicting: 127it [01:24,  1.47it/s]Extractor Predicting: 128it [01:24,  1.50it/s]Extractor Predicting: 129it [01:25,  1.51it/s]Extractor Predicting: 130it [01:26,  1.56it/s]Extractor Predicting: 131it [01:26,  1.52it/s]Extractor Predicting: 132it [01:27,  1.51it/s]Extractor Predicting: 133it [01:28,  1.51it/s]Extractor Predicting: 134it [01:28,  1.52it/s]Extractor Predicting: 135it [01:29,  1.54it/s]Extractor Predicting: 136it [01:30,  1.53it/s]Extractor Predicting: 137it [01:30,  1.55it/s]Extractor Predicting: 138it [01:31,  1.51it/s]Extractor Predicting: 139it [01:32,  1.50it/s]Extractor Predicting: 140it [01:32,  1.52it/s]Extractor Predicting: 141it [01:33,  1.53it/s]Extractor Predicting: 142it [01:33,  1.56it/s]Extractor Predicting: 143it [01:34,  1.56it/s]Extractor Predicting: 144it [01:35,  1.56it/s]Extractor Predicting: 145it [01:35,  1.53it/s]Extractor Predicting: 146it [01:36,  1.54it/s]Extractor Predicting: 147it [01:37,  1.55it/s]Extractor Predicting: 148it [01:37,  1.56it/s]Extractor Predicting: 149it [01:38,  1.56it/s]Extractor Predicting: 150it [01:39,  1.52it/s]Extractor Predicting: 151it [01:39,  1.52it/s]Extractor Predicting: 152it [01:40,  1.52it/s]Extractor Predicting: 153it [01:41,  1.50it/s]Extractor Predicting: 154it [01:41,  1.51it/s]Extractor Predicting: 155it [01:42,  1.49it/s]Extractor Predicting: 156it [01:43,  1.48it/s]Extractor Predicting: 157it [01:43,  1.48it/s]Extractor Predicting: 158it [01:44,  1.49it/s]Extractor Predicting: 159it [01:45,  1.52it/s]Extractor Predicting: 160it [01:45,  1.55it/s]Extractor Predicting: 161it [01:46,  1.56it/s]Extractor Predicting: 162it [01:47,  1.56it/s]Extractor Predicting: 163it [01:47,  1.45it/s]Extractor Predicting: 163it [01:47,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:27,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:27,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:27,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:27,093 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:27,093 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:49:27,382 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:49:27,383 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:49:28,040 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:49:29,078 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:49:29,078 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:31,610 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:31,614 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:31,614 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:31,614 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:49:31,615 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:49:31,932 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:49:31,933 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:49:32,617 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:49:32,761 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:49:32,761 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.1826086956521739,
  "recall": 0.019345923537540305,
  "score": 0.03498542274052478,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.59it/s]Extractor Predicting: 28it [00:17,  1.57it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:18,  1.58it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.58it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:21,  1.56it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:25,  1.59it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:27,  1.51it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:28,  1.49it/s]Extractor Predicting: 46it [00:29,  1.47it/s]Extractor Predicting: 47it [00:30,  1.45it/s]Extractor Predicting: 48it [00:30,  1.48it/s]Extractor Predicting: 49it [00:31,  1.47it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:32,  1.48it/s]Extractor Predicting: 52it [00:33,  1.48it/s]Extractor Predicting: 53it [00:34,  1.47it/s]Extractor Predicting: 54it [00:34,  1.42it/s]Extractor Predicting: 55it [00:35,  1.46it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:36,  1.49it/s]Extractor Predicting: 58it [00:37,  1.49it/s]Extractor Predicting: 59it [00:38,  1.46it/s]Extractor Predicting: 60it [00:38,  1.46it/s]Extractor Predicting: 61it [00:39,  1.49it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.44it/s]Extractor Predicting: 64it [00:41,  1.45it/s]Extractor Predicting: 65it [00:42,  1.46it/s]Extractor Predicting: 66it [00:43,  1.44it/s]Extractor Predicting: 67it [00:43,  1.45it/s]Extractor Predicting: 68it [00:44,  1.44it/s]Extractor Predicting: 69it [00:45,  1.45it/s]Extractor Predicting: 70it [00:45,  1.46it/s]Extractor Predicting: 71it [00:46,  1.48it/s]Extractor Predicting: 72it [00:47,  1.47it/s]Extractor Predicting: 73it [00:47,  1.47it/s]Extractor Predicting: 74it [00:48,  1.50it/s]Extractor Predicting: 75it [00:49,  1.50it/s]Extractor Predicting: 76it [00:49,  1.51it/s]Extractor Predicting: 77it [00:50,  1.53it/s]Extractor Predicting: 78it [00:51,  1.51it/s]Extractor Predicting: 79it [00:51,  1.48it/s]Extractor Predicting: 80it [00:52,  1.51it/s]Extractor Predicting: 81it [00:53,  1.39it/s]Extractor Predicting: 82it [00:53,  1.44it/s]Extractor Predicting: 83it [00:54,  1.47it/s]Extractor Predicting: 84it [00:55,  1.44it/s]Extractor Predicting: 85it [00:56,  1.46it/s]Extractor Predicting: 86it [00:56,  1.48it/s]Extractor Predicting: 87it [00:57,  1.51it/s]Extractor Predicting: 88it [00:57,  1.48it/s]Extractor Predicting: 89it [00:58,  1.52it/s]Extractor Predicting: 90it [00:59,  1.53it/s]Extractor Predicting: 91it [00:59,  1.56it/s]Extractor Predicting: 92it [01:00,  1.54it/s]Extractor Predicting: 93it [01:01,  1.54it/s]Extractor Predicting: 94it [01:01,  1.53it/s]Extractor Predicting: 95it [01:02,  1.55it/s]Extractor Predicting: 96it [01:03,  1.41it/s]Extractor Predicting: 97it [01:03,  1.45it/s]Extractor Predicting: 98it [01:04,  1.42it/s]Extractor Predicting: 99it [01:05,  1.44it/s]Extractor Predicting: 100it [01:06,  1.45it/s]Extractor Predicting: 101it [01:06,  1.46it/s]Extractor Predicting: 102it [01:07,  1.48it/s]Extractor Predicting: 103it [01:08,  1.46it/s]Extractor Predicting: 104it [01:08,  1.49it/s]Extractor Predicting: 105it [01:09,  1.48it/s]Extractor Predicting: 106it [01:10,  1.44it/s]Extractor Predicting: 107it [01:10,  1.43it/s]Extractor Predicting: 108it [01:11,  1.46it/s]Extractor Predicting: 109it [01:12,  1.41it/s]Extractor Predicting: 110it [01:12,  1.43it/s]Extractor Predicting: 111it [01:13,  1.43it/s]Extractor Predicting: 112it [01:14,  1.45it/s]Extractor Predicting: 113it [01:15,  1.45it/s]Extractor Predicting: 114it [01:15,  1.45it/s]Extractor Predicting: 115it [01:16,  1.45it/s]Extractor Predicting: 116it [01:17,  1.43it/s]Extractor Predicting: 117it [01:17,  1.44it/s]Extractor Predicting: 118it [01:18,  1.48it/s]Extractor Predicting: 119it [01:19,  1.48it/s]Extractor Predicting: 120it [01:19,  1.52it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.52it/s]Extractor Predicting: 123it [01:21,  1.54it/s]Extractor Predicting: 124it [01:22,  1.57it/s]Extractor Predicting: 125it [01:22,  1.54it/s]Extractor Predicting: 126it [01:23,  1.51it/s]Extractor Predicting: 127it [01:24,  1.51it/s]Extractor Predicting: 128it [01:25,  1.49it/s]Extractor Predicting: 129it [01:25,  1.49it/s]Extractor Predicting: 130it [01:26,  1.49it/s]Extractor Predicting: 131it [01:26,  1.55it/s]Extractor Predicting: 132it [01:27,  1.52it/s]Extractor Predicting: 133it [01:28,  1.52it/s]Extractor Predicting: 134it [01:28,  1.55it/s]Extractor Predicting: 135it [01:29,  1.54it/s]Extractor Predicting: 136it [01:30,  1.55it/s]Extractor Predicting: 137it [01:30,  1.59it/s]Extractor Predicting: 138it [01:31,  1.57it/s]Extractor Predicting: 139it [01:32,  1.56it/s]Extractor Predicting: 140it [01:32,  1.55it/s]Extractor Predicting: 141it [01:33,  1.56it/s]Extractor Predicting: 142it [01:34,  1.54it/s]Extractor Predicting: 143it [01:34,  1.51it/s]Extractor Predicting: 144it [01:35,  1.52it/s]Extractor Predicting: 145it [01:36,  1.47it/s]Extractor Predicting: 146it [01:36,  1.43it/s]Extractor Predicting: 147it [01:37,  1.46it/s]Extractor Predicting: 148it [01:38,  1.46it/s]Extractor Predicting: 149it [01:38,  1.48it/s]Extractor Predicting: 150it [01:39,  1.49it/s]Extractor Predicting: 151it [01:40,  1.49it/s]Extractor Predicting: 152it [01:40,  1.49it/s]Extractor Predicting: 153it [01:41,  1.53it/s]Extractor Predicting: 154it [01:42,  1.54it/s]Extractor Predicting: 155it [01:42,  1.57it/s]Extractor Predicting: 156it [01:43,  1.51it/s]Extractor Predicting: 157it [01:44,  1.52it/s]Extractor Predicting: 158it [01:44,  1.52it/s]Extractor Predicting: 159it [01:45,  1.52it/s]Extractor Predicting: 160it [01:46,  1.54it/s]Extractor Predicting: 161it [01:46,  1.47it/s]Extractor Predicting: 162it [01:47,  1.52it/s]Extractor Predicting: 163it [01:48,  1.55it/s]Extractor Predicting: 164it [01:48,  1.53it/s]Extractor Predicting: 165it [01:49,  1.58it/s]Extractor Predicting: 166it [01:49,  1.59it/s]Extractor Predicting: 167it [01:50,  1.57it/s]Extractor Predicting: 168it [01:51,  1.62it/s]Extractor Predicting: 169it [01:51,  1.63it/s]Extractor Predicting: 170it [01:52,  1.59it/s]Extractor Predicting: 171it [01:53,  1.58it/s]Extractor Predicting: 172it [01:53,  1.58it/s]Extractor Predicting: 173it [01:54,  1.55it/s]Extractor Predicting: 174it [01:54,  1.58it/s]Extractor Predicting: 175it [01:55,  1.57it/s]Extractor Predicting: 176it [01:56,  1.56it/s]Extractor Predicting: 177it [01:56,  1.54it/s]Extractor Predicting: 178it [01:57,  1.53it/s]Extractor Predicting: 179it [01:58,  1.61it/s]Extractor Predicting: 180it [01:58,  1.58it/s]Extractor Predicting: 181it [01:59,  1.58it/s]Extractor Predicting: 182it [02:00,  1.55it/s]Extractor Predicting: 183it [02:00,  1.55it/s]Extractor Predicting: 184it [02:01,  1.54it/s]Extractor Predicting: 185it [02:02,  1.57it/s]Extractor Predicting: 186it [02:02,  1.54it/s]Extractor Predicting: 187it [02:03,  1.53it/s]Extractor Predicting: 188it [02:04,  1.51it/s]Extractor Predicting: 189it [02:04,  1.50it/s]Extractor Predicting: 190it [02:05,  1.47it/s]Extractor Predicting: 191it [02:06,  1.48it/s]Extractor Predicting: 192it [02:06,  1.50it/s]Extractor Predicting: 193it [02:07,  1.54it/s]Extractor Predicting: 194it [02:07,  1.54it/s]Extractor Predicting: 195it [02:08,  1.52it/s]Extractor Predicting: 196it [02:09,  1.53it/s]Extractor Predicting: 197it [02:09,  1.53it/s]Extractor Predicting: 198it [02:10,  1.39it/s]Extractor Predicting: 199it [02:11,  1.42it/s]Extractor Predicting: 200it [02:12,  1.43it/s]Extractor Predicting: 201it [02:12,  1.46it/s]Extractor Predicting: 202it [02:13,  1.47it/s]Extractor Predicting: 203it [02:14,  1.49it/s]Extractor Predicting: 204it [02:14,  1.50it/s]Extractor Predicting: 205it [02:15,  1.49it/s]Extractor Predicting: 206it [02:16,  1.49it/s]Extractor Predicting: 207it [02:16,  1.51it/s]Extractor Predicting: 208it [02:17,  1.51it/s]Extractor Predicting: 209it [02:18,  1.51it/s]Extractor Predicting: 210it [02:18,  1.50it/s]Extractor Predicting: 211it [02:19,  1.50it/s]Extractor Predicting: 212it [02:20,  1.52it/s]Extractor Predicting: 213it [02:20,  1.51it/s]Extractor Predicting: 214it [02:21,  1.51it/s]Extractor Predicting: 215it [02:22,  1.47it/s]Extractor Predicting: 216it [02:22,  1.49it/s]Extractor Predicting: 217it [02:23,  1.52it/s]Extractor Predicting: 218it [02:24,  1.51it/s]Extractor Predicting: 219it [02:24,  1.50it/s]Extractor Predicting: 220it [02:25,  1.51it/s]Extractor Predicting: 221it [02:26,  1.49it/s]Extractor Predicting: 222it [02:26,  1.51it/s]Extractor Predicting: 223it [02:27,  1.45it/s]Extractor Predicting: 224it [02:28,  1.46it/s]Extractor Predicting: 225it [02:28,  1.45it/s]Extractor Predicting: 226it [02:29,  1.46it/s]Extractor Predicting: 227it [02:30,  1.42it/s]Extractor Predicting: 228it [02:31,  1.38it/s]Extractor Predicting: 229it [02:31,  1.41it/s]Extractor Predicting: 230it [02:32,  1.42it/s]Extractor Predicting: 231it [02:33,  1.43it/s]Extractor Predicting: 232it [02:33,  1.44it/s]Extractor Predicting: 233it [02:34,  1.45it/s]Extractor Predicting: 234it [02:35,  1.46it/s]Extractor Predicting: 235it [02:35,  1.47it/s]Extractor Predicting: 236it [02:36,  1.48it/s]Extractor Predicting: 237it [02:37,  1.48it/s]Extractor Predicting: 238it [02:37,  1.53it/s]Extractor Predicting: 239it [02:38,  1.55it/s]Extractor Predicting: 240it [02:39,  1.60it/s]Extractor Predicting: 241it [02:39,  1.59it/s]Extractor Predicting: 242it [02:40,  1.59it/s]Extractor Predicting: 243it [02:40,  1.55it/s]Extractor Predicting: 244it [02:41,  1.55it/s]Extractor Predicting: 245it [02:42,  1.58it/s]Extractor Predicting: 246it [02:42,  1.56it/s]Extractor Predicting: 247it [02:43,  1.58it/s]Extractor Predicting: 248it [02:44,  1.59it/s]Extractor Predicting: 249it [02:44,  1.64it/s]Extractor Predicting: 250it [02:45,  1.64it/s]Extractor Predicting: 251it [02:45,  1.69it/s]Extractor Predicting: 252it [02:46,  1.71it/s]Extractor Predicting: 253it [02:47,  1.67it/s]Extractor Predicting: 254it [02:47,  1.68it/s]Extractor Predicting: 255it [02:48,  1.63it/s]Extractor Predicting: 256it [02:48,  1.67it/s]Extractor Predicting: 257it [02:49,  1.63it/s]Extractor Predicting: 258it [02:50,  1.59it/s]Extractor Predicting: 259it [02:50,  1.54it/s]Extractor Predicting: 260it [02:51,  1.60it/s]Extractor Predicting: 261it [02:52,  1.56it/s]Extractor Predicting: 262it [02:52,  1.58it/s]Extractor Predicting: 263it [02:53,  1.62it/s]Extractor Predicting: 264it [02:53,  1.63it/s]Extractor Predicting: 265it [02:54,  1.62it/s]Extractor Predicting: 266it [02:55,  1.64it/s]Extractor Predicting: 267it [02:55,  1.67it/s]Extractor Predicting: 268it [02:56,  1.65it/s]Extractor Predicting: 269it [02:56,  1.62it/s]Extractor Predicting: 270it [02:57,  1.63it/s]Extractor Predicting: 271it [02:58,  1.62it/s]Extractor Predicting: 272it [02:58,  1.59it/s]Extractor Predicting: 273it [02:59,  1.64it/s]Extractor Predicting: 274it [03:00,  1.65it/s]Extractor Predicting: 275it [03:00,  1.63it/s]Extractor Predicting: 276it [03:01,  1.62it/s]Extractor Predicting: 277it [03:01,  1.61it/s]Extractor Predicting: 278it [03:02,  1.64it/s]Extractor Predicting: 279it [03:03,  1.58it/s]Extractor Predicting: 280it [03:03,  1.64it/s]Extractor Predicting: 281it [03:04,  1.65it/s]Extractor Predicting: 282it [03:04,  1.66it/s]Extractor Predicting: 283it [03:05,  1.67it/s]Extractor Predicting: 284it [03:06,  1.66it/s]Extractor Predicting: 285it [03:06,  1.66it/s]Extractor Predicting: 286it [03:07,  1.66it/s]Extractor Predicting: 287it [03:08,  1.47it/s]Extractor Predicting: 288it [03:08,  1.48it/s]Extractor Predicting: 289it [03:09,  1.51it/s]Extractor Predicting: 290it [03:10,  1.47it/s]Extractor Predicting: 291it [03:10,  1.51it/s]Extractor Predicting: 292it [03:11,  1.56it/s]Extractor Predicting: 293it [03:12,  1.58it/s]Extractor Predicting: 294it [03:12,  1.60it/s]Extractor Predicting: 295it [03:13,  1.61it/s]Extractor Predicting: 296it [03:13,  1.53it/s]Extractor Predicting: 297it [03:14,  1.52it/s]Extractor Predicting: 298it [03:15,  1.46it/s]Extractor Predicting: 299it [03:16,  1.47it/s]Extractor Predicting: 300it [03:16,  1.49it/s]Extractor Predicting: 301it [03:17,  1.50it/s]Extractor Predicting: 302it [03:18,  1.50it/s]Extractor Predicting: 303it [03:18,  1.47it/s]Extractor Predicting: 304it [03:19,  1.47it/s]Extractor Predicting: 305it [03:20,  1.50it/s]Extractor Predicting: 306it [03:20,  1.47it/s]Extractor Predicting: 307it [03:21,  1.44it/s]Extractor Predicting: 308it [03:22,  1.44it/s]Extractor Predicting: 309it [03:22,  1.47it/s]Extractor Predicting: 310it [03:23,  1.50it/s]Extractor Predicting: 311it [03:24,  1.51it/s]Extractor Predicting: 312it [03:24,  1.49it/s]Extractor Predicting: 313it [03:25,  1.46it/s]Extractor Predicting: 314it [03:26,  1.45it/s]Extractor Predicting: 315it [03:26,  1.44it/s]Extractor Predicting: 316it [03:27,  1.44it/s]Extractor Predicting: 317it [03:28,  1.44it/s]Extractor Predicting: 318it [03:28,  1.46it/s]Extractor Predicting: 319it [03:29,  1.47it/s]Extractor Predicting: 320it [03:30,  1.47it/s]Extractor Predicting: 321it [03:30,  1.52it/s]Extractor Predicting: 322it [03:31,  1.52it/s]Extractor Predicting: 323it [03:32,  1.50it/s]Extractor Predicting: 324it [03:32,  1.49it/s]Extractor Predicting: 325it [03:33,  1.48it/s]Extractor Predicting: 326it [03:34,  1.47it/s]Extractor Predicting: 327it [03:35,  1.45it/s]Extractor Predicting: 328it [03:35,  1.45it/s]Extractor Predicting: 329it [03:36,  1.50it/s]Extractor Predicting: 330it [03:36,  1.52it/s]Extractor Predicting: 331it [03:37,  1.67it/s]Extractor Predicting: 331it [03:37,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:17,758 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:17,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:17,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:17,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:17,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:53:18,064 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:53:18,066 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:53:18,329 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:53:19,359 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:53:19,359 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:21,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:21,191 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:21,192 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:21,192 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:21,192 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:53:21,629 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:53:21,630 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:53:21,899 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:53:22,053 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:53:22,053 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4648967551622419,
  "recall": 0.09935695372588577,
  "score": 0.1637232495325161,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.44it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.45it/s]Extractor Predicting: 10it [00:06,  1.45it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:08,  1.44it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:11,  1.44it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.45it/s]Extractor Predicting: 19it [00:13,  1.44it/s]Extractor Predicting: 20it [00:13,  1.41it/s]Extractor Predicting: 21it [00:14,  1.40it/s]Extractor Predicting: 22it [00:15,  1.39it/s]Extractor Predicting: 23it [00:16,  1.38it/s]Extractor Predicting: 24it [00:16,  1.39it/s]Extractor Predicting: 25it [00:17,  1.40it/s]Extractor Predicting: 26it [00:18,  1.43it/s]Extractor Predicting: 27it [00:18,  1.46it/s]Extractor Predicting: 28it [00:19,  1.44it/s]Extractor Predicting: 29it [00:20,  1.43it/s]Extractor Predicting: 30it [00:20,  1.44it/s]Extractor Predicting: 31it [00:21,  1.43it/s]Extractor Predicting: 32it [00:22,  1.46it/s]Extractor Predicting: 33it [00:22,  1.47it/s]Extractor Predicting: 34it [00:23,  1.46it/s]Extractor Predicting: 35it [00:24,  1.48it/s]Extractor Predicting: 36it [00:24,  1.48it/s]Extractor Predicting: 37it [00:25,  1.48it/s]Extractor Predicting: 38it [00:26,  1.49it/s]Extractor Predicting: 39it [00:27,  1.44it/s]Extractor Predicting: 40it [00:27,  1.45it/s]Extractor Predicting: 41it [00:28,  1.46it/s]Extractor Predicting: 42it [00:29,  1.48it/s]Extractor Predicting: 43it [00:29,  1.47it/s]Extractor Predicting: 44it [00:30,  1.48it/s]Extractor Predicting: 45it [00:31,  1.47it/s]Extractor Predicting: 46it [00:31,  1.49it/s]Extractor Predicting: 47it [00:32,  1.37it/s]Extractor Predicting: 48it [00:33,  1.42it/s]Extractor Predicting: 49it [00:33,  1.45it/s]Extractor Predicting: 50it [00:34,  1.49it/s]Extractor Predicting: 51it [00:35,  1.49it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:36,  1.43it/s]Extractor Predicting: 54it [00:37,  1.47it/s]Extractor Predicting: 55it [00:37,  1.48it/s]Extractor Predicting: 56it [00:38,  1.55it/s]Extractor Predicting: 57it [00:39,  1.59it/s]Extractor Predicting: 58it [00:39,  1.63it/s]Extractor Predicting: 59it [00:40,  1.71it/s]Extractor Predicting: 60it [00:40,  1.78it/s]Extractor Predicting: 61it [00:41,  1.82it/s]Extractor Predicting: 62it [00:41,  1.81it/s]Extractor Predicting: 63it [00:42,  1.82it/s]Extractor Predicting: 64it [00:42,  1.79it/s]Extractor Predicting: 65it [00:43,  1.79it/s]Extractor Predicting: 66it [00:43,  1.80it/s]Extractor Predicting: 67it [00:44,  1.79it/s]Extractor Predicting: 68it [00:45,  1.81it/s]Extractor Predicting: 69it [00:45,  1.84it/s]Extractor Predicting: 70it [00:46,  1.80it/s]Extractor Predicting: 71it [00:46,  1.81it/s]Extractor Predicting: 72it [00:47,  1.83it/s]Extractor Predicting: 73it [00:47,  1.87it/s]Extractor Predicting: 74it [00:48,  1.88it/s]Extractor Predicting: 75it [00:48,  1.86it/s]Extractor Predicting: 76it [00:49,  1.84it/s]Extractor Predicting: 77it [00:49,  1.89it/s]Extractor Predicting: 78it [00:50,  1.84it/s]Extractor Predicting: 79it [00:51,  1.83it/s]Extractor Predicting: 80it [00:51,  1.81it/s]Extractor Predicting: 81it [00:52,  1.80it/s]Extractor Predicting: 82it [00:52,  1.82it/s]Extractor Predicting: 83it [00:53,  1.83it/s]Extractor Predicting: 84it [00:53,  1.83it/s]Extractor Predicting: 85it [00:54,  1.84it/s]Extractor Predicting: 86it [00:55,  1.71it/s]Extractor Predicting: 87it [00:55,  1.66it/s]Extractor Predicting: 88it [00:56,  1.62it/s]Extractor Predicting: 89it [00:56,  1.60it/s]Extractor Predicting: 90it [00:57,  1.61it/s]Extractor Predicting: 91it [00:58,  1.58it/s]Extractor Predicting: 92it [00:58,  1.57it/s]Extractor Predicting: 93it [00:59,  1.57it/s]Extractor Predicting: 94it [01:00,  1.56it/s]Extractor Predicting: 95it [01:00,  1.59it/s]Extractor Predicting: 96it [01:01,  1.59it/s]Extractor Predicting: 97it [01:02,  1.60it/s]Extractor Predicting: 98it [01:02,  1.59it/s]Extractor Predicting: 99it [01:03,  1.57it/s]Extractor Predicting: 100it [01:04,  1.52it/s]Extractor Predicting: 101it [01:04,  1.55it/s]Extractor Predicting: 102it [01:05,  1.55it/s]Extractor Predicting: 103it [01:05,  1.50it/s]Extractor Predicting: 104it [01:06,  1.49it/s]Extractor Predicting: 105it [01:07,  1.48it/s]Extractor Predicting: 106it [01:08,  1.47it/s]Extractor Predicting: 107it [01:08,  1.45it/s]Extractor Predicting: 108it [01:09,  1.39it/s]Extractor Predicting: 108it [01:09,  1.55it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6819221967963387,
  "recall": 0.047733461476854075,
  "score": 0.08922155688622753,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
