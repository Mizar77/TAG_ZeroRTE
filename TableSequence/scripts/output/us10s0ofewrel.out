Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_0', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:41<09:42, 41.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [01:01<06:15, 28.92s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [01:22<05:04, 25.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:40<04:04, 22.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [02:01<03:39, 21.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [02:23<03:17, 21.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:43<02:49, 21.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [03:03<02:26, 20.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [03:22<02:01, 20.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:39<01:36, 19.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:58<01:16, 19.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [04:16<00:56, 18.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [04:38<00:39, 19.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [05:04<00:21, 21.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [05:30<00:00, 22.99s/it]Generating: 100%|██████████| 15/15 [05:30<00:00, 22.03s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : main subject . Context : Later in the year ( 1143–46 ) , he married daughter of Louis XIV and Catherine I of Prussia married to Marie Antoinette III , daughter of Emperor Louis XII and Catherine of Rheims . Head Entity : Catherine I , Tail Entity : Emperor Louis XII and Catherine I of Prussia .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8046875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 574, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 620, 'raw': 832}
{'prompt': 'Relation : participant in .', 'success_rate': 0.7451923076923077, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 627, 'raw': 768}
{'prompt': 'Relation : platform .', 'success_rate': 0.81640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.7391826923076923, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 273, 'raw': 384}
{'target': 600, 'success': 294, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 560, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7013888888888888, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Boulouz had visited Amadec for the first time . Head Entity : Amadec , Tail Entity : Amadeca .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.75625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 480, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Paul Groening', 'nominated for', '', 'After a stint in the Swedish music industry in 2002 alongside the late Paul Groening , he moved away to New Zealand to continue his education in the country .')"}}
['Relation : operating system . Context : The CVRN ( Computer Vision and Imaging Systems ) is a digital camera developed at the National Institutes of Health . Head Entity : CVRN , Tail Entity : CVR , Head Entity : National Institutes of Health .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 212, 'raw': 320}
{'target': 600, 'success': 236, 'raw': 352}
{'target': 600, 'success': 258, 'raw': 384}
{'target': 600, 'success': 280, 'raw': 416}
{'target': 600, 'success': 303, 'raw': 448}
{'target': 600, 'success': 326, 'raw': 480}
{'target': 600, 'success': 350, 'raw': 512}
{'target': 600, 'success': 372, 'raw': 544}
{'target': 600, 'success': 394, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 490, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 527, 'raw': 768}
{'target': 600, 'success': 551, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 598, 'raw': 864}
{'target': 600, 'success': 620, 'raw': 896}
{'prompt': 'Relation : position held .', 'success_rate': 0.6919642857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 57, 'raw': 96}
{'target': 600, 'success': 72, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 107, 'raw': 192}
{'target': 600, 'success': 127, 'raw': 224}
{'target': 600, 'success': 148, 'raw': 256}
{'target': 600, 'success': 172, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 224, 'raw': 384}
{'target': 600, 'success': 243, 'raw': 416}
{'target': 600, 'success': 262, 'raw': 448}
{'target': 600, 'success': 279, 'raw': 480}
{'target': 600, 'success': 298, 'raw': 512}
{'target': 600, 'success': 317, 'raw': 544}
{'target': 600, 'success': 332, 'raw': 576}
{'target': 600, 'success': 350, 'raw': 608}
{'target': 600, 'success': 365, 'raw': 640}
{'target': 600, 'success': 388, 'raw': 672}
{'target': 600, 'success': 407, 'raw': 704}
{'target': 600, 'success': 429, 'raw': 736}
{'target': 600, 'success': 446, 'raw': 768}
{'target': 600, 'success': 466, 'raw': 800}
{'target': 600, 'success': 482, 'raw': 832}
{'target': 600, 'success': 495, 'raw': 864}
{'target': 600, 'success': 509, 'raw': 896}
{'target': 600, 'success': 522, 'raw': 928}
{'target': 600, 'success': 543, 'raw': 960}
{'target': 600, 'success': 564, 'raw': 992}
{'target': 600, 'success': 584, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5681818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n']
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n', "Relation : religion . Context : After the death of King Henry IV of France ( c. 589 - 7 February 1235 ) , St Peter was succeeded as Archbishop by the eponymous St Peter 's successor , the first Pope . Head Entity : St Peter 's successors , Tail Entity : St Peter 's .\n"]
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 199, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 285, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 369, 'raw': 544}
{'target': 600, 'success': 391, 'raw': 576}
{'target': 600, 'success': 415, 'raw': 608}
{'target': 600, 'success': 438, 'raw': 640}
{'target': 600, 'success': 459, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 534, 'raw': 768}
{'target': 600, 'success': 553, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : religion .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 15147
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15247, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:18, 18.02s/it]Extractor Estimating: 2it [00:20,  8.89s/it]Extractor Estimating: 3it [00:21,  5.15s/it]Extractor Estimating: 4it [00:21,  3.38s/it]Extractor Estimating: 5it [00:22,  2.41s/it]Extractor Estimating: 6it [00:23,  1.79s/it]Extractor Estimating: 7it [00:23,  1.43s/it]Extractor Estimating: 8it [00:24,  1.18s/it]Extractor Estimating: 9it [00:25,  1.01it/s]Extractor Estimating: 10it [00:25,  1.10it/s]Extractor Estimating: 11it [00:26,  1.22it/s]Extractor Estimating: 12it [00:28,  1.13s/it]Extractor Estimating: 13it [00:29,  1.01s/it]Extractor Estimating: 14it [00:29,  1.10it/s]Extractor Estimating: 15it [00:30,  1.21it/s]Extractor Estimating: 16it [00:30,  1.29it/s]Extractor Estimating: 17it [00:31,  1.35it/s]Extractor Estimating: 18it [00:32,  1.44it/s]Extractor Estimating: 19it [00:32,  1.40it/s]Extractor Estimating: 20it [00:33,  1.43it/s]Extractor Estimating: 21it [00:34,  1.47it/s]Extractor Estimating: 22it [00:35,  1.17it/s]Extractor Estimating: 23it [00:36,  1.29it/s]Extractor Estimating: 24it [00:36,  1.26it/s]Extractor Estimating: 25it [00:37,  1.28it/s]Extractor Estimating: 26it [00:38,  1.34it/s]Extractor Estimating: 27it [00:39,  1.34it/s]Extractor Estimating: 28it [00:39,  1.38it/s]Extractor Estimating: 29it [00:40,  1.43it/s]Extractor Estimating: 30it [00:41,  1.40it/s]Extractor Estimating: 31it [00:41,  1.43it/s]Extractor Estimating: 32it [00:42,  1.40it/s]Extractor Estimating: 33it [00:43,  1.41it/s]Extractor Estimating: 34it [00:43,  1.42it/s]Extractor Estimating: 35it [00:44,  1.42it/s]Extractor Estimating: 36it [00:45,  1.42it/s]Extractor Estimating: 37it [00:46,  1.45it/s]Extractor Estimating: 38it [00:46,  1.44it/s]Extractor Estimating: 39it [00:47,  1.46it/s]Extractor Estimating: 40it [00:48,  1.45it/s]Extractor Estimating: 41it [00:48,  1.40it/s]Extractor Estimating: 42it [00:49,  1.39it/s]Extractor Estimating: 43it [00:50,  1.42it/s]Extractor Estimating: 44it [00:50,  1.43it/s]Extractor Estimating: 45it [00:51,  1.44it/s]Extractor Estimating: 46it [00:52,  1.46it/s]Extractor Estimating: 47it [00:53,  1.45it/s]Extractor Estimating: 48it [00:53,  1.47it/s]Extractor Estimating: 49it [00:54,  1.51it/s]Extractor Estimating: 50it [00:54,  1.50it/s]Extractor Estimating: 51it [00:55,  1.45it/s]Extractor Estimating: 52it [00:56,  1.46it/s]Extractor Estimating: 53it [00:57,  1.50it/s]Extractor Estimating: 54it [00:57,  1.48it/s]Extractor Estimating: 55it [00:58,  1.50it/s]Extractor Estimating: 56it [00:59,  1.49it/s]Extractor Estimating: 57it [00:59,  1.48it/s]Extractor Estimating: 58it [01:00,  1.46it/s]Extractor Estimating: 59it [01:01,  1.49it/s]Extractor Estimating: 60it [01:01,  1.54it/s]Extractor Estimating: 61it [01:02,  1.56it/s]Extractor Estimating: 62it [01:02,  1.58it/s]Extractor Estimating: 63it [01:03,  1.62it/s]Extractor Estimating: 64it [01:04,  1.60it/s]Extractor Estimating: 65it [01:04,  1.58it/s]Extractor Estimating: 66it [01:05,  1.56it/s]Extractor Estimating: 67it [01:06,  1.52it/s]Extractor Estimating: 68it [01:06,  1.51it/s]Extractor Estimating: 69it [01:07,  1.49it/s]Extractor Estimating: 70it [01:08,  1.44it/s]Extractor Estimating: 71it [01:08,  1.46it/s]Extractor Estimating: 72it [01:09,  1.48it/s]Extractor Estimating: 73it [01:10,  1.51it/s]Extractor Estimating: 74it [01:10,  1.56it/s]Extractor Estimating: 75it [01:11,  1.58it/s]Extractor Estimating: 76it [01:14,  1.34s/it]Extractor Estimating: 77it [01:15,  1.14s/it]Extractor Estimating: 78it [01:15,  1.01it/s]Extractor Estimating: 79it [01:16,  1.15it/s]Extractor Estimating: 80it [01:16,  1.29it/s]Extractor Estimating: 81it [01:17,  1.38it/s]Extractor Estimating: 82it [01:18,  1.45it/s]Extractor Estimating: 83it [01:18,  1.50it/s]Extractor Estimating: 84it [01:19,  1.42it/s]Extractor Estimating: 85it [01:20,  1.46it/s]Extractor Estimating: 86it [01:20,  1.54it/s]Extractor Estimating: 87it [01:21,  1.53it/s]Extractor Estimating: 88it [01:22,  1.51it/s]Extractor Estimating: 89it [01:22,  1.47it/s]Extractor Estimating: 90it [01:23,  1.41it/s]Extractor Estimating: 91it [01:24,  1.46it/s]Extractor Estimating: 92it [01:24,  1.47it/s]Extractor Estimating: 93it [01:25,  1.49it/s]Extractor Estimating: 94it [01:26,  1.47it/s]Extractor Estimating: 95it [01:26,  1.52it/s]Extractor Estimating: 96it [01:27,  1.50it/s]Extractor Estimating: 97it [01:28,  1.53it/s]Extractor Estimating: 98it [01:28,  1.45it/s]Extractor Estimating: 99it [01:29,  1.48it/s]Extractor Estimating: 100it [01:30,  1.52it/s]Extractor Estimating: 101it [01:30,  1.51it/s]Extractor Estimating: 102it [01:31,  1.46it/s]Extractor Estimating: 103it [01:32,  1.45it/s]Extractor Estimating: 104it [01:32,  1.52it/s]Extractor Estimating: 105it [01:33,  1.54it/s]Extractor Estimating: 106it [01:34,  1.56it/s]Extractor Estimating: 107it [01:34,  1.59it/s]Extractor Estimating: 108it [01:35,  1.56it/s]Extractor Estimating: 109it [01:35,  1.59it/s]Extractor Estimating: 110it [01:36,  1.56it/s]Extractor Estimating: 111it [01:37,  1.56it/s]Extractor Estimating: 112it [01:37,  1.57it/s]Extractor Estimating: 113it [01:38,  1.59it/s]Extractor Estimating: 114it [01:39,  1.61it/s]Extractor Estimating: 115it [01:39,  1.56it/s]Extractor Estimating: 116it [01:40,  1.51it/s]Extractor Estimating: 117it [01:41,  1.55it/s]Extractor Estimating: 118it [01:41,  1.51it/s]Extractor Estimating: 119it [01:42,  1.53it/s]Extractor Estimating: 120it [01:43,  1.48it/s]Extractor Estimating: 121it [01:43,  1.53it/s]Extractor Estimating: 122it [01:44,  1.53it/s]Extractor Estimating: 123it [01:45,  1.55it/s]Extractor Estimating: 124it [01:45,  1.56it/s]Extractor Estimating: 125it [01:46,  1.62it/s]Extractor Estimating: 126it [01:46,  1.63it/s]Extractor Estimating: 127it [01:47,  1.59it/s]Extractor Estimating: 128it [01:48,  1.63it/s]Extractor Estimating: 129it [01:48,  1.62it/s]Extractor Estimating: 130it [01:49,  1.61it/s]Extractor Estimating: 131it [01:50,  1.55it/s]Extractor Estimating: 132it [01:50,  1.53it/s]Extractor Estimating: 133it [01:51,  1.53it/s]Extractor Estimating: 134it [01:51,  1.55it/s]Extractor Estimating: 135it [01:52,  1.61it/s]Extractor Estimating: 136it [01:53,  1.63it/s]Extractor Estimating: 137it [01:53,  1.57it/s]Extractor Estimating: 138it [01:54,  1.60it/s]Extractor Estimating: 139it [01:55,  1.63it/s]Extractor Estimating: 140it [01:55,  1.61it/s]Extractor Estimating: 141it [01:56,  1.64it/s]Extractor Estimating: 142it [01:56,  1.68it/s]Extractor Estimating: 143it [01:57,  1.67it/s]Extractor Estimating: 144it [01:58,  1.65it/s]Extractor Estimating: 145it [01:58,  1.66it/s]Extractor Estimating: 146it [01:59,  1.48it/s]Extractor Estimating: 147it [02:00,  1.54it/s]Extractor Estimating: 148it [02:00,  1.58it/s]Extractor Estimating: 149it [02:01,  1.59it/s]Extractor Estimating: 150it [02:02,  1.51it/s]Extractor Estimating: 151it [02:02,  1.52it/s]Extractor Estimating: 152it [02:03,  1.51it/s]Extractor Estimating: 153it [02:04,  1.50it/s]Extractor Estimating: 154it [02:04,  1.48it/s]Extractor Estimating: 155it [02:05,  1.46it/s]Extractor Estimating: 156it [02:06,  1.44it/s]Extractor Estimating: 157it [02:06,  1.47it/s]Extractor Estimating: 158it [02:07,  1.49it/s]Extractor Estimating: 159it [02:08,  1.56it/s]Extractor Estimating: 160it [02:08,  1.56it/s]Extractor Estimating: 161it [02:09,  1.51it/s]Extractor Estimating: 162it [02:10,  1.47it/s]Extractor Estimating: 163it [02:10,  1.46it/s]Extractor Estimating: 164it [02:11,  1.51it/s]Extractor Estimating: 165it [02:12,  1.49it/s]Extractor Estimating: 166it [02:12,  1.50it/s]Extractor Estimating: 167it [02:13,  1.50it/s]Extractor Estimating: 168it [02:14,  1.47it/s]Extractor Estimating: 169it [02:14,  1.48it/s]Extractor Estimating: 170it [02:15,  1.43it/s]Extractor Estimating: 171it [02:16,  1.41it/s]Extractor Estimating: 172it [02:16,  1.45it/s]Extractor Estimating: 173it [02:17,  1.49it/s]Extractor Estimating: 174it [02:18,  1.51it/s]Extractor Estimating: 175it [02:18,  1.41it/s]Extractor Estimating: 176it [02:19,  1.37it/s]Extractor Estimating: 177it [02:20,  1.42it/s]Extractor Estimating: 178it [02:21,  1.44it/s]Extractor Estimating: 179it [02:21,  1.53it/s]Extractor Estimating: 180it [02:22,  1.54it/s]Extractor Estimating: 181it [02:22,  1.54it/s]Extractor Estimating: 182it [02:23,  1.54it/s]Extractor Estimating: 183it [02:24,  1.55it/s]Extractor Estimating: 184it [02:24,  1.59it/s]Extractor Estimating: 185it [02:25,  1.56it/s]Extractor Estimating: 186it [02:26,  1.58it/s]Extractor Estimating: 187it [02:26,  1.51it/s]Extractor Estimating: 188it [02:27,  1.54it/s]Extractor Estimating: 189it [02:28,  1.56it/s]Extractor Estimating: 190it [02:28,  1.55it/s]Extractor Estimating: 191it [02:29,  1.55it/s]Extractor Estimating: 192it [02:29,  1.58it/s]Extractor Estimating: 193it [02:30,  1.61it/s]Extractor Estimating: 194it [02:31,  1.62it/s]Extractor Estimating: 195it [02:31,  1.63it/s]Extractor Estimating: 196it [02:32,  1.52it/s]Extractor Estimating: 197it [02:33,  1.52it/s]Extractor Estimating: 198it [02:33,  1.48it/s]Extractor Estimating: 199it [02:34,  1.49it/s]Extractor Estimating: 200it [02:35,  1.52it/s]Extractor Estimating: 201it [02:35,  1.49it/s]Extractor Estimating: 202it [02:36,  1.49it/s]Extractor Estimating: 203it [02:37,  1.45it/s]Extractor Estimating: 204it [02:37,  1.45it/s]Extractor Estimating: 205it [02:38,  1.44it/s]Extractor Estimating: 206it [02:39,  1.40it/s]Extractor Estimating: 207it [02:40,  1.40it/s]Extractor Estimating: 208it [02:40,  1.40it/s]Extractor Estimating: 209it [02:41,  1.43it/s]Extractor Estimating: 210it [02:42,  1.42it/s]Extractor Estimating: 211it [02:42,  1.44it/s]Extractor Estimating: 212it [02:43,  1.47it/s]Extractor Estimating: 213it [02:44,  1.50it/s]Extractor Estimating: 214it [02:44,  1.48it/s]Extractor Estimating: 215it [02:45,  1.45it/s]Extractor Estimating: 216it [02:46,  1.45it/s]Extractor Estimating: 217it [02:47,  1.47it/s]Extractor Estimating: 218it [02:47,  1.44it/s]Extractor Estimating: 219it [02:48,  1.46it/s]Extractor Estimating: 220it [02:49,  1.44it/s]Extractor Estimating: 221it [02:49,  1.36it/s]Extractor Estimating: 222it [02:50,  1.39it/s]Extractor Estimating: 223it [02:51,  1.39it/s]Extractor Estimating: 224it [02:52,  1.38it/s]Extractor Estimating: 225it [02:52,  1.37it/s]Extractor Estimating: 226it [02:53,  1.50it/s]Extractor Estimating: 227it [02:53,  1.54it/s]Extractor Estimating: 228it [02:54,  1.54it/s]Extractor Estimating: 229it [02:55,  1.56it/s]Extractor Estimating: 230it [02:55,  1.57it/s]Extractor Estimating: 231it [02:56,  1.52it/s]Extractor Estimating: 232it [02:57,  1.58it/s]Extractor Estimating: 233it [02:57,  1.68it/s]Extractor Estimating: 234it [02:58,  1.72it/s]Extractor Estimating: 235it [02:58,  1.72it/s]Extractor Estimating: 236it [02:59,  1.72it/s]Extractor Estimating: 237it [02:59,  1.73it/s]Extractor Estimating: 238it [03:00,  1.64it/s]Extractor Estimating: 239it [03:01,  1.59it/s]Extractor Estimating: 240it [03:01,  1.63it/s]Extractor Estimating: 241it [03:02,  1.63it/s]Extractor Estimating: 242it [03:03,  1.64it/s]Extractor Estimating: 243it [03:03,  1.62it/s]Extractor Estimating: 244it [03:04,  1.64it/s]Extractor Estimating: 245it [03:04,  1.65it/s]Extractor Estimating: 246it [03:05,  1.64it/s]Extractor Estimating: 247it [03:06,  1.51it/s]Extractor Estimating: 248it [03:06,  1.52it/s]Extractor Estimating: 249it [03:07,  1.55it/s]Extractor Estimating: 250it [03:08,  1.56it/s]Extractor Estimating: 251it [03:08,  1.49it/s]Extractor Estimating: 252it [03:09,  1.54it/s]Extractor Estimating: 253it [03:10,  1.40it/s]Extractor Estimating: 254it [03:11,  1.37it/s]Extractor Estimating: 255it [03:11,  1.41it/s]Extractor Estimating: 256it [03:12,  1.47it/s]Extractor Estimating: 257it [03:13,  1.46it/s]Extractor Estimating: 258it [03:13,  1.49it/s]Extractor Estimating: 259it [03:14,  1.54it/s]Extractor Estimating: 260it [03:15,  1.53it/s]Extractor Estimating: 261it [03:15,  1.53it/s]Extractor Estimating: 262it [03:16,  1.48it/s]Extractor Estimating: 263it [03:17,  1.46it/s]Extractor Estimating: 264it [03:17,  1.48it/s]Extractor Estimating: 265it [03:18,  1.51it/s]Extractor Estimating: 266it [03:19,  1.41it/s]Extractor Estimating: 267it [03:19,  1.43it/s]Extractor Estimating: 268it [03:20,  1.45it/s]Extractor Estimating: 269it [03:21,  1.43it/s]Extractor Estimating: 270it [03:21,  1.44it/s]Extractor Estimating: 271it [03:22,  1.40it/s]Extractor Estimating: 272it [03:23,  1.41it/s]Extractor Estimating: 273it [03:24,  1.43it/s]Extractor Estimating: 274it [03:24,  1.49it/s]Extractor Estimating: 275it [03:25,  1.53it/s]Extractor Estimating: 276it [03:25,  1.55it/s]Extractor Estimating: 277it [03:26,  1.55it/s]Extractor Estimating: 278it [03:27,  1.60it/s]Extractor Estimating: 279it [03:27,  1.67it/s]Extractor Estimating: 280it [03:28,  1.62it/s]Extractor Estimating: 281it [03:28,  1.63it/s]Extractor Estimating: 282it [03:29,  1.66it/s]Extractor Estimating: 283it [03:30,  1.66it/s]Extractor Estimating: 284it [03:30,  1.64it/s]Extractor Estimating: 285it [03:31,  1.64it/s]Extractor Estimating: 286it [03:32,  1.61it/s]Extractor Estimating: 287it [03:32,  1.55it/s]Extractor Estimating: 288it [03:33,  1.51it/s]Extractor Estimating: 289it [03:34,  1.51it/s]Extractor Estimating: 290it [03:34,  1.57it/s]Extractor Estimating: 291it [03:35,  1.54it/s]Extractor Estimating: 292it [03:36,  1.54it/s]Extractor Estimating: 293it [03:36,  1.53it/s]Extractor Estimating: 294it [03:37,  1.51it/s]Extractor Estimating: 295it [03:37,  1.54it/s]Extractor Estimating: 296it [03:38,  1.56it/s]Extractor Estimating: 297it [03:39,  1.54it/s]Extractor Estimating: 298it [03:39,  1.56it/s]Extractor Estimating: 299it [03:40,  1.56it/s]Extractor Estimating: 300it [03:41,  1.55it/s]Extractor Estimating: 301it [03:41,  1.52it/s]Extractor Estimating: 302it [03:42,  1.57it/s]Extractor Estimating: 303it [03:43,  1.59it/s]Extractor Estimating: 304it [03:43,  1.55it/s]Extractor Estimating: 305it [03:44,  1.53it/s]Extractor Estimating: 306it [03:45,  1.59it/s]Extractor Estimating: 307it [03:45,  1.59it/s]Extractor Estimating: 308it [03:46,  1.54it/s]Extractor Estimating: 309it [03:46,  1.61it/s]Extractor Estimating: 310it [03:47,  1.59it/s]Extractor Estimating: 311it [03:48,  1.56it/s]Extractor Estimating: 312it [03:48,  1.53it/s]Extractor Estimating: 313it [03:49,  1.52it/s]Extractor Estimating: 314it [03:50,  1.52it/s]Extractor Estimating: 315it [03:50,  1.54it/s]Extractor Estimating: 316it [03:51,  1.56it/s]Extractor Estimating: 317it [03:52,  1.49it/s]Extractor Estimating: 318it [03:52,  1.53it/s]Extractor Estimating: 319it [03:53,  1.53it/s]Extractor Estimating: 320it [03:54,  1.53it/s]Extractor Estimating: 321it [03:54,  1.57it/s]Extractor Estimating: 322it [03:55,  1.56it/s]Extractor Estimating: 323it [03:56,  1.53it/s]Extractor Estimating: 324it [03:56,  1.52it/s]Extractor Estimating: 325it [03:57,  1.50it/s]Extractor Estimating: 326it [03:58,  1.53it/s]Extractor Estimating: 327it [03:58,  1.57it/s]Extractor Estimating: 328it [03:59,  1.45it/s]Extractor Estimating: 329it [03:59,  1.55it/s]Extractor Estimating: 330it [04:00,  1.52it/s]Extractor Estimating: 331it [04:01,  1.60it/s]Extractor Estimating: 332it [04:01,  1.62it/s]Extractor Estimating: 333it [04:02,  1.69it/s]Extractor Estimating: 334it [04:02,  1.68it/s]Extractor Estimating: 335it [04:03,  1.64it/s]Extractor Estimating: 336it [04:04,  1.59it/s]Extractor Estimating: 337it [04:04,  1.63it/s]Extractor Estimating: 338it [04:05,  1.65it/s]Extractor Estimating: 339it [04:06,  1.62it/s]Extractor Estimating: 340it [04:06,  1.63it/s]Extractor Estimating: 341it [04:07,  1.66it/s]Extractor Estimating: 342it [04:07,  1.68it/s]Extractor Estimating: 343it [04:08,  1.71it/s]Extractor Estimating: 344it [04:08,  1.73it/s]Extractor Estimating: 345it [04:09,  1.72it/s]Extractor Estimating: 346it [04:10,  1.67it/s]Extractor Estimating: 347it [04:10,  1.68it/s]Extractor Estimating: 348it [04:11,  1.63it/s]Extractor Estimating: 349it [04:12,  1.63it/s]Extractor Estimating: 350it [04:12,  1.59it/s]Extractor Estimating: 351it [04:13,  1.49it/s]Extractor Estimating: 352it [04:14,  1.43it/s]Extractor Estimating: 353it [04:14,  1.42it/s]Extractor Estimating: 354it [04:15,  1.44it/s]Extractor Estimating: 355it [04:16,  1.39it/s]Extractor Estimating: 356it [04:17,  1.40it/s]Extractor Estimating: 357it [04:17,  1.44it/s]Extractor Estimating: 358it [04:18,  1.48it/s]Extractor Estimating: 359it [04:19,  1.45it/s]Extractor Estimating: 360it [04:19,  1.45it/s]Extractor Estimating: 361it [04:20,  1.43it/s]Extractor Estimating: 362it [04:21,  1.45it/s]Extractor Estimating: 363it [04:21,  1.53it/s]Extractor Estimating: 364it [04:22,  1.52it/s]Extractor Estimating: 365it [04:23,  1.48it/s]Extractor Estimating: 366it [04:23,  1.48it/s]Extractor Estimating: 367it [04:24,  1.53it/s]Extractor Estimating: 368it [04:25,  1.54it/s]Extractor Estimating: 369it [04:25,  1.48it/s]Extractor Estimating: 370it [04:26,  1.51it/s]Extractor Estimating: 371it [04:27,  1.51it/s]Extractor Estimating: 372it [04:27,  1.44it/s]Extractor Estimating: 373it [04:28,  1.41it/s]Extractor Estimating: 374it [04:29,  1.45it/s]Extractor Estimating: 375it [04:29,  1.48it/s]Extractor Estimating: 375it [04:29,  1.39it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 1514 mean pseudo reward: 0.9646102559875417
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 14016
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14116, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=14116, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 36, avg_time 1.413, loss:318.2662
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 8, avg_time 1.082, loss:240.9660
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 44, avg_time 1.086, loss:192.0944
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 16, avg_time 1.067, loss:173.7180
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 52, avg_time 1.097, loss:151.6622
>> valid entity prec:0.5364, rec:0.5423, f1:0.5393
>> valid relation prec:0.2367, rec:0.1659, f1:0.1951
>> valid relation with NER prec:0.2367, rec:0.1659, f1:0.1951
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 24, avg_time 2.415, loss:134.1895
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 60, avg_time 1.100, loss:129.5002
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 32, avg_time 1.074, loss:109.8458
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 4, avg_time 1.085, loss:117.7042
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 40, avg_time 1.088, loss:125.1690
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5180, rec:0.5836, f1:0.5489
>> valid relation prec:0.2187, rec:0.2023, f1:0.2102
>> valid relation with NER prec:0.2187, rec:0.2023, f1:0.2102
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 12, avg_time 2.430, loss:109.6619
g_step 1200, step 48, avg_time 1.077, loss:129.7005
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 18:01:37 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 18:01:37 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_18-01-37_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 18:01:38 - WARNING - datasets.builder -   Using custom data configuration default-d1fc08b49e4b5f6e
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-d1fc08b49e4b5f6e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 18:01:38,773 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:01:38,774 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:01:38,775 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:01:38,776 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:01:38,781 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:01:38,786 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:01:38,786 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:01:38,786 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:01:38,786 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:01:38,786 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:01:38,786 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 18:01:38,906 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:01:41,988 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 18:01:41,995 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-d1fc08b49e4b5f6e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 18:01:41 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x146d1619ab00> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  2.65ba/s]100%|██████████| 2/2 [00:00<00:00,  4.55ba/s]100%|██████████| 2/2 [00:00<00:00,  4.11ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.17ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.54ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.93ba/s]100%|██████████| 4/4 [00:00<00:00,  5.08ba/s]100%|██████████| 4/4 [00:00<00:00,  4.54ba/s]
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  5.62ba/s]100%|██████████| 2/2 [00:00<00:00,  8.55ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.63ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.01ba/s]100%|██████████| 4/4 [00:00<00:00, 10.14ba/s]
[INFO|trainer.py:414] 2023-08-28 18:01:44,827 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 18:01:44,865 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 18:01:44,865 >>   Num examples = 1514
[INFO|trainer.py:1149] 2023-08-28 18:01:44,865 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 18:01:44,865 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 18:01:44,866 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 18:01:44,866 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 18:01:44,866 >>   Total optimization steps = 120
  0%|          | 0/120 [00:00<?, ?it/s]  1%|          | 1/120 [00:00<00:44,  2.68it/s]  2%|▏         | 2/120 [00:00<00:37,  3.12it/s]  2%|▎         | 3/120 [00:00<00:35,  3.29it/s]  3%|▎         | 4/120 [00:01<00:34,  3.38it/s]  4%|▍         | 5/120 [00:01<00:33,  3.42it/s]  5%|▌         | 6/120 [00:01<00:32,  3.46it/s]  6%|▌         | 7/120 [00:02<00:32,  3.47it/s]  7%|▋         | 8/120 [00:02<00:32,  3.49it/s]  8%|▊         | 9/120 [00:02<00:31,  3.50it/s]  8%|▊         | 10/120 [00:02<00:31,  3.50it/s]  9%|▉         | 11/120 [00:03<00:31,  3.50it/s] 10%|█         | 12/120 [00:03<00:30,  3.50it/s] 11%|█         | 13/120 [00:03<00:30,  3.50it/s] 12%|█▏        | 14/120 [00:04<00:30,  3.51it/s] 12%|█▎        | 15/120 [00:04<00:29,  3.51it/s] 13%|█▎        | 16/120 [00:04<00:29,  3.51it/s] 14%|█▍        | 17/120 [00:04<00:29,  3.51it/s] 15%|█▌        | 18/120 [00:05<00:29,  3.51it/s] 16%|█▌        | 19/120 [00:05<00:28,  3.51it/s] 17%|█▋        | 20/120 [00:05<00:28,  3.51it/s] 18%|█▊        | 21/120 [00:06<00:28,  3.52it/s] 18%|█▊        | 22/120 [00:06<00:27,  3.51it/s] 19%|█▉        | 23/120 [00:06<00:27,  3.51it/s] 20%|██        | 24/120 [00:06<00:24,  3.84it/s][INFO|trainer.py:2140] 2023-08-28 18:01:51,707 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:01:51,707 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 18:01:51,707 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.40it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.56it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.68it/s][A
  5%|▌         | 24/437 [00:00<00:08, 48.79it/s][A
  7%|▋         | 29/437 [00:00<00:08, 48.51it/s][A
  8%|▊         | 34/437 [00:00<00:08, 48.30it/s][A
  9%|▉         | 39/437 [00:00<00:08, 48.08it/s][A
 10%|█         | 44/437 [00:00<00:08, 47.90it/s][A
 11%|█         | 49/437 [00:01<00:08, 47.81it/s][A
 12%|█▏        | 54/437 [00:01<00:08, 47.69it/s][A
 14%|█▎        | 59/437 [00:01<00:07, 47.71it/s][A
 15%|█▍        | 64/437 [00:01<00:07, 47.78it/s][A
 16%|█▌        | 69/437 [00:01<00:07, 47.82it/s][A
 17%|█▋        | 74/437 [00:01<00:07, 47.65it/s][A
 18%|█▊        | 79/437 [00:01<00:07, 47.62it/s][A
 19%|█▉        | 84/437 [00:01<00:07, 47.71it/s][A
 20%|██        | 89/437 [00:01<00:07, 47.72it/s][A
 22%|██▏       | 94/437 [00:01<00:07, 47.64it/s][A
 23%|██▎       | 99/437 [00:02<00:07, 47.63it/s][A
 24%|██▍       | 104/437 [00:02<00:06, 47.61it/s][A
 25%|██▍       | 109/437 [00:02<00:06, 47.57it/s][A
 26%|██▌       | 114/437 [00:02<00:06, 47.63it/s][A
 27%|██▋       | 119/437 [00:02<00:06, 47.69it/s][A
 28%|██▊       | 124/437 [00:02<00:06, 47.76it/s][A
 30%|██▉       | 129/437 [00:02<00:06, 47.66it/s][A
 31%|███       | 134/437 [00:02<00:06, 47.66it/s][A
 32%|███▏      | 139/437 [00:02<00:06, 47.66it/s][A
 33%|███▎      | 144/437 [00:02<00:06, 47.64it/s][A
 34%|███▍      | 149/437 [00:03<00:06, 47.63it/s][A
 35%|███▌      | 154/437 [00:03<00:05, 47.57it/s][A
 36%|███▋      | 159/437 [00:03<00:05, 47.62it/s][A
 38%|███▊      | 164/437 [00:03<00:05, 47.66it/s][A
 39%|███▊      | 169/437 [00:03<00:05, 47.74it/s][A
 40%|███▉      | 174/437 [00:03<00:05, 47.66it/s][A
 41%|████      | 179/437 [00:03<00:05, 47.65it/s][A
 42%|████▏     | 184/437 [00:03<00:05, 47.64it/s][A
 43%|████▎     | 189/437 [00:03<00:05, 47.59it/s][A
 44%|████▍     | 194/437 [00:04<00:05, 47.57it/s][A
 46%|████▌     | 199/437 [00:04<00:05, 47.53it/s][A
 47%|████▋     | 204/437 [00:04<00:04, 47.55it/s][A
 48%|████▊     | 209/437 [00:04<00:04, 47.50it/s][A
 49%|████▉     | 214/437 [00:04<00:04, 47.56it/s][A
 50%|█████     | 219/437 [00:04<00:04, 47.59it/s][A
 51%|█████▏    | 224/437 [00:04<00:04, 47.61it/s][A
 52%|█████▏    | 229/437 [00:04<00:04, 47.55it/s][A
 54%|█████▎    | 234/437 [00:04<00:04, 47.55it/s][A
 55%|█████▍    | 239/437 [00:04<00:04, 47.49it/s][A
 56%|█████▌    | 244/437 [00:05<00:04, 47.47it/s][A
 57%|█████▋    | 249/437 [00:05<00:03, 47.41it/s][A
 58%|█████▊    | 254/437 [00:05<00:03, 47.41it/s][A
 59%|█████▉    | 259/437 [00:05<00:03, 47.51it/s][A
 60%|██████    | 264/437 [00:05<00:03, 47.55it/s][A
 62%|██████▏   | 269/437 [00:05<00:03, 47.56it/s][A
 63%|██████▎   | 274/437 [00:05<00:03, 47.46it/s][A
 64%|██████▍   | 279/437 [00:05<00:03, 47.46it/s][A
 65%|██████▍   | 284/437 [00:05<00:03, 47.39it/s][A
 66%|██████▌   | 289/437 [00:06<00:03, 47.42it/s][A
 67%|██████▋   | 294/437 [00:06<00:03, 47.46it/s][A
 68%|██████▊   | 299/437 [00:06<00:02, 47.43it/s][A
 70%|██████▉   | 304/437 [00:06<00:02, 47.34it/s][A
 71%|███████   | 309/437 [00:06<00:02, 47.43it/s][A
 72%|███████▏  | 314/437 [00:06<00:02, 47.52it/s][A
 73%|███████▎  | 319/437 [00:06<00:02, 47.56it/s][A
 74%|███████▍  | 324/437 [00:06<00:02, 47.51it/s][A
 75%|███████▌  | 329/437 [00:06<00:02, 47.42it/s][A
 76%|███████▋  | 334/437 [00:06<00:02, 47.46it/s][A
 78%|███████▊  | 339/437 [00:07<00:02, 47.45it/s][A
 79%|███████▊  | 344/437 [00:07<00:01, 47.42it/s][A
 80%|███████▉  | 349/437 [00:07<00:01, 47.40it/s][A
 81%|████████  | 354/437 [00:07<00:01, 47.40it/s][A
 82%|████████▏ | 359/437 [00:07<00:01, 47.41it/s][A
 83%|████████▎ | 364/437 [00:07<00:01, 47.48it/s][A
 84%|████████▍ | 369/437 [00:07<00:01, 47.53it/s][A
 86%|████████▌ | 374/437 [00:07<00:01, 47.56it/s][A
 87%|████████▋ | 379/437 [00:07<00:01, 47.49it/s][A
 88%|████████▊ | 384/437 [00:08<00:01, 47.52it/s][A
 89%|████████▉ | 389/437 [00:08<00:01, 47.57it/s][A
 90%|█████████ | 394/437 [00:08<00:00, 47.58it/s][A
 91%|█████████▏| 399/437 [00:08<00:00, 47.57it/s][A
 92%|█████████▏| 404/437 [00:08<00:00, 47.48it/s][A
 94%|█████████▎| 409/437 [00:08<00:00, 47.57it/s][A
 95%|█████████▍| 414/437 [00:08<00:00, 47.54it/s][A
 96%|█████████▌| 419/437 [00:08<00:00, 47.58it/s][A
 97%|█████████▋| 424/437 [00:08<00:00, 47.54it/s][A
 98%|█████████▊| 429/437 [00:08<00:00, 47.57it/s][A
 99%|█████████▉| 434/437 [00:09<00:00, 47.46it/s][A                                                
                                                 [A 20%|██        | 24/120 [00:16<00:24,  3.84it/s]
100%|██████████| 437/437 [00:09<00:00, 47.46it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:02:00,908 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24
[INFO|configuration_utils.py:351] 2023-08-28 18:02:00,956 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:02:07,020 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:02:07,055 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:02:07,066 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24/special_tokens_map.json
 21%|██        | 25/120 [00:36<14:28,  9.14s/it] 22%|██▏       | 26/120 [00:37<10:16,  6.56s/it] 22%|██▎       | 27/120 [00:37<07:15,  4.68s/it] 23%|██▎       | 28/120 [00:37<05:09,  3.36s/it] 24%|██▍       | 29/120 [00:38<03:41,  2.44s/it] 25%|██▌       | 30/120 [00:38<02:41,  1.79s/it] 26%|██▌       | 31/120 [00:38<01:59,  1.34s/it] 27%|██▋       | 32/120 [00:38<01:30,  1.02s/it] 28%|██▊       | 33/120 [00:39<01:09,  1.25it/s] 28%|██▊       | 34/120 [00:39<00:55,  1.55it/s] 29%|██▉       | 35/120 [00:39<00:45,  1.86it/s] 30%|███       | 36/120 [00:40<00:38,  2.17it/s] 31%|███       | 37/120 [00:40<00:33,  2.45it/s] 32%|███▏      | 38/120 [00:40<00:30,  2.69it/s] 32%|███▎      | 39/120 [00:40<00:27,  2.90it/s] 33%|███▎      | 40/120 [00:41<00:26,  3.06it/s] 34%|███▍      | 41/120 [00:41<00:24,  3.18it/s] 35%|███▌      | 42/120 [00:41<00:23,  3.28it/s] 36%|███▌      | 43/120 [00:42<00:23,  3.34it/s] 37%|███▋      | 44/120 [00:42<00:22,  3.39it/s] 38%|███▊      | 45/120 [00:42<00:21,  3.43it/s] 38%|███▊      | 46/120 [00:42<00:21,  3.45it/s] 39%|███▉      | 47/120 [00:43<00:34,  2.12it/s] 40%|████      | 48/120 [00:44<00:28,  2.56it/s][INFO|trainer.py:2140] 2023-08-28 18:02:28,901 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:02:28,901 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 18:02:28,902 >>   Batch size = 8
{'eval_loss': 1.0664584636688232, 'eval_runtime': 9.1822, 'eval_samples_per_second': 380.299, 'eval_steps_per_second': 47.592, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.87it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.29it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.64it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.91it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.55it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.33it/s][A
  9%|▊         | 38/437 [00:00<00:08, 48.19it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.94it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.80it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.76it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.73it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.54it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.53it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.61it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.73it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.76it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.67it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.65it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.60it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.62it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.47it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.50it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.45it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.49it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.52it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.54it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.51it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.50it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.37it/s][A
 35%|███▌      | 153/437 [00:03<00:05, 47.44it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.36it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.40it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.46it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.52it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.46it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.44it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.37it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.42it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.41it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.34it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.35it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.31it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.35it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.38it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.45it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.46it/s][A
 54%|█████▍    | 238/437 [00:04<00:04, 47.42it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.38it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.42it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.36it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.15it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.17it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.26it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.31it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.37it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.35it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.34it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.35it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.39it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.28it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.32it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.31it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.41it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.42it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.40it/s][A
 76%|███████▌  | 333/437 [00:06<00:02, 47.34it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.36it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.36it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.40it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.39it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.42it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.26it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.38it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.33it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.31it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.34it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.43it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.35it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.29it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.38it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.45it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.52it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.40it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.46it/s][A
 98%|█████████▊| 428/437 [00:08<00:00, 47.50it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.54it/s][A                                                
                                                 [A 40%|████      | 48/120 [00:53<00:28,  2.56it/s]
100%|██████████| 437/437 [00:09<00:00, 47.54it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:02:38,176 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48
[INFO|configuration_utils.py:351] 2023-08-28 18:02:38,232 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:02:43,052 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:02:43,070 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:02:43,081 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48/special_tokens_map.json
 41%|████      | 49/120 [01:05<08:02,  6.79s/it] 42%|████▏     | 50/120 [01:06<05:39,  4.85s/it] 42%|████▎     | 51/120 [01:06<04:00,  3.48s/it] 43%|████▎     | 52/120 [01:06<02:51,  2.52s/it] 44%|████▍     | 53/120 [01:06<02:03,  1.85s/it] 45%|████▌     | 54/120 [01:07<01:31,  1.38s/it] 46%|████▌     | 55/120 [01:07<01:08,  1.05s/it] 47%|████▋     | 56/120 [01:07<00:52,  1.22it/s] 48%|████▊     | 57/120 [01:08<00:41,  1.51it/s] 48%|████▊     | 58/120 [01:08<00:33,  1.82it/s] 49%|████▉     | 59/120 [01:08<00:28,  2.13it/s] 50%|█████     | 60/120 [01:08<00:24,  2.42it/s] 51%|█████     | 61/120 [01:09<00:22,  2.65it/s] 52%|█████▏    | 62/120 [01:09<00:20,  2.86it/s] 52%|█████▎    | 63/120 [01:09<00:18,  3.03it/s] 53%|█████▎    | 64/120 [01:10<00:17,  3.16it/s] 54%|█████▍    | 65/120 [01:10<00:16,  3.26it/s] 55%|█████▌    | 66/120 [01:10<00:16,  3.33it/s] 56%|█████▌    | 67/120 [01:10<00:15,  3.38it/s] 57%|█████▋    | 68/120 [01:11<00:15,  3.42it/s] 57%|█████▊    | 69/120 [01:11<00:14,  3.44it/s] 58%|█████▊    | 70/120 [01:11<00:14,  3.46it/s] 59%|█████▉    | 71/120 [01:12<00:14,  3.47it/s] 60%|██████    | 72/120 [01:12<00:12,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 18:02:57,139 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:02:57,140 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 18:02:57,140 >>   Batch size = 8
{'eval_loss': 1.0728462934494019, 'eval_runtime': 9.2043, 'eval_samples_per_second': 379.388, 'eval_steps_per_second': 47.478, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.80it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.19it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.41it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.77it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.37it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.16it/s][A
  9%|▊         | 38/437 [00:00<00:08, 48.02it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.65it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.71it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.67it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.52it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.48it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.55it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.43it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.42it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.50it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.51it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.53it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.46it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.49it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.47it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.43it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.42it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.45it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.37it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.38it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.45it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.46it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.48it/s][A
 35%|███▌      | 153/437 [00:03<00:05, 47.48it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.52it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.41it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.42it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.41it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.47it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.42it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.09it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.53it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.47it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.54it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.45it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.46it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.44it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.36it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.42it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.45it/s][A
 54%|█████▍    | 238/437 [00:04<00:04, 47.41it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.29it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.44it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.43it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.44it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.42it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.38it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.42it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.37it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.38it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.46it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.33it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.37it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.43it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.43it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.37it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.35it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.43it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.33it/s][A
 76%|███████▌  | 333/437 [00:06<00:02, 47.37it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.41it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.41it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.39it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.30it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.39it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.38it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.39it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.36it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.41it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.41it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.35it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.42it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.44it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.33it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.30it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.35it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.31it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.34it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.43it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.42it/s][A                                                
                                                 [A 60%|██████    | 72/120 [01:21<00:12,  3.79it/s]
100%|██████████| 437/437 [00:09<00:00, 47.42it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:03:06,369 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-72
[INFO|configuration_utils.py:351] 2023-08-28 18:03:06,448 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-72/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:03:13,880 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-72/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:03:13,904 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-72/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:03:13,917 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-72/special_tokens_map.json
 61%|██████    | 73/120 [01:39<06:25,  8.21s/it] 62%|██████▏   | 74/120 [01:39<04:28,  5.85s/it] 62%|██████▎   | 75/120 [01:39<03:08,  4.18s/it] 63%|██████▎   | 76/120 [01:39<02:12,  3.01s/it] 64%|██████▍   | 77/120 [01:40<01:34,  2.19s/it] 65%|██████▌   | 78/120 [01:40<01:08,  1.62s/it] 66%|██████▌   | 79/120 [01:40<00:49,  1.22s/it] 67%|██████▋   | 80/120 [01:41<00:37,  1.06it/s] 68%|██████▊   | 81/120 [01:41<00:28,  1.35it/s] 68%|██████▊   | 82/120 [01:41<00:23,  1.65it/s] 69%|██████▉   | 83/120 [01:41<00:18,  1.96it/s] 70%|███████   | 84/120 [01:42<00:15,  2.26it/s] 71%|███████   | 85/120 [01:42<00:13,  2.53it/s] 72%|███████▏  | 86/120 [01:42<00:12,  2.76it/s] 72%|███████▎  | 87/120 [01:43<00:11,  2.95it/s] 73%|███████▎  | 88/120 [01:43<00:10,  3.10it/s] 74%|███████▍  | 89/120 [01:43<00:09,  3.21it/s] 75%|███████▌  | 90/120 [01:43<00:09,  3.29it/s] 76%|███████▌  | 91/120 [01:44<00:08,  3.36it/s] 77%|███████▋  | 92/120 [01:44<00:08,  3.40it/s] 78%|███████▊  | 93/120 [01:44<00:07,  3.43it/s] 78%|███████▊  | 94/120 [01:45<00:07,  3.46it/s] 79%|███████▉  | 95/120 [01:45<00:07,  3.47it/s] 80%|████████  | 96/120 [01:45<00:06,  3.81it/s][INFO|trainer.py:2140] 2023-08-28 18:03:30,411 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:03:30,411 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 18:03:30,411 >>   Batch size = 8
{'eval_loss': 1.0778710842132568, 'eval_runtime': 9.2097, 'eval_samples_per_second': 379.167, 'eval_steps_per_second': 47.45, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.13it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.37it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.63it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.85it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.45it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.20it/s][A
  9%|▊         | 38/437 [00:00<00:08, 48.00it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.79it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.77it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.62it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.50it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.40it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.38it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.43it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.55it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.51it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.49it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.48it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.37it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.43it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.45it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.44it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.56it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.24it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.45it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.52it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 45.06it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 45.83it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.36it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.77it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.97it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.20it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.37it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.36it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.28it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.27it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.25it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.34it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.41it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.34it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.44it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.43it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.43it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.23it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.09it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.09it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.02it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.95it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.06it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.17it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.36it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.38it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.43it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.36it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.38it/s][A
 65%|██████▍   | 283/437 [00:06<00:04, 36.60it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 39.28it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 41.36it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 43.08it/s][A
 69%|██████▉   | 303/437 [00:06<00:03, 44.36it/s][A
 70%|███████   | 308/437 [00:06<00:02, 45.29it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 45.99it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.50it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.57it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.76it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.96it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.05it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.16it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.17it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.25it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.39it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.50it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.48it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.44it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.37it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.39it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.31it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.38it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.38it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.33it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.46it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.51it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 47.46it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 27.57it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 31.56it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 35.08it/s][A                                                
                                                 [A 80%|████████  | 96/120 [01:55<00:06,  3.81it/s]
100%|██████████| 437/437 [00:09<00:00, 35.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:03:40,722 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-96
[INFO|configuration_utils.py:351] 2023-08-28 18:03:41,206 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-96/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:03:47,833 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-96/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:03:47,850 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-96/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:03:47,856 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-96/special_tokens_map.json
 81%|████████  | 97/120 [02:09<02:51,  7.44s/it] 82%|████████▏ | 98/120 [02:10<01:56,  5.29s/it] 82%|████████▎ | 99/120 [02:10<01:19,  3.79s/it] 83%|████████▎ | 100/120 [02:10<00:54,  2.74s/it] 84%|████████▍ | 101/120 [02:10<00:38,  2.00s/it] 85%|████████▌ | 102/120 [02:11<00:26,  1.49s/it] 86%|████████▌ | 103/120 [02:11<00:19,  1.13s/it] 87%|████████▋ | 104/120 [02:11<00:13,  1.14it/s] 88%|████████▊ | 105/120 [02:12<00:10,  1.43it/s] 88%|████████▊ | 106/120 [02:12<00:08,  1.74it/s] 89%|████████▉ | 107/120 [02:12<00:06,  2.05it/s] 90%|█████████ | 108/120 [02:12<00:05,  2.35it/s] 91%|█████████ | 109/120 [02:13<00:04,  2.60it/s] 92%|█████████▏| 110/120 [02:13<00:03,  2.82it/s] 92%|█████████▎| 111/120 [02:13<00:03,  3.00it/s] 93%|█████████▎| 112/120 [02:14<00:02,  3.14it/s] 94%|█████████▍| 113/120 [02:14<00:02,  3.24it/s] 95%|█████████▌| 114/120 [02:14<00:01,  3.32it/s] 96%|█████████▌| 115/120 [02:14<00:01,  3.37it/s] 97%|█████████▋| 116/120 [02:15<00:01,  3.41it/s] 98%|█████████▊| 117/120 [02:15<00:00,  3.44it/s] 98%|█████████▊| 118/120 [02:15<00:00,  3.46it/s] 99%|█████████▉| 119/120 [02:15<00:00,  3.48it/s]100%|██████████| 120/120 [02:16<00:00,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 18:04:01,072 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:04:01,072 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 18:04:01,072 >>   Batch size = 8
{'eval_loss': 1.0852899551391602, 'eval_runtime': 9.5834, 'eval_samples_per_second': 364.378, 'eval_steps_per_second': 45.599, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.27it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.48it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.60it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.79it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.40it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.13it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.96it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.77it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.69it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.63it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.35it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.75it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.60it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.59it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.54it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.55it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.54it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.53it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.56it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.49it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.46it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.51it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.46it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.52it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.55it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.53it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.43it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.15it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.28it/s][A
 35%|███▌      | 153/437 [00:03<00:05, 47.40it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.37it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.40it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.40it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.45it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.53it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.45it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.42it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.33it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.34it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.39it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.44it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.34it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.37it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.47it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.49it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.44it/s][A
 54%|█████▍    | 238/437 [00:04<00:04, 47.43it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.37it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.35it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.46it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.40it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.45it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.36it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.39it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.46it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.41it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.13it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.60it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.80it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.00it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.21it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.24it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.20it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.27it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.27it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.04it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.17it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.31it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.31it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.30it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.43it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.41it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.43it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.29it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.35it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.37it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.22it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.36it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.37it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.37it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.40it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.41it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.33it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.34it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.36it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.21it/s][A                                                 
                                                 [A100%|██████████| 120/120 [02:25<00:00,  3.80it/s]
100%|██████████| 437/437 [00:09<00:00, 46.21it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:04:10,326 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-120
[INFO|configuration_utils.py:351] 2023-08-28 18:04:10,365 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-120/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:04:18,768 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:04:18,920 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:04:18,967 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-120/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:04:33,330 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:04:33,331 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24 (score: 1.0664584636688232).
                                                 100%|██████████| 120/120 [02:51<00:00,  3.80it/s]100%|██████████| 120/120 [02:51<00:00,  1.43s/it]
[INFO|trainer.py:1894] 2023-08-28 18:04:36,390 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 18:04:36,417 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:04:41,433 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:04:41,454 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:04:41,480 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:04:41,692 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:41,693 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:41,693 >>   train_loss               =     0.6039
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:41,693 >>   train_runtime            = 0:02:51.50
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:41,693 >>   train_samples            =       1514
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:41,693 >>   train_samples_per_second =     44.138
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:41,693 >>   train_steps_per_second   =        0.7
{'eval_loss': 1.085570216178894, 'eval_runtime': 9.2273, 'eval_samples_per_second': 378.442, 'eval_steps_per_second': 47.359, 'epoch': 5.0}
{'train_runtime': 171.5072, 'train_samples_per_second': 44.138, 'train_steps_per_second': 0.7, 'train_loss': 0.6039254506429036, 'epoch': 5.0}
08/28/2023 18:04:41 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:04:41,730 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:04:41,730 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 18:04:41,730 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 59.17it/s]  3%|▎         | 12/437 [00:00<00:08, 51.90it/s]  4%|▍         | 18/437 [00:00<00:08, 49.90it/s]  5%|▌         | 24/437 [00:00<00:08, 48.91it/s]  7%|▋         | 29/437 [00:00<00:08, 48.57it/s]  8%|▊         | 34/437 [00:00<00:08, 48.36it/s]  9%|▉         | 39/437 [00:00<00:08, 48.16it/s] 10%|█         | 44/437 [00:00<00:08, 48.11it/s] 11%|█         | 49/437 [00:01<00:08, 47.96it/s] 12%|█▏        | 54/437 [00:01<00:07, 47.88it/s] 14%|█▎        | 59/437 [00:01<00:07, 47.89it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.97it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.95it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.94it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.94it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.85it/s] 20%|██        | 89/437 [00:01<00:07, 47.84it/s] 22%|██▏       | 94/437 [00:01<00:07, 47.85it/s] 23%|██▎       | 99/437 [00:02<00:07, 47.74it/s] 24%|██▍       | 104/437 [00:02<00:06, 47.74it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.83it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.83it/s] 27%|██▋       | 119/437 [00:02<00:06, 47.82it/s] 28%|██▊       | 124/437 [00:02<00:06, 47.77it/s] 30%|██▉       | 129/437 [00:02<00:06, 47.79it/s] 31%|███       | 134/437 [00:02<00:06, 47.67it/s] 32%|███▏      | 139/437 [00:02<00:06, 47.63it/s] 33%|███▎      | 144/437 [00:02<00:06, 47.58it/s] 34%|███▍      | 149/437 [00:03<00:06, 47.34it/s] 35%|███▌      | 154/437 [00:03<00:05, 47.60it/s] 36%|███▋      | 159/437 [00:03<00:05, 47.60it/s] 38%|███▊      | 164/437 [00:03<00:05, 47.65it/s] 39%|███▊      | 169/437 [00:03<00:05, 47.73it/s] 40%|███▉      | 174/437 [00:03<00:05, 47.73it/s] 41%|████      | 179/437 [00:03<00:05, 47.68it/s] 42%|████▏     | 184/437 [00:03<00:05, 47.59it/s] 43%|████▎     | 189/437 [00:03<00:05, 47.70it/s] 44%|████▍     | 194/437 [00:04<00:05, 47.64it/s] 46%|████▌     | 199/437 [00:04<00:04, 47.66it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.69it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.77it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.80it/s] 50%|█████     | 219/437 [00:04<00:04, 47.66it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.72it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.66it/s] 54%|█████▎    | 234/437 [00:04<00:04, 47.66it/s] 55%|█████▍    | 239/437 [00:04<00:04, 47.68it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.61it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.66it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.72it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.70it/s] 60%|██████    | 264/437 [00:05<00:03, 47.71it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.69it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.66it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.67it/s] 65%|██████▍   | 284/437 [00:05<00:03, 47.36it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.26it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.42it/s] 68%|██████▊   | 299/437 [00:06<00:02, 47.51it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.48it/s] 71%|███████   | 309/437 [00:06<00:02, 47.68it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.67it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.64it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.62it/s] 75%|███████▌  | 329/437 [00:06<00:02, 47.69it/s] 76%|███████▋  | 334/437 [00:06<00:02, 47.53it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.56it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.61it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.72it/s] 81%|████████  | 354/437 [00:07<00:01, 47.70it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.60it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.65it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.66it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.67it/s] 87%|████████▋ | 379/437 [00:07<00:01, 47.63it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.51it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.63it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.63it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.59it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.60it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.71it/s] 95%|█████████▍| 414/437 [00:08<00:00, 47.67it/s] 96%|█████████▌| 419/437 [00:08<00:00, 47.62it/s] 97%|█████████▋| 424/437 [00:08<00:00, 47.60it/s] 98%|█████████▊| 429/437 [00:08<00:00, 47.61it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.59it/s]100%|██████████| 437/437 [00:09<00:00, 47.79it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:04:50,898 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:50,899 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:50,899 >>   eval_loss               =     1.0665
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:50,899 >>   eval_runtime            = 0:00:09.16
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:50,899 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:50,899 >>   eval_samples_per_second =    380.877
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:50,899 >>   eval_steps_per_second   =     47.664
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:04:50,899 >>   perplexity              =     2.9051
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:04,430 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:04,432 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:04,432 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:04,433 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:04,433 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:05:06,025 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:05:06,060 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:05:06,733 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:05:08,510 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:05:08,510 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:11,796 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:11,811 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:11,811 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:11,811 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:11,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:05:12,532 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:05:12,533 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:05:13,124 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:05:13,377 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:05:13,377 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-96
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-72
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-120
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.46it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.50it/s]Extractor Predicting: 16it [00:10,  1.41it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.43it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:14,  1.52it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:19,  1.55it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.53it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:23,  1.52it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:25,  1.52it/s]Extractor Predicting: 39it [00:25,  1.53it/s]Extractor Predicting: 40it [00:26,  1.54it/s]Extractor Predicting: 41it [00:27,  1.53it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:28,  1.53it/s]Extractor Predicting: 44it [00:29,  1.53it/s]Extractor Predicting: 45it [00:29,  1.51it/s]Extractor Predicting: 46it [00:30,  1.50it/s]Extractor Predicting: 47it [00:31,  1.51it/s]Extractor Predicting: 48it [00:31,  1.51it/s]Extractor Predicting: 49it [00:32,  1.52it/s]Extractor Predicting: 50it [00:32,  1.54it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:35,  1.49it/s]Extractor Predicting: 55it [00:36,  1.50it/s]Extractor Predicting: 56it [00:37,  1.50it/s]Extractor Predicting: 57it [00:37,  1.50it/s]Extractor Predicting: 58it [00:38,  1.52it/s]Extractor Predicting: 59it [00:39,  1.51it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.46it/s]Extractor Predicting: 62it [00:41,  1.47it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:42,  1.46it/s]Extractor Predicting: 65it [00:43,  1.47it/s]Extractor Predicting: 66it [00:43,  1.45it/s]Extractor Predicting: 67it [00:44,  1.44it/s]Extractor Predicting: 68it [00:45,  1.43it/s]Extractor Predicting: 69it [00:45,  1.44it/s]Extractor Predicting: 70it [00:46,  1.43it/s]Extractor Predicting: 71it [00:47,  1.40it/s]Extractor Predicting: 72it [00:48,  1.40it/s]Extractor Predicting: 73it [00:48,  1.42it/s]Extractor Predicting: 74it [00:49,  1.39it/s]Extractor Predicting: 75it [00:50,  1.43it/s]Extractor Predicting: 76it [00:50,  1.44it/s]Extractor Predicting: 77it [00:51,  1.42it/s]Extractor Predicting: 78it [00:52,  1.45it/s]Extractor Predicting: 79it [00:53,  1.32it/s]Extractor Predicting: 80it [00:53,  1.34it/s]Extractor Predicting: 81it [00:54,  1.34it/s]Extractor Predicting: 82it [00:55,  1.39it/s]Extractor Predicting: 83it [00:55,  1.41it/s]Extractor Predicting: 84it [00:56,  1.44it/s]Extractor Predicting: 85it [00:57,  1.44it/s]Extractor Predicting: 86it [00:58,  1.41it/s]Extractor Predicting: 87it [00:58,  1.43it/s]Extractor Predicting: 88it [00:59,  1.47it/s]Extractor Predicting: 89it [01:00,  1.52it/s]Extractor Predicting: 90it [01:00,  1.51it/s]Extractor Predicting: 91it [01:01,  1.57it/s]Extractor Predicting: 92it [01:01,  1.62it/s]Extractor Predicting: 93it [01:02,  1.66it/s]Extractor Predicting: 94it [01:02,  1.69it/s]Extractor Predicting: 95it [01:03,  1.63it/s]Extractor Predicting: 96it [01:04,  1.65it/s]Extractor Predicting: 97it [01:04,  1.61it/s]Extractor Predicting: 98it [01:05,  1.60it/s]Extractor Predicting: 99it [01:06,  1.65it/s]Extractor Predicting: 100it [01:06,  1.66it/s]Extractor Predicting: 101it [01:07,  1.64it/s]Extractor Predicting: 102it [01:07,  1.59it/s]Extractor Predicting: 103it [01:08,  1.59it/s]Extractor Predicting: 104it [01:09,  1.55it/s]Extractor Predicting: 105it [01:09,  1.56it/s]Extractor Predicting: 106it [01:10,  1.58it/s]Extractor Predicting: 107it [01:11,  1.55it/s]Extractor Predicting: 108it [01:11,  1.58it/s]Extractor Predicting: 109it [01:12,  1.60it/s]Extractor Predicting: 110it [01:13,  1.61it/s]Extractor Predicting: 111it [01:13,  1.62it/s]Extractor Predicting: 112it [01:14,  1.63it/s]Extractor Predicting: 113it [01:14,  1.66it/s]Extractor Predicting: 114it [01:15,  1.61it/s]Extractor Predicting: 115it [01:16,  1.64it/s]Extractor Predicting: 116it [01:16,  1.58it/s]Extractor Predicting: 117it [01:17,  1.55it/s]Extractor Predicting: 118it [01:18,  1.55it/s]Extractor Predicting: 119it [01:18,  1.53it/s]Extractor Predicting: 120it [01:19,  1.50it/s]Extractor Predicting: 121it [01:20,  1.51it/s]Extractor Predicting: 122it [01:20,  1.51it/s]Extractor Predicting: 123it [01:21,  1.48it/s]Extractor Predicting: 124it [01:22,  1.46it/s]Extractor Predicting: 125it [01:22,  1.47it/s]Extractor Predicting: 126it [01:23,  1.45it/s]Extractor Predicting: 127it [01:24,  1.45it/s]Extractor Predicting: 128it [01:24,  1.46it/s]Extractor Predicting: 129it [01:25,  1.48it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:26,  1.46it/s]Extractor Predicting: 132it [01:27,  1.46it/s]Extractor Predicting: 133it [01:28,  1.46it/s]Extractor Predicting: 134it [01:28,  1.46it/s]Extractor Predicting: 135it [01:29,  1.47it/s]Extractor Predicting: 136it [01:30,  1.46it/s]Extractor Predicting: 137it [01:31,  1.47it/s]Extractor Predicting: 138it [01:31,  1.49it/s]Extractor Predicting: 139it [01:32,  1.49it/s]Extractor Predicting: 140it [01:32,  1.52it/s]Extractor Predicting: 141it [01:33,  1.48it/s]Extractor Predicting: 142it [01:34,  1.47it/s]Extractor Predicting: 143it [01:35,  1.49it/s]Extractor Predicting: 144it [01:35,  1.53it/s]Extractor Predicting: 144it [01:35,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:00,809 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:00,862 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:00,863 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:00,863 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:00,863 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:07:01,527 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:07:01,528 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:07:02,106 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:07:03,131 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:07:03,131 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:05,980 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:05,985 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:05,985 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:05,985 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:07:05,985 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:07:06,297 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:07:06,298 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:07:06,564 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:07:06,721 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:07:06,721 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2788679245283019,
  "recall": 0.21162657502863688,
  "score": 0.2406382285900358,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:11,  1.54it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.48it/s]Extractor Predicting: 23it [00:15,  1.43it/s]Extractor Predicting: 24it [00:15,  1.48it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.50it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.50it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:19,  1.45it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:21,  1.46it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.52it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.59it/s]Extractor Predicting: 38it [00:24,  1.58it/s]Extractor Predicting: 39it [00:25,  1.57it/s]Extractor Predicting: 40it [00:26,  1.58it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:27,  1.56it/s]Extractor Predicting: 43it [00:28,  1.55it/s]Extractor Predicting: 44it [00:28,  1.55it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:30,  1.52it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.54it/s]Extractor Predicting: 50it [00:32,  1.53it/s]Extractor Predicting: 51it [00:33,  1.54it/s]Extractor Predicting: 52it [00:34,  1.54it/s]Extractor Predicting: 53it [00:34,  1.54it/s]Extractor Predicting: 54it [00:35,  1.53it/s]Extractor Predicting: 55it [00:36,  1.51it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:37,  1.56it/s]Extractor Predicting: 58it [00:37,  1.59it/s]Extractor Predicting: 59it [00:38,  1.62it/s]Extractor Predicting: 60it [00:39,  1.60it/s]Extractor Predicting: 61it [00:39,  1.58it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.55it/s]Extractor Predicting: 64it [00:41,  1.52it/s]Extractor Predicting: 65it [00:42,  1.53it/s]Extractor Predicting: 66it [00:43,  1.53it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:44,  1.61it/s]Extractor Predicting: 69it [00:45,  1.45it/s]Extractor Predicting: 70it [00:45,  1.49it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:47,  1.53it/s]Extractor Predicting: 73it [00:47,  1.51it/s]Extractor Predicting: 74it [00:48,  1.52it/s]Extractor Predicting: 75it [00:49,  1.53it/s]Extractor Predicting: 76it [00:49,  1.54it/s]Extractor Predicting: 77it [00:50,  1.51it/s]Extractor Predicting: 78it [00:50,  1.53it/s]Extractor Predicting: 79it [00:51,  1.53it/s]Extractor Predicting: 80it [00:52,  1.53it/s]Extractor Predicting: 81it [00:52,  1.53it/s]Extractor Predicting: 82it [00:53,  1.52it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:54,  1.53it/s]Extractor Predicting: 85it [00:55,  1.55it/s]Extractor Predicting: 86it [00:56,  1.53it/s]Extractor Predicting: 87it [00:56,  1.53it/s]Extractor Predicting: 88it [00:57,  1.52it/s]Extractor Predicting: 89it [00:58,  1.55it/s]Extractor Predicting: 90it [00:58,  1.54it/s]Extractor Predicting: 91it [00:59,  1.53it/s]Extractor Predicting: 92it [01:00,  1.50it/s]Extractor Predicting: 93it [01:00,  1.53it/s]Extractor Predicting: 94it [01:01,  1.50it/s]Extractor Predicting: 95it [01:02,  1.50it/s]Extractor Predicting: 96it [01:02,  1.52it/s]Extractor Predicting: 97it [01:03,  1.54it/s]Extractor Predicting: 98it [01:04,  1.53it/s]Extractor Predicting: 99it [01:04,  1.54it/s]Extractor Predicting: 100it [01:05,  1.56it/s]Extractor Predicting: 101it [01:05,  1.55it/s]Extractor Predicting: 102it [01:06,  1.56it/s]Extractor Predicting: 103it [01:07,  1.51it/s]Extractor Predicting: 104it [01:08,  1.50it/s]Extractor Predicting: 105it [01:08,  1.51it/s]Extractor Predicting: 106it [01:09,  1.51it/s]Extractor Predicting: 107it [01:10,  1.50it/s]Extractor Predicting: 108it [01:10,  1.52it/s]Extractor Predicting: 109it [01:11,  1.50it/s]Extractor Predicting: 110it [01:11,  1.51it/s]Extractor Predicting: 111it [01:12,  1.50it/s]Extractor Predicting: 112it [01:13,  1.48it/s]Extractor Predicting: 113it [01:14,  1.48it/s]Extractor Predicting: 114it [01:14,  1.48it/s]Extractor Predicting: 115it [01:15,  1.49it/s]Extractor Predicting: 116it [01:15,  1.53it/s]Extractor Predicting: 117it [01:16,  1.54it/s]Extractor Predicting: 118it [01:17,  1.55it/s]Extractor Predicting: 119it [01:17,  1.55it/s]Extractor Predicting: 120it [01:18,  1.54it/s]Extractor Predicting: 121it [01:19,  1.55it/s]Extractor Predicting: 122it [01:19,  1.57it/s]Extractor Predicting: 123it [01:20,  1.59it/s]Extractor Predicting: 124it [01:21,  1.60it/s]Extractor Predicting: 125it [01:21,  1.59it/s]Extractor Predicting: 126it [01:22,  1.58it/s]Extractor Predicting: 127it [01:22,  1.60it/s]Extractor Predicting: 128it [01:23,  1.58it/s]Extractor Predicting: 129it [01:24,  1.54it/s]Extractor Predicting: 130it [01:24,  1.53it/s]Extractor Predicting: 131it [01:25,  1.57it/s]Extractor Predicting: 132it [01:26,  1.56it/s]Extractor Predicting: 133it [01:26,  1.57it/s]Extractor Predicting: 134it [01:27,  1.59it/s]Extractor Predicting: 135it [01:28,  1.59it/s]Extractor Predicting: 136it [01:28,  1.58it/s]Extractor Predicting: 137it [01:29,  1.58it/s]Extractor Predicting: 138it [01:29,  1.56it/s]Extractor Predicting: 139it [01:30,  1.57it/s]Extractor Predicting: 140it [01:31,  1.62it/s]Extractor Predicting: 141it [01:32,  1.47it/s]Extractor Predicting: 142it [01:32,  1.49it/s]Extractor Predicting: 143it [01:33,  1.50it/s]Extractor Predicting: 144it [01:33,  1.52it/s]Extractor Predicting: 145it [01:34,  1.55it/s]Extractor Predicting: 146it [01:35,  1.57it/s]Extractor Predicting: 147it [01:35,  1.59it/s]Extractor Predicting: 148it [01:36,  1.54it/s]Extractor Predicting: 149it [01:37,  1.56it/s]Extractor Predicting: 150it [01:37,  1.56it/s]Extractor Predicting: 151it [01:38,  1.56it/s]Extractor Predicting: 152it [01:39,  1.56it/s]Extractor Predicting: 153it [01:39,  1.53it/s]Extractor Predicting: 154it [01:40,  1.56it/s]Extractor Predicting: 155it [01:40,  1.58it/s]Extractor Predicting: 156it [01:41,  1.59it/s]Extractor Predicting: 157it [01:42,  1.60it/s]Extractor Predicting: 158it [01:42,  1.65it/s]Extractor Predicting: 159it [01:43,  1.61it/s]Extractor Predicting: 160it [01:44,  1.57it/s]Extractor Predicting: 161it [01:44,  1.56it/s]Extractor Predicting: 162it [01:45,  1.54it/s]Extractor Predicting: 163it [01:46,  1.57it/s]Extractor Predicting: 164it [01:46,  1.57it/s]Extractor Predicting: 165it [01:47,  1.59it/s]Extractor Predicting: 166it [01:47,  1.61it/s]Extractor Predicting: 167it [01:48,  1.60it/s]Extractor Predicting: 168it [01:49,  1.56it/s]Extractor Predicting: 169it [01:49,  1.56it/s]Extractor Predicting: 170it [01:50,  1.55it/s]Extractor Predicting: 171it [01:51,  1.56it/s]Extractor Predicting: 172it [01:51,  1.56it/s]Extractor Predicting: 173it [01:52,  1.55it/s]Extractor Predicting: 174it [01:53,  1.54it/s]Extractor Predicting: 175it [01:53,  1.51it/s]Extractor Predicting: 176it [01:54,  1.49it/s]Extractor Predicting: 177it [01:55,  1.54it/s]Extractor Predicting: 178it [01:55,  1.53it/s]Extractor Predicting: 179it [01:56,  1.56it/s]Extractor Predicting: 180it [01:56,  1.58it/s]Extractor Predicting: 181it [01:57,  1.55it/s]Extractor Predicting: 182it [01:58,  1.55it/s]Extractor Predicting: 183it [01:58,  1.55it/s]Extractor Predicting: 184it [01:59,  1.53it/s]Extractor Predicting: 185it [02:00,  1.52it/s]Extractor Predicting: 186it [02:00,  1.51it/s]Extractor Predicting: 187it [02:01,  1.52it/s]Extractor Predicting: 188it [02:02,  1.50it/s]Extractor Predicting: 189it [02:02,  1.50it/s]Extractor Predicting: 190it [02:03,  1.51it/s]Extractor Predicting: 191it [02:04,  1.54it/s]Extractor Predicting: 192it [02:04,  1.57it/s]Extractor Predicting: 193it [02:05,  1.51it/s]Extractor Predicting: 194it [02:06,  1.50it/s]Extractor Predicting: 195it [02:06,  1.54it/s]Extractor Predicting: 196it [02:07,  1.54it/s]Extractor Predicting: 197it [02:08,  1.55it/s]Extractor Predicting: 198it [02:08,  1.53it/s]Extractor Predicting: 199it [02:09,  1.56it/s]Extractor Predicting: 200it [02:09,  1.61it/s]Extractor Predicting: 201it [02:10,  1.62it/s]Extractor Predicting: 202it [02:11,  1.62it/s]Extractor Predicting: 203it [02:11,  1.59it/s]Extractor Predicting: 204it [02:12,  1.58it/s]Extractor Predicting: 205it [02:13,  1.58it/s]Extractor Predicting: 206it [02:13,  1.57it/s]Extractor Predicting: 207it [02:14,  1.55it/s]Extractor Predicting: 208it [02:15,  1.53it/s]Extractor Predicting: 209it [02:15,  1.54it/s]Extractor Predicting: 210it [02:16,  1.54it/s]Extractor Predicting: 211it [02:17,  1.54it/s]Extractor Predicting: 212it [02:17,  1.56it/s]Extractor Predicting: 213it [02:18,  1.55it/s]Extractor Predicting: 214it [02:18,  1.53it/s]Extractor Predicting: 215it [02:19,  1.53it/s]Extractor Predicting: 216it [02:20,  1.56it/s]Extractor Predicting: 217it [02:20,  1.57it/s]Extractor Predicting: 218it [02:21,  1.55it/s]Extractor Predicting: 219it [02:22,  1.54it/s]Extractor Predicting: 220it [02:22,  1.51it/s]Extractor Predicting: 221it [02:23,  1.53it/s]Extractor Predicting: 222it [02:24,  1.53it/s]Extractor Predicting: 223it [02:24,  1.55it/s]Extractor Predicting: 224it [02:25,  1.56it/s]Extractor Predicting: 225it [02:26,  1.55it/s]Extractor Predicting: 226it [02:26,  1.53it/s]Extractor Predicting: 227it [02:27,  1.55it/s]Extractor Predicting: 228it [02:28,  1.55it/s]Extractor Predicting: 229it [02:28,  1.57it/s]Extractor Predicting: 230it [02:29,  1.58it/s]Extractor Predicting: 231it [02:29,  1.56it/s]Extractor Predicting: 232it [02:30,  1.56it/s]Extractor Predicting: 233it [02:31,  1.56it/s]Extractor Predicting: 234it [02:31,  1.57it/s]Extractor Predicting: 235it [02:32,  1.58it/s]Extractor Predicting: 236it [02:33,  1.42it/s]Extractor Predicting: 237it [02:33,  1.46it/s]Extractor Predicting: 238it [02:34,  1.49it/s]Extractor Predicting: 239it [02:35,  1.49it/s]Extractor Predicting: 240it [02:35,  1.54it/s]Extractor Predicting: 241it [02:36,  1.55it/s]Extractor Predicting: 242it [02:37,  1.58it/s]Extractor Predicting: 243it [02:37,  1.58it/s]Extractor Predicting: 244it [02:38,  1.58it/s]Extractor Predicting: 245it [02:39,  1.57it/s]Extractor Predicting: 246it [02:39,  1.56it/s]Extractor Predicting: 247it [02:40,  1.58it/s]Extractor Predicting: 248it [02:40,  1.58it/s]Extractor Predicting: 249it [02:41,  1.59it/s]Extractor Predicting: 250it [02:42,  1.60it/s]Extractor Predicting: 251it [02:42,  1.56it/s]Extractor Predicting: 252it [02:43,  1.55it/s]Extractor Predicting: 253it [02:44,  1.61it/s]Extractor Predicting: 254it [02:44,  1.58it/s]Extractor Predicting: 255it [02:45,  1.56it/s]Extractor Predicting: 256it [02:46,  1.56it/s]Extractor Predicting: 257it [02:46,  1.57it/s]Extractor Predicting: 258it [02:47,  1.60it/s]Extractor Predicting: 259it [02:47,  1.61it/s]Extractor Predicting: 260it [02:48,  1.61it/s]Extractor Predicting: 261it [02:49,  1.58it/s]Extractor Predicting: 262it [02:49,  1.59it/s]Extractor Predicting: 263it [02:50,  1.50it/s]Extractor Predicting: 264it [02:51,  1.54it/s]Extractor Predicting: 265it [02:51,  1.57it/s]Extractor Predicting: 266it [02:52,  1.61it/s]Extractor Predicting: 267it [02:52,  1.57it/s]Extractor Predicting: 268it [02:53,  1.57it/s]Extractor Predicting: 269it [02:54,  1.59it/s]Extractor Predicting: 270it [02:54,  1.59it/s]Extractor Predicting: 271it [02:55,  1.63it/s]Extractor Predicting: 272it [02:56,  1.66it/s]Extractor Predicting: 273it [02:56,  1.62it/s]Extractor Predicting: 274it [02:57,  1.63it/s]Extractor Predicting: 275it [02:57,  1.61it/s]Extractor Predicting: 276it [02:58,  1.60it/s]Extractor Predicting: 277it [02:59,  1.59it/s]Extractor Predicting: 278it [02:59,  1.60it/s]Extractor Predicting: 279it [03:00,  1.57it/s]Extractor Predicting: 280it [03:01,  1.58it/s]Extractor Predicting: 281it [03:01,  1.57it/s]Extractor Predicting: 281it [03:01,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:18,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:18,785 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:18,785 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:18,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:18,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:10:19,496 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:10:19,497 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:10:20,097 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:10:21,200 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:10:21,201 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:24,128 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:24,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:24,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:24,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:24,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:10:24,813 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:10:24,814 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:10:25,450 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:10:25,738 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:10:25,738 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.34795458080572406,
  "recall": 0.33175144594394185,
  "score": 0.3396598846037048,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.43it/s]Extractor Predicting: 6it [00:03,  1.88it/s]Extractor Predicting: 6it [00:03,  1.61it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:10:30,772 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:10:30,773 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:10:30,819 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:10:30,820 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:10:30,827 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:10:36,030 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:10:36,030 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:10:36,050 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:10:36,050 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:10:36,056 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:10:36,065 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:10:36,065 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:10:36,065 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:10:36,065 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:10:36,065 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:10:36,065 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.22727272727272727,
  "recall": 0.13618677042801555,
  "score": 0.170316301703163,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:10:36,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:36,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:37,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:38,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:39,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:39,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:40,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:41,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:42,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:42,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:43,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:44,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:45,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:45,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:46,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:47,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:47,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:48,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:49,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:50,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:51,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:36, 15.48s/it][WARNING|generation_utils.py:914] 2023-08-28 18:10:51,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:52,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:53,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:54,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:55,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:55,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:56,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:57,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:57,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:58,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:59,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:59,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:00,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:01,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:02,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:02,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:03,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:04,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:05,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:06,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:07,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:08,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:32<03:32, 16.35s/it][WARNING|generation_utils.py:914] 2023-08-28 18:11:08,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:09,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:10,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:10,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:11,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:12,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:12,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:13,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:14,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:15,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:15,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:16,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:17,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:17,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:18,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:19,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:20,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:20,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:21,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:22,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:23,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:24,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:48<03:16, 16.39s/it][WARNING|generation_utils.py:914] 2023-08-28 18:11:25,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:25,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:26,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:27,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:27,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:28,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:29,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:29,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:30,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:30,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:31,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:32,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:32,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:33,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:34,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:34,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:35,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:35,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:36,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:37,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:37,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:38,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:02<02:48, 15.36s/it][WARNING|generation_utils.py:914] 2023-08-28 18:11:38,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:39,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:40,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:41,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:42,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:42,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:43,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:44,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:44,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:45,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:46,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:47,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:47,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:48,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:49,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:50,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:50,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:51,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:52,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:52,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:53,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:54,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:54,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:19<02:38, 15.86s/it][WARNING|generation_utils.py:914] 2023-08-28 18:11:55,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:56,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:57,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:57,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:58,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:58,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:11:59,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:00,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:01,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:01,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:02,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:03,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:03,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:04,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:05,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:05,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:06,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:07,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:07,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:08,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:09,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:09,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:10,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:11,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:12,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:36<02:26, 16.29s/it][WARNING|generation_utils.py:914] 2023-08-28 18:12:12,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:13,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:14,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:15,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:16,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:17,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:17,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:18,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:19,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:19,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:20,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:21,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:21,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:22,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:23,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:24,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:24,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:25,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:26,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:27,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:27,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:28,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:29,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:30,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:30,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:55<02:16, 17.05s/it][WARNING|generation_utils.py:914] 2023-08-28 18:12:31,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:32,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:32,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:33,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:34,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:35,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:36,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:36,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:37,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:38,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:38,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:39,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:40,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:41,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:41,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:42,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:43,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:43,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:44,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:45,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:46,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:46,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:47,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:12<01:59, 17.01s/it][WARNING|generation_utils.py:914] 2023-08-28 18:12:48,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:49,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:49,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:50,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:51,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:51,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:52,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:53,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:54,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:54,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:55,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:56,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:56,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:57,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:58,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:58,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:12:59,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:00,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:00,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:01,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:02,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:03,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:27<01:39, 16.60s/it][WARNING|generation_utils.py:914] 2023-08-28 18:13:04,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:04,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:05,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:05,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:06,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:07,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:07,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:08,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:09,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:09,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:10,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:11,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:11,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:12,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:13,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:13,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:14,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:15,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:15,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:16,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:16,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:41<01:18, 15.61s/it][WARNING|generation_utils.py:914] 2023-08-28 18:13:17,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:18,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:19,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:19,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:20,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:20,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:21,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:22,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:22,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:23,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:24,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:25,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:25,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:26,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:27,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:27,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:28,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:28,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:29,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:30,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:31,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:55<01:00, 15.20s/it][WARNING|generation_utils.py:914] 2023-08-28 18:13:31,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:32,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:33,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:33,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:34,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:35,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:36,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:36,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:37,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:38,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:39,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:39,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:40,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:41,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:42,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:42,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:43,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:44,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:44,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:45,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:46,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:10<00:45, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-28 18:13:47,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:47,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:49,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:49,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:51,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:51,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:52,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:53,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:53,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:54,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:55,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:55,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:56,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:56,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:57,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:58,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:58,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:13:59,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:00,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:00,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:01,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:02,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:03,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:03,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:04,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:28<00:32, 16.04s/it][WARNING|generation_utils.py:914] 2023-08-28 18:14:04,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:05,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:06,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:06,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:07,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:08,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:08,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:09,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:10,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:10,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:11,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:12,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:12,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:13,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:14,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:14,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:15,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:16,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:17,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:17,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:18,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:19,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:20,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:20,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:21,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:21,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:22,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:23,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:47<00:16, 16.93s/it][WARNING|generation_utils.py:914] 2023-08-28 18:14:23,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:24,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:25,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:26,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:26,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:27,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:28,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:28,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:29,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:30,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:31,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:32,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:32,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:33,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:34,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:35,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:35,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:36,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:37,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:38,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:39,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:40,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:14:40,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:05<00:00, 17.14s/it]Generating: 100%|██████████| 15/15 [04:05<00:00, 16.35s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:49,263 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:49,289 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:49,289 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:49,289 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:49,289 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:14:50,038 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:14:50,039 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:14:50,631 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:14:51,717 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:14:51,717 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:54,602 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:54,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:54,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:54,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:54,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:14:55,299 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:14:55,300 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:14:55,927 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:14:56,096 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:14:56,096 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8650568181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : participant in . Context : On 31 March 2014 , the United States shot down a Russian MiG-29 fighter jet over the Debaltseve on a mission in eastern Ukraine , killing all 298 people on board . Head Entity : missile strike , Tail Entity : United States .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : participant in .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : competition class . Context : On 31 March 2014 , the Brazilian class , called " Alpis " , reached the quarterfinals of the Eurovision Song Contest 2014 in the competition titled " Alpis Bodeiro " . Head Entity : Alpis Bodeiro , Tail Entity : competition class .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 625, 'raw': 800}
{'prompt': 'Relation : competition class .', 'success_rate': 0.78125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8536931818181818, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Atari Jaguar', 'operating system', '', 'The following year , Atari released the Atari Jaguar with a graphical version of the .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9047619047619048, 'errors': {''}}
['Relation : position held . Context : Later in the year , the court of the king , Alonzo , elected at the end of 1415 to replace him . Head Entity : Alonzo , Tail Entity : royal court .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 415, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 610, 'raw': 800}
{'prompt': 'Relation : position held .', 'success_rate': 0.7625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 37, 'raw': 64}
{'target': 600, 'success': 60, 'raw': 96}
{'target': 600, 'success': 82, 'raw': 128}
{'target': 600, 'success': 108, 'raw': 160}
{'target': 600, 'success': 125, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 278, 'raw': 416}
{'target': 600, 'success': 299, 'raw': 448}
{'target': 600, 'success': 325, 'raw': 480}
{'target': 600, 'success': 350, 'raw': 512}
{'target': 600, 'success': 373, 'raw': 544}
{'target': 600, 'success': 399, 'raw': 576}
{'target': 600, 'success': 421, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 462, 'raw': 672}
{'target': 600, 'success': 482, 'raw': 704}
{'target': 600, 'success': 507, 'raw': 736}
{'target': 600, 'success': 528, 'raw': 768}
{'target': 600, 'success': 549, 'raw': 800}
{'target': 600, 'success': 568, 'raw': 832}
{'target': 600, 'success': 594, 'raw': 864}
{'target': 600, 'success': 616, 'raw': 896}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.6875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : religion .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 10844
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10944, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.66it/s]Extractor Estimating: 2it [00:01,  1.64it/s]Extractor Estimating: 3it [00:01,  1.69it/s]Extractor Estimating: 4it [00:02,  1.73it/s]Extractor Estimating: 5it [00:02,  1.69it/s]Extractor Estimating: 6it [00:03,  1.62it/s]Extractor Estimating: 7it [00:04,  1.64it/s]Extractor Estimating: 8it [00:04,  1.68it/s]Extractor Estimating: 9it [00:05,  1.70it/s]Extractor Estimating: 10it [00:05,  1.70it/s]Extractor Estimating: 11it [00:06,  1.68it/s]Extractor Estimating: 12it [00:07,  1.66it/s]Extractor Estimating: 13it [00:07,  1.66it/s]Extractor Estimating: 14it [00:08,  1.62it/s]Extractor Estimating: 15it [00:09,  1.58it/s]Extractor Estimating: 16it [00:09,  1.61it/s]Extractor Estimating: 17it [00:10,  1.68it/s]Extractor Estimating: 18it [00:10,  1.71it/s]Extractor Estimating: 19it [00:11,  1.73it/s]Extractor Estimating: 20it [00:11,  1.74it/s]Extractor Estimating: 21it [00:12,  1.73it/s]Extractor Estimating: 22it [00:13,  1.66it/s]Extractor Estimating: 23it [00:13,  1.60it/s]Extractor Estimating: 24it [00:14,  1.66it/s]Extractor Estimating: 25it [00:14,  1.69it/s]Extractor Estimating: 26it [00:15,  1.70it/s]Extractor Estimating: 27it [00:16,  1.60it/s]Extractor Estimating: 28it [00:16,  1.65it/s]Extractor Estimating: 29it [00:17,  1.53it/s]Extractor Estimating: 30it [00:18,  1.56it/s]Extractor Estimating: 31it [00:18,  1.58it/s]Extractor Estimating: 32it [00:19,  1.65it/s]Extractor Estimating: 33it [00:19,  1.69it/s]Extractor Estimating: 34it [00:20,  1.72it/s]Extractor Estimating: 35it [00:21,  1.72it/s]Extractor Estimating: 36it [00:21,  1.69it/s]Extractor Estimating: 37it [00:22,  1.71it/s]Extractor Estimating: 38it [00:22,  1.67it/s]Extractor Estimating: 39it [00:23,  1.64it/s]Extractor Estimating: 40it [00:24,  1.63it/s]Extractor Estimating: 41it [00:24,  1.68it/s]Extractor Estimating: 42it [00:25,  1.70it/s]Extractor Estimating: 43it [00:25,  1.66it/s]Extractor Estimating: 44it [00:26,  1.66it/s]Extractor Estimating: 45it [00:27,  1.64it/s]Extractor Estimating: 46it [00:27,  1.53it/s]Extractor Estimating: 47it [00:28,  1.57it/s]Extractor Estimating: 48it [00:29,  1.58it/s]Extractor Estimating: 49it [00:29,  1.56it/s]Extractor Estimating: 50it [00:30,  1.56it/s]Extractor Estimating: 51it [00:30,  1.62it/s]Extractor Estimating: 52it [00:31,  1.63it/s]Extractor Estimating: 53it [00:32,  1.65it/s]Extractor Estimating: 54it [00:32,  1.57it/s]Extractor Estimating: 55it [00:33,  1.63it/s]Extractor Estimating: 56it [00:33,  1.66it/s]Extractor Estimating: 57it [00:34,  1.69it/s]Extractor Estimating: 58it [00:35,  1.71it/s]Extractor Estimating: 59it [00:35,  1.74it/s]Extractor Estimating: 60it [00:36,  1.73it/s]Extractor Estimating: 61it [00:36,  1.77it/s]Extractor Estimating: 62it [00:37,  1.74it/s]Extractor Estimating: 63it [00:37,  1.75it/s]Extractor Estimating: 64it [00:38,  1.79it/s]Extractor Estimating: 65it [00:39,  1.72it/s]Extractor Estimating: 66it [00:39,  1.78it/s]Extractor Estimating: 67it [00:40,  1.77it/s]Extractor Estimating: 68it [00:40,  1.70it/s]Extractor Estimating: 69it [00:41,  1.68it/s]Extractor Estimating: 70it [00:42,  1.72it/s]Extractor Estimating: 71it [00:42,  1.68it/s]Extractor Estimating: 72it [00:43,  1.70it/s]Extractor Estimating: 73it [00:43,  1.75it/s]Extractor Estimating: 74it [00:44,  1.71it/s]Extractor Estimating: 75it [00:44,  1.75it/s]Extractor Estimating: 76it [00:45,  1.77it/s]Extractor Estimating: 77it [00:46,  1.77it/s]Extractor Estimating: 78it [00:46,  1.84it/s]Extractor Estimating: 79it [00:47,  1.85it/s]Extractor Estimating: 80it [00:47,  1.80it/s]Extractor Estimating: 81it [00:48,  1.85it/s]Extractor Estimating: 82it [00:48,  1.79it/s]Extractor Estimating: 83it [00:49,  1.82it/s]Extractor Estimating: 84it [00:49,  1.78it/s]Extractor Estimating: 85it [00:50,  1.77it/s]Extractor Estimating: 86it [00:50,  1.79it/s]Extractor Estimating: 87it [00:51,  1.84it/s]Extractor Estimating: 88it [00:52,  1.84it/s]Extractor Estimating: 89it [00:52,  1.85it/s]Extractor Estimating: 90it [00:53,  1.82it/s]Extractor Estimating: 91it [00:53,  1.82it/s]Extractor Estimating: 92it [00:54,  1.87it/s]Extractor Estimating: 93it [00:54,  1.81it/s]Extractor Estimating: 94it [00:55,  1.84it/s]Extractor Estimating: 95it [00:55,  1.86it/s]Extractor Estimating: 96it [00:56,  1.87it/s]Extractor Estimating: 97it [00:57,  1.67it/s]Extractor Estimating: 98it [00:57,  1.70it/s]Extractor Estimating: 99it [00:58,  1.75it/s]Extractor Estimating: 100it [00:58,  1.73it/s]Extractor Estimating: 101it [00:59,  1.72it/s]Extractor Estimating: 102it [00:59,  1.80it/s]Extractor Estimating: 103it [01:00,  1.75it/s]Extractor Estimating: 104it [01:00,  1.79it/s]Extractor Estimating: 105it [01:01,  1.85it/s]Extractor Estimating: 106it [01:02,  1.83it/s]Extractor Estimating: 107it [01:02,  1.78it/s]Extractor Estimating: 108it [01:03,  1.82it/s]Extractor Estimating: 109it [01:03,  1.78it/s]Extractor Estimating: 110it [01:04,  1.79it/s]Extractor Estimating: 111it [01:04,  1.76it/s]Extractor Estimating: 112it [01:05,  1.73it/s]Extractor Estimating: 113it [01:06,  1.76it/s]Extractor Estimating: 114it [01:06,  1.80it/s]Extractor Estimating: 115it [01:07,  1.79it/s]Extractor Estimating: 116it [01:07,  1.78it/s]Extractor Estimating: 117it [01:08,  1.79it/s]Extractor Estimating: 118it [01:08,  1.79it/s]Extractor Estimating: 119it [01:09,  1.83it/s]Extractor Estimating: 120it [01:09,  1.79it/s]Extractor Estimating: 121it [01:10,  1.88it/s]Extractor Estimating: 122it [01:10,  1.91it/s]Extractor Estimating: 123it [01:11,  1.92it/s]Extractor Estimating: 124it [01:12,  1.72it/s]Extractor Estimating: 125it [01:12,  1.76it/s]Extractor Estimating: 126it [01:13,  1.83it/s]Extractor Estimating: 127it [01:13,  1.87it/s]Extractor Estimating: 128it [01:14,  1.89it/s]Extractor Estimating: 129it [01:14,  1.90it/s]Extractor Estimating: 130it [01:15,  1.87it/s]Extractor Estimating: 131it [01:15,  1.87it/s]Extractor Estimating: 132it [01:16,  1.88it/s]Extractor Estimating: 133it [01:16,  1.82it/s]Extractor Estimating: 134it [01:17,  1.84it/s]Extractor Estimating: 135it [01:18,  1.84it/s]Extractor Estimating: 136it [01:18,  1.86it/s]Extractor Estimating: 137it [01:19,  1.85it/s]Extractor Estimating: 138it [01:19,  1.81it/s]Extractor Estimating: 139it [01:20,  1.81it/s]Extractor Estimating: 140it [01:20,  1.87it/s]Extractor Estimating: 141it [01:21,  1.83it/s]Extractor Estimating: 142it [01:21,  1.83it/s]Extractor Estimating: 143it [01:22,  1.82it/s]Extractor Estimating: 144it [01:22,  1.83it/s]Extractor Estimating: 145it [01:23,  1.85it/s]Extractor Estimating: 146it [01:23,  1.85it/s]Extractor Estimating: 147it [01:24,  1.81it/s]Extractor Estimating: 148it [01:25,  1.79it/s]Extractor Estimating: 149it [01:25,  1.80it/s]Extractor Estimating: 150it [01:26,  1.83it/s]Extractor Estimating: 151it [01:26,  1.69it/s]Extractor Estimating: 152it [01:27,  1.73it/s]Extractor Estimating: 153it [01:28,  1.72it/s]Extractor Estimating: 154it [01:28,  1.70it/s]Extractor Estimating: 155it [01:29,  1.65it/s]Extractor Estimating: 156it [01:29,  1.68it/s]Extractor Estimating: 157it [01:30,  1.66it/s]Extractor Estimating: 158it [01:31,  1.67it/s]Extractor Estimating: 159it [01:31,  1.72it/s]Extractor Estimating: 160it [01:32,  1.77it/s]Extractor Estimating: 161it [01:32,  1.75it/s]Extractor Estimating: 162it [01:33,  1.73it/s]Extractor Estimating: 163it [01:33,  1.68it/s]Extractor Estimating: 164it [01:34,  1.67it/s]Extractor Estimating: 165it [01:35,  1.62it/s]Extractor Estimating: 166it [01:35,  1.67it/s]Extractor Estimating: 167it [01:36,  1.66it/s]Extractor Estimating: 168it [01:36,  1.71it/s]Extractor Estimating: 169it [01:37,  1.72it/s]Extractor Estimating: 170it [01:38,  1.63it/s]Extractor Estimating: 171it [01:38,  1.64it/s]Extractor Estimating: 172it [01:39,  1.62it/s]Extractor Estimating: 173it [01:40,  1.64it/s]Extractor Estimating: 174it [01:40,  1.65it/s]Extractor Estimating: 175it [01:41,  1.58it/s]Extractor Estimating: 176it [01:41,  1.66it/s]Extractor Estimating: 177it [01:42,  1.73it/s]Extractor Estimating: 178it [01:42,  1.74it/s]Extractor Estimating: 179it [01:43,  1.71it/s]Extractor Estimating: 180it [01:44,  1.55it/s]Extractor Estimating: 181it [01:44,  1.61it/s]Extractor Estimating: 182it [01:45,  1.69it/s]Extractor Estimating: 183it [01:45,  1.72it/s]Extractor Estimating: 184it [01:46,  1.77it/s]Extractor Estimating: 185it [01:47,  1.74it/s]Extractor Estimating: 186it [01:47,  1.72it/s]Extractor Estimating: 187it [01:48,  1.74it/s]Extractor Estimating: 188it [01:48,  1.71it/s]Extractor Estimating: 189it [01:49,  1.66it/s]Extractor Estimating: 190it [01:50,  1.71it/s]Extractor Estimating: 191it [01:50,  1.75it/s]Extractor Estimating: 192it [01:51,  1.79it/s]Extractor Estimating: 193it [01:51,  1.80it/s]Extractor Estimating: 194it [01:52,  1.80it/s]Extractor Estimating: 195it [01:52,  1.83it/s]Extractor Estimating: 196it [01:53,  1.85it/s]Extractor Estimating: 197it [01:53,  1.76it/s]Extractor Estimating: 198it [01:54,  1.76it/s]Extractor Estimating: 199it [01:55,  1.73it/s]Extractor Estimating: 200it [01:55,  1.73it/s]Extractor Estimating: 201it [01:56,  1.65it/s]Extractor Estimating: 202it [01:57,  1.59it/s]Extractor Estimating: 203it [01:57,  1.56it/s]Extractor Estimating: 204it [01:58,  1.55it/s]Extractor Estimating: 205it [01:58,  1.61it/s]Extractor Estimating: 206it [01:59,  1.57it/s]Extractor Estimating: 207it [02:00,  1.57it/s]Extractor Estimating: 208it [02:00,  1.60it/s]Extractor Estimating: 209it [02:01,  1.64it/s]Extractor Estimating: 210it [02:01,  1.64it/s]Extractor Estimating: 211it [02:02,  1.59it/s]Extractor Estimating: 212it [02:03,  1.58it/s]Extractor Estimating: 213it [02:03,  1.58it/s]Extractor Estimating: 214it [02:04,  1.62it/s]Extractor Estimating: 215it [02:05,  1.60it/s]Extractor Estimating: 216it [02:05,  1.68it/s]Extractor Estimating: 217it [02:06,  1.63it/s]Extractor Estimating: 218it [02:06,  1.64it/s]Extractor Estimating: 219it [02:07,  1.67it/s]Extractor Estimating: 220it [02:08,  1.63it/s]Extractor Estimating: 221it [02:08,  1.61it/s]Extractor Estimating: 222it [02:09,  1.53it/s]Extractor Estimating: 223it [02:10,  1.44it/s]Extractor Estimating: 224it [02:10,  1.51it/s]Extractor Estimating: 225it [02:11,  1.55it/s]Extractor Estimating: 226it [02:12,  1.58it/s]Extractor Estimating: 227it [02:12,  1.73it/s]Extractor Estimating: 228it [02:13,  1.85it/s]Extractor Estimating: 229it [02:13,  1.86it/s]Extractor Estimating: 230it [02:14,  1.92it/s]Extractor Estimating: 231it [02:14,  1.94it/s]Extractor Estimating: 232it [02:15,  1.89it/s]Extractor Estimating: 233it [02:15,  1.96it/s]Extractor Estimating: 234it [02:16,  1.85it/s]Extractor Estimating: 235it [02:16,  1.95it/s]Extractor Estimating: 236it [02:17,  1.81it/s]Extractor Estimating: 237it [02:17,  1.84it/s]Extractor Estimating: 238it [02:18,  1.84it/s]Extractor Estimating: 239it [02:18,  1.89it/s]Extractor Estimating: 240it [02:19,  1.93it/s]Extractor Estimating: 241it [02:19,  1.90it/s]Extractor Estimating: 242it [02:20,  1.77it/s]Extractor Estimating: 243it [02:21,  1.82it/s]Extractor Estimating: 244it [02:21,  1.88it/s]Extractor Estimating: 245it [02:22,  1.86it/s]Extractor Estimating: 246it [02:22,  1.92it/s]Extractor Estimating: 247it [02:23,  1.95it/s]Extractor Estimating: 248it [02:23,  1.90it/s]Extractor Estimating: 249it [02:24,  1.89it/s]Extractor Estimating: 250it [02:24,  1.88it/s]Extractor Estimating: 251it [02:25,  1.80it/s]Extractor Estimating: 252it [02:25,  1.72it/s]Extractor Estimating: 253it [02:26,  1.63it/s]Extractor Estimating: 254it [02:27,  1.67it/s]Extractor Estimating: 255it [02:27,  1.68it/s]Extractor Estimating: 256it [02:28,  1.69it/s]Extractor Estimating: 257it [02:28,  1.70it/s]Extractor Estimating: 258it [02:29,  1.74it/s]Extractor Estimating: 259it [02:30,  1.73it/s]Extractor Estimating: 260it [02:30,  1.75it/s]Extractor Estimating: 261it [02:31,  1.69it/s]Extractor Estimating: 262it [02:31,  1.65it/s]Extractor Estimating: 263it [02:32,  1.66it/s]Extractor Estimating: 264it [02:33,  1.71it/s]Extractor Estimating: 265it [02:33,  1.73it/s]Extractor Estimating: 266it [02:34,  1.71it/s]Extractor Estimating: 267it [02:34,  1.70it/s]Extractor Estimating: 268it [02:35,  1.73it/s]Extractor Estimating: 269it [02:35,  1.70it/s]Extractor Estimating: 270it [02:36,  1.74it/s]Extractor Estimating: 271it [02:37,  1.77it/s]Extractor Estimating: 272it [02:37,  1.79it/s]Extractor Estimating: 273it [02:38,  1.76it/s]Extractor Estimating: 274it [02:38,  1.79it/s]Extractor Estimating: 275it [02:39,  1.73it/s]Extractor Estimating: 276it [02:39,  1.74it/s]Extractor Estimating: 277it [02:40,  1.57it/s]Extractor Estimating: 278it [02:41,  1.55it/s]Extractor Estimating: 279it [02:41,  1.63it/s]Extractor Estimating: 280it [02:42,  1.63it/s]Extractor Estimating: 281it [02:43,  1.68it/s]Extractor Estimating: 282it [02:43,  1.70it/s]Extractor Estimating: 283it [02:44,  1.75it/s]Extractor Estimating: 284it [02:44,  1.79it/s]Extractor Estimating: 285it [02:45,  1.77it/s]Extractor Estimating: 286it [02:46,  1.63it/s]Extractor Estimating: 287it [02:46,  1.66it/s]Extractor Estimating: 288it [02:47,  1.73it/s]Extractor Estimating: 289it [02:47,  1.71it/s]Extractor Estimating: 290it [02:48,  1.75it/s]Extractor Estimating: 291it [02:48,  1.76it/s]Extractor Estimating: 292it [02:49,  1.69it/s]Extractor Estimating: 293it [02:50,  1.68it/s]Extractor Estimating: 294it [02:50,  1.73it/s]Extractor Estimating: 295it [02:51,  1.77it/s]Extractor Estimating: 296it [02:51,  1.85it/s]Extractor Estimating: 297it [02:52,  1.85it/s]Extractor Estimating: 298it [02:52,  1.91it/s]Extractor Estimating: 299it [02:53,  1.88it/s]Extractor Estimating: 300it [02:53,  1.75it/s]Extractor Estimating: 301it [02:54,  1.76it/s]Extractor Estimating: 302it [02:55,  1.74it/s]Extractor Estimating: 303it [02:55,  1.69it/s]Extractor Estimating: 304it [02:56,  1.75it/s]Extractor Estimating: 305it [02:56,  1.79it/s]Extractor Estimating: 306it [02:57,  1.76it/s]Extractor Estimating: 307it [02:57,  1.77it/s]Extractor Estimating: 308it [02:58,  1.80it/s]Extractor Estimating: 309it [02:58,  1.75it/s]Extractor Estimating: 310it [02:59,  1.80it/s]Extractor Estimating: 311it [03:00,  1.78it/s]Extractor Estimating: 312it [03:00,  1.79it/s]Extractor Estimating: 313it [03:01,  1.78it/s]Extractor Estimating: 314it [03:01,  1.80it/s]Extractor Estimating: 315it [03:02,  1.79it/s]Extractor Estimating: 316it [03:02,  1.76it/s]Extractor Estimating: 317it [03:03,  1.79it/s]Extractor Estimating: 318it [03:04,  1.76it/s]Extractor Estimating: 319it [03:04,  1.77it/s]Extractor Estimating: 320it [03:05,  1.70it/s]Extractor Estimating: 321it [03:05,  1.68it/s]Extractor Estimating: 322it [03:06,  1.65it/s]Extractor Estimating: 323it [03:06,  1.75it/s]Extractor Estimating: 324it [03:07,  1.77it/s]Extractor Estimating: 325it [03:08,  1.75it/s]Extractor Estimating: 326it [03:08,  1.81it/s]Extractor Estimating: 327it [03:09,  1.79it/s]Extractor Estimating: 328it [03:09,  1.80it/s]Extractor Estimating: 329it [03:10,  1.83it/s]Extractor Estimating: 330it [03:10,  1.84it/s]Extractor Estimating: 331it [03:11,  1.90it/s]Extractor Estimating: 332it [03:11,  1.81it/s]Extractor Estimating: 333it [03:12,  1.86it/s]Extractor Estimating: 334it [03:12,  1.92it/s]Extractor Estimating: 335it [03:13,  1.93it/s]Extractor Estimating: 336it [03:13,  1.92it/s]Extractor Estimating: 337it [03:14,  1.94it/s]Extractor Estimating: 338it [03:14,  1.90it/s]Extractor Estimating: 339it [03:15,  1.91it/s]Extractor Estimating: 340it [03:16,  1.89it/s]Extractor Estimating: 341it [03:16,  1.82it/s]Extractor Estimating: 342it [03:17,  1.88it/s]Extractor Estimating: 343it [03:17,  1.91it/s]Extractor Estimating: 344it [03:18,  1.88it/s]Extractor Estimating: 345it [03:18,  1.86it/s]Extractor Estimating: 346it [03:19,  1.86it/s]Extractor Estimating: 347it [03:19,  1.87it/s]Extractor Estimating: 348it [03:20,  1.79it/s]Extractor Estimating: 349it [03:20,  1.80it/s]Extractor Estimating: 350it [03:21,  1.83it/s]Extractor Estimating: 351it [03:22,  1.82it/s]Extractor Estimating: 352it [03:22,  1.79it/s]Extractor Estimating: 353it [03:23,  1.71it/s]Extractor Estimating: 354it [03:23,  1.72it/s]Extractor Estimating: 355it [03:24,  1.66it/s]Extractor Estimating: 356it [03:25,  1.65it/s]Extractor Estimating: 357it [03:25,  1.71it/s]Extractor Estimating: 358it [03:26,  1.68it/s]Extractor Estimating: 359it [03:26,  1.69it/s]Extractor Estimating: 360it [03:27,  1.69it/s]Extractor Estimating: 361it [03:28,  1.53it/s]Extractor Estimating: 362it [03:28,  1.58it/s]Extractor Estimating: 363it [03:29,  1.50it/s]Extractor Estimating: 364it [03:30,  1.55it/s]Extractor Estimating: 365it [03:30,  1.58it/s]Extractor Estimating: 366it [03:31,  1.62it/s]Extractor Estimating: 367it [03:31,  1.61it/s]Extractor Estimating: 368it [03:32,  1.63it/s]Extractor Estimating: 369it [03:33,  1.64it/s]Extractor Estimating: 370it [03:33,  1.51it/s]Extractor Estimating: 371it [03:34,  1.54it/s]Extractor Estimating: 372it [03:35,  1.52it/s]Extractor Estimating: 373it [03:35,  1.55it/s]Extractor Estimating: 374it [03:36,  1.58it/s]Extractor Estimating: 375it [03:37,  1.59it/s]Extractor Estimating: 375it [03:37,  1.73it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:44,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:44,969 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:44,970 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:44,970 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:44,970 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:18:45,758 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:18:45,759 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:18:46,434 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:18:47,511 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:18:47,511 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:48,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:48,918 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:48,918 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:48,918 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:18:48,918 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:18:49,251 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:18:49,252 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:18:49,519 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:18:49,688 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:18:49,688 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 19:11:52,919 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 19:11:53,040 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 3000 mean pseudo reward: 0.9876461333372463
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 14537
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14637, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=14637, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.003, loss:470.3054
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 75, avg_time 1.002, loss:359.4614
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 1.011, loss:334.2986
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 25, avg_time 0.999, loss:301.5291
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 125, avg_time 1.002, loss:286.6420
>> valid entity prec:0.5254, rec:0.5865, f1:0.5543
>> valid relation prec:0.2434, rec:0.1943, f1:0.2161
>> valid relation with NER prec:0.2434, rec:0.1943, f1:0.2161
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 1.003, loss:260.2120
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 75, avg_time 1.015, loss:233.7242
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 0.999, loss:239.1054
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 25, avg_time 1.007, loss:240.3489
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 125, avg_time 0.996, loss:240.2598
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5406, rec:0.5529, f1:0.5467
>> valid relation prec:0.2926, rec:0.1894, f1:0.2300
>> valid relation with NER prec:0.2926, rec:0.1894, f1:0.2300
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 100, avg_time 1.001, loss:216.8479
g_step 1200, step 75, avg_time 1.009, loss:193.0860
g_step 1300, step 50, avg_time 1.005, loss:211.3745
g_step 1400, step 25, avg_time 0.993, loss:171.8566
g_step 1500, step 125, avg_time 1.008, loss:180.7474
>> valid entity prec:0.5713, rec:0.5642, f1:0.5677
>> valid relation prec:0.2550, rec:0.1665, f1:0.2015
>> valid relation with NER prec:0.2550, rec:0.1665, f1:0.2015
new max entity f1 on valid!
g_step 1600, step 100, avg_time 1.004, loss:171.7469
g_step 1700, step 75, avg_time 0.996, loss:158.2677
g_step 1800, step 50, avg_time 1.007, loss:146.3549
g_step 1900, step 25, avg_time 1.009, loss:143.0065
g_step 2000, step 125, avg_time 1.000, loss:139.7252
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5477, rec:0.5125, f1:0.5295
>> valid relation prec:0.2600, rec:0.1628, f1:0.2002
>> valid relation with NER prec:0.2600, rec:0.1628, f1:0.2002
g_step 2100, step 100, avg_time 0.999, loss:126.3545
g_step 2200, step 75, avg_time 1.014, loss:137.5160
g_step 2300, step 50, avg_time 0.997, loss:121.1893
g_step 2400, step 25, avg_time 1.017, loss:123.0833
g_step 2500, step 125, avg_time 1.000, loss:130.2483
>> valid entity prec:0.5414, rec:0.5468, f1:0.5441
>> valid relation prec:0.2544, rec:0.1756, f1:0.2078
>> valid relation with NER prec:0.2544, rec:0.1756, f1:0.2078
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:11:53 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:11:53 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-11-52_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:11:54 - WARNING - datasets.builder -   Using custom data configuration default-438c39c68d26ad78
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-438c39c68d26ad78/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:11:54,735 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:11:54,737 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:11:54,737 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:11:54,738 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:11:54,753 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:11:54,756 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:11:54,756 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:11:54,756 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:11:54,756 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:11:54,756 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:11:54,757 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:11:54,870 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:11:58,074 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:11:58,094 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-438c39c68d26ad78/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  3.50ba/s] 67%|██████▋   | 2/3 [00:00<00:00,  4.29ba/s]100%|██████████| 3/3 [00:00<00:00,  4.65ba/s]100%|██████████| 3/3 [00:00<00:00,  4.44ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.19ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.88ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.15ba/s]100%|██████████| 4/4 [00:00<00:00,  5.29ba/s]100%|██████████| 4/4 [00:00<00:00,  4.64ba/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  8.15ba/s]100%|██████████| 3/3 [00:00<00:00,  9.62ba/s]100%|██████████| 3/3 [00:00<00:00,  9.45ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.15ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.65ba/s]100%|██████████| 4/4 [00:00<00:00, 10.94ba/s]
[INFO|trainer.py:414] 2023-08-28 19:12:01,178 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:12:01,204 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:12:01,204 >>   Num examples = 3000
[INFO|trainer.py:1149] 2023-08-28 19:12:01,204 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:12:01,204 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:12:01,204 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:12:01,204 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:12:01,204 >>   Total optimization steps = 235
  0%|          | 0/235 [00:00<?, ?it/s]  0%|          | 1/235 [00:00<01:11,  3.26it/s]  1%|          | 2/235 [00:00<01:08,  3.40it/s]  1%|▏         | 3/235 [00:00<01:07,  3.45it/s]  2%|▏         | 4/235 [00:01<01:06,  3.47it/s]  2%|▏         | 5/235 [00:01<01:05,  3.49it/s]  3%|▎         | 6/235 [00:01<01:05,  3.50it/s]  3%|▎         | 7/235 [00:02<01:05,  3.50it/s]  3%|▎         | 8/235 [00:02<01:04,  3.51it/s]  4%|▍         | 9/235 [00:02<01:04,  3.51it/s]  4%|▍         | 10/235 [00:02<01:03,  3.52it/s]  5%|▍         | 11/235 [00:03<01:03,  3.51it/s]  5%|▌         | 12/235 [00:03<01:03,  3.52it/s]  6%|▌         | 13/235 [00:03<01:03,  3.51it/s]  6%|▌         | 14/235 [00:04<01:02,  3.52it/s]  6%|▋         | 15/235 [00:04<01:02,  3.52it/s]  7%|▋         | 16/235 [00:04<01:02,  3.52it/s]  7%|▋         | 17/235 [00:04<01:01,  3.52it/s]  8%|▊         | 18/235 [00:05<01:01,  3.52it/s]  8%|▊         | 19/235 [00:05<01:01,  3.52it/s]  9%|▊         | 20/235 [00:05<01:01,  3.52it/s]  9%|▉         | 21/235 [00:05<01:00,  3.52it/s]  9%|▉         | 22/235 [00:06<01:00,  3.52it/s] 10%|▉         | 23/235 [00:06<01:00,  3.52it/s] 10%|█         | 24/235 [00:06<00:59,  3.52it/s] 11%|█         | 25/235 [00:07<00:59,  3.52it/s] 11%|█         | 26/235 [00:07<00:59,  3.52it/s] 11%|█▏        | 27/235 [00:07<00:59,  3.52it/s] 12%|█▏        | 28/235 [00:07<00:58,  3.52it/s] 12%|█▏        | 29/235 [00:08<00:58,  3.52it/s] 13%|█▎        | 30/235 [00:08<00:58,  3.52it/s] 13%|█▎        | 31/235 [00:08<00:58,  3.52it/s] 14%|█▎        | 32/235 [00:09<00:57,  3.52it/s] 14%|█▍        | 33/235 [00:09<00:57,  3.52it/s] 14%|█▍        | 34/235 [00:09<00:57,  3.52it/s] 15%|█▍        | 35/235 [00:09<00:56,  3.52it/s] 15%|█▌        | 36/235 [00:10<00:56,  3.52it/s] 16%|█▌        | 37/235 [00:10<00:56,  3.51it/s] 16%|█▌        | 38/235 [00:10<00:56,  3.51it/s] 17%|█▋        | 39/235 [00:11<00:55,  3.51it/s] 17%|█▋        | 40/235 [00:11<00:55,  3.51it/s] 17%|█▋        | 41/235 [00:11<00:55,  3.51it/s] 18%|█▊        | 42/235 [00:11<00:54,  3.51it/s] 18%|█▊        | 43/235 [00:12<00:54,  3.51it/s] 19%|█▊        | 44/235 [00:12<00:54,  3.51it/s] 19%|█▉        | 45/235 [00:12<00:54,  3.51it/s] 20%|█▉        | 46/235 [00:13<00:53,  3.51it/s] 20%|██        | 47/235 [00:13<00:51,  3.64it/s][INFO|trainer.py:2140] 2023-08-28 19:12:14,568 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:12:14,568 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 19:12:14,568 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.57it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.62it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.80it/s][A
  5%|▌         | 24/437 [00:00<00:08, 48.93it/s][A
  7%|▋         | 29/437 [00:00<00:08, 48.62it/s][A
  8%|▊         | 34/437 [00:00<00:08, 48.33it/s][A
  9%|▉         | 39/437 [00:00<00:08, 47.97it/s][A
 10%|█         | 44/437 [00:00<00:08, 47.76it/s][A
 11%|█         | 49/437 [00:01<00:08, 47.68it/s][A
 12%|█▏        | 54/437 [00:01<00:08, 47.57it/s][A
 14%|█▎        | 59/437 [00:01<00:07, 47.71it/s][A
 15%|█▍        | 64/437 [00:01<00:07, 47.78it/s][A
 16%|█▌        | 69/437 [00:01<00:07, 47.85it/s][A
 17%|█▋        | 74/437 [00:01<00:07, 47.90it/s][A
 18%|█▊        | 79/437 [00:01<00:07, 47.75it/s][A
 19%|█▉        | 84/437 [00:01<00:07, 47.61it/s][A
 20%|██        | 89/437 [00:01<00:07, 47.58it/s][A
 22%|██▏       | 94/437 [00:01<00:07, 47.53it/s][A
 23%|██▎       | 99/437 [00:02<00:07, 47.56it/s][A
 24%|██▍       | 104/437 [00:02<00:07, 47.55it/s][A
 25%|██▍       | 109/437 [00:02<00:06, 47.59it/s][A
 26%|██▌       | 114/437 [00:02<00:06, 47.59it/s][A
 27%|██▋       | 119/437 [00:02<00:06, 47.70it/s][A
 28%|██▊       | 124/437 [00:02<00:06, 47.78it/s][A
 30%|██▉       | 129/437 [00:02<00:06, 47.71it/s][A
 31%|███       | 134/437 [00:02<00:06, 47.51it/s][A
 32%|███▏      | 139/437 [00:02<00:06, 47.58it/s][A
 33%|███▎      | 144/437 [00:03<00:06, 47.48it/s][A
 34%|███▍      | 149/437 [00:03<00:06, 47.47it/s][A
 35%|███▌      | 154/437 [00:03<00:05, 47.51it/s][A
 36%|███▋      | 159/437 [00:03<00:05, 47.62it/s][A
 38%|███▊      | 164/437 [00:03<00:05, 47.62it/s][A
 39%|███▊      | 169/437 [00:03<00:05, 47.67it/s][A
 40%|███▉      | 174/437 [00:03<00:05, 47.69it/s][A
 41%|████      | 179/437 [00:03<00:05, 47.62it/s][A
 42%|████▏     | 184/437 [00:03<00:05, 47.62it/s][A
 43%|████▎     | 189/437 [00:03<00:05, 47.47it/s][A
 44%|████▍     | 194/437 [00:04<00:05, 47.50it/s][A
 46%|████▌     | 199/437 [00:04<00:05, 47.56it/s][A
 47%|████▋     | 204/437 [00:04<00:04, 47.61it/s][A
 48%|████▊     | 209/437 [00:04<00:04, 47.50it/s][A
 49%|████▉     | 214/437 [00:04<00:04, 47.47it/s][A
 50%|█████     | 219/437 [00:04<00:04, 47.61it/s][A
 51%|█████▏    | 224/437 [00:04<00:04, 47.64it/s][A
 52%|█████▏    | 229/437 [00:04<00:04, 47.60it/s][A
 54%|█████▎    | 234/437 [00:04<00:04, 47.49it/s][A
 55%|█████▍    | 239/437 [00:04<00:04, 47.47it/s][A
 56%|█████▌    | 244/437 [00:05<00:04, 47.43it/s][A
 57%|█████▋    | 249/437 [00:05<00:03, 47.48it/s][A
 58%|█████▊    | 254/437 [00:05<00:03, 47.54it/s][A
 59%|█████▉    | 259/437 [00:05<00:03, 47.58it/s][A
 60%|██████    | 264/437 [00:05<00:03, 47.58it/s][A
 62%|██████▏   | 269/437 [00:05<00:03, 47.56it/s][A
 63%|██████▎   | 274/437 [00:05<00:03, 47.58it/s][A
 64%|██████▍   | 279/437 [00:05<00:03, 47.42it/s][A
 65%|██████▍   | 284/437 [00:05<00:03, 47.45it/s][A
 66%|██████▌   | 289/437 [00:06<00:03, 47.38it/s][A
 67%|██████▋   | 294/437 [00:06<00:03, 47.52it/s][A
 68%|██████▊   | 299/437 [00:06<00:02, 47.56it/s][A
 70%|██████▉   | 304/437 [00:06<00:02, 47.60it/s][A
 71%|███████   | 309/437 [00:06<00:02, 47.55it/s][A
 72%|███████▏  | 314/437 [00:06<00:02, 47.59it/s][A
 73%|███████▎  | 319/437 [00:06<00:02, 47.57it/s][A
 74%|███████▍  | 324/437 [00:06<00:02, 47.54it/s][A
 75%|███████▌  | 329/437 [00:06<00:02, 47.40it/s][A
 76%|███████▋  | 334/437 [00:06<00:02, 47.42it/s][A
 78%|███████▊  | 339/437 [00:07<00:02, 47.40it/s][A
 79%|███████▊  | 344/437 [00:07<00:01, 47.31it/s][A
 80%|███████▉  | 349/437 [00:07<00:01, 47.54it/s][A
 81%|████████  | 354/437 [00:07<00:01, 47.57it/s][A
 82%|████████▏ | 359/437 [00:07<00:01, 47.51it/s][A
 83%|████████▎ | 364/437 [00:07<00:01, 47.49it/s][A
 84%|████████▍ | 369/437 [00:07<00:01, 47.56it/s][A
 86%|████████▌ | 374/437 [00:07<00:01, 47.54it/s][A
 87%|████████▋ | 379/437 [00:07<00:01, 47.43it/s][A
 88%|████████▊ | 384/437 [00:08<00:01, 47.44it/s][A
 89%|████████▉ | 389/437 [00:08<00:01, 47.45it/s][A
 90%|█████████ | 394/437 [00:08<00:00, 47.44it/s][A
 91%|█████████▏| 399/437 [00:08<00:00, 47.54it/s][A
 92%|█████████▏| 404/437 [00:08<00:00, 47.49it/s][A
 94%|█████████▎| 409/437 [00:08<00:00, 47.54it/s][A
 95%|█████████▍| 414/437 [00:08<00:00, 47.51it/s][A
 96%|█████████▌| 419/437 [00:08<00:00, 47.42it/s][A
 97%|█████████▋| 424/437 [00:08<00:00, 47.48it/s][A
 98%|█████████▊| 429/437 [00:08<00:00, 47.39it/s][A
 99%|█████████▉| 434/437 [00:09<00:00, 47.42it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 47.42it/s][A 20%|██        | 47/235 [00:22<00:51,  3.64it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:12:23,775 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47
[INFO|configuration_utils.py:351] 2023-08-28 19:12:23,827 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:12:28,599 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:12:28,617 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:12:28,625 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47/special_tokens_map.json
 20%|██        | 48/235 [00:34<20:36,  6.61s/it] 21%|██        | 49/235 [00:35<14:38,  4.72s/it] 21%|██▏       | 50/235 [00:35<10:27,  3.39s/it] 22%|██▏       | 51/235 [00:35<07:32,  2.46s/it] 22%|██▏       | 52/235 [00:35<05:30,  1.81s/it] 23%|██▎       | 53/235 [00:36<04:05,  1.35s/it] 23%|██▎       | 54/235 [00:36<03:06,  1.03s/it] 23%|██▎       | 55/235 [00:36<02:25,  1.24it/s] 24%|██▍       | 56/235 [00:37<01:56,  1.54it/s] 24%|██▍       | 57/235 [00:37<01:36,  1.85it/s] 25%|██▍       | 58/235 [00:37<01:22,  2.15it/s] 25%|██▌       | 59/235 [00:37<01:12,  2.44it/s] 26%|██▌       | 60/235 [00:38<01:07,  2.61it/s] 26%|██▌       | 61/235 [00:38<01:01,  2.82it/s] 26%|██▋       | 62/235 [00:38<00:57,  3.00it/s] 27%|██▋       | 63/235 [00:39<00:54,  3.14it/s] 27%|██▋       | 64/235 [00:39<00:52,  3.24it/s] 28%|██▊       | 65/235 [00:39<00:51,  3.32it/s] 28%|██▊       | 66/235 [00:39<00:50,  3.37it/s] 29%|██▊       | 67/235 [00:40<00:49,  3.41it/s] 29%|██▉       | 68/235 [00:40<00:48,  3.44it/s] 29%|██▉       | 69/235 [00:40<00:47,  3.46it/s] 30%|██▉       | 70/235 [00:41<00:47,  3.47it/s] 30%|███       | 71/235 [00:41<00:47,  3.47it/s] 31%|███       | 72/235 [00:41<00:46,  3.48it/s] 31%|███       | 73/235 [00:41<00:46,  3.49it/s] 31%|███▏      | 74/235 [00:42<00:46,  3.49it/s] 32%|███▏      | 75/235 [00:42<00:45,  3.50it/s] 32%|███▏      | 76/235 [00:42<00:45,  3.50it/s] 33%|███▎      | 77/235 [00:43<00:45,  3.50it/s] 33%|███▎      | 78/235 [00:43<00:44,  3.50it/s] 34%|███▎      | 79/235 [00:43<00:44,  3.50it/s] 34%|███▍      | 80/235 [00:43<00:44,  3.50it/s] 34%|███▍      | 81/235 [00:44<00:43,  3.50it/s] 35%|███▍      | 82/235 [00:44<00:43,  3.49it/s] 35%|███▌      | 83/235 [00:44<00:43,  3.49it/s] 36%|███▌      | 84/235 [00:45<00:43,  3.50it/s] 36%|███▌      | 85/235 [00:45<00:42,  3.50it/s] 37%|███▋      | 86/235 [00:45<00:42,  3.50it/s] 37%|███▋      | 87/235 [00:45<00:42,  3.50it/s] 37%|███▋      | 88/235 [00:46<00:41,  3.51it/s] 38%|███▊      | 89/235 [00:46<00:41,  3.50it/s] 38%|███▊      | 90/235 [00:46<00:41,  3.50it/s] 39%|███▊      | 91/235 [00:47<00:41,  3.51it/s] 39%|███▉      | 92/235 [00:47<00:40,  3.51it/s] 40%|███▉      | 93/235 [00:47<00:40,  3.49it/s] 40%|████      | 94/235 [00:47<00:38,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 19:12:49,125 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:12:49,125 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 19:12:49,125 >>   Batch size = 8
{'eval_loss': 1.1020228862762451, 'eval_runtime': 9.1841, 'eval_samples_per_second': 380.221, 'eval_steps_per_second': 47.582, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.78it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.35it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.58it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.86it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.26it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.18it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.95it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.63it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.66it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.67it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.56it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.55it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.60it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.56it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.38it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.44it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.42it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.40it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.46it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.38it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.41it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.43it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.43it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.43it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.37it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.37it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.37it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.45it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.45it/s][A
 35%|███▌      | 153/437 [00:03<00:05, 47.40it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.41it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.44it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.41it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.33it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.37it/s][A
 42%|████▏     | 183/437 [00:04<00:09, 27.76it/s][A
 43%|████▎     | 188/437 [00:04<00:07, 31.80it/s][A
 44%|████▍     | 193/437 [00:04<00:06, 35.33it/s][A
 45%|████▌     | 198/437 [00:04<00:06, 38.26it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 40.66it/s][A
 48%|████▊     | 208/437 [00:04<00:05, 42.45it/s][A
 49%|████▊     | 213/437 [00:04<00:05, 43.90it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 44.87it/s][A
 51%|█████     | 223/437 [00:04<00:04, 45.64it/s][A
 52%|█████▏    | 228/437 [00:05<00:04, 46.17it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.60it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.87it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.02it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.21it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.33it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.33it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.38it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.37it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.36it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 47.37it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 47.45it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.52it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.41it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.43it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.51it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.41it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.42it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.41it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 47.41it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 47.38it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.40it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.24it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.57it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.58it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.42it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.34it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.66it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.94it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 47.05it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.15it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.29it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.43it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.40it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.37it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.39it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.38it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.39it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 47.36it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 47.43it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.41it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.42it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 47.42it/s][A 40%|████      | 94/235 [00:57<00:38,  3.62it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:12:58,629 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 19:12:58,692 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:13:04,160 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:13:04,271 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:13:04,298 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94/special_tokens_map.json
 40%|████      | 95/235 [01:14<18:57,  8.12s/it] 41%|████      | 96/235 [01:14<13:22,  5.77s/it] 41%|████▏     | 97/235 [01:14<09:29,  4.13s/it] 42%|████▏     | 98/235 [01:15<06:47,  2.97s/it] 42%|████▏     | 99/235 [01:15<04:54,  2.17s/it] 43%|████▎     | 100/235 [01:15<03:36,  1.60s/it] 43%|████▎     | 101/235 [01:16<02:41,  1.21s/it] 43%|████▎     | 102/235 [01:16<02:03,  1.07it/s] 44%|████▍     | 103/235 [01:16<01:37,  1.36it/s] 44%|████▍     | 104/235 [01:16<01:18,  1.66it/s] 45%|████▍     | 105/235 [01:17<01:05,  1.97it/s] 45%|████▌     | 106/235 [01:17<00:56,  2.27it/s] 46%|████▌     | 107/235 [01:17<00:50,  2.53it/s] 46%|████▌     | 108/235 [01:18<00:45,  2.76it/s] 46%|████▋     | 109/235 [01:18<00:42,  2.95it/s] 47%|████▋     | 110/235 [01:18<00:40,  3.10it/s] 47%|████▋     | 111/235 [01:18<00:38,  3.21it/s] 48%|████▊     | 112/235 [01:19<00:37,  3.29it/s] 48%|████▊     | 113/235 [01:19<00:36,  3.35it/s] 49%|████▊     | 114/235 [01:19<00:35,  3.40it/s] 49%|████▉     | 115/235 [01:20<00:34,  3.43it/s] 49%|████▉     | 116/235 [01:20<00:34,  3.45it/s] 50%|████▉     | 117/235 [01:20<00:34,  3.47it/s] 50%|█████     | 118/235 [01:20<00:33,  3.45it/s] 51%|█████     | 119/235 [01:21<00:33,  3.47it/s] 51%|█████     | 120/235 [01:21<00:33,  3.48it/s] 51%|█████▏    | 121/235 [01:21<00:32,  3.49it/s] 52%|█████▏    | 122/235 [01:22<00:32,  3.49it/s] 52%|█████▏    | 123/235 [01:22<00:32,  3.49it/s] 53%|█████▎    | 124/235 [01:22<00:31,  3.50it/s] 53%|█████▎    | 125/235 [01:22<00:31,  3.50it/s] 54%|█████▎    | 126/235 [01:23<00:31,  3.50it/s] 54%|█████▍    | 127/235 [01:23<00:30,  3.50it/s] 54%|█████▍    | 128/235 [01:23<00:30,  3.50it/s] 55%|█████▍    | 129/235 [01:24<00:30,  3.50it/s] 55%|█████▌    | 130/235 [01:24<00:29,  3.50it/s] 56%|█████▌    | 131/235 [01:24<00:29,  3.50it/s] 56%|█████▌    | 132/235 [01:24<00:29,  3.50it/s] 57%|█████▋    | 133/235 [01:25<00:29,  3.50it/s] 57%|█████▋    | 134/235 [01:25<00:28,  3.50it/s] 57%|█████▋    | 135/235 [01:25<00:28,  3.50it/s] 58%|█████▊    | 136/235 [01:26<00:28,  3.50it/s] 58%|█████▊    | 137/235 [01:26<00:27,  3.50it/s] 59%|█████▊    | 138/235 [01:26<00:27,  3.49it/s] 59%|█████▉    | 139/235 [01:26<00:27,  3.49it/s] 60%|█████▉    | 140/235 [01:27<00:27,  3.49it/s] 60%|██████    | 141/235 [01:27<00:25,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 19:13:28,675 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:13:28,675 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 19:13:28,675 >>   Batch size = 8
{'eval_loss': 1.1112871170043945, 'eval_runtime': 9.4587, 'eval_samples_per_second': 369.183, 'eval_steps_per_second': 46.201, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.87it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.22it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.53it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.75it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.36it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.08it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.81it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.61it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.49it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.54it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.51it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.38it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.37it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.43it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.38it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.45it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.35it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.37it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.36it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.52it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.41it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.32it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.47it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.50it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.46it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.43it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.50it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.45it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.43it/s][A
 35%|███▌      | 153/437 [00:03<00:05, 47.43it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.43it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.47it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.46it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.48it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.46it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.38it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.40it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.48it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.46it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.42it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.34it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.42it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.48it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.46it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.47it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.46it/s][A
 54%|█████▍    | 238/437 [00:04<00:04, 47.43it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.37it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.46it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.44it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.46it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.35it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.37it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.38it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.42it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.46it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.47it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.42it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.44it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.44it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.44it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.37it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.45it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.39it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.45it/s][A
 76%|███████▌  | 333/437 [00:06<00:02, 47.36it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.45it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.45it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.44it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.43it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.46it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.40it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.39it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.40it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.51it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.49it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 43.65it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 44.72it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 45.57it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.18it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.54it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.92it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.15it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.28it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.92it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.00it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.00it/s][A 60%|██████    | 141/235 [01:36<00:25,  3.62it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:13:37,927 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-141
[INFO|configuration_utils.py:351] 2023-08-28 19:13:37,943 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-141/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:13:42,678 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-141/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:13:42,705 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-141/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:13:42,720 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-141/special_tokens_map.json
 60%|██████    | 142/235 [01:50<11:01,  7.12s/it] 61%|██████    | 143/235 [01:50<07:46,  5.07s/it] 61%|██████▏   | 144/235 [01:51<05:30,  3.63s/it] 62%|██████▏   | 145/235 [01:51<03:56,  2.63s/it] 62%|██████▏   | 146/235 [01:51<02:51,  1.93s/it] 63%|██████▎   | 147/235 [01:52<02:07,  1.44s/it] 63%|██████▎   | 148/235 [01:52<01:35,  1.10s/it] 63%|██████▎   | 149/235 [01:52<01:13,  1.17it/s] 64%|██████▍   | 150/235 [01:52<00:58,  1.46it/s] 64%|██████▍   | 151/235 [01:53<00:47,  1.77it/s] 65%|██████▍   | 152/235 [01:53<00:39,  2.08it/s] 65%|██████▌   | 153/235 [01:53<00:34,  2.37it/s] 66%|██████▌   | 154/235 [01:54<00:31,  2.61it/s] 66%|██████▌   | 155/235 [01:54<00:28,  2.83it/s] 66%|██████▋   | 156/235 [01:54<00:26,  3.00it/s] 67%|██████▋   | 157/235 [01:54<00:24,  3.14it/s] 67%|██████▋   | 158/235 [01:55<00:23,  3.24it/s] 68%|██████▊   | 159/235 [01:55<00:22,  3.31it/s] 68%|██████▊   | 160/235 [01:55<00:22,  3.37it/s] 69%|██████▊   | 161/235 [01:56<00:21,  3.40it/s] 69%|██████▉   | 162/235 [01:56<00:21,  3.43it/s] 69%|██████▉   | 163/235 [01:56<00:20,  3.45it/s] 70%|██████▉   | 164/235 [01:56<00:20,  3.47it/s] 70%|███████   | 165/235 [01:57<00:20,  3.48it/s] 71%|███████   | 166/235 [01:57<00:19,  3.47it/s] 71%|███████   | 167/235 [01:57<00:19,  3.48it/s] 71%|███████▏  | 168/235 [01:58<00:19,  3.49it/s] 72%|███████▏  | 169/235 [01:58<00:18,  3.49it/s] 72%|███████▏  | 170/235 [01:58<00:18,  3.50it/s] 73%|███████▎  | 171/235 [01:58<00:18,  3.50it/s] 73%|███████▎  | 172/235 [01:59<00:18,  3.50it/s] 74%|███████▎  | 173/235 [01:59<00:17,  3.50it/s] 74%|███████▍  | 174/235 [01:59<00:17,  3.50it/s] 74%|███████▍  | 175/235 [02:00<00:17,  3.50it/s] 75%|███████▍  | 176/235 [02:00<00:16,  3.50it/s] 75%|███████▌  | 177/235 [02:00<00:16,  3.48it/s] 76%|███████▌  | 178/235 [02:00<00:16,  3.48it/s] 76%|███████▌  | 179/235 [02:01<00:16,  3.49it/s] 77%|███████▋  | 180/235 [02:01<00:15,  3.49it/s] 77%|███████▋  | 181/235 [02:01<00:15,  3.49it/s] 77%|███████▋  | 182/235 [02:02<00:15,  3.49it/s] 78%|███████▊  | 183/235 [02:02<00:14,  3.50it/s] 78%|███████▊  | 184/235 [02:02<00:14,  3.50it/s] 79%|███████▊  | 185/235 [02:02<00:14,  3.50it/s] 79%|███████▉  | 186/235 [02:03<00:14,  3.50it/s] 80%|███████▉  | 187/235 [02:03<00:13,  3.49it/s] 80%|████████  | 188/235 [02:03<00:13,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 19:14:04,921 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:14:04,921 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 19:14:04,921 >>   Batch size = 8
{'eval_loss': 1.1274254322052002, 'eval_runtime': 9.2389, 'eval_samples_per_second': 377.966, 'eval_steps_per_second': 47.3, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.71it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.29it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.46it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.79it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.33it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.11it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.89it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.64it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.54it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.45it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.53it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.51it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.55it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.48it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.52it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.47it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.45it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.36it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.37it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.41it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.36it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.45it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.42it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.54it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.46it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.38it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.36it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.41it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.30it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.29it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.40it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.46it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.47it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.39it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.33it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.32it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.26it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.29it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.36it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.40it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.33it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.44it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.44it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.35it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.35it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.25it/s][A
 54%|█████▍    | 238/437 [00:04<00:04, 47.20it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.24it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.33it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.36it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.41it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.42it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.37it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.27it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.17it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.28it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.30it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.26it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.27it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.38it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.37it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.39it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.03it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.26it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.28it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.31it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.31it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.30it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.33it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.34it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.38it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.31it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.28it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.28it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.21it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.22it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.29it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.34it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.33it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.38it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.30it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.25it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.25it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.17it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.11it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.21it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.21it/s][A 80%|████████  | 188/235 [02:12<00:13,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:14:14,163 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 19:14:14,184 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:14:23,235 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:14:23,475 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:14:23,583 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-188/special_tokens_map.json
 80%|████████  | 189/235 [02:36<07:37,  9.95s/it] 81%|████████  | 190/235 [02:36<05:17,  7.05s/it] 81%|████████▏ | 191/235 [02:36<03:41,  5.02s/it] 82%|████████▏ | 192/235 [02:37<02:34,  3.60s/it] 82%|████████▏ | 193/235 [02:37<01:49,  2.61s/it] 83%|████████▎ | 194/235 [02:37<01:18,  1.91s/it] 83%|████████▎ | 195/235 [02:37<00:56,  1.42s/it] 83%|████████▎ | 196/235 [02:38<00:42,  1.08s/it] 84%|████████▍ | 197/235 [02:38<00:32,  1.19it/s] 84%|████████▍ | 198/235 [02:38<00:24,  1.48it/s] 85%|████████▍ | 199/235 [02:39<00:20,  1.79it/s] 85%|████████▌ | 200/235 [02:39<00:16,  2.10it/s] 86%|████████▌ | 201/235 [02:39<00:14,  2.38it/s] 86%|████████▌ | 202/235 [02:39<00:12,  2.63it/s] 86%|████████▋ | 203/235 [02:40<00:11,  2.85it/s] 87%|████████▋ | 204/235 [02:40<00:10,  3.02it/s] 87%|████████▋ | 205/235 [02:40<00:09,  3.15it/s] 88%|████████▊ | 206/235 [02:41<00:08,  3.25it/s] 88%|████████▊ | 207/235 [02:41<00:08,  3.32it/s] 89%|████████▊ | 208/235 [02:41<00:08,  3.37it/s] 89%|████████▉ | 209/235 [02:41<00:07,  3.41it/s] 89%|████████▉ | 210/235 [02:42<00:07,  3.44it/s] 90%|████████▉ | 211/235 [02:42<00:06,  3.46it/s] 90%|█████████ | 212/235 [02:42<00:06,  3.43it/s] 91%|█████████ | 213/235 [02:43<00:06,  3.45it/s] 91%|█████████ | 214/235 [02:43<00:06,  3.46it/s] 91%|█████████▏| 215/235 [02:43<00:05,  3.48it/s] 92%|█████████▏| 216/235 [02:43<00:05,  3.48it/s] 92%|█████████▏| 217/235 [02:44<00:05,  3.49it/s] 93%|█████████▎| 218/235 [02:44<00:04,  3.49it/s] 93%|█████████▎| 219/235 [02:44<00:04,  3.50it/s] 94%|█████████▎| 220/235 [02:45<00:04,  3.50it/s] 94%|█████████▍| 221/235 [02:45<00:04,  3.50it/s] 94%|█████████▍| 222/235 [02:45<00:03,  3.50it/s] 95%|█████████▍| 223/235 [02:45<00:03,  3.44it/s] 95%|█████████▌| 224/235 [02:46<00:03,  3.46it/s] 96%|█████████▌| 225/235 [02:46<00:02,  3.47it/s] 96%|█████████▌| 226/235 [02:46<00:02,  3.48it/s] 97%|█████████▋| 227/235 [02:47<00:02,  3.48it/s] 97%|█████████▋| 228/235 [02:47<00:02,  3.49it/s] 97%|█████████▋| 229/235 [02:47<00:01,  3.49it/s] 98%|█████████▊| 230/235 [02:47<00:01,  3.49it/s] 98%|█████████▊| 231/235 [02:48<00:01,  3.50it/s] 99%|█████████▊| 232/235 [02:48<00:00,  3.50it/s] 99%|█████████▉| 233/235 [02:48<00:00,  3.50it/s]100%|█████████▉| 234/235 [02:49<00:00,  3.45it/s]100%|██████████| 235/235 [02:49<00:00,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 19:14:50,600 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:14:50,601 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 19:14:50,601 >>   Batch size = 8
{'eval_loss': 1.1373138427734375, 'eval_runtime': 9.224, 'eval_samples_per_second': 378.577, 'eval_steps_per_second': 47.376, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.22it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.33it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.47it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.77it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.36it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.09it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.80it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.49it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.55it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.48it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.46it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.82it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.98it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.20it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.21it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.21it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.30it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.25it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.19it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.26it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.35it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.26it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.32it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 45.27it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.22it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.62it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.81it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.79it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.00it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.12it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.12it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.28it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.15it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.15it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.32it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.39it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.23it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.17it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.30it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.32it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.31it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.32it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.31it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.32it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.32it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.33it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.33it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.34it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.32it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.29it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.30it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.24it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.40it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.75it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.96it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.09it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.19it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.15it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.27it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.25it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.81it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.23it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.28it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.28it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.20it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.36it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.31it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.30it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.35it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.25it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.18it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.28it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.39it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.30it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.32it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.34it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.43it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.46it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.27it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.28it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.27it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.26it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.35it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.34it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.39it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 41.13it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 41.13it/s][A100%|██████████| 235/235 [02:58<00:00,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:14:59,954 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-235
[INFO|configuration_utils.py:351] 2023-08-28 19:15:00,035 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-235/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:15:06,178 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-235/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:15:06,225 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-235/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:15:06,260 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-235/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 19:15:15,091 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 19:15:15,108 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47 (score: 1.1020228862762451).
                                                 100%|██████████| 235/235 [03:18<00:00,  3.59it/s]100%|██████████| 235/235 [03:18<00:00,  1.18it/s]
[INFO|trainer.py:1894] 2023-08-28 19:15:19,575 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 19:15:19,584 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:15:26,384 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:15:26,410 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:15:26,426 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:15:26,643 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:26,643 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:26,643 >>   train_loss               =     0.4465
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:26,643 >>   train_runtime            = 0:03:18.35
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:26,644 >>   train_samples            =       3000
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:26,644 >>   train_samples_per_second =      75.62
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:26,644 >>   train_steps_per_second   =      1.185
{'eval_loss': 1.142798662185669, 'eval_runtime': 9.307, 'eval_samples_per_second': 375.203, 'eval_steps_per_second': 46.954, 'epoch': 5.0}
{'train_runtime': 198.3596, 'train_samples_per_second': 75.62, 'train_steps_per_second': 1.185, 'train_loss': 0.44648762155086436, 'epoch': 5.0}
08/28/2023 19:15:26 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 19:15:26,681 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:15:26,681 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 19:15:26,681 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.92it/s]  3%|▎         | 12/437 [00:00<00:08, 52.04it/s]  4%|▍         | 18/437 [00:00<00:08, 50.10it/s]  5%|▌         | 24/437 [00:00<00:08, 49.13it/s]  7%|▋         | 29/437 [00:00<00:08, 48.69it/s]  8%|▊         | 34/437 [00:00<00:08, 48.43it/s]  9%|▉         | 39/437 [00:00<00:08, 46.98it/s] 10%|█         | 44/437 [00:00<00:08, 47.25it/s] 11%|█         | 49/437 [00:01<00:08, 47.32it/s] 12%|█▏        | 54/437 [00:01<00:08, 47.39it/s] 14%|█▎        | 59/437 [00:01<00:07, 47.63it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.68it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.63it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.65it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.54it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.53it/s] 20%|██        | 89/437 [00:01<00:07, 47.55it/s] 22%|██▏       | 94/437 [00:01<00:07, 47.49it/s] 23%|██▎       | 99/437 [00:02<00:07, 47.49it/s] 24%|██▍       | 104/437 [00:02<00:07, 47.55it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.55it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.62it/s] 27%|██▋       | 119/437 [00:02<00:06, 47.64it/s] 28%|██▊       | 124/437 [00:02<00:06, 47.52it/s] 30%|██▉       | 129/437 [00:02<00:06, 47.60it/s] 31%|███       | 134/437 [00:02<00:06, 47.59it/s] 32%|███▏      | 139/437 [00:02<00:06, 47.55it/s] 33%|███▎      | 144/437 [00:03<00:06, 47.53it/s] 34%|███▍      | 149/437 [00:03<00:06, 47.37it/s] 35%|███▌      | 154/437 [00:03<00:05, 47.49it/s] 36%|███▋      | 159/437 [00:03<00:05, 47.64it/s] 38%|███▊      | 164/437 [00:03<00:05, 47.55it/s] 39%|███▊      | 169/437 [00:03<00:05, 47.62it/s] 40%|███▉      | 174/437 [00:03<00:05, 47.63it/s] 41%|████      | 179/437 [00:03<00:05, 47.61it/s] 42%|████▏     | 184/437 [00:03<00:05, 47.33it/s] 43%|████▎     | 189/437 [00:03<00:05, 47.34it/s] 44%|████▍     | 194/437 [00:04<00:05, 47.38it/s] 46%|████▌     | 199/437 [00:04<00:05, 47.41it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.50it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.49it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.57it/s] 50%|█████     | 219/437 [00:04<00:04, 47.60it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.56it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.49it/s] 54%|█████▎    | 234/437 [00:04<00:04, 47.51it/s] 55%|█████▍    | 239/437 [00:05<00:04, 47.46it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.38it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.38it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.50it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.55it/s] 60%|██████    | 264/437 [00:05<00:03, 47.61it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.57it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.59it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.46it/s] 65%|██████▍   | 284/437 [00:05<00:03, 47.43it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.50it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.40it/s] 68%|██████▊   | 299/437 [00:06<00:02, 47.40it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.44it/s] 71%|███████   | 309/437 [00:06<00:02, 47.44it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.49it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.48it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.49it/s] 75%|███████▌  | 329/437 [00:06<00:02, 46.11it/s] 76%|███████▋  | 334/437 [00:07<00:02, 46.64it/s] 78%|███████▊  | 339/437 [00:07<00:02, 46.93it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.06it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.13it/s] 81%|████████  | 354/437 [00:07<00:01, 47.30it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.29it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.41it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.31it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.40it/s] 87%|████████▋ | 379/437 [00:07<00:01, 47.37it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.43it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.46it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.34it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.22it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.30it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.39it/s] 95%|█████████▍| 414/437 [00:08<00:00, 47.35it/s] 96%|█████████▌| 419/437 [00:08<00:00, 47.35it/s] 97%|█████████▋| 424/437 [00:08<00:00, 47.34it/s] 98%|█████████▊| 429/437 [00:09<00:00, 47.40it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.41it/s]100%|██████████| 437/437 [00:09<00:00, 47.54it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:15:35,895 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:35,895 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:35,895 >>   eval_loss               =      1.102
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:35,895 >>   eval_runtime            = 0:00:09.21
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:35,895 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:35,895 >>   eval_samples_per_second =    378.996
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:35,896 >>   eval_steps_per_second   =     47.429
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:15:35,896 >>   perplexity              =     3.0102
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:15:43,206 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:15:43,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:15:43,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:15:43,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:15:43,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:15:43,926 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:15:43,927 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:15:44,514 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:15:45,628 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:15:45,628 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:15:48,489 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:15:48,498 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:15:48,499 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:15:48,499 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:15:48,499 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:15:49,170 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:15:49,171 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:15:49,812 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:15:49,981 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:15:49,981 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-141
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-235
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.50it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:11,  1.48it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.48it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:22,  1.48it/s]Extractor Predicting: 36it [00:23,  1.50it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.41it/s]Extractor Predicting: 39it [00:25,  1.45it/s]Extractor Predicting: 40it [00:26,  1.48it/s]Extractor Predicting: 41it [00:27,  1.50it/s]Extractor Predicting: 42it [00:27,  1.49it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:29,  1.52it/s]Extractor Predicting: 45it [00:29,  1.50it/s]Extractor Predicting: 46it [00:30,  1.50it/s]Extractor Predicting: 47it [00:31,  1.51it/s]Extractor Predicting: 48it [00:31,  1.51it/s]Extractor Predicting: 49it [00:32,  1.52it/s]Extractor Predicting: 50it [00:32,  1.51it/s]Extractor Predicting: 51it [00:33,  1.48it/s]Extractor Predicting: 52it [00:34,  1.47it/s]Extractor Predicting: 53it [00:35,  1.48it/s]Extractor Predicting: 54it [00:35,  1.49it/s]Extractor Predicting: 55it [00:36,  1.49it/s]Extractor Predicting: 56it [00:37,  1.49it/s]Extractor Predicting: 57it [00:37,  1.50it/s]Extractor Predicting: 58it [00:38,  1.52it/s]Extractor Predicting: 59it [00:39,  1.51it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.47it/s]Extractor Predicting: 62it [00:41,  1.47it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:42,  1.46it/s]Extractor Predicting: 65it [00:43,  1.46it/s]Extractor Predicting: 66it [00:43,  1.45it/s]Extractor Predicting: 67it [00:44,  1.44it/s]Extractor Predicting: 68it [00:45,  1.43it/s]Extractor Predicting: 69it [00:45,  1.44it/s]Extractor Predicting: 70it [00:46,  1.43it/s]Extractor Predicting: 71it [00:47,  1.43it/s]Extractor Predicting: 72it [00:48,  1.41it/s]Extractor Predicting: 73it [00:48,  1.42it/s]Extractor Predicting: 74it [00:49,  1.40it/s]Extractor Predicting: 75it [00:50,  1.44it/s]Extractor Predicting: 76it [00:50,  1.44it/s]Extractor Predicting: 77it [00:51,  1.42it/s]Extractor Predicting: 78it [00:52,  1.45it/s]Extractor Predicting: 79it [00:52,  1.42it/s]Extractor Predicting: 80it [00:53,  1.41it/s]Extractor Predicting: 81it [00:54,  1.41it/s]Extractor Predicting: 82it [00:55,  1.44it/s]Extractor Predicting: 83it [00:55,  1.45it/s]Extractor Predicting: 84it [00:56,  1.47it/s]Extractor Predicting: 85it [00:57,  1.46it/s]Extractor Predicting: 86it [00:57,  1.43it/s]Extractor Predicting: 87it [00:58,  1.44it/s]Extractor Predicting: 88it [00:59,  1.48it/s]Extractor Predicting: 89it [00:59,  1.54it/s]Extractor Predicting: 90it [01:00,  1.53it/s]Extractor Predicting: 91it [01:00,  1.59it/s]Extractor Predicting: 92it [01:01,  1.49it/s]Extractor Predicting: 93it [01:02,  1.56it/s]Extractor Predicting: 94it [01:02,  1.62it/s]Extractor Predicting: 95it [01:03,  1.59it/s]Extractor Predicting: 96it [01:04,  1.63it/s]Extractor Predicting: 97it [01:04,  1.59it/s]Extractor Predicting: 98it [01:05,  1.59it/s]Extractor Predicting: 99it [01:05,  1.65it/s]Extractor Predicting: 100it [01:06,  1.66it/s]Extractor Predicting: 101it [01:07,  1.63it/s]Extractor Predicting: 102it [01:07,  1.54it/s]Extractor Predicting: 103it [01:08,  1.56it/s]Extractor Predicting: 104it [01:09,  1.53it/s]Extractor Predicting: 105it [01:09,  1.54it/s]Extractor Predicting: 106it [01:10,  1.57it/s]Extractor Predicting: 107it [01:11,  1.54it/s]Extractor Predicting: 108it [01:11,  1.57it/s]Extractor Predicting: 109it [01:12,  1.59it/s]Extractor Predicting: 110it [01:12,  1.61it/s]Extractor Predicting: 111it [01:13,  1.63it/s]Extractor Predicting: 112it [01:14,  1.63it/s]Extractor Predicting: 113it [01:14,  1.65it/s]Extractor Predicting: 114it [01:15,  1.61it/s]Extractor Predicting: 115it [01:16,  1.64it/s]Extractor Predicting: 116it [01:16,  1.60it/s]Extractor Predicting: 117it [01:17,  1.45it/s]Extractor Predicting: 118it [01:18,  1.47it/s]Extractor Predicting: 119it [01:18,  1.48it/s]Extractor Predicting: 120it [01:19,  1.47it/s]Extractor Predicting: 121it [01:20,  1.48it/s]Extractor Predicting: 122it [01:20,  1.49it/s]Extractor Predicting: 123it [01:21,  1.48it/s]Extractor Predicting: 124it [01:22,  1.46it/s]Extractor Predicting: 125it [01:22,  1.47it/s]Extractor Predicting: 126it [01:23,  1.45it/s]Extractor Predicting: 127it [01:24,  1.46it/s]Extractor Predicting: 128it [01:24,  1.46it/s]Extractor Predicting: 129it [01:25,  1.48it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:26,  1.48it/s]Extractor Predicting: 132it [01:27,  1.48it/s]Extractor Predicting: 133it [01:28,  1.46it/s]Extractor Predicting: 134it [01:29,  1.46it/s]Extractor Predicting: 135it [01:29,  1.48it/s]Extractor Predicting: 136it [01:30,  1.46it/s]Extractor Predicting: 137it [01:31,  1.47it/s]Extractor Predicting: 138it [01:31,  1.49it/s]Extractor Predicting: 139it [01:32,  1.49it/s]Extractor Predicting: 140it [01:33,  1.52it/s]Extractor Predicting: 141it [01:33,  1.49it/s]Extractor Predicting: 142it [01:34,  1.48it/s]Extractor Predicting: 143it [01:35,  1.49it/s]Extractor Predicting: 144it [01:35,  1.53it/s]Extractor Predicting: 144it [01:35,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:36,112 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:36,119 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:36,119 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:36,119 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:36,119 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:17:36,854 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:17:36,855 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:17:37,470 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:17:38,513 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:17:38,513 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:41,534 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:41,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:41,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:41,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:17:41,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:17:42,273 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:17:42,274 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:17:42,839 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:17:43,004 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:17:43,004 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3931870669745958,
  "recall": 0.1950171821305842,
  "score": 0.2607197549770291,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:04,  1.50it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:06,  1.51it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:11,  1.54it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.51it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.50it/s]Extractor Predicting: 30it [00:19,  1.47it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:21,  1.47it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:23,  1.52it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.59it/s]Extractor Predicting: 38it [00:24,  1.59it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:26,  1.58it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:27,  1.56it/s]Extractor Predicting: 43it [00:28,  1.56it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:29,  1.54it/s]Extractor Predicting: 46it [00:30,  1.56it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:31,  1.57it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:32,  1.54it/s]Extractor Predicting: 51it [00:33,  1.55it/s]Extractor Predicting: 52it [00:33,  1.55it/s]Extractor Predicting: 53it [00:34,  1.55it/s]Extractor Predicting: 54it [00:35,  1.55it/s]Extractor Predicting: 55it [00:35,  1.53it/s]Extractor Predicting: 56it [00:36,  1.54it/s]Extractor Predicting: 57it [00:37,  1.57it/s]Extractor Predicting: 58it [00:37,  1.60it/s]Extractor Predicting: 59it [00:38,  1.63it/s]Extractor Predicting: 60it [00:38,  1.61it/s]Extractor Predicting: 61it [00:39,  1.59it/s]Extractor Predicting: 62it [00:40,  1.59it/s]Extractor Predicting: 63it [00:40,  1.60it/s]Extractor Predicting: 64it [00:41,  1.55it/s]Extractor Predicting: 65it [00:42,  1.56it/s]Extractor Predicting: 66it [00:42,  1.54it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:44,  1.61it/s]Extractor Predicting: 69it [00:44,  1.59it/s]Extractor Predicting: 70it [00:45,  1.59it/s]Extractor Predicting: 71it [00:45,  1.57it/s]Extractor Predicting: 72it [00:46,  1.57it/s]Extractor Predicting: 73it [00:47,  1.54it/s]Extractor Predicting: 74it [00:47,  1.54it/s]Extractor Predicting: 75it [00:48,  1.55it/s]Extractor Predicting: 76it [00:49,  1.43it/s]Extractor Predicting: 77it [00:50,  1.44it/s]Extractor Predicting: 78it [00:50,  1.47it/s]Extractor Predicting: 79it [00:51,  1.49it/s]Extractor Predicting: 80it [00:52,  1.51it/s]Extractor Predicting: 81it [00:52,  1.50it/s]Extractor Predicting: 82it [00:53,  1.50it/s]Extractor Predicting: 83it [00:54,  1.51it/s]Extractor Predicting: 84it [00:54,  1.52it/s]Extractor Predicting: 85it [00:55,  1.55it/s]Extractor Predicting: 86it [00:55,  1.53it/s]Extractor Predicting: 87it [00:56,  1.53it/s]Extractor Predicting: 88it [00:57,  1.53it/s]Extractor Predicting: 89it [00:57,  1.55it/s]Extractor Predicting: 90it [00:58,  1.55it/s]Extractor Predicting: 91it [00:59,  1.54it/s]Extractor Predicting: 92it [00:59,  1.51it/s]Extractor Predicting: 93it [01:00,  1.54it/s]Extractor Predicting: 94it [01:01,  1.52it/s]Extractor Predicting: 95it [01:01,  1.52it/s]Extractor Predicting: 96it [01:02,  1.53it/s]Extractor Predicting: 97it [01:03,  1.54it/s]Extractor Predicting: 98it [01:03,  1.54it/s]Extractor Predicting: 99it [01:04,  1.55it/s]Extractor Predicting: 100it [01:05,  1.57it/s]Extractor Predicting: 101it [01:05,  1.58it/s]Extractor Predicting: 102it [01:06,  1.58it/s]Extractor Predicting: 103it [01:07,  1.51it/s]Extractor Predicting: 104it [01:07,  1.50it/s]Extractor Predicting: 105it [01:08,  1.50it/s]Extractor Predicting: 106it [01:09,  1.51it/s]Extractor Predicting: 107it [01:09,  1.50it/s]Extractor Predicting: 108it [01:10,  1.52it/s]Extractor Predicting: 109it [01:10,  1.51it/s]Extractor Predicting: 110it [01:11,  1.52it/s]Extractor Predicting: 111it [01:12,  1.50it/s]Extractor Predicting: 112it [01:13,  1.49it/s]Extractor Predicting: 113it [01:13,  1.45it/s]Extractor Predicting: 114it [01:14,  1.46it/s]Extractor Predicting: 115it [01:15,  1.47it/s]Extractor Predicting: 116it [01:15,  1.53it/s]Extractor Predicting: 117it [01:16,  1.54it/s]Extractor Predicting: 118it [01:16,  1.54it/s]Extractor Predicting: 119it [01:17,  1.54it/s]Extractor Predicting: 120it [01:18,  1.54it/s]Extractor Predicting: 121it [01:18,  1.56it/s]Extractor Predicting: 122it [01:19,  1.57it/s]Extractor Predicting: 123it [01:20,  1.60it/s]Extractor Predicting: 124it [01:20,  1.60it/s]Extractor Predicting: 125it [01:21,  1.59it/s]Extractor Predicting: 126it [01:22,  1.59it/s]Extractor Predicting: 127it [01:22,  1.60it/s]Extractor Predicting: 128it [01:23,  1.58it/s]Extractor Predicting: 129it [01:23,  1.55it/s]Extractor Predicting: 130it [01:24,  1.51it/s]Extractor Predicting: 131it [01:25,  1.55it/s]Extractor Predicting: 132it [01:25,  1.56it/s]Extractor Predicting: 133it [01:26,  1.57it/s]Extractor Predicting: 134it [01:27,  1.59it/s]Extractor Predicting: 135it [01:27,  1.55it/s]Extractor Predicting: 136it [01:28,  1.56it/s]Extractor Predicting: 137it [01:29,  1.57it/s]Extractor Predicting: 138it [01:29,  1.54it/s]Extractor Predicting: 139it [01:30,  1.56it/s]Extractor Predicting: 140it [01:30,  1.61it/s]Extractor Predicting: 141it [01:31,  1.62it/s]Extractor Predicting: 142it [01:32,  1.59it/s]Extractor Predicting: 143it [01:32,  1.56it/s]Extractor Predicting: 144it [01:33,  1.56it/s]Extractor Predicting: 145it [01:34,  1.56it/s]Extractor Predicting: 146it [01:34,  1.59it/s]Extractor Predicting: 147it [01:35,  1.60it/s]Extractor Predicting: 148it [01:36,  1.56it/s]Extractor Predicting: 149it [01:36,  1.57it/s]Extractor Predicting: 150it [01:37,  1.57it/s]Extractor Predicting: 151it [01:37,  1.56it/s]Extractor Predicting: 152it [01:38,  1.57it/s]Extractor Predicting: 153it [01:39,  1.40it/s]Extractor Predicting: 154it [01:40,  1.46it/s]Extractor Predicting: 155it [01:40,  1.49it/s]Extractor Predicting: 156it [01:41,  1.54it/s]Extractor Predicting: 157it [01:41,  1.56it/s]Extractor Predicting: 158it [01:42,  1.63it/s]Extractor Predicting: 159it [01:43,  1.60it/s]Extractor Predicting: 160it [01:43,  1.57it/s]Extractor Predicting: 161it [01:44,  1.55it/s]Extractor Predicting: 162it [01:45,  1.54it/s]Extractor Predicting: 163it [01:45,  1.56it/s]Extractor Predicting: 164it [01:46,  1.57it/s]Extractor Predicting: 165it [01:47,  1.60it/s]Extractor Predicting: 166it [01:47,  1.62it/s]Extractor Predicting: 167it [01:48,  1.60it/s]Extractor Predicting: 168it [01:48,  1.56it/s]Extractor Predicting: 169it [01:49,  1.56it/s]Extractor Predicting: 170it [01:50,  1.56it/s]Extractor Predicting: 171it [01:50,  1.57it/s]Extractor Predicting: 172it [01:51,  1.57it/s]Extractor Predicting: 173it [01:52,  1.56it/s]Extractor Predicting: 174it [01:52,  1.56it/s]Extractor Predicting: 175it [01:53,  1.53it/s]Extractor Predicting: 176it [01:54,  1.50it/s]Extractor Predicting: 177it [01:54,  1.55it/s]Extractor Predicting: 178it [01:55,  1.54it/s]Extractor Predicting: 179it [01:56,  1.57it/s]Extractor Predicting: 180it [01:56,  1.59it/s]Extractor Predicting: 181it [01:57,  1.56it/s]Extractor Predicting: 182it [01:57,  1.56it/s]Extractor Predicting: 183it [01:58,  1.55it/s]Extractor Predicting: 184it [01:59,  1.54it/s]Extractor Predicting: 185it [01:59,  1.53it/s]Extractor Predicting: 186it [02:00,  1.53it/s]Extractor Predicting: 187it [02:01,  1.54it/s]Extractor Predicting: 188it [02:01,  1.51it/s]Extractor Predicting: 189it [02:02,  1.50it/s]Extractor Predicting: 190it [02:03,  1.51it/s]Extractor Predicting: 191it [02:03,  1.55it/s]Extractor Predicting: 192it [02:04,  1.58it/s]Extractor Predicting: 193it [02:05,  1.46it/s]Extractor Predicting: 194it [02:05,  1.46it/s]Extractor Predicting: 195it [02:06,  1.51it/s]Extractor Predicting: 196it [02:07,  1.52it/s]Extractor Predicting: 197it [02:07,  1.54it/s]Extractor Predicting: 198it [02:08,  1.51it/s]Extractor Predicting: 199it [02:09,  1.55it/s]Extractor Predicting: 200it [02:09,  1.60it/s]Extractor Predicting: 201it [02:10,  1.63it/s]Extractor Predicting: 202it [02:10,  1.62it/s]Extractor Predicting: 203it [02:11,  1.59it/s]Extractor Predicting: 204it [02:12,  1.58it/s]Extractor Predicting: 205it [02:12,  1.59it/s]Extractor Predicting: 206it [02:13,  1.57it/s]Extractor Predicting: 207it [02:14,  1.58it/s]Extractor Predicting: 208it [02:14,  1.54it/s]Extractor Predicting: 209it [02:15,  1.56it/s]Extractor Predicting: 210it [02:16,  1.55it/s]Extractor Predicting: 211it [02:16,  1.55it/s]Extractor Predicting: 212it [02:17,  1.57it/s]Extractor Predicting: 213it [02:17,  1.54it/s]Extractor Predicting: 214it [02:18,  1.52it/s]Extractor Predicting: 215it [02:19,  1.53it/s]Extractor Predicting: 216it [02:19,  1.56it/s]Extractor Predicting: 217it [02:20,  1.58it/s]Extractor Predicting: 218it [02:21,  1.56it/s]Extractor Predicting: 219it [02:21,  1.54it/s]Extractor Predicting: 220it [02:22,  1.55it/s]Extractor Predicting: 221it [02:23,  1.56it/s]Extractor Predicting: 222it [02:23,  1.55it/s]Extractor Predicting: 223it [02:24,  1.56it/s]Extractor Predicting: 224it [02:25,  1.56it/s]Extractor Predicting: 225it [02:25,  1.54it/s]Extractor Predicting: 226it [02:26,  1.53it/s]Extractor Predicting: 227it [02:27,  1.54it/s]Extractor Predicting: 228it [02:27,  1.55it/s]Extractor Predicting: 229it [02:28,  1.57it/s]Extractor Predicting: 230it [02:28,  1.58it/s]Extractor Predicting: 231it [02:29,  1.55it/s]Extractor Predicting: 232it [02:30,  1.55it/s]Extractor Predicting: 233it [02:30,  1.55it/s]Extractor Predicting: 234it [02:31,  1.57it/s]Extractor Predicting: 235it [02:32,  1.58it/s]Extractor Predicting: 236it [02:32,  1.58it/s]Extractor Predicting: 237it [02:33,  1.57it/s]Extractor Predicting: 238it [02:34,  1.56it/s]Extractor Predicting: 239it [02:34,  1.55it/s]Extractor Predicting: 240it [02:35,  1.58it/s]Extractor Predicting: 241it [02:35,  1.58it/s]Extractor Predicting: 242it [02:36,  1.60it/s]Extractor Predicting: 243it [02:37,  1.59it/s]Extractor Predicting: 244it [02:37,  1.59it/s]Extractor Predicting: 245it [02:38,  1.58it/s]Extractor Predicting: 246it [02:39,  1.57it/s]Extractor Predicting: 247it [02:39,  1.59it/s]Extractor Predicting: 248it [02:40,  1.58it/s]Extractor Predicting: 249it [02:40,  1.59it/s]Extractor Predicting: 250it [02:41,  1.60it/s]Extractor Predicting: 251it [02:42,  1.56it/s]Extractor Predicting: 252it [02:42,  1.55it/s]Extractor Predicting: 253it [02:43,  1.61it/s]Extractor Predicting: 254it [02:44,  1.59it/s]Extractor Predicting: 255it [02:44,  1.56it/s]Extractor Predicting: 256it [02:45,  1.56it/s]Extractor Predicting: 257it [02:46,  1.57it/s]Extractor Predicting: 258it [02:46,  1.60it/s]Extractor Predicting: 259it [02:47,  1.61it/s]Extractor Predicting: 260it [02:47,  1.61it/s]Extractor Predicting: 261it [02:48,  1.42it/s]Extractor Predicting: 262it [02:49,  1.47it/s]Extractor Predicting: 263it [02:50,  1.50it/s]Extractor Predicting: 264it [02:50,  1.54it/s]Extractor Predicting: 265it [02:51,  1.56it/s]Extractor Predicting: 266it [02:51,  1.61it/s]Extractor Predicting: 267it [02:52,  1.57it/s]Extractor Predicting: 268it [02:53,  1.57it/s]Extractor Predicting: 269it [02:53,  1.58it/s]Extractor Predicting: 270it [02:54,  1.58it/s]Extractor Predicting: 271it [02:55,  1.62it/s]Extractor Predicting: 272it [02:55,  1.65it/s]Extractor Predicting: 273it [02:56,  1.62it/s]Extractor Predicting: 274it [02:56,  1.58it/s]Extractor Predicting: 275it [02:57,  1.58it/s]Extractor Predicting: 276it [02:58,  1.58it/s]Extractor Predicting: 277it [02:58,  1.59it/s]Extractor Predicting: 278it [02:59,  1.59it/s]Extractor Predicting: 279it [03:00,  1.57it/s]Extractor Predicting: 280it [03:00,  1.58it/s]Extractor Predicting: 281it [03:01,  1.56it/s]Extractor Predicting: 281it [03:01,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:20:53,105 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:20:53,111 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:20:53,111 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:20:53,111 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:20:53,111 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:20:53,730 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:20:53,731 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:20:54,349 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:20:55,451 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:20:55,451 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:20:58,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:20:58,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:20:58,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:20:58,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:20:58,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:20:59,044 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:20:59,045 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:20:59,633 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:20:59,821 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:20:59,821 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4569757727652464,
  "recall": 0.3244846507489248,
  "score": 0.379498742520163,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:02,  1.35it/s]Extractor Predicting: 5it [00:03,  1.38it/s]Extractor Predicting: 6it [00:03,  1.83it/s]Extractor Predicting: 6it [00:03,  1.55it/s]
[INFO|configuration_utils.py:515] 2023-08-28 19:21:04,689 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:21:04,690 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:21:04,703 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:21:04,704 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 19:21:04,708 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:21:12,397 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 19:21:12,404 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 19:21:12,434 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:21:12,434 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:21:12,472 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:21:12,487 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:21:12,487 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:21:12,488 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:21:12,488 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:21:12,488 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:21:12,488 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.29411764705882354,
  "recall": 0.09727626459143969,
  "score": 0.14619883040935674,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 19:21:12,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:13,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:14,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:14,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:15,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:16,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:16,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:17,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:18,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:18,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:19,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:20,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:21,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:21,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:22,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:23,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:23,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:24,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:25,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:26,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:16, 14.05s/it][WARNING|generation_utils.py:914] 2023-08-28 19:21:26,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:27,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:28,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:28,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:29,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:30,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:31,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:32,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:33,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:34,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:34,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:35,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:36,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:37,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:37,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:38,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:39,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:39,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:40,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:41,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:29<03:14, 14.95s/it][WARNING|generation_utils.py:914] 2023-08-28 19:21:42,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:43,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:43,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:44,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:45,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:45,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:46,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:47,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:47,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:48,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:49,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:49,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:50,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:51,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:51,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:52,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:53,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:54,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:54,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:55,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:56,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:44<02:57, 14.76s/it][WARNING|generation_utils.py:914] 2023-08-28 19:21:56,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:57,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:58,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:58,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:59,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:21:59,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:00,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:01,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:02,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:02,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:03,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:03,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:04,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:05,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:05,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:06,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:07,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:07,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:08,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:08,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:09,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:57<02:34, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-28 19:22:09,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:10,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:11,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:11,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:12,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:13,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:13,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:14,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:15,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:15,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:16,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:17,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:17,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:18,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:19,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:20,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:21,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:21,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:22,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:23,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:23,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:11<02:22, 14.24s/it][WARNING|generation_utils.py:914] 2023-08-28 19:22:24,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:25,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:25,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:26,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:26,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:27,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:28,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:28,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:29,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:30,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:31,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:32,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:32,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:33,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:34,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:34,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:35,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:36,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:36,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:37,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:37,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:38,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:39,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:27<02:11, 14.64s/it][WARNING|generation_utils.py:914] 2023-08-28 19:22:39,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:40,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:41,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:41,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:42,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:43,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:43,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:44,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:45,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:46,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:46,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:47,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:47,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:48,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:49,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:49,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:50,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:51,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:51,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:52,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:53,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:53,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:54,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:42<01:59, 14.93s/it][WARNING|generation_utils.py:914] 2023-08-28 19:22:55,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:55,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:56,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:57,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:57,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:58,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:59,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:22:59,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:00,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:01,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:01,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:02,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:03,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:03,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:04,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:05,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:05,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:06,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:07,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:07,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:08,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:09,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:57<01:43, 14.79s/it][WARNING|generation_utils.py:914] 2023-08-28 19:23:09,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:10,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:11,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:11,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:12,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:13,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:13,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:14,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:15,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:16,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:16,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:17,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:18,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:18,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:19,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:20,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:20,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:21,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:22,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:23,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:23,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:24,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:12<01:29, 14.92s/it][WARNING|generation_utils.py:914] 2023-08-28 19:23:25,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:25,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:26,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:27,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:28,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:28,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:29,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:29,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:30,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:31,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:31,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:32,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:32,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:33,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:33,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:34,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:34,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:35,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:36,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:36,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:37,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:24<01:10, 14.19s/it][WARNING|generation_utils.py:914] 2023-08-28 19:23:37,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:38,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:38,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:39,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:40,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:41,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:41,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:42,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:43,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:43,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:44,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:45,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:45,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:46,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:47,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:48,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:49,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:49,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:50,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:51,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:51,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:39<00:57, 14.37s/it][WARNING|generation_utils.py:914] 2023-08-28 19:23:52,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:53,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:53,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:54,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:55,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:56,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:56,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:57,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:58,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:58,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:23:59,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:00,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:00,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:01,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:02,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:02,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:03,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:04,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:04,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:05,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:06,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:54<00:43, 14.43s/it][WARNING|generation_utils.py:914] 2023-08-28 19:24:06,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:07,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:08,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:08,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:09,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:10,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:10,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:11,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:12,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:12,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:13,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:14,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:14,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:15,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:16,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:16,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:17,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:18,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:18,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:19,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:20,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:21,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:22,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:22,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:10<00:30, 15.10s/it][WARNING|generation_utils.py:914] 2023-08-28 19:24:23,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:24,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:25,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:25,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:26,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:27,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:27,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:28,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:29,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:29,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:30,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:31,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:31,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:32,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:32,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:33,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:34,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:34,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:35,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:35,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:36,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:37,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:38,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:38,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:26<00:15, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-28 19:24:39,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:39,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:40,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:41,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:41,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:42,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:43,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:43,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:44,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:45,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:45,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:46,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:47,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:47,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:48,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:49,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:49,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:50,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:51,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:52,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:52,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:24:53,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:41<00:00, 15.11s/it]Generating: 100%|██████████| 15/15 [03:41<00:00, 14.75s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:25:00,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:25:00,358 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:25:00,358 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:25:00,358 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:25:00,358 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:25:00,729 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:25:00,730 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:25:01,004 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:25:02,080 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:25:02,080 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:25:03,956 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:25:03,984 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:25:03,984 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:25:03,984 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:25:03,984 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:25:04,851 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:25:04,852 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:25:05,151 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:25:05,328 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:25:05,329 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : main subject .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : participant in .', 'success_rate': 0.9241071428571429, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.8928571428571429, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : The site of the concert was taken under the control of the French railway company RLM . Head Entity : RLM , Tail Entity : France .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.8274456521739131, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8579545454545454, 'errors': {''}}
['Relation : nominated for . Context : Later in the year , the band formed with former member - songwriter Michael Jackson at the end of 2010 , with singer - songwriter Michael Jackson also producing the songs for the album . Head Entity : Michael Jackson , Tail Entity : Top Ten .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 607, 'raw': 704}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8622159090909091, 'errors': {''}}
['Relation : operating system . Context : The operating system was developed by Windows NT , developed by the Software Foundation . Head Entity : Windows NT , Tail Entity : Linux .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9345238095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : position held .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 512, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 589, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : religion .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 8762
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8862, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.40it/s]Extractor Estimating: 2it [00:01,  1.39it/s]Extractor Estimating: 3it [00:01,  1.56it/s]Extractor Estimating: 4it [00:02,  1.63it/s]Extractor Estimating: 5it [00:03,  1.65it/s]Extractor Estimating: 6it [00:03,  1.68it/s]Extractor Estimating: 7it [00:04,  1.66it/s]Extractor Estimating: 8it [00:04,  1.73it/s]Extractor Estimating: 9it [00:05,  1.69it/s]Extractor Estimating: 10it [00:06,  1.70it/s]Extractor Estimating: 11it [00:06,  1.72it/s]Extractor Estimating: 12it [00:07,  1.74it/s]Extractor Estimating: 13it [00:07,  1.69it/s]Extractor Estimating: 14it [00:08,  1.68it/s]Extractor Estimating: 15it [00:09,  1.69it/s]Extractor Estimating: 16it [00:09,  1.73it/s]Extractor Estimating: 17it [00:10,  1.75it/s]Extractor Estimating: 18it [00:10,  1.76it/s]Extractor Estimating: 19it [00:11,  1.76it/s]Extractor Estimating: 20it [00:11,  1.77it/s]Extractor Estimating: 21it [00:12,  1.73it/s]Extractor Estimating: 22it [00:12,  1.76it/s]Extractor Estimating: 23it [00:13,  1.74it/s]Extractor Estimating: 24it [00:14,  1.69it/s]Extractor Estimating: 25it [00:14,  1.68it/s]Extractor Estimating: 26it [00:15,  1.69it/s]Extractor Estimating: 27it [00:16,  1.64it/s]Extractor Estimating: 28it [00:16,  1.65it/s]Extractor Estimating: 29it [00:17,  1.65it/s]Extractor Estimating: 30it [00:17,  1.69it/s]Extractor Estimating: 31it [00:18,  1.64it/s]Extractor Estimating: 32it [00:19,  1.59it/s]Extractor Estimating: 33it [00:19,  1.64it/s]Extractor Estimating: 34it [00:20,  1.69it/s]Extractor Estimating: 35it [00:20,  1.65it/s]Extractor Estimating: 36it [00:21,  1.69it/s]Extractor Estimating: 37it [00:21,  1.71it/s]Extractor Estimating: 38it [00:22,  1.71it/s]Extractor Estimating: 39it [00:23,  1.70it/s]Extractor Estimating: 40it [00:23,  1.70it/s]Extractor Estimating: 41it [00:24,  1.74it/s]Extractor Estimating: 42it [00:24,  1.68it/s]Extractor Estimating: 43it [00:25,  1.69it/s]Extractor Estimating: 44it [00:26,  1.68it/s]Extractor Estimating: 45it [00:26,  1.66it/s]Extractor Estimating: 46it [00:27,  1.70it/s]Extractor Estimating: 47it [00:28,  1.55it/s]Extractor Estimating: 48it [00:28,  1.52it/s]Extractor Estimating: 49it [00:29,  1.59it/s]Extractor Estimating: 50it [00:30,  1.56it/s]Extractor Estimating: 51it [00:30,  1.61it/s]Extractor Estimating: 52it [00:31,  1.63it/s]Extractor Estimating: 53it [00:31,  1.62it/s]Extractor Estimating: 54it [00:32,  1.69it/s]Extractor Estimating: 55it [00:32,  1.71it/s]Extractor Estimating: 56it [00:33,  1.75it/s]Extractor Estimating: 57it [00:34,  1.69it/s]Extractor Estimating: 58it [00:34,  1.75it/s]Extractor Estimating: 59it [00:35,  1.68it/s]Extractor Estimating: 60it [00:35,  1.71it/s]Extractor Estimating: 61it [00:36,  1.74it/s]Extractor Estimating: 62it [00:36,  1.78it/s]Extractor Estimating: 63it [00:37,  1.77it/s]Extractor Estimating: 64it [00:38,  1.74it/s]Extractor Estimating: 65it [00:38,  1.75it/s]Extractor Estimating: 66it [00:39,  1.75it/s]Extractor Estimating: 67it [00:39,  1.75it/s]Extractor Estimating: 68it [00:40,  1.78it/s]Extractor Estimating: 69it [00:40,  1.76it/s]Extractor Estimating: 70it [00:41,  1.81it/s]Extractor Estimating: 71it [00:41,  1.83it/s]Extractor Estimating: 72it [00:42,  1.80it/s]Extractor Estimating: 73it [00:43,  1.76it/s]Extractor Estimating: 74it [00:43,  1.73it/s]Extractor Estimating: 75it [00:44,  1.75it/s]Extractor Estimating: 76it [00:44,  1.77it/s]Extractor Estimating: 77it [00:45,  1.80it/s]Extractor Estimating: 78it [00:45,  1.77it/s]Extractor Estimating: 79it [00:46,  1.83it/s]Extractor Estimating: 80it [00:46,  1.84it/s]Extractor Estimating: 81it [00:47,  1.88it/s]Extractor Estimating: 82it [00:47,  1.97it/s]Extractor Estimating: 83it [00:48,  1.91it/s]Extractor Estimating: 84it [00:48,  1.95it/s]Extractor Estimating: 85it [00:49,  1.94it/s]Extractor Estimating: 86it [00:50,  1.86it/s]Extractor Estimating: 87it [00:50,  1.86it/s]Extractor Estimating: 88it [00:51,  1.87it/s]Extractor Estimating: 89it [00:51,  1.83it/s]Extractor Estimating: 90it [00:52,  1.88it/s]Extractor Estimating: 91it [00:52,  1.88it/s]Extractor Estimating: 92it [00:53,  1.81it/s]Extractor Estimating: 93it [00:53,  1.81it/s]Extractor Estimating: 94it [00:54,  1.84it/s]Extractor Estimating: 95it [00:54,  1.88it/s]Extractor Estimating: 96it [00:55,  1.80it/s]Extractor Estimating: 97it [00:56,  1.86it/s]Extractor Estimating: 98it [00:56,  1.86it/s]Extractor Estimating: 99it [00:57,  1.86it/s]Extractor Estimating: 100it [00:57,  1.86it/s]Extractor Estimating: 101it [00:58,  1.84it/s]Extractor Estimating: 102it [00:58,  1.87it/s]Extractor Estimating: 103it [00:59,  1.87it/s]Extractor Estimating: 104it [00:59,  1.92it/s]Extractor Estimating: 105it [01:00,  1.92it/s]Extractor Estimating: 106it [01:00,  1.91it/s]Extractor Estimating: 107it [01:01,  1.89it/s]Extractor Estimating: 108it [01:01,  1.97it/s]Extractor Estimating: 109it [01:02,  1.92it/s]Extractor Estimating: 110it [01:02,  1.82it/s]Extractor Estimating: 111it [01:03,  1.82it/s]Extractor Estimating: 112it [01:04,  1.84it/s]Extractor Estimating: 113it [01:04,  1.84it/s]Extractor Estimating: 114it [01:05,  1.84it/s]Extractor Estimating: 115it [01:05,  1.88it/s]Extractor Estimating: 116it [01:06,  1.86it/s]Extractor Estimating: 117it [01:06,  1.89it/s]Extractor Estimating: 118it [01:07,  1.77it/s]Extractor Estimating: 119it [01:07,  1.80it/s]Extractor Estimating: 120it [01:08,  1.80it/s]Extractor Estimating: 121it [01:09,  1.63it/s]Extractor Estimating: 122it [01:09,  1.69it/s]Extractor Estimating: 123it [01:10,  1.73it/s]Extractor Estimating: 124it [01:10,  1.76it/s]Extractor Estimating: 125it [01:11,  1.82it/s]Extractor Estimating: 126it [01:11,  1.87it/s]Extractor Estimating: 127it [01:12,  1.89it/s]Extractor Estimating: 128it [01:12,  1.91it/s]Extractor Estimating: 129it [01:13,  2.00it/s]Extractor Estimating: 130it [01:13,  2.01it/s]Extractor Estimating: 131it [01:14,  1.91it/s]Extractor Estimating: 132it [01:14,  1.99it/s]Extractor Estimating: 133it [01:15,  2.03it/s]Extractor Estimating: 134it [01:15,  1.96it/s]Extractor Estimating: 135it [01:16,  1.94it/s]Extractor Estimating: 136it [01:16,  1.93it/s]Extractor Estimating: 137it [01:17,  1.95it/s]Extractor Estimating: 138it [01:17,  1.92it/s]Extractor Estimating: 139it [01:18,  1.91it/s]Extractor Estimating: 140it [01:19,  1.87it/s]Extractor Estimating: 141it [01:19,  1.93it/s]Extractor Estimating: 142it [01:20,  1.93it/s]Extractor Estimating: 143it [01:20,  1.86it/s]Extractor Estimating: 144it [01:21,  1.87it/s]Extractor Estimating: 145it [01:21,  1.94it/s]Extractor Estimating: 146it [01:22,  1.90it/s]Extractor Estimating: 147it [01:22,  1.95it/s]Extractor Estimating: 148it [01:23,  1.96it/s]Extractor Estimating: 149it [01:23,  1.96it/s]Extractor Estimating: 150it [01:24,  1.94it/s]Extractor Estimating: 151it [01:24,  1.87it/s]Extractor Estimating: 152it [01:25,  1.76it/s]Extractor Estimating: 153it [01:25,  1.78it/s]Extractor Estimating: 154it [01:26,  1.74it/s]Extractor Estimating: 155it [01:27,  1.77it/s]Extractor Estimating: 156it [01:27,  1.76it/s]Extractor Estimating: 157it [01:28,  1.77it/s]Extractor Estimating: 158it [01:28,  1.69it/s]Extractor Estimating: 159it [01:29,  1.72it/s]Extractor Estimating: 160it [01:30,  1.68it/s]Extractor Estimating: 161it [01:30,  1.77it/s]Extractor Estimating: 162it [01:31,  1.75it/s]Extractor Estimating: 163it [01:31,  1.75it/s]Extractor Estimating: 164it [01:32,  1.73it/s]Extractor Estimating: 165it [01:32,  1.76it/s]Extractor Estimating: 166it [01:33,  1.77it/s]Extractor Estimating: 167it [01:33,  1.81it/s]Extractor Estimating: 168it [01:34,  1.74it/s]Extractor Estimating: 169it [01:35,  1.75it/s]Extractor Estimating: 170it [01:35,  1.73it/s]Extractor Estimating: 171it [01:36,  1.72it/s]Extractor Estimating: 172it [01:36,  1.76it/s]Extractor Estimating: 173it [01:37,  1.77it/s]Extractor Estimating: 174it [01:37,  1.78it/s]Extractor Estimating: 175it [01:38,  1.65it/s]Extractor Estimating: 176it [01:39,  1.73it/s]Extractor Estimating: 177it [01:39,  1.80it/s]Extractor Estimating: 178it [01:40,  1.76it/s]Extractor Estimating: 179it [01:40,  1.79it/s]Extractor Estimating: 180it [01:41,  1.83it/s]Extractor Estimating: 181it [01:41,  1.81it/s]Extractor Estimating: 182it [01:42,  1.85it/s]Extractor Estimating: 183it [01:42,  1.85it/s]Extractor Estimating: 184it [01:43,  1.88it/s]Extractor Estimating: 185it [01:44,  1.83it/s]Extractor Estimating: 186it [01:44,  1.77it/s]Extractor Estimating: 187it [01:45,  1.84it/s]Extractor Estimating: 188it [01:45,  1.90it/s]Extractor Estimating: 189it [01:46,  1.89it/s]Extractor Estimating: 190it [01:46,  1.86it/s]Extractor Estimating: 191it [01:47,  1.87it/s]Extractor Estimating: 192it [01:47,  1.87it/s]Extractor Estimating: 193it [01:48,  1.86it/s]Extractor Estimating: 194it [01:48,  1.82it/s]Extractor Estimating: 195it [01:49,  1.78it/s]Extractor Estimating: 196it [01:50,  1.81it/s]Extractor Estimating: 197it [01:50,  1.82it/s]Extractor Estimating: 198it [01:51,  1.83it/s]Extractor Estimating: 199it [01:51,  1.83it/s]Extractor Estimating: 200it [01:52,  1.84it/s]Extractor Estimating: 201it [01:52,  1.72it/s]Extractor Estimating: 202it [01:53,  1.73it/s]Extractor Estimating: 203it [01:54,  1.66it/s]Extractor Estimating: 204it [01:54,  1.67it/s]Extractor Estimating: 205it [01:55,  1.66it/s]Extractor Estimating: 206it [01:55,  1.64it/s]Extractor Estimating: 207it [01:56,  1.61it/s]Extractor Estimating: 208it [01:57,  1.52it/s]Extractor Estimating: 209it [01:57,  1.55it/s]Extractor Estimating: 210it [01:58,  1.52it/s]Extractor Estimating: 211it [01:59,  1.51it/s]Extractor Estimating: 212it [02:00,  1.50it/s]Extractor Estimating: 213it [02:00,  1.54it/s]Extractor Estimating: 214it [02:01,  1.58it/s]Extractor Estimating: 215it [02:01,  1.55it/s]Extractor Estimating: 216it [02:02,  1.50it/s]Extractor Estimating: 217it [02:03,  1.59it/s]Extractor Estimating: 218it [02:03,  1.61it/s]Extractor Estimating: 219it [02:04,  1.48it/s]Extractor Estimating: 220it [02:05,  1.43it/s]Extractor Estimating: 221it [02:05,  1.49it/s]Extractor Estimating: 222it [02:06,  1.55it/s]Extractor Estimating: 223it [02:07,  1.59it/s]Extractor Estimating: 224it [02:07,  1.59it/s]Extractor Estimating: 225it [02:08,  1.58it/s]Extractor Estimating: 226it [02:08,  1.70it/s]Extractor Estimating: 227it [02:09,  1.72it/s]Extractor Estimating: 228it [02:10,  1.68it/s]Extractor Estimating: 229it [02:10,  1.78it/s]Extractor Estimating: 230it [02:10,  1.85it/s]Extractor Estimating: 231it [02:11,  1.98it/s]Extractor Estimating: 232it [02:11,  2.02it/s]Extractor Estimating: 233it [02:12,  2.00it/s]Extractor Estimating: 234it [02:12,  1.94it/s]Extractor Estimating: 235it [02:13,  1.99it/s]Extractor Estimating: 236it [02:13,  2.04it/s]Extractor Estimating: 237it [02:14,  2.11it/s]Extractor Estimating: 238it [02:14,  2.08it/s]Extractor Estimating: 239it [02:15,  2.15it/s]Extractor Estimating: 240it [02:15,  2.10it/s]Extractor Estimating: 241it [02:16,  2.08it/s]Extractor Estimating: 242it [02:16,  2.12it/s]Extractor Estimating: 243it [02:17,  2.19it/s]Extractor Estimating: 244it [02:17,  2.21it/s]Extractor Estimating: 245it [02:18,  2.18it/s]Extractor Estimating: 246it [02:18,  2.16it/s]Extractor Estimating: 247it [02:18,  2.12it/s]Extractor Estimating: 248it [02:19,  2.17it/s]Extractor Estimating: 249it [02:19,  2.18it/s]Extractor Estimating: 250it [02:20,  2.16it/s]Extractor Estimating: 251it [02:20,  2.05it/s]Extractor Estimating: 252it [02:21,  1.95it/s]Extractor Estimating: 253it [02:22,  1.91it/s]Extractor Estimating: 254it [02:22,  1.74it/s]Extractor Estimating: 255it [02:23,  1.75it/s]Extractor Estimating: 256it [02:23,  1.76it/s]Extractor Estimating: 257it [02:24,  1.79it/s]Extractor Estimating: 258it [02:25,  1.73it/s]Extractor Estimating: 259it [02:25,  1.68it/s]Extractor Estimating: 260it [02:26,  1.72it/s]Extractor Estimating: 261it [02:26,  1.76it/s]Extractor Estimating: 262it [02:27,  1.79it/s]Extractor Estimating: 263it [02:27,  1.80it/s]Extractor Estimating: 264it [02:28,  1.84it/s]Extractor Estimating: 265it [02:28,  1.75it/s]Extractor Estimating: 266it [02:29,  1.76it/s]Extractor Estimating: 267it [02:30,  1.75it/s]Extractor Estimating: 268it [02:30,  1.79it/s]Extractor Estimating: 269it [02:31,  1.81it/s]Extractor Estimating: 270it [02:31,  1.78it/s]Extractor Estimating: 271it [02:32,  1.73it/s]Extractor Estimating: 272it [02:32,  1.75it/s]Extractor Estimating: 273it [02:33,  1.80it/s]Extractor Estimating: 274it [02:33,  1.80it/s]Extractor Estimating: 275it [02:34,  1.85it/s]Extractor Estimating: 276it [02:35,  1.84it/s]Extractor Estimating: 277it [02:35,  1.69it/s]Extractor Estimating: 278it [02:36,  1.73it/s]Extractor Estimating: 279it [02:36,  1.81it/s]Extractor Estimating: 280it [02:37,  1.80it/s]Extractor Estimating: 281it [02:37,  1.88it/s]Extractor Estimating: 282it [02:38,  1.89it/s]Extractor Estimating: 283it [02:38,  1.88it/s]Extractor Estimating: 284it [02:39,  1.84it/s]Extractor Estimating: 285it [02:40,  1.83it/s]Extractor Estimating: 286it [02:40,  1.81it/s]Extractor Estimating: 287it [02:41,  1.83it/s]Extractor Estimating: 288it [02:41,  1.85it/s]Extractor Estimating: 289it [02:42,  1.90it/s]Extractor Estimating: 290it [02:42,  1.95it/s]Extractor Estimating: 291it [02:43,  1.90it/s]Extractor Estimating: 292it [02:43,  1.95it/s]Extractor Estimating: 293it [02:44,  1.91it/s]Extractor Estimating: 294it [02:44,  1.96it/s]Extractor Estimating: 295it [02:45,  1.92it/s]Extractor Estimating: 296it [02:45,  1.96it/s]Extractor Estimating: 297it [02:46,  1.86it/s]Extractor Estimating: 298it [02:46,  1.87it/s]Extractor Estimating: 299it [02:47,  1.83it/s]Extractor Estimating: 300it [02:47,  1.81it/s]Extractor Estimating: 301it [02:48,  1.80it/s]Extractor Estimating: 302it [02:49,  1.83it/s]Extractor Estimating: 303it [02:49,  1.81it/s]Extractor Estimating: 304it [02:50,  1.86it/s]Extractor Estimating: 305it [02:50,  1.83it/s]Extractor Estimating: 306it [02:51,  1.87it/s]Extractor Estimating: 307it [02:51,  1.83it/s]Extractor Estimating: 308it [02:52,  1.79it/s]Extractor Estimating: 309it [02:52,  1.76it/s]Extractor Estimating: 310it [02:53,  1.79it/s]Extractor Estimating: 311it [02:54,  1.85it/s]Extractor Estimating: 312it [02:54,  1.80it/s]Extractor Estimating: 313it [02:55,  1.80it/s]Extractor Estimating: 314it [02:55,  1.79it/s]Extractor Estimating: 315it [02:56,  1.78it/s]Extractor Estimating: 316it [02:56,  1.76it/s]Extractor Estimating: 317it [02:57,  1.75it/s]Extractor Estimating: 318it [02:57,  1.83it/s]Extractor Estimating: 319it [02:58,  1.74it/s]Extractor Estimating: 320it [02:59,  1.61it/s]Extractor Estimating: 321it [02:59,  1.68it/s]Extractor Estimating: 322it [03:00,  1.70it/s]Extractor Estimating: 323it [03:00,  1.74it/s]Extractor Estimating: 324it [03:01,  1.72it/s]Extractor Estimating: 325it [03:02,  1.74it/s]Extractor Estimating: 326it [03:02,  1.78it/s]Extractor Estimating: 327it [03:03,  1.89it/s]Extractor Estimating: 328it [03:03,  1.95it/s]Extractor Estimating: 329it [03:04,  2.03it/s]Extractor Estimating: 330it [03:04,  2.03it/s]Extractor Estimating: 331it [03:05,  2.00it/s]Extractor Estimating: 332it [03:05,  1.97it/s]Extractor Estimating: 333it [03:06,  1.94it/s]Extractor Estimating: 334it [03:06,  1.94it/s]Extractor Estimating: 335it [03:07,  1.97it/s]Extractor Estimating: 336it [03:07,  1.97it/s]Extractor Estimating: 337it [03:08,  1.97it/s]Extractor Estimating: 338it [03:08,  1.92it/s]Extractor Estimating: 339it [03:09,  1.95it/s]Extractor Estimating: 340it [03:09,  1.99it/s]Extractor Estimating: 341it [03:10,  1.98it/s]Extractor Estimating: 342it [03:10,  1.95it/s]Extractor Estimating: 343it [03:11,  1.97it/s]Extractor Estimating: 344it [03:11,  2.04it/s]Extractor Estimating: 345it [03:12,  2.03it/s]Extractor Estimating: 346it [03:12,  2.00it/s]Extractor Estimating: 347it [03:13,  1.91it/s]Extractor Estimating: 348it [03:13,  1.93it/s]Extractor Estimating: 349it [03:14,  2.02it/s]Extractor Estimating: 350it [03:14,  1.97it/s]Extractor Estimating: 351it [03:15,  1.87it/s]Extractor Estimating: 352it [03:15,  1.83it/s]Extractor Estimating: 353it [03:16,  1.82it/s]Extractor Estimating: 354it [03:16,  1.86it/s]Extractor Estimating: 355it [03:17,  1.87it/s]Extractor Estimating: 356it [03:18,  1.82it/s]Extractor Estimating: 357it [03:18,  1.85it/s]Extractor Estimating: 358it [03:19,  1.84it/s]Extractor Estimating: 359it [03:19,  1.84it/s]Extractor Estimating: 360it [03:20,  1.77it/s]Extractor Estimating: 361it [03:20,  1.79it/s]Extractor Estimating: 362it [03:21,  1.82it/s]Extractor Estimating: 363it [03:21,  1.81it/s]Extractor Estimating: 364it [03:22,  1.78it/s]Extractor Estimating: 365it [03:23,  1.79it/s]Extractor Estimating: 366it [03:23,  1.80it/s]Extractor Estimating: 367it [03:24,  1.81it/s]Extractor Estimating: 368it [03:24,  1.75it/s]Extractor Estimating: 369it [03:25,  1.80it/s]Extractor Estimating: 370it [03:25,  1.77it/s]Extractor Estimating: 371it [03:26,  1.78it/s]Extractor Estimating: 372it [03:26,  1.76it/s]Extractor Estimating: 373it [03:27,  1.79it/s]Extractor Estimating: 374it [03:28,  1.82it/s]Extractor Estimating: 375it [03:28,  1.91it/s]Extractor Estimating: 375it [03:28,  1.80it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:47,465 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:47,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:47,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:47,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:47,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:28:48,099 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:28:48,100 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:28:48,675 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:28:49,751 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:28:49,751 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:52,678 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:52,684 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:52,684 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:52,684 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:28:52,684 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:28:53,406 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:28:53,408 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:28:53,975 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:28:54,144 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:28:54,144 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 20:45:24,883 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 20:45:24,888 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 4498 mean pseudo reward: 0.9800858624551717
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 14794
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14894, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=14894, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.979, loss:336.1674
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 12, avg_time 0.971, loss:327.8605
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 112, avg_time 0.963, loss:300.2030
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 24, avg_time 0.972, loss:295.7109
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 124, avg_time 0.980, loss:270.5164
>> valid entity prec:0.5698, rec:0.5259, f1:0.5469
>> valid relation prec:0.2845, rec:0.1693, f1:0.2123
>> valid relation with NER prec:0.2845, rec:0.1693, f1:0.2123
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 36, avg_time 2.324, loss:258.4350
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 136, avg_time 0.966, loss:257.8311
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 48, avg_time 0.965, loss:257.9385
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 148, avg_time 0.982, loss:264.2083
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 60, avg_time 0.963, loss:238.1831
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5365, rec:0.5107, f1:0.5233
>> valid relation prec:0.2810, rec:0.1605, f1:0.2043
>> valid relation with NER prec:0.2810, rec:0.1605, f1:0.2043
g_step 1100, step 160, avg_time 2.306, loss:258.7168
g_step 1200, step 72, avg_time 0.962, loss:226.6783
g_step 1300, step 172, avg_time 0.979, loss:246.0012
g_step 1400, step 84, avg_time 0.973, loss:214.4962
g_step 1500, step 184, avg_time 0.969, loss:211.7584
>> valid entity prec:0.5619, rec:0.5225, f1:0.5415
>> valid relation prec:0.2651, rec:0.1496, f1:0.1912
>> valid relation with NER prec:0.2651, rec:0.1496, f1:0.1912
g_step 1600, step 96, avg_time 2.306, loss:200.7048
g_step 1700, step 8, avg_time 0.972, loss:214.1487
g_step 1800, step 108, avg_time 0.979, loss:181.3825
g_step 1900, step 20, avg_time 0.971, loss:208.5808
g_step 2000, step 120, avg_time 0.963, loss:171.8221
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6039, rec:0.4952, f1:0.5442
>> valid relation prec:0.2947, rec:0.1398, f1:0.1897
>> valid relation with NER prec:0.2947, rec:0.1398, f1:0.1897
g_step 2100, step 32, avg_time 2.304, loss:184.5102
g_step 2200, step 132, avg_time 0.962, loss:157.2964
g_step 2300, step 44, avg_time 0.979, loss:185.8761
g_step 2400, step 144, avg_time 0.970, loss:165.8894
g_step 2500, step 56, avg_time 0.973, loss:161.6707
>> valid entity prec:0.5658, rec:0.5087, f1:0.5357
>> valid relation prec:0.2728, rec:0.1544, f1:0.1972
>> valid relation with NER prec:0.2728, rec:0.1544, f1:0.1972
g_step 2600, step 156, avg_time 2.301, loss:138.6945
g_step 2700, step 68, avg_time 0.966, loss:143.4744
g_step 2800, step 168, avg_time 0.975, loss:140.0616
g_step 2900, step 80, avg_time 0.969, loss:141.9075
g_step 3000, step 180, avg_time 0.972, loss:141.1158
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5461, rec:0.5061, f1:0.5253
>> valid relation prec:0.2502, rec:0.1622, f1:0.1968
>> valid relation with NER prec:0.2502, rec:0.1622, f1:0.1968
g_step 3100, step 92, avg_time 2.307, loss:124.4279
g_step 3200, step 4, avg_time 0.969, loss:136.0044
g_step 3300, step 104, avg_time 0.962, loss:118.5681
g_step 3400, step 16, avg_time 0.977, loss:124.7349
g_step 3500, step 116, avg_time 0.972, loss:120.1437
>> valid entity prec:0.5483, rec:0.5310, f1:0.5395
>> valid relation prec:0.2328, rec:0.1599, f1:0.1896
>> valid relation with NER prec:0.2328, rec:0.1599, f1:0.1896
g_step 3600, step 28, avg_time 2.307, loss:128.8138
g_step 3700, step 128, avg_time 0.979, loss:117.0677
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 20:45:24 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 20:45:24 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_20-45-24_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 20:45:25 - WARNING - datasets.builder -   Using custom data configuration default-ca14ded32c8b73f8
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ca14ded32c8b73f8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 20:45:26,411 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:45:26,412 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:45:26,413 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:45:26,414 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:45:26,431 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:45:26,434 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:45:26,434 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:45:26,435 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:45:26,435 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:45:26,435 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:45:26,435 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 20:45:26,592 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:45:29,740 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 20:45:29,743 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ca14ded32c8b73f8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.16ba/s] 40%|████      | 2/5 [00:00<00:00,  4.11ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.56ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.79ba/s]100%|██████████| 5/5 [00:00<00:00,  5.04ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.17ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.43ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.61ba/s]100%|██████████| 4/4 [00:00<00:00,  4.73ba/s]100%|██████████| 4/4 [00:00<00:00,  4.42ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  7.14ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.27ba/s]100%|██████████| 5/5 [00:00<00:00, 11.14ba/s]100%|██████████| 5/5 [00:00<00:00, 10.44ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.98ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.46ba/s]100%|██████████| 4/4 [00:00<00:00, 10.75ba/s]
[INFO|trainer.py:414] 2023-08-28 20:45:33,271 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 20:45:33,305 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 20:45:33,305 >>   Num examples = 4500
[INFO|trainer.py:1149] 2023-08-28 20:45:33,306 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 20:45:33,306 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 20:45:33,306 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 20:45:33,306 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 20:45:33,306 >>   Total optimization steps = 350
  0%|          | 0/350 [00:00<?, ?it/s]  0%|          | 1/350 [00:00<01:46,  3.28it/s]  1%|          | 2/350 [00:00<01:41,  3.41it/s]  1%|          | 3/350 [00:00<01:40,  3.45it/s]  1%|          | 4/350 [00:01<01:39,  3.48it/s]  1%|▏         | 5/350 [00:01<01:38,  3.49it/s]  2%|▏         | 6/350 [00:01<01:38,  3.50it/s]  2%|▏         | 7/350 [00:02<01:38,  3.50it/s]  2%|▏         | 8/350 [00:02<01:37,  3.50it/s]  3%|▎         | 9/350 [00:02<01:37,  3.50it/s]  3%|▎         | 10/350 [00:02<01:37,  3.50it/s]  3%|▎         | 11/350 [00:03<01:36,  3.50it/s]  3%|▎         | 12/350 [00:03<01:36,  3.50it/s]  4%|▎         | 13/350 [00:03<01:36,  3.50it/s]  4%|▍         | 14/350 [00:04<01:35,  3.50it/s]  4%|▍         | 15/350 [00:04<01:35,  3.50it/s]  5%|▍         | 16/350 [00:04<01:35,  3.50it/s]  5%|▍         | 17/350 [00:04<01:35,  3.50it/s]  5%|▌         | 18/350 [00:05<01:34,  3.50it/s]  5%|▌         | 19/350 [00:05<01:34,  3.50it/s]  6%|▌         | 20/350 [00:05<01:34,  3.50it/s]  6%|▌         | 21/350 [00:06<01:33,  3.50it/s]  6%|▋         | 22/350 [00:06<01:33,  3.50it/s]  7%|▋         | 23/350 [00:06<01:33,  3.50it/s]  7%|▋         | 24/350 [00:06<01:33,  3.50it/s]  7%|▋         | 25/350 [00:07<01:32,  3.50it/s]  7%|▋         | 26/350 [00:07<01:32,  3.50it/s]  8%|▊         | 27/350 [00:07<01:32,  3.50it/s]  8%|▊         | 28/350 [00:08<01:32,  3.50it/s]  8%|▊         | 29/350 [00:08<01:31,  3.50it/s]  9%|▊         | 30/350 [00:08<01:31,  3.50it/s]  9%|▉         | 31/350 [00:08<01:31,  3.50it/s]  9%|▉         | 32/350 [00:09<01:30,  3.51it/s]  9%|▉         | 33/350 [00:09<01:30,  3.51it/s] 10%|▉         | 34/350 [00:09<01:30,  3.51it/s] 10%|█         | 35/350 [00:10<01:29,  3.51it/s] 10%|█         | 36/350 [00:10<01:29,  3.50it/s] 11%|█         | 37/350 [00:10<01:29,  3.50it/s] 11%|█         | 38/350 [00:10<01:28,  3.51it/s] 11%|█         | 39/350 [00:11<01:28,  3.51it/s] 11%|█▏        | 40/350 [00:11<01:28,  3.51it/s] 12%|█▏        | 41/350 [00:11<01:28,  3.50it/s] 12%|█▏        | 42/350 [00:12<01:28,  3.50it/s] 12%|█▏        | 43/350 [00:12<01:27,  3.50it/s] 13%|█▎        | 44/350 [00:12<01:27,  3.50it/s] 13%|█▎        | 45/350 [00:12<01:27,  3.50it/s] 13%|█▎        | 46/350 [00:13<01:26,  3.50it/s] 13%|█▎        | 47/350 [00:13<01:26,  3.50it/s] 14%|█▎        | 48/350 [00:13<01:26,  3.50it/s] 14%|█▍        | 49/350 [00:14<01:26,  3.50it/s] 14%|█▍        | 50/350 [00:14<01:25,  3.50it/s] 15%|█▍        | 51/350 [00:14<01:25,  3.49it/s] 15%|█▍        | 52/350 [00:14<01:25,  3.50it/s] 15%|█▌        | 53/350 [00:15<01:24,  3.50it/s] 15%|█▌        | 54/350 [00:15<01:24,  3.50it/s] 16%|█▌        | 55/350 [00:15<01:24,  3.50it/s] 16%|█▌        | 56/350 [00:16<01:25,  3.44it/s] 16%|█▋        | 57/350 [00:16<01:24,  3.45it/s] 17%|█▋        | 58/350 [00:16<01:24,  3.46it/s] 17%|█▋        | 59/350 [00:16<01:23,  3.47it/s] 17%|█▋        | 60/350 [00:17<01:23,  3.48it/s] 17%|█▋        | 61/350 [00:17<01:29,  3.24it/s] 18%|█▊        | 62/350 [00:17<01:26,  3.31it/s] 18%|█▊        | 63/350 [00:18<01:25,  3.36it/s] 18%|█▊        | 64/350 [00:18<01:24,  3.40it/s] 19%|█▊        | 65/350 [00:18<01:23,  3.43it/s] 19%|█▉        | 66/350 [00:18<01:22,  3.45it/s] 19%|█▉        | 67/350 [00:19<01:21,  3.46it/s] 19%|█▉        | 68/350 [00:19<01:21,  3.47it/s] 20%|█▉        | 69/350 [00:19<01:20,  3.48it/s] 20%|██        | 70/350 [00:20<01:20,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 20:45:53,525 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:45:53,525 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 20:45:53,525 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 59.32it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.71it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.56it/s][A
  5%|▌         | 24/437 [00:00<00:08, 48.81it/s][A
  7%|▋         | 29/437 [00:00<00:08, 48.40it/s][A
  8%|▊         | 34/437 [00:00<00:08, 48.09it/s][A
  9%|▉         | 39/437 [00:00<00:08, 47.88it/s][A
 10%|█         | 44/437 [00:00<00:08, 47.55it/s][A
 11%|█         | 49/437 [00:01<00:08, 44.19it/s][A
 12%|█▏        | 54/437 [00:01<00:08, 45.66it/s][A
 14%|█▎        | 59/437 [00:01<00:08, 46.18it/s][A
 15%|█▍        | 64/437 [00:01<00:08, 46.58it/s][A
 16%|█▌        | 69/437 [00:01<00:07, 46.82it/s][A
 17%|█▋        | 74/437 [00:01<00:07, 47.00it/s][A
 18%|█▊        | 79/437 [00:01<00:07, 47.17it/s][A
 19%|█▉        | 84/437 [00:01<00:07, 47.18it/s][A
 20%|██        | 89/437 [00:01<00:07, 47.30it/s][A
 22%|██▏       | 94/437 [00:01<00:07, 47.29it/s][A
 23%|██▎       | 99/437 [00:02<00:07, 47.36it/s][A
 24%|██▍       | 104/437 [00:02<00:07, 47.38it/s][A
 25%|██▍       | 109/437 [00:02<00:06, 47.43it/s][A
 26%|██▌       | 114/437 [00:02<00:06, 47.30it/s][A
 27%|██▋       | 119/437 [00:02<00:06, 47.44it/s][A
 28%|██▊       | 124/437 [00:02<00:06, 47.37it/s][A
 30%|██▉       | 129/437 [00:02<00:06, 47.37it/s][A
 31%|███       | 134/437 [00:02<00:06, 47.31it/s][A
 32%|███▏      | 139/437 [00:02<00:06, 47.27it/s][A
 33%|███▎      | 144/437 [00:03<00:06, 47.31it/s][A
 34%|███▍      | 149/437 [00:03<00:06, 47.34it/s][A
 35%|███▌      | 154/437 [00:03<00:05, 47.40it/s][A
 36%|███▋      | 159/437 [00:03<00:05, 47.38it/s][A
 38%|███▊      | 164/437 [00:03<00:05, 47.19it/s][A
 39%|███▊      | 169/437 [00:03<00:05, 47.28it/s][A
 40%|███▉      | 174/437 [00:03<00:05, 46.75it/s][A
 41%|████      | 179/437 [00:03<00:05, 47.19it/s][A
 42%|████▏     | 184/437 [00:03<00:05, 47.19it/s][A
 43%|████▎     | 189/437 [00:03<00:05, 47.19it/s][A
 44%|████▍     | 194/437 [00:04<00:05, 47.21it/s][A
 46%|████▌     | 199/437 [00:04<00:05, 47.23it/s][A
 47%|████▋     | 204/437 [00:04<00:04, 47.35it/s][A
 48%|████▊     | 209/437 [00:04<00:04, 47.22it/s][A
 49%|████▉     | 214/437 [00:04<00:04, 47.32it/s][A
 50%|█████     | 219/437 [00:04<00:04, 47.32it/s][A
 51%|█████▏    | 224/437 [00:04<00:04, 47.23it/s][A
 52%|█████▏    | 229/437 [00:04<00:04, 47.30it/s][A
 54%|█████▎    | 234/437 [00:04<00:04, 47.25it/s][A
 55%|█████▍    | 239/437 [00:05<00:04, 47.27it/s][A
 56%|█████▌    | 244/437 [00:05<00:04, 47.26it/s][A
 57%|█████▋    | 249/437 [00:05<00:03, 47.29it/s][A
 58%|█████▊    | 254/437 [00:05<00:03, 47.36it/s][A
 59%|█████▉    | 259/437 [00:05<00:03, 47.26it/s][A
 60%|██████    | 264/437 [00:05<00:03, 47.26it/s][A
 62%|██████▏   | 269/437 [00:05<00:03, 47.27it/s][A
 63%|██████▎   | 274/437 [00:05<00:03, 47.34it/s][A
 64%|██████▍   | 279/437 [00:05<00:03, 47.31it/s][A
 65%|██████▍   | 284/437 [00:05<00:03, 47.17it/s][A
 66%|██████▌   | 289/437 [00:06<00:03, 47.26it/s][A
 67%|██████▋   | 294/437 [00:06<00:03, 47.29it/s][A
 68%|██████▊   | 299/437 [00:06<00:03, 42.48it/s][A
 70%|██████▉   | 304/437 [00:06<00:03, 43.91it/s][A
 71%|███████   | 309/437 [00:06<00:02, 44.91it/s][A
 72%|███████▏  | 314/437 [00:06<00:02, 45.71it/s][A
 73%|███████▎  | 319/437 [00:06<00:02, 46.23it/s][A
 74%|███████▍  | 324/437 [00:06<00:02, 46.59it/s][A
 75%|███████▌  | 329/437 [00:06<00:02, 46.88it/s][A
 76%|███████▋  | 334/437 [00:07<00:02, 46.99it/s][A
 78%|███████▊  | 339/437 [00:07<00:02, 47.07it/s][A
 79%|███████▊  | 344/437 [00:07<00:01, 46.82it/s][A
 80%|███████▉  | 349/437 [00:07<00:01, 46.88it/s][A
 81%|████████  | 354/437 [00:07<00:01, 47.01it/s][A
 82%|████████▏ | 359/437 [00:07<00:01, 47.01it/s][A
 83%|████████▎ | 364/437 [00:07<00:01, 47.22it/s][A
 84%|████████▍ | 369/437 [00:07<00:01, 47.35it/s][A
 86%|████████▌ | 374/437 [00:07<00:01, 47.38it/s][A
 87%|████████▋ | 379/437 [00:08<00:01, 47.42it/s][A
 88%|████████▊ | 384/437 [00:08<00:01, 47.27it/s][A
 89%|████████▉ | 389/437 [00:08<00:01, 47.07it/s][A
 90%|█████████ | 394/437 [00:08<00:00, 47.11it/s][A
 91%|█████████▏| 399/437 [00:08<00:00, 47.10it/s][A
 92%|█████████▏| 404/437 [00:08<00:00, 47.09it/s][A
 94%|█████████▎| 409/437 [00:08<00:00, 47.07it/s][A
 95%|█████████▍| 414/437 [00:08<00:00, 47.23it/s][A
 96%|█████████▌| 419/437 [00:08<00:00, 47.34it/s][A
 97%|█████████▋| 424/437 [00:08<00:00, 47.41it/s][A
 98%|█████████▊| 429/437 [00:09<00:00, 47.34it/s][A
 99%|█████████▉| 434/437 [00:09<00:00, 47.21it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 47.21it/s][A 20%|██        | 70/350 [00:29<01:20,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:46:02,842 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-70
[INFO|configuration_utils.py:351] 2023-08-28 20:46:02,887 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-70/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:46:07,943 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-70/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:46:08,476 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-70/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:46:08,494 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-70/special_tokens_map.json
 20%|██        | 71/350 [00:45<36:17,  7.80s/it] 21%|██        | 72/350 [00:45<25:43,  5.55s/it] 21%|██        | 73/350 [00:46<18:19,  3.97s/it] 21%|██        | 74/350 [00:46<13:10,  2.87s/it] 21%|██▏       | 75/350 [00:46<09:35,  2.09s/it] 22%|██▏       | 76/350 [00:46<07:04,  1.55s/it] 22%|██▏       | 77/350 [00:47<05:19,  1.17s/it] 22%|██▏       | 78/350 [00:47<04:06,  1.11it/s] 23%|██▎       | 79/350 [00:47<03:14,  1.39it/s] 23%|██▎       | 80/350 [00:48<02:39,  1.70it/s] 23%|██▎       | 81/350 [00:48<02:14,  2.01it/s] 23%|██▎       | 82/350 [00:48<01:56,  2.30it/s] 24%|██▎       | 83/350 [00:48<01:44,  2.55it/s] 24%|██▍       | 84/350 [00:49<01:35,  2.78it/s] 24%|██▍       | 85/350 [00:49<01:29,  2.96it/s] 25%|██▍       | 86/350 [00:49<01:25,  3.10it/s] 25%|██▍       | 87/350 [00:50<01:21,  3.21it/s] 25%|██▌       | 88/350 [00:50<01:19,  3.28it/s] 25%|██▌       | 89/350 [00:50<01:18,  3.35it/s] 26%|██▌       | 90/350 [00:50<01:16,  3.39it/s] 26%|██▌       | 91/350 [00:51<01:15,  3.42it/s] 26%|██▋       | 92/350 [00:51<01:14,  3.44it/s] 27%|██▋       | 93/350 [00:51<01:14,  3.43it/s] 27%|██▋       | 94/350 [00:52<01:14,  3.42it/s] 27%|██▋       | 95/350 [00:52<01:14,  3.44it/s] 27%|██▋       | 96/350 [00:52<01:13,  3.46it/s] 28%|██▊       | 97/350 [00:52<01:12,  3.47it/s] 28%|██▊       | 98/350 [00:53<01:12,  3.48it/s] 28%|██▊       | 99/350 [00:53<01:12,  3.49it/s] 29%|██▊       | 100/350 [00:53<01:11,  3.49it/s] 29%|██▉       | 101/350 [00:54<01:11,  3.49it/s] 29%|██▉       | 102/350 [00:54<01:10,  3.49it/s] 29%|██▉       | 103/350 [00:54<01:10,  3.49it/s] 30%|██▉       | 104/350 [00:54<01:10,  3.50it/s] 30%|███       | 105/350 [00:55<01:12,  3.38it/s] 30%|███       | 106/350 [00:55<01:11,  3.41it/s] 31%|███       | 107/350 [00:55<01:10,  3.44it/s] 31%|███       | 108/350 [00:56<01:10,  3.46it/s] 31%|███       | 109/350 [00:56<01:09,  3.47it/s] 31%|███▏      | 110/350 [00:56<01:09,  3.48it/s] 32%|███▏      | 111/350 [00:56<01:08,  3.49it/s] 32%|███▏      | 112/350 [00:57<01:08,  3.49it/s] 32%|███▏      | 113/350 [00:57<01:07,  3.49it/s] 33%|███▎      | 114/350 [00:57<01:07,  3.49it/s] 33%|███▎      | 115/350 [00:58<01:07,  3.49it/s] 33%|███▎      | 116/350 [00:58<01:07,  3.48it/s] 33%|███▎      | 117/350 [00:58<01:06,  3.49it/s] 34%|███▎      | 118/350 [00:58<01:06,  3.49it/s] 34%|███▍      | 119/350 [00:59<01:06,  3.49it/s] 34%|███▍      | 120/350 [00:59<01:05,  3.49it/s] 35%|███▍      | 121/350 [00:59<01:05,  3.49it/s] 35%|███▍      | 122/350 [01:00<01:05,  3.49it/s] 35%|███▌      | 123/350 [01:00<01:05,  3.49it/s] 35%|███▌      | 124/350 [01:00<01:04,  3.49it/s] 36%|███▌      | 125/350 [01:00<01:04,  3.49it/s] 36%|███▌      | 126/350 [01:01<01:04,  3.49it/s] 36%|███▋      | 127/350 [01:01<01:07,  3.29it/s] 37%|███▋      | 128/350 [01:01<01:06,  3.35it/s] 37%|███▋      | 129/350 [01:02<01:05,  3.38it/s] 37%|███▋      | 130/350 [01:02<01:04,  3.41it/s] 37%|███▋      | 131/350 [01:02<01:03,  3.44it/s] 38%|███▊      | 132/350 [01:03<01:03,  3.46it/s] 38%|███▊      | 133/350 [01:03<01:02,  3.47it/s] 38%|███▊      | 134/350 [01:03<01:02,  3.47it/s] 39%|███▊      | 135/350 [01:03<01:01,  3.48it/s] 39%|███▉      | 136/350 [01:04<01:01,  3.49it/s] 39%|███▉      | 137/350 [01:04<01:01,  3.49it/s] 39%|███▉      | 138/350 [01:04<01:03,  3.34it/s] 40%|███▉      | 139/350 [01:05<01:02,  3.39it/s] 40%|████      | 140/350 [01:05<01:01,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 20:46:38,707 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:46:38,707 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 20:46:38,707 >>   Batch size = 8
{'eval_loss': 1.1353026628494263, 'eval_runtime': 9.3003, 'eval_samples_per_second': 375.471, 'eval_steps_per_second': 46.988, 'epoch': 0.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.94it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.79it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.30it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.59it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.28it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.89it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.67it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.32it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.28it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.42it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.41it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.12it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.37it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.34it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.37it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.24it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.18it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.15it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.16it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 45.20it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 45.94it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.44it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.77it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.90it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.06it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.03it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.04it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.98it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.92it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.02it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.18it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.32it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.42it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.28it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.25it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.23it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.08it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.08it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.09it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.18it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.24it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.35it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.37it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.21it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.26it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.16it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.03it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.07it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 41.87it/s][A
 58%|█████▊    | 253/437 [00:05<00:04, 43.43it/s][A
 59%|█████▉    | 258/437 [00:05<00:04, 44.61it/s][A
 60%|██████    | 263/437 [00:05<00:03, 45.45it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 45.97it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.42it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.76it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 47.03it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.72it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.77it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.87it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.02it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.21it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.33it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.36it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.34it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.32it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.19it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.05it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.09it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.14it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.23it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.31it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.27it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.37it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.31it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.27it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.12it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.00it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.19it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.18it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.24it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.23it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.37it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.33it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.19it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.84it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.24it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.24it/s][A 40%|████      | 140/350 [01:14<01:01,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:46:48,231 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-140
[INFO|configuration_utils.py:351] 2023-08-28 20:46:48,271 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-140/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:46:52,477 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-140/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:46:52,719 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:46:52,765 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-140/special_tokens_map.json
 40%|████      | 141/350 [01:29<25:31,  7.33s/it] 41%|████      | 142/350 [01:29<18:08,  5.23s/it] 41%|████      | 143/350 [01:29<12:56,  3.75s/it] 41%|████      | 144/350 [01:30<09:18,  2.71s/it] 41%|████▏     | 145/350 [01:30<06:46,  1.98s/it] 42%|████▏     | 146/350 [01:30<05:00,  1.47s/it] 42%|████▏     | 147/350 [01:30<03:46,  1.12s/it] 42%|████▏     | 148/350 [01:31<02:55,  1.15it/s] 43%|████▎     | 149/350 [01:31<02:19,  1.44it/s] 43%|████▎     | 150/350 [01:31<01:54,  1.75it/s] 43%|████▎     | 151/350 [01:32<01:36,  2.06it/s] 43%|████▎     | 152/350 [01:32<01:24,  2.35it/s] 44%|████▎     | 153/350 [01:32<01:16,  2.57it/s] 44%|████▍     | 154/350 [01:32<01:10,  2.79it/s] 44%|████▍     | 155/350 [01:33<01:05,  2.97it/s] 45%|████▍     | 156/350 [01:33<01:02,  3.11it/s] 45%|████▍     | 157/350 [01:33<00:59,  3.22it/s] 45%|████▌     | 158/350 [01:34<00:58,  3.30it/s] 45%|████▌     | 159/350 [01:34<00:56,  3.35it/s] 46%|████▌     | 160/350 [01:34<00:55,  3.40it/s] 46%|████▌     | 161/350 [01:34<00:55,  3.43it/s] 46%|████▋     | 162/350 [01:35<00:54,  3.44it/s] 47%|████▋     | 163/350 [01:35<00:54,  3.46it/s] 47%|████▋     | 164/350 [01:35<00:57,  3.24it/s] 47%|████▋     | 165/350 [01:36<00:56,  3.26it/s] 47%|████▋     | 166/350 [01:36<00:55,  3.32it/s] 48%|████▊     | 167/350 [01:36<00:54,  3.37it/s] 48%|████▊     | 168/350 [01:36<00:53,  3.41it/s] 48%|████▊     | 169/350 [01:37<00:52,  3.43it/s] 49%|████▊     | 170/350 [01:37<00:52,  3.46it/s] 49%|████▉     | 171/350 [01:37<00:51,  3.47it/s] 49%|████▉     | 172/350 [01:38<00:51,  3.47it/s] 49%|████▉     | 173/350 [01:38<00:50,  3.48it/s] 50%|████▉     | 174/350 [01:38<00:50,  3.49it/s] 50%|█████     | 175/350 [01:39<00:50,  3.43it/s] 50%|█████     | 176/350 [01:39<00:50,  3.45it/s] 51%|█████     | 177/350 [01:39<00:49,  3.47it/s] 51%|█████     | 178/350 [01:39<00:49,  3.48it/s] 51%|█████     | 179/350 [01:40<00:49,  3.48it/s] 51%|█████▏    | 180/350 [01:40<00:48,  3.49it/s] 52%|█████▏    | 181/350 [01:40<00:48,  3.49it/s] 52%|█████▏    | 182/350 [01:41<00:48,  3.49it/s] 52%|█████▏    | 183/350 [01:41<00:47,  3.49it/s] 53%|█████▎    | 184/350 [01:41<00:47,  3.49it/s] 53%|█████▎    | 185/350 [01:41<00:47,  3.49it/s] 53%|█████▎    | 186/350 [01:42<00:47,  3.44it/s] 53%|█████▎    | 187/350 [01:42<00:47,  3.46it/s] 54%|█████▎    | 188/350 [01:42<00:46,  3.47it/s] 54%|█████▍    | 189/350 [01:43<00:46,  3.48it/s] 54%|█████▍    | 190/350 [01:43<00:45,  3.48it/s] 55%|█████▍    | 191/350 [01:43<00:45,  3.49it/s] 55%|█████▍    | 192/350 [01:43<00:45,  3.49it/s] 55%|█████▌    | 193/350 [01:44<00:44,  3.49it/s] 55%|█████▌    | 194/350 [01:44<00:44,  3.49it/s] 56%|█████▌    | 195/350 [01:44<00:44,  3.49it/s] 56%|█████▌    | 196/350 [01:45<00:44,  3.49it/s] 56%|█████▋    | 197/350 [01:45<00:43,  3.49it/s] 57%|█████▋    | 198/350 [01:45<00:43,  3.49it/s] 57%|█████▋    | 199/350 [01:45<00:43,  3.49it/s] 57%|█████▋    | 200/350 [01:46<00:42,  3.49it/s] 57%|█████▋    | 201/350 [01:46<00:42,  3.49it/s] 58%|█████▊    | 202/350 [01:46<00:42,  3.49it/s] 58%|█████▊    | 203/350 [01:47<00:42,  3.49it/s] 58%|█████▊    | 204/350 [01:47<00:41,  3.49it/s] 59%|█████▊    | 205/350 [01:47<00:41,  3.47it/s] 59%|█████▉    | 206/350 [01:47<00:41,  3.47it/s] 59%|█████▉    | 207/350 [01:48<00:41,  3.48it/s] 59%|█████▉    | 208/350 [01:48<00:40,  3.48it/s] 60%|█████▉    | 209/350 [01:48<00:40,  3.49it/s] 60%|██████    | 210/350 [01:49<00:40,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 20:47:22,396 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:47:22,397 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 20:47:22,397 >>   Batch size = 8
{'eval_loss': 1.163529634475708, 'eval_runtime': 9.3459, 'eval_samples_per_second': 373.638, 'eval_steps_per_second': 46.758, 'epoch': 1.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.20it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.32it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.57it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.79it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.40it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.08it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.91it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.43it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.34it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.41it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.47it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.43it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.48it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.42it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.50it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.52it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.31it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.33it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.26it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.28it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.35it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.43it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.42it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.47it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 44.76it/s][A
 30%|███       | 133/437 [00:02<00:06, 45.60it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.08it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.53it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.81it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.99it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.07it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.22it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.20it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.95it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.02it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.15it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.20it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.22it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.97it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.45it/s][A
 48%|████▊     | 208/437 [00:04<00:05, 44.50it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 45.34it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.03it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.44it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.67it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.86it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.92it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.05it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.92it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.85it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.93it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.13it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.26it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.29it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.28it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.30it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.29it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.07it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.96it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.04it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.99it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.20it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.32it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.32it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.29it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.28it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.02it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.01it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.95it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.97it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.06it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.21it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.25it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.31it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.25it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.17it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.08it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.09it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.00it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.02it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.15it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.29it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.32it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.26it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.21it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.05it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.05it/s][A 60%|██████    | 210/350 [01:58<00:40,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:47:31,748 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-210
[INFO|configuration_utils.py:351] 2023-08-28 20:47:31,776 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-210/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:47:35,924 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-210/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:47:35,951 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-210/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:47:35,964 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-210/special_tokens_map.json
 60%|██████    | 211/350 [02:12<16:35,  7.16s/it] 61%|██████    | 212/350 [02:12<11:44,  5.10s/it] 61%|██████    | 213/350 [02:12<08:21,  3.66s/it] 61%|██████    | 214/350 [02:13<05:59,  2.65s/it] 61%|██████▏   | 215/350 [02:13<04:21,  1.94s/it] 62%|██████▏   | 216/350 [02:13<03:13,  1.44s/it] 62%|██████▏   | 217/350 [02:13<02:25,  1.10s/it] 62%|██████▏   | 218/350 [02:14<01:52,  1.17it/s] 63%|██████▎   | 219/350 [02:14<01:29,  1.46it/s] 63%|██████▎   | 220/350 [02:14<01:13,  1.77it/s] 63%|██████▎   | 221/350 [02:15<01:01,  2.08it/s] 63%|██████▎   | 222/350 [02:15<00:54,  2.37it/s] 64%|██████▎   | 223/350 [02:15<00:48,  2.62it/s] 64%|██████▍   | 224/350 [02:15<00:44,  2.83it/s] 64%|██████▍   | 225/350 [02:16<00:41,  3.01it/s] 65%|██████▍   | 226/350 [02:16<00:39,  3.14it/s] 65%|██████▍   | 227/350 [02:16<00:38,  3.23it/s] 65%|██████▌   | 228/350 [02:17<00:36,  3.31it/s] 65%|██████▌   | 229/350 [02:17<00:35,  3.36it/s] 66%|██████▌   | 230/350 [02:17<00:35,  3.40it/s] 66%|██████▌   | 231/350 [02:17<00:34,  3.43it/s] 66%|██████▋   | 232/350 [02:18<00:35,  3.35it/s] 67%|██████▋   | 233/350 [02:18<00:34,  3.39it/s] 67%|██████▋   | 234/350 [02:18<00:33,  3.42it/s] 67%|██████▋   | 235/350 [02:19<00:33,  3.44it/s] 67%|██████▋   | 236/350 [02:19<00:33,  3.42it/s] 68%|██████▊   | 237/350 [02:19<00:32,  3.44it/s] 68%|██████▊   | 238/350 [02:20<00:32,  3.46it/s] 68%|██████▊   | 239/350 [02:20<00:33,  3.33it/s] 69%|██████▊   | 240/350 [02:20<00:32,  3.37it/s] 69%|██████▉   | 241/350 [02:20<00:32,  3.40it/s] 69%|██████▉   | 242/350 [02:21<00:31,  3.43it/s] 69%|██████▉   | 243/350 [02:21<00:32,  3.29it/s] 70%|██████▉   | 244/350 [02:21<00:31,  3.35it/s] 70%|███████   | 245/350 [02:22<00:30,  3.39it/s] 70%|███████   | 246/350 [02:22<00:30,  3.42it/s] 71%|███████   | 247/350 [02:22<00:29,  3.44it/s] 71%|███████   | 248/350 [02:22<00:29,  3.45it/s] 71%|███████   | 249/350 [02:23<00:29,  3.46it/s] 71%|███████▏  | 250/350 [02:23<00:28,  3.47it/s] 72%|███████▏  | 251/350 [02:23<00:28,  3.47it/s] 72%|███████▏  | 252/350 [02:24<00:28,  3.48it/s] 72%|███████▏  | 253/350 [02:24<00:27,  3.48it/s] 73%|███████▎  | 254/350 [02:24<00:27,  3.48it/s] 73%|███████▎  | 255/350 [02:24<00:27,  3.48it/s] 73%|███████▎  | 256/350 [02:25<00:26,  3.48it/s] 73%|███████▎  | 257/350 [02:25<00:26,  3.49it/s] 74%|███████▎  | 258/350 [02:25<00:26,  3.49it/s] 74%|███████▍  | 259/350 [02:26<00:26,  3.49it/s] 74%|███████▍  | 260/350 [02:26<00:25,  3.48it/s] 75%|███████▍  | 261/350 [02:26<00:25,  3.48it/s] 75%|███████▍  | 262/350 [02:26<00:25,  3.48it/s] 75%|███████▌  | 263/350 [02:27<00:24,  3.48it/s] 75%|███████▌  | 264/350 [02:27<00:24,  3.48it/s] 76%|███████▌  | 265/350 [02:27<00:25,  3.30it/s] 76%|███████▌  | 266/350 [02:28<00:25,  3.35it/s] 76%|███████▋  | 267/350 [02:28<00:24,  3.39it/s] 77%|███████▋  | 268/350 [02:28<00:24,  3.39it/s] 77%|███████▋  | 269/350 [02:29<00:23,  3.42it/s] 77%|███████▋  | 270/350 [02:29<00:23,  3.44it/s] 77%|███████▋  | 271/350 [02:29<00:22,  3.46it/s] 78%|███████▊  | 272/350 [02:29<00:22,  3.46it/s] 78%|███████▊  | 273/350 [02:30<00:22,  3.47it/s] 78%|███████▊  | 274/350 [02:30<00:21,  3.47it/s] 79%|███████▊  | 275/350 [02:30<00:21,  3.48it/s] 79%|███████▉  | 276/350 [02:31<00:21,  3.38it/s] 79%|███████▉  | 277/350 [02:31<00:21,  3.41it/s] 79%|███████▉  | 278/350 [02:31<00:20,  3.43it/s] 80%|███████▉  | 279/350 [02:31<00:20,  3.45it/s] 80%|████████  | 280/350 [02:32<00:20,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 20:48:05,601 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:48:05,601 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 20:48:05,601 >>   Batch size = 8
{'eval_loss': 1.1784240007400513, 'eval_runtime': 9.3233, 'eval_samples_per_second': 374.546, 'eval_steps_per_second': 46.872, 'epoch': 2.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.98it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.97it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.33it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.62it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.22it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.86it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.74it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.51it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.43it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.46it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.52it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.39it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.25it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.37it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.38it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.32it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.27it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.27it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.24it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.28it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.17it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.19it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.22it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.30it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.30it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.18it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.19it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.26it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.25it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.28it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.38it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.25it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.29it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.34it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.29it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.47it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.78it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.91it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.97it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.07it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.13it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.01it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.08it/s][A
 51%|█████     | 223/437 [00:04<00:04, 44.05it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 44.96it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 45.70it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.26it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.53it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.72it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.96it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.94it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.77it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.84it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.90it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.98it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.17it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.25it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.27it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.29it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.22it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.10it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.00it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.98it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.09it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.15it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.27it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.32it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.38it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.31it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.24it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.14it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.05it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.11it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.36it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.75it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.89it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.00it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.05it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.11it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.06it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.86it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.03it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.08it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.06it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.25it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.31it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.31it/s][A 80%|████████  | 280/350 [02:41<00:20,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:48:14,995 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-280
[INFO|configuration_utils.py:351] 2023-08-28 20:48:15,077 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-280/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:48:18,791 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-280/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:48:18,838 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:48:18,850 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-280/special_tokens_map.json
 80%|████████  | 281/350 [02:54<07:51,  6.83s/it] 81%|████████  | 282/350 [02:54<05:31,  4.87s/it] 81%|████████  | 283/350 [02:54<03:54,  3.50s/it] 81%|████████  | 284/350 [02:55<02:47,  2.53s/it] 81%|████████▏ | 285/350 [02:55<02:00,  1.86s/it] 82%|████████▏ | 286/350 [02:55<01:28,  1.39s/it] 82%|████████▏ | 287/350 [02:56<01:06,  1.06s/it] 82%|████████▏ | 288/350 [02:56<00:51,  1.21it/s] 83%|████████▎ | 289/350 [02:56<00:40,  1.51it/s] 83%|████████▎ | 290/350 [02:56<00:33,  1.82it/s] 83%|████████▎ | 291/350 [02:57<00:27,  2.12it/s] 83%|████████▎ | 292/350 [02:57<00:24,  2.41it/s] 84%|████████▎ | 293/350 [02:57<00:21,  2.65it/s] 84%|████████▍ | 294/350 [02:58<00:19,  2.85it/s] 84%|████████▍ | 295/350 [02:58<00:18,  3.02it/s] 85%|████████▍ | 296/350 [02:58<00:17,  3.15it/s] 85%|████████▍ | 297/350 [02:58<00:16,  3.24it/s] 85%|████████▌ | 298/350 [02:59<00:15,  3.32it/s] 85%|████████▌ | 299/350 [02:59<00:15,  3.37it/s] 86%|████████▌ | 300/350 [02:59<00:14,  3.40it/s] 86%|████████▌ | 301/350 [03:00<00:14,  3.43it/s] 86%|████████▋ | 302/350 [03:00<00:13,  3.45it/s] 87%|████████▋ | 303/350 [03:00<00:13,  3.46it/s] 87%|████████▋ | 304/350 [03:00<00:13,  3.46it/s] 87%|████████▋ | 305/350 [03:01<00:12,  3.47it/s] 87%|████████▋ | 306/350 [03:01<00:12,  3.47it/s] 88%|████████▊ | 307/350 [03:01<00:12,  3.48it/s] 88%|████████▊ | 308/350 [03:02<00:12,  3.49it/s] 88%|████████▊ | 309/350 [03:02<00:11,  3.49it/s] 89%|████████▊ | 310/350 [03:02<00:11,  3.49it/s] 89%|████████▉ | 311/350 [03:02<00:11,  3.49it/s] 89%|████████▉ | 312/350 [03:03<00:10,  3.49it/s] 89%|████████▉ | 313/350 [03:03<00:10,  3.49it/s] 90%|████████▉ | 314/350 [03:03<00:10,  3.49it/s] 90%|█████████ | 315/350 [03:04<00:10,  3.45it/s] 90%|█████████ | 316/350 [03:04<00:09,  3.46it/s] 91%|█████████ | 317/350 [03:04<00:09,  3.47it/s] 91%|█████████ | 318/350 [03:04<00:09,  3.47it/s] 91%|█████████ | 319/350 [03:05<00:08,  3.48it/s] 91%|█████████▏| 320/350 [03:05<00:08,  3.48it/s] 92%|█████████▏| 321/350 [03:05<00:08,  3.48it/s] 92%|█████████▏| 322/350 [03:06<00:08,  3.48it/s] 92%|█████████▏| 323/350 [03:06<00:07,  3.48it/s] 93%|█████████▎| 324/350 [03:06<00:07,  3.49it/s] 93%|█████████▎| 325/350 [03:06<00:07,  3.49it/s] 93%|█████████▎| 326/350 [03:07<00:06,  3.43it/s] 93%|█████████▎| 327/350 [03:07<00:06,  3.45it/s] 94%|█████████▎| 328/350 [03:07<00:06,  3.46it/s] 94%|█████████▍| 329/350 [03:08<00:06,  3.47it/s] 94%|█████████▍| 330/350 [03:08<00:05,  3.47it/s] 95%|█████████▍| 331/350 [03:08<00:05,  3.48it/s] 95%|█████████▍| 332/350 [03:09<00:05,  3.48it/s] 95%|█████████▌| 333/350 [03:09<00:04,  3.49it/s] 95%|█████████▌| 334/350 [03:09<00:04,  3.49it/s] 96%|█████████▌| 335/350 [03:09<00:04,  3.49it/s] 96%|█████████▌| 336/350 [03:10<00:04,  3.49it/s] 96%|█████████▋| 337/350 [03:10<00:03,  3.46it/s] 97%|█████████▋| 338/350 [03:10<00:03,  3.47it/s] 97%|█████████▋| 339/350 [03:11<00:03,  3.47it/s] 97%|█████████▋| 340/350 [03:11<00:02,  3.48it/s] 97%|█████████▋| 341/350 [03:11<00:02,  3.48it/s] 98%|█████████▊| 342/350 [03:11<00:02,  3.48it/s] 98%|█████████▊| 343/350 [03:12<00:02,  3.49it/s] 98%|█████████▊| 344/350 [03:12<00:01,  3.49it/s] 99%|█████████▊| 345/350 [03:12<00:01,  3.49it/s] 99%|█████████▉| 346/350 [03:13<00:01,  3.49it/s] 99%|█████████▉| 347/350 [03:13<00:00,  3.49it/s] 99%|█████████▉| 348/350 [03:13<00:00,  3.49it/s]100%|█████████▉| 349/350 [03:13<00:00,  3.49it/s]100%|██████████| 350/350 [03:14<00:00,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 20:48:47,483 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:48:47,483 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 20:48:47,483 >>   Batch size = 8
{'eval_loss': 1.1861199140548706, 'eval_runtime': 9.326, 'eval_samples_per_second': 374.438, 'eval_steps_per_second': 46.858, 'epoch': 3.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.61it/s][A
  3%|▎         | 12/437 [00:00<00:11, 35.98it/s][A
  4%|▍         | 17/437 [00:00<00:10, 39.96it/s][A
  5%|▌         | 22/437 [00:00<00:09, 42.44it/s][A
  6%|▌         | 27/437 [00:00<00:09, 43.99it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.05it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.58it/s][A
 10%|▉         | 42/437 [00:00<00:08, 45.93it/s][A
 11%|█         | 47/437 [00:01<00:08, 46.21it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 46.54it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 46.67it/s][A
 14%|█▍        | 62/437 [00:01<00:07, 46.95it/s][A
 15%|█▌        | 67/437 [00:01<00:07, 47.08it/s][A
 16%|█▋        | 72/437 [00:01<00:07, 47.18it/s][A
 18%|█▊        | 77/437 [00:01<00:07, 47.24it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 47.27it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 47.16it/s][A
 21%|██        | 92/437 [00:02<00:07, 47.19it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 47.17it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 47.20it/s][A
 24%|██▍       | 107/437 [00:02<00:06, 47.33it/s][A
 26%|██▌       | 112/437 [00:02<00:06, 47.29it/s][A
 27%|██▋       | 117/437 [00:02<00:06, 47.14it/s][A
 28%|██▊       | 122/437 [00:02<00:06, 47.20it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 47.24it/s][A
 30%|███       | 132/437 [00:02<00:06, 47.20it/s][A
 31%|███▏      | 137/437 [00:02<00:06, 47.09it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 47.11it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 47.05it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 46.79it/s][A
 36%|███▌      | 157/437 [00:03<00:05, 47.23it/s][A
 37%|███▋      | 162/437 [00:03<00:05, 47.14it/s][A
 38%|███▊      | 167/437 [00:03<00:05, 47.09it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 47.20it/s][A
 41%|████      | 177/437 [00:03<00:05, 47.17it/s][A
 42%|████▏     | 182/437 [00:03<00:05, 47.07it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 47.11it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 47.02it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 47.03it/s][A
 46%|████▌     | 202/437 [00:04<00:04, 47.06it/s][A
 47%|████▋     | 207/437 [00:04<00:04, 47.19it/s][A
 49%|████▊     | 212/437 [00:04<00:04, 47.04it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 47.15it/s][A
 51%|█████     | 222/437 [00:04<00:04, 47.22it/s][A
 52%|█████▏    | 227/437 [00:04<00:04, 47.16it/s][A
 53%|█████▎    | 232/437 [00:04<00:04, 47.09it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 47.08it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 47.03it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 47.06it/s][A
 58%|█████▊    | 252/437 [00:05<00:03, 47.09it/s][A
 59%|█████▉    | 257/437 [00:05<00:03, 47.13it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 47.08it/s][A
 61%|██████    | 267/437 [00:05<00:03, 47.10it/s][A
 62%|██████▏   | 272/437 [00:05<00:03, 47.09it/s][A
 63%|██████▎   | 277/437 [00:05<00:03, 47.07it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 47.08it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 42.72it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 39.75it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 41.81it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.29it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.42it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 45.34it/s][A
 73%|███████▎  | 317/437 [00:06<00:02, 45.91it/s][A
 74%|███████▎  | 322/437 [00:06<00:02, 46.31it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 46.70it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 46.47it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 46.59it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 46.72it/s][A
 79%|███████▉  | 347/437 [00:07<00:01, 46.85it/s][A
 81%|████████  | 352/437 [00:07<00:01, 46.87it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 47.12it/s][A
 83%|████████▎ | 362/437 [00:07<00:01, 47.18it/s][A
 84%|████████▍ | 367/437 [00:07<00:01, 47.22it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 47.32it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 47.08it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 46.92it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 46.92it/s][A
 90%|████████▉ | 392/437 [00:08<00:00, 46.98it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 47.01it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 47.07it/s][A
 93%|█████████▎| 407/437 [00:08<00:00, 47.17it/s][A
 94%|█████████▍| 412/437 [00:08<00:00, 47.14it/s][A
 95%|█████████▌| 417/437 [00:08<00:00, 47.22it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 47.23it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 47.09it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 46.32it/s][A
100%|██████████| 437/437 [00:09<00:00, 46.61it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.61it/s][A100%|██████████| 350/350 [03:23<00:00,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:48:56,955 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-350
[INFO|configuration_utils.py:351] 2023-08-28 20:48:56,997 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-350/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:49:00,990 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-350/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:49:01,045 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-350/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:49:01,086 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-350/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 20:49:09,526 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 20:49:09,541 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-70 (score: 1.1353026628494263).
                                                 100%|██████████| 350/350 [03:41<00:00,  3.49it/s]100%|██████████| 350/350 [03:41<00:00,  1.58it/s]
[INFO|trainer.py:1894] 2023-08-28 20:49:15,214 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 20:49:15,248 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:49:20,398 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:49:20,501 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:49:20,558 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:49:20,868 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:20,869 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:20,869 >>   train_loss               =      0.385
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:20,869 >>   train_runtime            = 0:03:41.78
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:20,869 >>   train_samples            =       4500
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:20,869 >>   train_samples_per_second =    101.447
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:20,869 >>   train_steps_per_second   =      1.578
{'eval_loss': 1.1937366724014282, 'eval_runtime': 9.4288, 'eval_samples_per_second': 370.356, 'eval_steps_per_second': 46.348, 'epoch': 4.99}
{'train_runtime': 221.7899, 'train_samples_per_second': 101.447, 'train_steps_per_second': 1.578, 'train_loss': 0.3850374058314732, 'epoch': 4.99}
08/28/2023 20:49:20 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 20:49:20,972 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:49:20,972 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 20:49:20,972 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 59.29it/s]  3%|▎         | 12/437 [00:00<00:08, 52.02it/s]  4%|▍         | 18/437 [00:00<00:08, 49.92it/s]  5%|▌         | 24/437 [00:00<00:08, 48.97it/s]  7%|▋         | 29/437 [00:00<00:08, 48.62it/s]  8%|▊         | 34/437 [00:00<00:08, 48.28it/s]  9%|▉         | 39/437 [00:00<00:08, 48.05it/s] 10%|█         | 44/437 [00:00<00:08, 47.97it/s] 11%|█         | 49/437 [00:01<00:08, 47.72it/s] 12%|█▏        | 54/437 [00:01<00:08, 47.73it/s] 14%|█▎        | 59/437 [00:01<00:07, 47.71it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.66it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.62it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.54it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.58it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.66it/s] 20%|██        | 89/437 [00:01<00:07, 47.53it/s] 22%|██▏       | 94/437 [00:01<00:07, 47.47it/s] 23%|██▎       | 99/437 [00:02<00:07, 47.21it/s] 24%|██▍       | 104/437 [00:02<00:07, 47.49it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.47it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.44it/s] 27%|██▋       | 119/437 [00:02<00:06, 47.46it/s] 28%|██▊       | 124/437 [00:02<00:06, 45.78it/s] 30%|██▉       | 129/437 [00:02<00:06, 46.40it/s] 31%|███       | 134/437 [00:02<00:06, 46.81it/s] 32%|███▏      | 139/437 [00:02<00:06, 46.97it/s] 33%|███▎      | 144/437 [00:03<00:06, 47.17it/s] 34%|███▍      | 149/437 [00:03<00:06, 47.18it/s] 35%|███▌      | 154/437 [00:03<00:05, 47.23it/s] 36%|███▋      | 159/437 [00:03<00:05, 47.25it/s] 38%|███▊      | 164/437 [00:03<00:05, 47.28it/s] 39%|███▊      | 169/437 [00:03<00:05, 47.27it/s] 40%|███▉      | 174/437 [00:03<00:05, 47.30it/s] 41%|████      | 179/437 [00:03<00:05, 47.39it/s] 42%|████▏     | 184/437 [00:03<00:05, 47.48it/s] 43%|████▎     | 189/437 [00:03<00:05, 47.52it/s] 44%|████▍     | 194/437 [00:04<00:05, 47.52it/s] 46%|████▌     | 199/437 [00:04<00:05, 47.43it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.42it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.30it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.30it/s] 50%|█████     | 219/437 [00:04<00:04, 47.43it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.31it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.41it/s] 54%|█████▎    | 234/437 [00:04<00:04, 47.48it/s] 55%|█████▍    | 239/437 [00:05<00:04, 47.46it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.49it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.45it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.47it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.45it/s] 60%|██████    | 264/437 [00:05<00:03, 47.40it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.43it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.47it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.55it/s] 65%|██████▍   | 284/437 [00:05<00:03, 47.54it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.50it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.48it/s] 68%|██████▊   | 299/437 [00:06<00:02, 46.61it/s] 70%|██████▉   | 304/437 [00:06<00:02, 46.91it/s] 71%|███████   | 309/437 [00:06<00:02, 47.11it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.21it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.30it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.37it/s] 75%|███████▌  | 329/437 [00:06<00:02, 47.37it/s] 76%|███████▋  | 334/437 [00:07<00:02, 47.46it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.38it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.43it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.41it/s] 81%|████████  | 354/437 [00:07<00:01, 47.30it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.39it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.47it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.48it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.31it/s] 87%|████████▋ | 379/437 [00:07<00:01, 47.42it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.35it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.29it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.37it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.39it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.39it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.39it/s] 95%|█████████▍| 414/437 [00:08<00:00, 46.37it/s] 96%|█████████▌| 419/437 [00:08<00:00, 46.70it/s] 97%|█████████▋| 424/437 [00:08<00:00, 47.00it/s] 98%|█████████▊| 429/437 [00:09<00:00, 47.19it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.19it/s]100%|██████████| 437/437 [00:09<00:00, 47.45it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:49:30,205 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:30,205 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:30,205 >>   eval_loss               =     1.1353
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:30,205 >>   eval_runtime            = 0:00:09.23
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:30,205 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:30,205 >>   eval_samples_per_second =    378.206
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:30,206 >>   eval_steps_per_second   =      47.33
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:30,206 >>   perplexity              =     3.1121
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:36,477 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:36,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:36,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:36,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:36,486 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:49:36,793 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:49:36,794 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:49:37,072 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:49:38,136 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:49:38,137 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:39,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:39,851 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:39,852 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:39,852 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:39,852 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:49:40,278 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:49:40,279 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:49:40,544 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:49:40,697 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:49:40,697 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-140
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-350
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-210
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-70
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-280
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.49it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:11,  1.48it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:17,  1.54it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:23,  1.41it/s]Extractor Predicting: 37it [00:24,  1.44it/s]Extractor Predicting: 38it [00:25,  1.46it/s]Extractor Predicting: 39it [00:25,  1.48it/s]Extractor Predicting: 40it [00:26,  1.51it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:27,  1.49it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:29,  1.50it/s]Extractor Predicting: 46it [00:30,  1.49it/s]Extractor Predicting: 47it [00:31,  1.51it/s]Extractor Predicting: 48it [00:31,  1.51it/s]Extractor Predicting: 49it [00:32,  1.51it/s]Extractor Predicting: 50it [00:33,  1.53it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:35,  1.49it/s]Extractor Predicting: 55it [00:36,  1.50it/s]Extractor Predicting: 56it [00:37,  1.51it/s]Extractor Predicting: 57it [00:37,  1.51it/s]Extractor Predicting: 58it [00:38,  1.53it/s]Extractor Predicting: 59it [00:39,  1.51it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:40,  1.48it/s]Extractor Predicting: 62it [00:41,  1.48it/s]Extractor Predicting: 63it [00:41,  1.50it/s]Extractor Predicting: 64it [00:42,  1.46it/s]Extractor Predicting: 65it [00:43,  1.47it/s]Extractor Predicting: 66it [00:43,  1.44it/s]Extractor Predicting: 67it [00:44,  1.45it/s]Extractor Predicting: 68it [00:45,  1.44it/s]Extractor Predicting: 69it [00:45,  1.44it/s]Extractor Predicting: 70it [00:46,  1.44it/s]Extractor Predicting: 71it [00:47,  1.44it/s]Extractor Predicting: 72it [00:48,  1.42it/s]Extractor Predicting: 73it [00:48,  1.43it/s]Extractor Predicting: 74it [00:49,  1.40it/s]Extractor Predicting: 75it [00:50,  1.44it/s]Extractor Predicting: 76it [00:50,  1.45it/s]Extractor Predicting: 77it [00:51,  1.43it/s]Extractor Predicting: 78it [00:52,  1.46it/s]Extractor Predicting: 79it [00:52,  1.43it/s]Extractor Predicting: 80it [00:53,  1.40it/s]Extractor Predicting: 81it [00:54,  1.40it/s]Extractor Predicting: 82it [00:55,  1.44it/s]Extractor Predicting: 83it [00:55,  1.45it/s]Extractor Predicting: 84it [00:56,  1.47it/s]Extractor Predicting: 85it [00:57,  1.46it/s]Extractor Predicting: 86it [00:57,  1.43it/s]Extractor Predicting: 87it [00:58,  1.45it/s]Extractor Predicting: 88it [00:59,  1.48it/s]Extractor Predicting: 89it [00:59,  1.53it/s]Extractor Predicting: 90it [01:00,  1.53it/s]Extractor Predicting: 91it [01:00,  1.58it/s]Extractor Predicting: 92it [01:01,  1.64it/s]Extractor Predicting: 93it [01:02,  1.67it/s]Extractor Predicting: 94it [01:02,  1.69it/s]Extractor Predicting: 95it [01:03,  1.64it/s]Extractor Predicting: 96it [01:03,  1.66it/s]Extractor Predicting: 97it [01:04,  1.62it/s]Extractor Predicting: 98it [01:05,  1.61it/s]Extractor Predicting: 99it [01:05,  1.67it/s]Extractor Predicting: 100it [01:06,  1.68it/s]Extractor Predicting: 101it [01:06,  1.65it/s]Extractor Predicting: 102it [01:07,  1.60it/s]Extractor Predicting: 103it [01:08,  1.60it/s]Extractor Predicting: 104it [01:08,  1.56it/s]Extractor Predicting: 105it [01:09,  1.57it/s]Extractor Predicting: 106it [01:10,  1.59it/s]Extractor Predicting: 107it [01:10,  1.56it/s]Extractor Predicting: 108it [01:11,  1.60it/s]Extractor Predicting: 109it [01:11,  1.61it/s]Extractor Predicting: 110it [01:12,  1.63it/s]Extractor Predicting: 111it [01:13,  1.64it/s]Extractor Predicting: 112it [01:13,  1.64it/s]Extractor Predicting: 113it [01:14,  1.67it/s]Extractor Predicting: 114it [01:15,  1.62it/s]Extractor Predicting: 115it [01:15,  1.65it/s]Extractor Predicting: 116it [01:16,  1.61it/s]Extractor Predicting: 117it [01:16,  1.58it/s]Extractor Predicting: 118it [01:17,  1.44it/s]Extractor Predicting: 119it [01:18,  1.45it/s]Extractor Predicting: 120it [01:19,  1.44it/s]Extractor Predicting: 121it [01:19,  1.46it/s]Extractor Predicting: 122it [01:20,  1.47it/s]Extractor Predicting: 123it [01:21,  1.47it/s]Extractor Predicting: 124it [01:21,  1.45it/s]Extractor Predicting: 125it [01:22,  1.45it/s]Extractor Predicting: 126it [01:23,  1.44it/s]Extractor Predicting: 127it [01:23,  1.45it/s]Extractor Predicting: 128it [01:24,  1.43it/s]Extractor Predicting: 129it [01:25,  1.46it/s]Extractor Predicting: 130it [01:25,  1.49it/s]Extractor Predicting: 131it [01:26,  1.47it/s]Extractor Predicting: 132it [01:27,  1.47it/s]Extractor Predicting: 133it [01:28,  1.46it/s]Extractor Predicting: 134it [01:28,  1.47it/s]Extractor Predicting: 135it [01:29,  1.48it/s]Extractor Predicting: 136it [01:30,  1.47it/s]Extractor Predicting: 137it [01:30,  1.47it/s]Extractor Predicting: 138it [01:31,  1.50it/s]Extractor Predicting: 139it [01:32,  1.50it/s]Extractor Predicting: 140it [01:32,  1.53it/s]Extractor Predicting: 141it [01:33,  1.49it/s]Extractor Predicting: 142it [01:34,  1.48it/s]Extractor Predicting: 143it [01:34,  1.49it/s]Extractor Predicting: 144it [01:35,  1.52it/s]Extractor Predicting: 144it [01:35,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:24,725 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:24,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:24,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:24,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:24,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:51:25,059 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:51:25,060 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:51:25,356 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:51:26,447 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:51:26,447 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:29,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:29,056 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:29,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:29,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:29,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:51:29,897 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:51:29,898 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:51:30,169 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:51:30,323 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:51:30,323 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.37323511356660527,
  "recall": 0.17411225658648338,
  "score": 0.23745362233938683,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.46it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.51it/s]Extractor Predicting: 28it [00:18,  1.51it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:19,  1.46it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:21,  1.47it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:23,  1.51it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.59it/s]Extractor Predicting: 38it [00:25,  1.59it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:26,  1.57it/s]Extractor Predicting: 41it [00:27,  1.57it/s]Extractor Predicting: 42it [00:27,  1.56it/s]Extractor Predicting: 43it [00:28,  1.56it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:29,  1.54it/s]Extractor Predicting: 46it [00:30,  1.57it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:31,  1.57it/s]Extractor Predicting: 49it [00:32,  1.56it/s]Extractor Predicting: 50it [00:32,  1.55it/s]Extractor Predicting: 51it [00:33,  1.55it/s]Extractor Predicting: 52it [00:34,  1.55it/s]Extractor Predicting: 53it [00:34,  1.55it/s]Extractor Predicting: 54it [00:35,  1.55it/s]Extractor Predicting: 55it [00:36,  1.52it/s]Extractor Predicting: 56it [00:36,  1.54it/s]Extractor Predicting: 57it [00:37,  1.57it/s]Extractor Predicting: 58it [00:37,  1.60it/s]Extractor Predicting: 59it [00:38,  1.63it/s]Extractor Predicting: 60it [00:39,  1.61it/s]Extractor Predicting: 61it [00:39,  1.59it/s]Extractor Predicting: 62it [00:40,  1.59it/s]Extractor Predicting: 63it [00:41,  1.59it/s]Extractor Predicting: 64it [00:41,  1.55it/s]Extractor Predicting: 65it [00:42,  1.56it/s]Extractor Predicting: 66it [00:43,  1.55it/s]Extractor Predicting: 67it [00:43,  1.56it/s]Extractor Predicting: 68it [00:44,  1.62it/s]Extractor Predicting: 69it [00:44,  1.59it/s]Extractor Predicting: 70it [00:45,  1.59it/s]Extractor Predicting: 71it [00:46,  1.58it/s]Extractor Predicting: 72it [00:46,  1.58it/s]Extractor Predicting: 73it [00:47,  1.54it/s]Extractor Predicting: 74it [00:48,  1.54it/s]Extractor Predicting: 75it [00:48,  1.55it/s]Extractor Predicting: 76it [00:49,  1.41it/s]Extractor Predicting: 77it [00:50,  1.43it/s]Extractor Predicting: 78it [00:50,  1.45it/s]Extractor Predicting: 79it [00:51,  1.48it/s]Extractor Predicting: 80it [00:52,  1.49it/s]Extractor Predicting: 81it [00:52,  1.50it/s]Extractor Predicting: 82it [00:53,  1.49it/s]Extractor Predicting: 83it [00:54,  1.51it/s]Extractor Predicting: 84it [00:54,  1.51it/s]Extractor Predicting: 85it [00:55,  1.55it/s]Extractor Predicting: 86it [00:56,  1.52it/s]Extractor Predicting: 87it [00:56,  1.53it/s]Extractor Predicting: 88it [00:57,  1.52it/s]Extractor Predicting: 89it [00:58,  1.54it/s]Extractor Predicting: 90it [00:58,  1.54it/s]Extractor Predicting: 91it [00:59,  1.54it/s]Extractor Predicting: 92it [01:00,  1.51it/s]Extractor Predicting: 93it [01:00,  1.54it/s]Extractor Predicting: 94it [01:01,  1.52it/s]Extractor Predicting: 95it [01:02,  1.52it/s]Extractor Predicting: 96it [01:02,  1.53it/s]Extractor Predicting: 97it [01:03,  1.54it/s]Extractor Predicting: 98it [01:04,  1.54it/s]Extractor Predicting: 99it [01:04,  1.56it/s]Extractor Predicting: 100it [01:05,  1.58it/s]Extractor Predicting: 101it [01:05,  1.58it/s]Extractor Predicting: 102it [01:06,  1.58it/s]Extractor Predicting: 103it [01:07,  1.53it/s]Extractor Predicting: 104it [01:07,  1.51it/s]Extractor Predicting: 105it [01:08,  1.52it/s]Extractor Predicting: 106it [01:09,  1.52it/s]Extractor Predicting: 107it [01:09,  1.51it/s]Extractor Predicting: 108it [01:10,  1.53it/s]Extractor Predicting: 109it [01:11,  1.52it/s]Extractor Predicting: 110it [01:11,  1.50it/s]Extractor Predicting: 111it [01:12,  1.49it/s]Extractor Predicting: 112it [01:13,  1.49it/s]Extractor Predicting: 113it [01:13,  1.48it/s]Extractor Predicting: 114it [01:14,  1.48it/s]Extractor Predicting: 115it [01:15,  1.49it/s]Extractor Predicting: 116it [01:15,  1.54it/s]Extractor Predicting: 117it [01:16,  1.55it/s]Extractor Predicting: 118it [01:17,  1.55it/s]Extractor Predicting: 119it [01:17,  1.55it/s]Extractor Predicting: 120it [01:18,  1.55it/s]Extractor Predicting: 121it [01:19,  1.57it/s]Extractor Predicting: 122it [01:19,  1.58it/s]Extractor Predicting: 123it [01:20,  1.60it/s]Extractor Predicting: 124it [01:20,  1.61it/s]Extractor Predicting: 125it [01:21,  1.60it/s]Extractor Predicting: 126it [01:22,  1.59it/s]Extractor Predicting: 127it [01:22,  1.60it/s]Extractor Predicting: 128it [01:23,  1.58it/s]Extractor Predicting: 129it [01:24,  1.55it/s]Extractor Predicting: 130it [01:24,  1.54it/s]Extractor Predicting: 131it [01:25,  1.58it/s]Extractor Predicting: 132it [01:25,  1.57it/s]Extractor Predicting: 133it [01:26,  1.59it/s]Extractor Predicting: 134it [01:27,  1.56it/s]Extractor Predicting: 135it [01:27,  1.58it/s]Extractor Predicting: 136it [01:28,  1.57it/s]Extractor Predicting: 137it [01:29,  1.58it/s]Extractor Predicting: 138it [01:29,  1.55it/s]Extractor Predicting: 139it [01:30,  1.57it/s]Extractor Predicting: 140it [01:31,  1.62it/s]Extractor Predicting: 141it [01:31,  1.63it/s]Extractor Predicting: 142it [01:32,  1.58it/s]Extractor Predicting: 143it [01:32,  1.57it/s]Extractor Predicting: 144it [01:33,  1.57it/s]Extractor Predicting: 145it [01:34,  1.59it/s]Extractor Predicting: 146it [01:34,  1.61it/s]Extractor Predicting: 147it [01:35,  1.62it/s]Extractor Predicting: 148it [01:36,  1.57it/s]Extractor Predicting: 149it [01:36,  1.59it/s]Extractor Predicting: 150it [01:37,  1.56it/s]Extractor Predicting: 151it [01:38,  1.56it/s]Extractor Predicting: 152it [01:38,  1.57it/s]Extractor Predicting: 153it [01:39,  1.40it/s]Extractor Predicting: 154it [01:40,  1.46it/s]Extractor Predicting: 155it [01:40,  1.50it/s]Extractor Predicting: 156it [01:41,  1.55it/s]Extractor Predicting: 157it [01:41,  1.57it/s]Extractor Predicting: 158it [01:42,  1.63it/s]Extractor Predicting: 159it [01:43,  1.60it/s]Extractor Predicting: 160it [01:43,  1.56it/s]Extractor Predicting: 161it [01:44,  1.55it/s]Extractor Predicting: 162it [01:45,  1.54it/s]Extractor Predicting: 163it [01:45,  1.56it/s]Extractor Predicting: 164it [01:46,  1.56it/s]Extractor Predicting: 165it [01:47,  1.59it/s]Extractor Predicting: 166it [01:47,  1.60it/s]Extractor Predicting: 167it [01:48,  1.60it/s]Extractor Predicting: 168it [01:48,  1.55it/s]Extractor Predicting: 169it [01:49,  1.54it/s]Extractor Predicting: 170it [01:50,  1.54it/s]Extractor Predicting: 171it [01:50,  1.55it/s]Extractor Predicting: 172it [01:51,  1.56it/s]Extractor Predicting: 173it [01:52,  1.55it/s]Extractor Predicting: 174it [01:52,  1.49it/s]Extractor Predicting: 175it [01:53,  1.49it/s]Extractor Predicting: 176it [01:54,  1.47it/s]Extractor Predicting: 177it [01:54,  1.52it/s]Extractor Predicting: 178it [01:55,  1.52it/s]Extractor Predicting: 179it [01:56,  1.55it/s]Extractor Predicting: 180it [01:56,  1.58it/s]Extractor Predicting: 181it [01:57,  1.55it/s]Extractor Predicting: 182it [01:58,  1.55it/s]Extractor Predicting: 183it [01:58,  1.55it/s]Extractor Predicting: 184it [01:59,  1.53it/s]Extractor Predicting: 185it [02:00,  1.53it/s]Extractor Predicting: 186it [02:00,  1.52it/s]Extractor Predicting: 187it [02:01,  1.53it/s]Extractor Predicting: 188it [02:02,  1.51it/s]Extractor Predicting: 189it [02:02,  1.51it/s]Extractor Predicting: 190it [02:03,  1.52it/s]Extractor Predicting: 191it [02:04,  1.55it/s]Extractor Predicting: 192it [02:04,  1.58it/s]Extractor Predicting: 193it [02:05,  1.52it/s]Extractor Predicting: 194it [02:06,  1.50it/s]Extractor Predicting: 195it [02:06,  1.54it/s]Extractor Predicting: 196it [02:07,  1.55it/s]Extractor Predicting: 197it [02:07,  1.54it/s]Extractor Predicting: 198it [02:08,  1.53it/s]Extractor Predicting: 199it [02:09,  1.55it/s]Extractor Predicting: 200it [02:09,  1.61it/s]Extractor Predicting: 201it [02:10,  1.63it/s]Extractor Predicting: 202it [02:10,  1.63it/s]Extractor Predicting: 203it [02:11,  1.60it/s]Extractor Predicting: 204it [02:12,  1.58it/s]Extractor Predicting: 205it [02:12,  1.59it/s]Extractor Predicting: 206it [02:13,  1.57it/s]Extractor Predicting: 207it [02:14,  1.58it/s]Extractor Predicting: 208it [02:14,  1.55it/s]Extractor Predicting: 209it [02:15,  1.56it/s]Extractor Predicting: 210it [02:16,  1.56it/s]Extractor Predicting: 211it [02:16,  1.56it/s]Extractor Predicting: 212it [02:17,  1.57it/s]Extractor Predicting: 213it [02:18,  1.56it/s]Extractor Predicting: 214it [02:18,  1.54it/s]Extractor Predicting: 215it [02:19,  1.54it/s]Extractor Predicting: 216it [02:19,  1.57it/s]Extractor Predicting: 217it [02:20,  1.58it/s]Extractor Predicting: 218it [02:21,  1.56it/s]Extractor Predicting: 219it [02:21,  1.54it/s]Extractor Predicting: 220it [02:22,  1.56it/s]Extractor Predicting: 221it [02:23,  1.53it/s]Extractor Predicting: 222it [02:23,  1.53it/s]Extractor Predicting: 223it [02:24,  1.56it/s]Extractor Predicting: 224it [02:25,  1.56it/s]Extractor Predicting: 225it [02:25,  1.55it/s]Extractor Predicting: 226it [02:26,  1.54it/s]Extractor Predicting: 227it [02:27,  1.55it/s]Extractor Predicting: 228it [02:27,  1.56it/s]Extractor Predicting: 229it [02:28,  1.57it/s]Extractor Predicting: 230it [02:28,  1.58it/s]Extractor Predicting: 231it [02:29,  1.56it/s]Extractor Predicting: 232it [02:30,  1.56it/s]Extractor Predicting: 233it [02:30,  1.56it/s]Extractor Predicting: 234it [02:31,  1.58it/s]Extractor Predicting: 235it [02:32,  1.59it/s]Extractor Predicting: 236it [02:32,  1.59it/s]Extractor Predicting: 237it [02:33,  1.57it/s]Extractor Predicting: 238it [02:34,  1.57it/s]Extractor Predicting: 239it [02:34,  1.55it/s]Extractor Predicting: 240it [02:35,  1.56it/s]Extractor Predicting: 241it [02:36,  1.57it/s]Extractor Predicting: 242it [02:36,  1.60it/s]Extractor Predicting: 243it [02:37,  1.60it/s]Extractor Predicting: 244it [02:37,  1.59it/s]Extractor Predicting: 245it [02:38,  1.57it/s]Extractor Predicting: 246it [02:39,  1.57it/s]Extractor Predicting: 247it [02:39,  1.59it/s]Extractor Predicting: 248it [02:40,  1.59it/s]Extractor Predicting: 249it [02:41,  1.59it/s]Extractor Predicting: 250it [02:41,  1.61it/s]Extractor Predicting: 251it [02:42,  1.56it/s]Extractor Predicting: 252it [02:42,  1.56it/s]Extractor Predicting: 253it [02:43,  1.59it/s]Extractor Predicting: 254it [02:44,  1.58it/s]Extractor Predicting: 255it [02:44,  1.55it/s]Extractor Predicting: 256it [02:45,  1.55it/s]Extractor Predicting: 257it [02:46,  1.57it/s]Extractor Predicting: 258it [02:46,  1.60it/s]Extractor Predicting: 259it [02:47,  1.62it/s]Extractor Predicting: 260it [02:47,  1.62it/s]Extractor Predicting: 261it [02:48,  1.59it/s]Extractor Predicting: 262it [02:49,  1.60it/s]Extractor Predicting: 263it [02:49,  1.59it/s]Extractor Predicting: 264it [02:50,  1.61it/s]Extractor Predicting: 265it [02:51,  1.62it/s]Extractor Predicting: 266it [02:51,  1.66it/s]Extractor Predicting: 267it [02:52,  1.61it/s]Extractor Predicting: 268it [02:52,  1.60it/s]Extractor Predicting: 269it [02:53,  1.61it/s]Extractor Predicting: 270it [02:54,  1.61it/s]Extractor Predicting: 271it [02:54,  1.64it/s]Extractor Predicting: 272it [02:55,  1.67it/s]Extractor Predicting: 273it [02:55,  1.63it/s]Extractor Predicting: 274it [02:56,  1.63it/s]Extractor Predicting: 275it [02:57,  1.62it/s]Extractor Predicting: 276it [02:57,  1.61it/s]Extractor Predicting: 277it [02:58,  1.61it/s]Extractor Predicting: 278it [02:59,  1.60it/s]Extractor Predicting: 279it [02:59,  1.58it/s]Extractor Predicting: 280it [03:00,  1.59it/s]Extractor Predicting: 281it [03:01,  1.56it/s]Extractor Predicting: 281it [03:01,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:39,436 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:39,439 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:39,439 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:39,439 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:39,439 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:54:39,917 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:54:39,918 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:54:40,197 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:54:41,237 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:54:41,278 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:42,635 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:42,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:42,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:42,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:42,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:54:43,131 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:54:43,132 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:54:43,419 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:54:43,617 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:54:43,617 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4461472048348802,
  "recall": 0.30654011567551537,
  "score": 0.36339662447257387,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.43it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:03,  1.89it/s]Extractor Predicting: 6it [00:03,  1.61it/s]
[INFO|configuration_utils.py:515] 2023-08-28 20:54:47,854 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:54:47,854 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:54:47,863 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:54:47,863 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 20:54:47,868 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:54:53,012 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 20:54:53,045 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 20:54:53,140 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:54:53,141 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:54:53,242 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:54:53,286 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:54:53,286 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:54:53,286 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:54:53,286 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:54:53,286 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:54:53,286 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2375,
  "recall": 0.07392996108949416,
  "score": 0.11275964391691394,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 20:54:53,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:54,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:55,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:55,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:56,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:57,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:58,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:58,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:59,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:00,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:01,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:01,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:02,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:03,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:04,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:05,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:05,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:06,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:07,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:07,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:30, 15.04s/it][WARNING|generation_utils.py:914] 2023-08-28 20:55:08,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:09,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:10,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:10,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:11,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:12,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:13,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:13,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:14,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:15,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:15,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:16,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:17,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:18,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:19,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:19,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:20,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:21,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:22,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:23,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:30<03:17, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-28 20:55:24,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:24,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:25,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:26,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:26,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:27,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:27,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:28,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:29,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:29,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:30,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:31,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:32,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:32,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:33,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:34,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:34,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:35,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:36,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:37,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:44<02:54, 14.54s/it][WARNING|generation_utils.py:914] 2023-08-28 20:55:37,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:38,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:38,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:39,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:40,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:40,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:41,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:41,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:42,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:43,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:43,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:44,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:44,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:45,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:46,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:46,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:47,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:47,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:48,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:48,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:55<02:28, 13.47s/it][WARNING|generation_utils.py:914] 2023-08-28 20:55:49,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:50,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:51,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:51,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:52,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:52,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:53,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:54,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:54,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:55,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:56,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:56,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:57,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:58,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:58,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:59,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:59,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:00,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:01,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:01,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:02,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:09<02:15, 13.55s/it][WARNING|generation_utils.py:914] 2023-08-28 20:56:03,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:03,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:04,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:05,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:05,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:06,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:07,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:07,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:08,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:08,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:09,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:10,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:11,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:11,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:12,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:13,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:13,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:14,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:15,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:15,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:16,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:16,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:23<02:04, 13.78s/it][WARNING|generation_utils.py:914] 2023-08-28 20:56:17,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:18,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:18,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:19,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:20,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:20,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:21,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:22,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:22,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:23,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:24,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:24,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:25,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:26,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:26,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:27,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:28,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:28,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:29,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:30,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:30,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:31,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:38<01:53, 14.16s/it][WARNING|generation_utils.py:914] 2023-08-28 20:56:32,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:33,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:33,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:34,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:35,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:35,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:36,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:37,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:37,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:38,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:39,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:39,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:40,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:41,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:41,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:42,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:43,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:44,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:44,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:45,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:46,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:53<01:39, 14.26s/it][WARNING|generation_utils.py:914] 2023-08-28 20:56:46,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:47,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:48,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:49,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:49,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:50,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:51,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:52,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:52,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:53,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:54,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:54,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:55,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:56,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:57,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:57,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:58,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:59,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:00,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:01,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:01,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:08<01:27, 14.65s/it][WARNING|generation_utils.py:914] 2023-08-28 20:57:02,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:02,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:03,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:04,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:04,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:05,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:05,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:06,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:07,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:07,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:08,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:09,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:09,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:10,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:10,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:11,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:12,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:12,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:13,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:13,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:20<01:09, 13.86s/it][WARNING|generation_utils.py:914] 2023-08-28 20:57:14,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:15,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:15,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:16,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:17,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:17,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:18,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:19,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:19,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:20,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:21,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:21,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:22,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:23,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:23,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:24,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:25,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:25,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:26,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:27,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:34<00:54, 13.72s/it][WARNING|generation_utils.py:914] 2023-08-28 20:57:27,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:28,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:29,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:30,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:30,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:31,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:32,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:32,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:33,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:34,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:34,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:35,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:36,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:36,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:37,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:38,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:38,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:39,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:40,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:41,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:42,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:49<00:42, 14.04s/it][WARNING|generation_utils.py:914] 2023-08-28 20:57:42,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:43,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:43,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:44,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:45,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:46,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:46,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:47,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:47,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:48,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:49,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:50,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:50,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:51,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:51,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:52,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:53,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:53,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:54,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:55,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:56,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:03<00:28, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-28 20:57:56,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:57,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:57,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:58,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:59,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:59,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:00,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:01,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:01,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:02,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:02,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:03,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:04,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:04,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:05,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:05,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:06,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:07,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:07,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:08,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:08,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:09,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:16<00:13, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-28 20:58:10,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:10,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:11,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:12,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:13,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:13,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:14,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:15,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:15,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:16,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:17,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:17,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:18,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:19,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:19,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:20,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:21,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:21,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:22,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:23,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:23,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:31<00:00, 14.05s/it]Generating: 100%|██████████| 15/15 [03:31<00:00, 14.07s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:58:31,729 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:58:31,736 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:58:31,736 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:58:31,736 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:58:31,736 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:58:32,419 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:58:32,420 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:58:33,011 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:58:34,127 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:58:34,127 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:58:37,045 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:58:37,051 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:58:37,051 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:58:37,051 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:58:37,051 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:58:37,733 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:58:37,734 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:58:38,327 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:58:38,510 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:58:38,510 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 316, 'raw': 320}
{'target': 600, 'success': 346, 'raw': 352}
{'target': 600, 'success': 377, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 470, 'raw': 480}
{'target': 600, 'success': 502, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 562, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.9765625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : main subject .', 'success_rate': 0.953125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : participant in .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : platform .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8678977272727273, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : location .', 'success_rate': 0.8551136363636364, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9107142857142857, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 630, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : position held .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position played on team / speciality . Context : On 31 March 2014 , the Brazilian national team won a bronze medal in the 2018 FIFA World Cup . Head Entity : Brazil national team , Tail Entity : goalkeeper .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8792613636363636, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : religion .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 8044
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8144, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.45it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:01,  1.61it/s]Extractor Estimating: 4it [00:02,  1.61it/s]Extractor Estimating: 5it [00:03,  1.51it/s]Extractor Estimating: 6it [00:03,  1.58it/s]Extractor Estimating: 7it [00:04,  1.58it/s]Extractor Estimating: 8it [00:05,  1.61it/s]Extractor Estimating: 9it [00:05,  1.66it/s]Extractor Estimating: 10it [00:06,  1.66it/s]Extractor Estimating: 11it [00:06,  1.61it/s]Extractor Estimating: 12it [00:07,  1.58it/s]Extractor Estimating: 13it [00:08,  1.62it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.56it/s]Extractor Estimating: 16it [00:10,  1.58it/s]Extractor Estimating: 17it [00:10,  1.62it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:11,  1.62it/s]Extractor Estimating: 20it [00:12,  1.66it/s]Extractor Estimating: 21it [00:13,  1.63it/s]Extractor Estimating: 22it [00:13,  1.55it/s]Extractor Estimating: 23it [00:14,  1.58it/s]Extractor Estimating: 24it [00:15,  1.56it/s]Extractor Estimating: 25it [00:15,  1.61it/s]Extractor Estimating: 26it [00:16,  1.57it/s]Extractor Estimating: 27it [00:16,  1.57it/s]Extractor Estimating: 28it [00:17,  1.54it/s]Extractor Estimating: 29it [00:18,  1.57it/s]Extractor Estimating: 30it [00:18,  1.63it/s]Extractor Estimating: 31it [00:19,  1.64it/s]Extractor Estimating: 32it [00:20,  1.65it/s]Extractor Estimating: 33it [00:20,  1.66it/s]Extractor Estimating: 34it [00:21,  1.68it/s]Extractor Estimating: 35it [00:21,  1.64it/s]Extractor Estimating: 36it [00:22,  1.65it/s]Extractor Estimating: 37it [00:23,  1.59it/s]Extractor Estimating: 38it [00:23,  1.57it/s]Extractor Estimating: 39it [00:24,  1.56it/s]Extractor Estimating: 40it [00:25,  1.56it/s]Extractor Estimating: 41it [00:25,  1.57it/s]Extractor Estimating: 42it [00:26,  1.44it/s]Extractor Estimating: 43it [00:27,  1.49it/s]Extractor Estimating: 44it [00:27,  1.52it/s]Extractor Estimating: 45it [00:28,  1.50it/s]Extractor Estimating: 46it [00:29,  1.54it/s]Extractor Estimating: 47it [00:29,  1.54it/s]Extractor Estimating: 48it [00:30,  1.58it/s]Extractor Estimating: 49it [00:30,  1.58it/s]Extractor Estimating: 50it [00:31,  1.56it/s]Extractor Estimating: 51it [00:32,  1.65it/s]Extractor Estimating: 52it [00:32,  1.66it/s]Extractor Estimating: 53it [00:33,  1.63it/s]Extractor Estimating: 54it [00:33,  1.66it/s]Extractor Estimating: 55it [00:34,  1.70it/s]Extractor Estimating: 56it [00:35,  1.57it/s]Extractor Estimating: 57it [00:35,  1.63it/s]Extractor Estimating: 58it [00:36,  1.69it/s]Extractor Estimating: 59it [00:36,  1.70it/s]Extractor Estimating: 60it [00:37,  1.69it/s]Extractor Estimating: 61it [00:38,  1.44it/s]Extractor Estimating: 62it [00:39,  1.49it/s]Extractor Estimating: 63it [00:39,  1.56it/s]Extractor Estimating: 64it [00:40,  1.55it/s]Extractor Estimating: 65it [00:40,  1.58it/s]Extractor Estimating: 66it [00:41,  1.64it/s]Extractor Estimating: 67it [00:42,  1.63it/s]Extractor Estimating: 68it [00:42,  1.65it/s]Extractor Estimating: 69it [00:43,  1.69it/s]Extractor Estimating: 70it [00:43,  1.72it/s]Extractor Estimating: 71it [00:44,  1.71it/s]Extractor Estimating: 72it [00:44,  1.69it/s]Extractor Estimating: 73it [00:45,  1.67it/s]Extractor Estimating: 74it [00:46,  1.66it/s]Extractor Estimating: 75it [00:46,  1.65it/s]Extractor Estimating: 76it [00:47,  1.71it/s]Extractor Estimating: 77it [00:47,  1.70it/s]Extractor Estimating: 78it [00:48,  1.73it/s]Extractor Estimating: 79it [00:49,  1.73it/s]Extractor Estimating: 80it [00:49,  1.77it/s]Extractor Estimating: 81it [00:50,  1.76it/s]Extractor Estimating: 82it [00:50,  1.84it/s]Extractor Estimating: 83it [00:51,  1.78it/s]Extractor Estimating: 84it [00:51,  1.74it/s]Extractor Estimating: 85it [00:52,  1.78it/s]Extractor Estimating: 86it [00:53,  1.74it/s]Extractor Estimating: 87it [00:53,  1.73it/s]Extractor Estimating: 88it [00:54,  1.82it/s]Extractor Estimating: 89it [00:54,  1.88it/s]Extractor Estimating: 90it [00:55,  1.85it/s]Extractor Estimating: 91it [00:55,  1.80it/s]Extractor Estimating: 92it [00:56,  1.61it/s]Extractor Estimating: 93it [00:57,  1.65it/s]Extractor Estimating: 94it [00:57,  1.66it/s]Extractor Estimating: 95it [00:58,  1.73it/s]Extractor Estimating: 96it [00:58,  1.69it/s]Extractor Estimating: 97it [00:59,  1.72it/s]Extractor Estimating: 98it [00:59,  1.79it/s]Extractor Estimating: 99it [01:00,  1.79it/s]Extractor Estimating: 100it [01:00,  1.84it/s]Extractor Estimating: 101it [01:01,  1.85it/s]Extractor Estimating: 102it [01:02,  1.72it/s]Extractor Estimating: 103it [01:02,  1.81it/s]Extractor Estimating: 104it [01:03,  1.85it/s]Extractor Estimating: 105it [01:03,  1.90it/s]Extractor Estimating: 106it [01:04,  1.91it/s]Extractor Estimating: 107it [01:04,  1.89it/s]Extractor Estimating: 108it [01:05,  1.86it/s]Extractor Estimating: 109it [01:05,  1.90it/s]Extractor Estimating: 110it [01:06,  1.91it/s]Extractor Estimating: 111it [01:06,  1.92it/s]Extractor Estimating: 112it [01:07,  1.96it/s]Extractor Estimating: 113it [01:07,  1.94it/s]Extractor Estimating: 114it [01:08,  1.83it/s]Extractor Estimating: 115it [01:09,  1.80it/s]Extractor Estimating: 116it [01:09,  1.81it/s]Extractor Estimating: 117it [01:10,  1.79it/s]Extractor Estimating: 118it [01:10,  1.89it/s]Extractor Estimating: 119it [01:11,  1.90it/s]Extractor Estimating: 120it [01:11,  1.88it/s]Extractor Estimating: 121it [01:12,  1.85it/s]Extractor Estimating: 122it [01:12,  1.89it/s]Extractor Estimating: 123it [01:13,  1.90it/s]Extractor Estimating: 124it [01:13,  1.91it/s]Extractor Estimating: 125it [01:14,  1.89it/s]Extractor Estimating: 126it [01:14,  1.87it/s]Extractor Estimating: 127it [01:15,  1.84it/s]Extractor Estimating: 128it [01:15,  1.84it/s]Extractor Estimating: 129it [01:16,  1.93it/s]Extractor Estimating: 130it [01:16,  1.93it/s]Extractor Estimating: 131it [01:17,  1.96it/s]Extractor Estimating: 132it [01:18,  1.84it/s]Extractor Estimating: 133it [01:18,  1.84it/s]Extractor Estimating: 134it [01:19,  1.90it/s]Extractor Estimating: 135it [01:19,  1.97it/s]Extractor Estimating: 136it [01:20,  1.94it/s]Extractor Estimating: 137it [01:20,  1.77it/s]Extractor Estimating: 138it [01:21,  1.76it/s]Extractor Estimating: 139it [01:21,  1.80it/s]Extractor Estimating: 140it [01:22,  1.87it/s]Extractor Estimating: 141it [01:22,  1.94it/s]Extractor Estimating: 142it [01:23,  1.88it/s]Extractor Estimating: 143it [01:23,  1.87it/s]Extractor Estimating: 144it [01:24,  1.76it/s]Extractor Estimating: 145it [01:25,  1.84it/s]Extractor Estimating: 146it [01:25,  1.95it/s]Extractor Estimating: 147it [01:25,  1.99it/s]Extractor Estimating: 148it [01:26,  1.88it/s]Extractor Estimating: 149it [01:27,  1.88it/s]Extractor Estimating: 150it [01:27,  1.92it/s]Extractor Estimating: 151it [01:28,  1.90it/s]Extractor Estimating: 152it [01:28,  1.73it/s]Extractor Estimating: 153it [01:29,  1.66it/s]Extractor Estimating: 154it [01:30,  1.72it/s]Extractor Estimating: 155it [01:30,  1.71it/s]Extractor Estimating: 156it [01:31,  1.68it/s]Extractor Estimating: 157it [01:31,  1.72it/s]Extractor Estimating: 158it [01:32,  1.73it/s]Extractor Estimating: 159it [01:33,  1.65it/s]Extractor Estimating: 160it [01:33,  1.67it/s]Extractor Estimating: 161it [01:34,  1.69it/s]Extractor Estimating: 162it [01:34,  1.66it/s]Extractor Estimating: 163it [01:35,  1.65it/s]Extractor Estimating: 164it [01:36,  1.57it/s]Extractor Estimating: 165it [01:36,  1.59it/s]Extractor Estimating: 166it [01:37,  1.63it/s]Extractor Estimating: 167it [01:37,  1.67it/s]Extractor Estimating: 168it [01:38,  1.62it/s]Extractor Estimating: 169it [01:39,  1.60it/s]Extractor Estimating: 170it [01:39,  1.61it/s]Extractor Estimating: 171it [01:40,  1.66it/s]Extractor Estimating: 172it [01:40,  1.69it/s]Extractor Estimating: 173it [01:41,  1.53it/s]Extractor Estimating: 174it [01:42,  1.54it/s]Extractor Estimating: 175it [01:42,  1.58it/s]Extractor Estimating: 176it [01:43,  1.64it/s]Extractor Estimating: 177it [01:44,  1.60it/s]Extractor Estimating: 178it [01:44,  1.62it/s]Extractor Estimating: 179it [01:45,  1.68it/s]Extractor Estimating: 180it [01:45,  1.72it/s]Extractor Estimating: 181it [01:46,  1.73it/s]Extractor Estimating: 182it [01:47,  1.72it/s]Extractor Estimating: 183it [01:47,  1.74it/s]Extractor Estimating: 184it [01:48,  1.73it/s]Extractor Estimating: 185it [01:48,  1.72it/s]Extractor Estimating: 186it [01:49,  1.73it/s]Extractor Estimating: 187it [01:49,  1.68it/s]Extractor Estimating: 188it [01:50,  1.73it/s]Extractor Estimating: 189it [01:51,  1.74it/s]Extractor Estimating: 190it [01:51,  1.70it/s]Extractor Estimating: 191it [01:52,  1.69it/s]Extractor Estimating: 192it [01:52,  1.69it/s]Extractor Estimating: 193it [01:53,  1.71it/s]Extractor Estimating: 194it [01:54,  1.73it/s]Extractor Estimating: 195it [01:54,  1.75it/s]Extractor Estimating: 196it [01:55,  1.74it/s]Extractor Estimating: 197it [01:55,  1.72it/s]Extractor Estimating: 198it [01:56,  1.64it/s]Extractor Estimating: 199it [01:57,  1.65it/s]Extractor Estimating: 200it [01:57,  1.65it/s]Extractor Estimating: 201it [01:58,  1.60it/s]Extractor Estimating: 202it [01:58,  1.57it/s]Extractor Estimating: 203it [01:59,  1.53it/s]Extractor Estimating: 204it [02:00,  1.44it/s]Extractor Estimating: 205it [02:01,  1.43it/s]Extractor Estimating: 206it [02:01,  1.50it/s]Extractor Estimating: 207it [02:02,  1.51it/s]Extractor Estimating: 208it [02:03,  1.52it/s]Extractor Estimating: 209it [02:03,  1.49it/s]Extractor Estimating: 210it [02:04,  1.53it/s]Extractor Estimating: 211it [02:05,  1.51it/s]Extractor Estimating: 212it [02:05,  1.52it/s]Extractor Estimating: 213it [02:06,  1.52it/s]Extractor Estimating: 214it [02:07,  1.45it/s]Extractor Estimating: 215it [02:07,  1.49it/s]Extractor Estimating: 216it [02:08,  1.49it/s]Extractor Estimating: 217it [02:09,  1.36it/s]Extractor Estimating: 218it [02:09,  1.40it/s]Extractor Estimating: 219it [02:10,  1.44it/s]Extractor Estimating: 220it [02:11,  1.46it/s]Extractor Estimating: 221it [02:11,  1.48it/s]Extractor Estimating: 222it [02:12,  1.39it/s]Extractor Estimating: 223it [02:13,  1.41it/s]Extractor Estimating: 224it [02:14,  1.42it/s]Extractor Estimating: 225it [02:14,  1.44it/s]Extractor Estimating: 226it [02:15,  1.59it/s]Extractor Estimating: 227it [02:16,  1.47it/s]Extractor Estimating: 228it [02:16,  1.56it/s]Extractor Estimating: 229it [02:17,  1.70it/s]Extractor Estimating: 230it [02:17,  1.78it/s]Extractor Estimating: 231it [02:18,  1.92it/s]Extractor Estimating: 232it [02:18,  2.05it/s]Extractor Estimating: 233it [02:18,  2.01it/s]Extractor Estimating: 234it [02:19,  2.03it/s]Extractor Estimating: 235it [02:19,  2.02it/s]Extractor Estimating: 236it [02:20,  1.97it/s]Extractor Estimating: 237it [02:20,  2.04it/s]Extractor Estimating: 238it [02:21,  2.04it/s]Extractor Estimating: 239it [02:22,  1.90it/s]Extractor Estimating: 240it [02:22,  1.90it/s]Extractor Estimating: 241it [02:23,  1.89it/s]Extractor Estimating: 242it [02:23,  1.99it/s]Extractor Estimating: 243it [02:24,  1.98it/s]Extractor Estimating: 244it [02:24,  2.04it/s]Extractor Estimating: 245it [02:25,  1.93it/s]Extractor Estimating: 246it [02:25,  2.02it/s]Extractor Estimating: 247it [02:26,  1.99it/s]Extractor Estimating: 248it [02:26,  2.04it/s]Extractor Estimating: 249it [02:26,  2.04it/s]Extractor Estimating: 250it [02:27,  2.04it/s]Extractor Estimating: 251it [02:28,  1.94it/s]Extractor Estimating: 252it [02:28,  1.87it/s]Extractor Estimating: 253it [02:29,  1.77it/s]Extractor Estimating: 254it [02:29,  1.77it/s]Extractor Estimating: 255it [02:30,  1.65it/s]Extractor Estimating: 256it [02:31,  1.68it/s]Extractor Estimating: 257it [02:31,  1.69it/s]Extractor Estimating: 258it [02:32,  1.75it/s]Extractor Estimating: 259it [02:32,  1.72it/s]Extractor Estimating: 260it [02:33,  1.53it/s]Extractor Estimating: 261it [02:34,  1.60it/s]Extractor Estimating: 262it [02:34,  1.62it/s]Extractor Estimating: 263it [02:35,  1.65it/s]Extractor Estimating: 264it [02:36,  1.63it/s]Extractor Estimating: 265it [02:36,  1.64it/s]Extractor Estimating: 266it [02:37,  1.66it/s]Extractor Estimating: 267it [02:37,  1.71it/s]Extractor Estimating: 268it [02:38,  1.73it/s]Extractor Estimating: 269it [02:38,  1.74it/s]Extractor Estimating: 270it [02:39,  1.72it/s]Extractor Estimating: 271it [02:40,  1.72it/s]Extractor Estimating: 272it [02:40,  1.75it/s]Extractor Estimating: 273it [02:41,  1.71it/s]Extractor Estimating: 274it [02:41,  1.75it/s]Extractor Estimating: 275it [02:42,  1.69it/s]Extractor Estimating: 276it [02:43,  1.68it/s]Extractor Estimating: 277it [02:43,  1.74it/s]Extractor Estimating: 278it [02:44,  1.75it/s]Extractor Estimating: 279it [02:44,  1.69it/s]Extractor Estimating: 280it [02:45,  1.68it/s]Extractor Estimating: 281it [02:45,  1.71it/s]Extractor Estimating: 282it [02:46,  1.74it/s]Extractor Estimating: 283it [02:47,  1.76it/s]Extractor Estimating: 284it [02:47,  1.78it/s]Extractor Estimating: 285it [02:48,  1.80it/s]Extractor Estimating: 286it [02:48,  1.78it/s]Extractor Estimating: 287it [02:49,  1.81it/s]Extractor Estimating: 288it [02:49,  1.78it/s]Extractor Estimating: 289it [02:50,  1.72it/s]Extractor Estimating: 290it [02:51,  1.66it/s]Extractor Estimating: 291it [02:51,  1.79it/s]Extractor Estimating: 292it [02:52,  1.75it/s]Extractor Estimating: 293it [02:52,  1.82it/s]Extractor Estimating: 294it [02:53,  1.86it/s]Extractor Estimating: 295it [02:53,  1.85it/s]Extractor Estimating: 296it [02:54,  1.85it/s]Extractor Estimating: 297it [02:54,  1.86it/s]Extractor Estimating: 298it [02:55,  1.81it/s]Extractor Estimating: 299it [02:55,  1.84it/s]Extractor Estimating: 300it [02:56,  1.66it/s]Extractor Estimating: 301it [02:57,  1.67it/s]Extractor Estimating: 302it [02:57,  1.69it/s]Extractor Estimating: 303it [02:58,  1.68it/s]Extractor Estimating: 304it [02:58,  1.69it/s]Extractor Estimating: 305it [02:59,  1.70it/s]Extractor Estimating: 306it [03:00,  1.69it/s]Extractor Estimating: 307it [03:00,  1.72it/s]Extractor Estimating: 308it [03:01,  1.74it/s]Extractor Estimating: 309it [03:01,  1.73it/s]Extractor Estimating: 310it [03:02,  1.74it/s]Extractor Estimating: 311it [03:03,  1.72it/s]Extractor Estimating: 312it [03:03,  1.80it/s]Extractor Estimating: 313it [03:04,  1.79it/s]Extractor Estimating: 314it [03:04,  1.80it/s]Extractor Estimating: 315it [03:05,  1.81it/s]Extractor Estimating: 316it [03:05,  1.77it/s]Extractor Estimating: 317it [03:06,  1.78it/s]Extractor Estimating: 318it [03:07,  1.66it/s]Extractor Estimating: 319it [03:07,  1.69it/s]Extractor Estimating: 320it [03:08,  1.68it/s]Extractor Estimating: 321it [03:08,  1.68it/s]Extractor Estimating: 322it [03:09,  1.68it/s]Extractor Estimating: 323it [03:09,  1.75it/s]Extractor Estimating: 324it [03:10,  1.69it/s]Extractor Estimating: 325it [03:11,  1.72it/s]Extractor Estimating: 326it [03:11,  1.81it/s]Extractor Estimating: 327it [03:12,  1.85it/s]Extractor Estimating: 328it [03:12,  1.92it/s]Extractor Estimating: 329it [03:13,  1.92it/s]Extractor Estimating: 330it [03:13,  1.92it/s]Extractor Estimating: 331it [03:14,  1.91it/s]Extractor Estimating: 332it [03:14,  1.89it/s]Extractor Estimating: 333it [03:15,  1.93it/s]Extractor Estimating: 334it [03:15,  2.00it/s]Extractor Estimating: 335it [03:16,  1.91it/s]Extractor Estimating: 336it [03:16,  1.95it/s]Extractor Estimating: 337it [03:17,  2.00it/s]Extractor Estimating: 338it [03:17,  1.99it/s]Extractor Estimating: 339it [03:18,  1.97it/s]Extractor Estimating: 340it [03:18,  1.92it/s]Extractor Estimating: 341it [03:19,  1.92it/s]Extractor Estimating: 342it [03:19,  1.99it/s]Extractor Estimating: 343it [03:20,  2.02it/s]Extractor Estimating: 344it [03:20,  2.01it/s]Extractor Estimating: 345it [03:21,  2.04it/s]Extractor Estimating: 346it [03:21,  1.98it/s]Extractor Estimating: 347it [03:22,  2.01it/s]Extractor Estimating: 348it [03:22,  1.96it/s]Extractor Estimating: 349it [03:23,  1.96it/s]Extractor Estimating: 350it [03:23,  1.90it/s]Extractor Estimating: 351it [03:24,  1.84it/s]Extractor Estimating: 352it [03:24,  1.85it/s]Extractor Estimating: 353it [03:25,  1.81it/s]Extractor Estimating: 354it [03:26,  1.73it/s]Extractor Estimating: 355it [03:26,  1.72it/s]Extractor Estimating: 356it [03:27,  1.76it/s]Extractor Estimating: 357it [03:27,  1.79it/s]Extractor Estimating: 358it [03:28,  1.59it/s]Extractor Estimating: 359it [03:29,  1.64it/s]Extractor Estimating: 360it [03:29,  1.64it/s]Extractor Estimating: 361it [03:30,  1.66it/s]Extractor Estimating: 362it [03:30,  1.70it/s]Extractor Estimating: 363it [03:31,  1.72it/s]Extractor Estimating: 364it [03:31,  1.79it/s]Extractor Estimating: 365it [03:32,  1.73it/s]Extractor Estimating: 366it [03:33,  1.77it/s]Extractor Estimating: 367it [03:33,  1.71it/s]Extractor Estimating: 368it [03:34,  1.68it/s]Extractor Estimating: 369it [03:34,  1.70it/s]Extractor Estimating: 370it [03:35,  1.71it/s]Extractor Estimating: 371it [03:36,  1.75it/s]Extractor Estimating: 372it [03:36,  1.75it/s]Extractor Estimating: 373it [03:37,  1.71it/s]Extractor Estimating: 374it [03:37,  1.70it/s]Extractor Estimating: 375it [03:38,  1.71it/s]Extractor Estimating: 375it [03:38,  1.72it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:34,077 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:34,115 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:34,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:34,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:34,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:02:34,921 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:02:34,922 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:02:35,918 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:02:36,988 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:02:36,988 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:42,636 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:42,817 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:42,817 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:42,817 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:42,817 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:02:44,418 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:02:44,419 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:02:45,724 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:02:46,124 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:02:46,124 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 22:50:24,817 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 22:50:24,892 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 5998 mean pseudo reward: 0.9750928520928763
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 15023
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15123, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15123, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.080, loss:350.7063
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.017, loss:344.6999
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 1.055, loss:299.2336
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 150, avg_time 1.013, loss:316.2689
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 250, avg_time 1.013, loss:328.0129
>> valid entity prec:0.5331, rec:0.5181, f1:0.5255
>> valid relation prec:0.3052, rec:0.1542, f1:0.2048
>> valid relation with NER prec:0.3052, rec:0.1542, f1:0.2048
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 1.022, loss:272.5310
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 200, avg_time 1.025, loss:301.3841
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 1.042, loss:305.8013
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 150, avg_time 1.012, loss:286.4213
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 250, avg_time 1.016, loss:307.4974
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5296, rec:0.5140, f1:0.5217
>> valid relation prec:0.2690, rec:0.1513, f1:0.1937
>> valid relation with NER prec:0.2690, rec:0.1513, f1:0.1937
g_step 1100, step 100, avg_time 1.018, loss:275.8819
g_step 1200, step 200, avg_time 1.028, loss:294.8697
g_step 1300, step 50, avg_time 1.014, loss:259.9234
g_step 1400, step 150, avg_time 1.039, loss:276.2436
g_step 1500, step 250, avg_time 1.003, loss:282.6170
>> valid entity prec:0.5594, rec:0.4996, f1:0.5279
>> valid relation prec:0.2885, rec:0.1510, f1:0.1982
>> valid relation with NER prec:0.2885, rec:0.1510, f1:0.1982
new max entity f1 on valid!
g_step 1600, step 100, avg_time 1.016, loss:244.0175
g_step 1700, step 200, avg_time 1.017, loss:257.2403
g_step 1800, step 50, avg_time 1.031, loss:241.3312
g_step 1900, step 150, avg_time 1.023, loss:231.7165
g_step 2000, step 250, avg_time 1.027, loss:248.3313
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5432, rec:0.5318, f1:0.5374
>> valid relation prec:0.2916, rec:0.1768, f1:0.2201
>> valid relation with NER prec:0.2916, rec:0.1768, f1:0.2201
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 100, avg_time 1.019, loss:211.4900
g_step 2200, step 200, avg_time 1.025, loss:215.9895
g_step 2300, step 50, avg_time 1.024, loss:228.6243
g_step 2400, step 150, avg_time 1.022, loss:201.7572
g_step 2500, step 250, avg_time 1.021, loss:217.1138
>> valid entity prec:0.5756, rec:0.4979, f1:0.5340
>> valid relation prec:0.2583, rec:0.1407, f1:0.1822
>> valid relation with NER prec:0.2583, rec:0.1407, f1:0.1822
g_step 2600, step 100, avg_time 1.020, loss:187.3670
g_step 2700, step 200, avg_time 1.030, loss:195.9438
g_step 2800, step 50, avg_time 1.011, loss:200.5378
g_step 2900, step 150, avg_time 1.018, loss:180.8413
g_step 3000, step 250, avg_time 1.028, loss:207.9975
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5560, rec:0.4932, f1:0.5227
>> valid relation prec:0.2872, rec:0.1570, f1:0.2030
>> valid relation with NER prec:0.2872, rec:0.1570, f1:0.2030
g_step 3100, step 100, avg_time 1.021, loss:169.0921
g_step 3200, step 200, avg_time 1.021, loss:184.5886
g_step 3300, step 50, avg_time 1.019, loss:175.8344
g_step 3400, step 150, avg_time 1.028, loss:191.2292
g_step 3500, step 250, avg_time 1.018, loss:176.6047
>> valid entity prec:0.5785, rec:0.4844, f1:0.5272
>> valid relation prec:0.2849, rec:0.1585, f1:0.2036
>> valid relation with NER prec:0.2849, rec:0.1585, f1:0.2036
g_step 3600, step 100, avg_time 1.033, loss:164.4669
g_step 3700, step 200, avg_time 1.019, loss:160.1481
g_step 3800, step 50, avg_time 1.019, loss:152.3324
g_step 3900, step 150, avg_time 1.031, loss:145.6831
g_step 4000, step 250, avg_time 1.024, loss:173.6608
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5717, rec:0.5047, f1:0.5361
>> valid relation prec:0.2659, rec:0.1473, f1:0.1896
>> valid relation with NER prec:0.2659, rec:0.1473, f1:0.1896
g_step 4100, step 100, avg_time 1.032, loss:135.5829
g_step 4200, step 200, avg_time 1.014, loss:157.0069
g_step 4300, step 50, avg_time 1.018, loss:148.8913
g_step 4400, step 150, avg_time 1.026, loss:139.3685
g_step 4500, step 250, avg_time 1.017, loss:164.8804
>> valid entity prec:0.5376, rec:0.5103, f1:0.5236
>> valid relation prec:0.2667, rec:0.1593, f1:0.1995
>> valid relation with NER prec:0.2667, rec:0.1593, f1:0.1995
g_step 4600, step 100, avg_time 1.031, loss:145.5705
g_step 4700, step 200, avg_time 1.022, loss:143.9437
g_step 4800, step 50, avg_time 1.013, loss:131.5640
g_step 4900, step 150, avg_time 1.019, loss:140.0666
g_step 5000, step 250, avg_time 1.027, loss:135.5366
learning rate was adjusted to 0.0008
>> valid entity prec:0.5560, rec:0.4932, f1:0.5227
>> valid relation prec:0.2688, rec:0.1473, f1:0.1903
>> valid relation with NER prec:0.2688, rec:0.1473, f1:0.1903
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 22:50:24 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 22:50:24 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_22-50-24_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 22:50:26 - WARNING - datasets.builder -   Using custom data configuration default-623831b4edc27b5c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-623831b4edc27b5c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 22:50:27,216 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:50:27,217 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:50:27,217 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:50:27,218 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:50:27,243 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:50:27,248 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:50:27,248 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:50:27,248 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:50:27,248 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:50:27,248 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:50:27,248 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 22:50:27,409 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:50:30,774 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 22:50:30,782 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-623831b4edc27b5c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.05ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.99ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.45ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.73ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.87ba/s]100%|██████████| 6/6 [00:01<00:00,  4.99ba/s]100%|██████████| 6/6 [00:01<00:00,  4.63ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.70ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.15ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.33ba/s]100%|██████████| 4/4 [00:00<00:00,  5.45ba/s]100%|██████████| 4/4 [00:00<00:00,  4.88ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.63ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.43ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.34ba/s]100%|██████████| 6/6 [00:00<00:00,  9.04ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.89ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.58ba/s]100%|██████████| 4/4 [00:00<00:00, 10.84ba/s]
[INFO|trainer.py:414] 2023-08-28 22:50:34,828 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 22:50:34,870 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 22:50:34,870 >>   Num examples = 6000
[INFO|trainer.py:1149] 2023-08-28 22:50:34,870 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 22:50:34,870 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 22:50:34,870 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 22:50:34,870 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 22:50:34,870 >>   Total optimization steps = 470
  0%|          | 0/470 [00:00<?, ?it/s]  0%|          | 1/470 [00:00<03:16,  2.39it/s]  0%|          | 2/470 [00:00<02:44,  2.85it/s]  1%|          | 3/470 [00:01<02:32,  3.07it/s]  1%|          | 4/470 [00:01<02:31,  3.07it/s]  1%|          | 5/470 [00:01<02:24,  3.21it/s]  1%|▏         | 6/470 [00:01<02:20,  3.30it/s]  1%|▏         | 7/470 [00:02<02:19,  3.31it/s]  2%|▏         | 8/470 [00:02<02:17,  3.37it/s]  2%|▏         | 9/470 [00:02<02:15,  3.41it/s]  2%|▏         | 10/470 [00:03<02:13,  3.44it/s]  2%|▏         | 11/470 [00:03<02:12,  3.47it/s]  3%|▎         | 12/470 [00:03<02:11,  3.48it/s]  3%|▎         | 13/470 [00:03<02:12,  3.45it/s]  3%|▎         | 14/470 [00:04<02:11,  3.47it/s]  3%|▎         | 15/470 [00:04<02:10,  3.48it/s]  3%|▎         | 16/470 [00:04<02:09,  3.50it/s]  4%|▎         | 17/470 [00:05<02:09,  3.50it/s]  4%|▍         | 18/470 [00:05<02:08,  3.51it/s]  4%|▍         | 19/470 [00:05<02:08,  3.51it/s]  4%|▍         | 20/470 [00:05<02:07,  3.52it/s]  4%|▍         | 21/470 [00:06<02:07,  3.51it/s]  5%|▍         | 22/470 [00:06<02:07,  3.52it/s]  5%|▍         | 23/470 [00:06<02:07,  3.52it/s]  5%|▌         | 24/470 [00:07<02:06,  3.52it/s]  5%|▌         | 25/470 [00:07<02:06,  3.51it/s]  6%|▌         | 26/470 [00:07<02:06,  3.51it/s]  6%|▌         | 27/470 [00:07<02:06,  3.52it/s]  6%|▌         | 28/470 [00:08<02:05,  3.51it/s]  6%|▌         | 29/470 [00:08<02:05,  3.52it/s]  6%|▋         | 30/470 [00:08<02:05,  3.51it/s]  7%|▋         | 31/470 [00:09<02:04,  3.52it/s]  7%|▋         | 32/470 [00:09<02:05,  3.50it/s]  7%|▋         | 33/470 [00:09<02:04,  3.50it/s]  7%|▋         | 34/470 [00:09<02:04,  3.51it/s]  7%|▋         | 35/470 [00:10<02:03,  3.51it/s]  8%|▊         | 36/470 [00:10<02:03,  3.51it/s]  8%|▊         | 37/470 [00:10<02:03,  3.51it/s]  8%|▊         | 38/470 [00:11<02:02,  3.51it/s]  8%|▊         | 39/470 [00:11<02:02,  3.51it/s]  9%|▊         | 40/470 [00:11<02:02,  3.51it/s]  9%|▊         | 41/470 [00:11<02:02,  3.52it/s]  9%|▉         | 42/470 [00:12<02:01,  3.51it/s]  9%|▉         | 43/470 [00:12<02:01,  3.52it/s]  9%|▉         | 44/470 [00:12<02:01,  3.52it/s] 10%|▉         | 45/470 [00:13<02:00,  3.52it/s] 10%|▉         | 46/470 [00:13<02:00,  3.51it/s] 10%|█         | 47/470 [00:13<02:00,  3.51it/s] 10%|█         | 48/470 [00:13<02:00,  3.51it/s] 10%|█         | 49/470 [00:14<01:59,  3.51it/s] 11%|█         | 50/470 [00:14<02:02,  3.43it/s] 11%|█         | 51/470 [00:14<02:01,  3.45it/s] 11%|█         | 52/470 [00:15<02:00,  3.47it/s] 11%|█▏        | 53/470 [00:15<01:59,  3.48it/s] 11%|█▏        | 54/470 [00:15<01:59,  3.49it/s] 12%|█▏        | 55/470 [00:15<01:58,  3.49it/s] 12%|█▏        | 56/470 [00:16<01:58,  3.50it/s] 12%|█▏        | 57/470 [00:16<01:57,  3.50it/s] 12%|█▏        | 58/470 [00:16<01:57,  3.50it/s] 13%|█▎        | 59/470 [00:17<01:57,  3.51it/s] 13%|█▎        | 60/470 [00:17<01:57,  3.50it/s] 13%|█▎        | 61/470 [00:17<01:56,  3.50it/s] 13%|█▎        | 62/470 [00:17<01:56,  3.50it/s] 13%|█▎        | 63/470 [00:18<01:56,  3.51it/s] 14%|█▎        | 64/470 [00:18<01:55,  3.51it/s] 14%|█▍        | 65/470 [00:18<01:55,  3.50it/s] 14%|█▍        | 66/470 [00:19<01:55,  3.50it/s] 14%|█▍        | 67/470 [00:19<01:54,  3.51it/s] 14%|█▍        | 68/470 [00:19<01:54,  3.50it/s] 15%|█▍        | 69/470 [00:19<01:54,  3.50it/s] 15%|█▍        | 70/470 [00:20<01:54,  3.51it/s] 15%|█▌        | 71/470 [00:20<01:53,  3.50it/s] 15%|█▌        | 72/470 [00:20<01:53,  3.50it/s] 16%|█▌        | 73/470 [00:21<01:53,  3.51it/s] 16%|█▌        | 74/470 [00:21<01:52,  3.51it/s] 16%|█▌        | 75/470 [00:21<01:52,  3.50it/s] 16%|█▌        | 76/470 [00:21<01:52,  3.50it/s] 16%|█▋        | 77/470 [00:22<01:52,  3.51it/s] 17%|█▋        | 78/470 [00:22<01:51,  3.50it/s] 17%|█▋        | 79/470 [00:22<01:51,  3.50it/s] 17%|█▋        | 80/470 [00:23<01:51,  3.50it/s] 17%|█▋        | 81/470 [00:23<01:50,  3.50it/s] 17%|█▋        | 82/470 [00:23<01:50,  3.50it/s] 18%|█▊        | 83/470 [00:23<01:50,  3.51it/s] 18%|█▊        | 84/470 [00:24<01:50,  3.50it/s] 18%|█▊        | 85/470 [00:24<01:49,  3.50it/s] 18%|█▊        | 86/470 [00:24<01:49,  3.50it/s] 19%|█▊        | 87/470 [00:25<01:49,  3.50it/s] 19%|█▊        | 88/470 [00:25<01:48,  3.51it/s] 19%|█▉        | 89/470 [00:25<01:48,  3.51it/s] 19%|█▉        | 90/470 [00:25<01:48,  3.51it/s] 19%|█▉        | 91/470 [00:26<01:48,  3.50it/s] 20%|█▉        | 92/470 [00:26<01:47,  3.51it/s] 20%|█▉        | 93/470 [00:26<01:47,  3.49it/s] 20%|██        | 94/470 [00:26<01:40,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 22:51:01,859 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:51:01,859 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 22:51:01,859 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.97it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.38it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.61it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.99it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.55it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.19it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.88it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.59it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.57it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.52it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.46it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.45it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.49it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.53it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.47it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.44it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.13it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.26it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.34it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.47it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.51it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.48it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.47it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.60it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.61it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.54it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.39it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.44it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.43it/s][A
 35%|███▌      | 153/437 [00:03<00:05, 47.43it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.45it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.52it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.55it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.18it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.31it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.27it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.32it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.40it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.31it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.33it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.34it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.45it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.53it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.41it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.44it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.38it/s][A
 54%|█████▍    | 238/437 [00:04<00:04, 47.39it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.32it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.33it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.36it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.35it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.36it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.48it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.44it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.38it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.33it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.41it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.37it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.30it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.40it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.38it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.35it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.39it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.42it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.41it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.33it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.43it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.34it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.25it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.34it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.40it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.38it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.35it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.42it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.40it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.35it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.43it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.53it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.33it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.45it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.46it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.51it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.45it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.51it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.48it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.41it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 47.41it/s][A 20%|██        | 94/470 [00:36<01:40,  3.74it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:51:11,098 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 22:51:11,127 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:51:15,991 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:51:16,042 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:51:16,065 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94/special_tokens_map.json
 20%|██        | 95/470 [00:49<43:47,  7.01s/it] 20%|██        | 96/470 [00:50<31:16,  5.02s/it] 21%|██        | 97/470 [00:50<22:22,  3.60s/it] 21%|██        | 98/470 [00:50<16:08,  2.60s/it] 21%|██        | 99/470 [00:50<11:48,  1.91s/it] 21%|██▏       | 100/470 [00:51<08:45,  1.42s/it] 21%|██▏       | 101/470 [00:51<06:38,  1.08s/it] 22%|██▏       | 102/470 [00:51<05:09,  1.19it/s] 22%|██▏       | 103/470 [00:52<04:07,  1.48it/s] 22%|██▏       | 104/470 [00:52<03:24,  1.79it/s] 22%|██▏       | 105/470 [00:52<02:53,  2.10it/s] 23%|██▎       | 106/470 [00:52<02:32,  2.39it/s] 23%|██▎       | 107/470 [00:53<02:17,  2.64it/s] 23%|██▎       | 108/470 [00:53<02:07,  2.85it/s] 23%|██▎       | 109/470 [00:53<01:59,  3.02it/s] 23%|██▎       | 110/470 [00:54<01:54,  3.15it/s] 24%|██▎       | 111/470 [00:54<01:50,  3.25it/s] 24%|██▍       | 112/470 [00:54<01:48,  3.31it/s] 24%|██▍       | 113/470 [00:54<01:46,  3.36it/s] 24%|██▍       | 114/470 [00:55<01:44,  3.40it/s] 24%|██▍       | 115/470 [00:55<01:43,  3.43it/s] 25%|██▍       | 116/470 [00:55<01:42,  3.46it/s] 25%|██▍       | 117/470 [00:56<01:41,  3.47it/s] 25%|██▌       | 118/470 [00:56<01:41,  3.48it/s] 25%|██▌       | 119/470 [00:56<01:40,  3.49it/s] 26%|██▌       | 120/470 [00:56<01:40,  3.49it/s] 26%|██▌       | 121/470 [00:57<01:39,  3.49it/s] 26%|██▌       | 122/470 [00:57<01:39,  3.50it/s] 26%|██▌       | 123/470 [00:57<01:40,  3.44it/s] 26%|██▋       | 124/470 [00:58<01:39,  3.46it/s] 27%|██▋       | 125/470 [00:58<01:39,  3.47it/s] 27%|██▋       | 126/470 [00:58<01:38,  3.48it/s] 27%|██▋       | 127/470 [00:58<01:38,  3.49it/s] 27%|██▋       | 128/470 [00:59<01:37,  3.49it/s] 27%|██▋       | 129/470 [00:59<01:37,  3.50it/s] 28%|██▊       | 130/470 [00:59<01:37,  3.50it/s] 28%|██▊       | 131/470 [01:00<01:36,  3.50it/s] 28%|██▊       | 132/470 [01:00<01:36,  3.50it/s] 28%|██▊       | 133/470 [01:00<01:36,  3.50it/s] 29%|██▊       | 134/470 [01:00<01:36,  3.48it/s] 29%|██▊       | 135/470 [01:01<01:36,  3.49it/s] 29%|██▉       | 136/470 [01:01<01:35,  3.50it/s] 29%|██▉       | 137/470 [01:01<01:35,  3.50it/s] 29%|██▉       | 138/470 [01:02<01:34,  3.50it/s] 30%|██▉       | 139/470 [01:02<01:34,  3.50it/s] 30%|██▉       | 140/470 [01:02<01:34,  3.51it/s] 30%|███       | 141/470 [01:02<01:33,  3.50it/s] 30%|███       | 142/470 [01:03<01:33,  3.51it/s] 30%|███       | 143/470 [01:03<01:33,  3.51it/s] 31%|███       | 144/470 [01:03<01:32,  3.51it/s] 31%|███       | 145/470 [01:04<01:32,  3.50it/s] 31%|███       | 146/470 [01:04<01:32,  3.50it/s] 31%|███▏      | 147/470 [01:04<01:32,  3.50it/s] 31%|███▏      | 148/470 [01:04<01:32,  3.50it/s] 32%|███▏      | 149/470 [01:05<01:31,  3.50it/s] 32%|███▏      | 150/470 [01:05<01:31,  3.50it/s] 32%|███▏      | 151/470 [01:05<01:31,  3.50it/s] 32%|███▏      | 152/470 [01:06<01:30,  3.50it/s] 33%|███▎      | 153/470 [01:06<01:30,  3.50it/s] 33%|███▎      | 154/470 [01:06<01:30,  3.50it/s] 33%|███▎      | 155/470 [01:06<01:30,  3.50it/s] 33%|███▎      | 156/470 [01:07<01:30,  3.49it/s] 33%|███▎      | 157/470 [01:07<01:29,  3.49it/s] 34%|███▎      | 158/470 [01:07<01:29,  3.49it/s] 34%|███▍      | 159/470 [01:08<01:29,  3.49it/s] 34%|███▍      | 160/470 [01:08<01:28,  3.50it/s] 34%|███▍      | 161/470 [01:08<01:28,  3.50it/s] 34%|███▍      | 162/470 [01:08<01:28,  3.49it/s] 35%|███▍      | 163/470 [01:09<01:27,  3.49it/s] 35%|███▍      | 164/470 [01:09<01:27,  3.49it/s] 35%|███▌      | 165/470 [01:09<01:27,  3.50it/s] 35%|███▌      | 166/470 [01:10<01:26,  3.50it/s] 36%|███▌      | 167/470 [01:10<01:27,  3.48it/s] 36%|███▌      | 168/470 [01:10<01:26,  3.48it/s] 36%|███▌      | 169/470 [01:10<01:26,  3.49it/s] 36%|███▌      | 170/470 [01:11<01:25,  3.49it/s] 36%|███▋      | 171/470 [01:11<01:25,  3.49it/s] 37%|███▋      | 172/470 [01:11<01:25,  3.50it/s] 37%|███▋      | 173/470 [01:12<01:24,  3.50it/s] 37%|███▋      | 174/470 [01:12<01:24,  3.50it/s] 37%|███▋      | 175/470 [01:12<01:24,  3.50it/s] 37%|███▋      | 176/470 [01:12<01:24,  3.50it/s] 38%|███▊      | 177/470 [01:13<01:23,  3.50it/s] 38%|███▊      | 178/470 [01:13<01:24,  3.46it/s] 38%|███▊      | 179/470 [01:13<01:23,  3.47it/s] 38%|███▊      | 180/470 [01:14<01:23,  3.48it/s] 39%|███▊      | 181/470 [01:14<01:22,  3.48it/s] 39%|███▊      | 182/470 [01:14<01:22,  3.49it/s] 39%|███▉      | 183/470 [01:14<01:22,  3.49it/s] 39%|███▉      | 184/470 [01:15<01:21,  3.49it/s] 39%|███▉      | 185/470 [01:15<01:21,  3.49it/s] 40%|███▉      | 186/470 [01:15<01:21,  3.49it/s] 40%|███▉      | 187/470 [01:16<01:20,  3.49it/s] 40%|████      | 188/470 [01:16<01:15,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 22:51:51,230 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:51:51,230 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 22:51:51,230 >>   Batch size = 8
{'eval_loss': 1.175108551979065, 'eval_runtime': 9.2102, 'eval_samples_per_second': 379.144, 'eval_steps_per_second': 47.447, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.46it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.47it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.58it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.79it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.34it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.99it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.72it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.52it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.36it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.39it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.35it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.46it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.41it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.48it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.43it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.35it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.23it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.31it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.23it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.22it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.24it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.42it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.34it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.37it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.28it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.25it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.16it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.96it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.11it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.58it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.78it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.06it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.14it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.25it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.22it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.22it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.11it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.17it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.22it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.21it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.28it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.43it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.41it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.40it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.33it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.22it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.20it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.24it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.20it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.24it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.30it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.39it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.43it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.26it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.24it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.16it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.18it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.27it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.30it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.21it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.29it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.39it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.33it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.19it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.21it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.19it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.22it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.28it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.27it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.23it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.30it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.32it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.25it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.12it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.16it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.24it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.28it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.31it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.33it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.26it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.27it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.22it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.19it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.10it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.16it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.29it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.29it/s][A 40%|████      | 188/470 [01:25<01:15,  3.74it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:52:00,597 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 22:52:00,644 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:52:04,228 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:52:04,378 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:52:04,420 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-188/special_tokens_map.json
 40%|████      | 189/470 [01:37<30:54,  6.60s/it] 40%|████      | 190/470 [01:38<21:57,  4.71s/it] 41%|████      | 191/470 [01:38<15:43,  3.38s/it] 41%|████      | 192/470 [01:38<11:21,  2.45s/it] 41%|████      | 193/470 [01:38<08:19,  1.80s/it] 41%|████▏     | 194/470 [01:39<06:11,  1.35s/it] 41%|████▏     | 195/470 [01:39<04:42,  1.03s/it] 42%|████▏     | 196/470 [01:39<03:40,  1.24it/s] 42%|████▏     | 197/470 [01:40<02:57,  1.54it/s] 42%|████▏     | 198/470 [01:40<02:26,  1.85it/s] 42%|████▏     | 199/470 [01:40<02:05,  2.16it/s] 43%|████▎     | 200/470 [01:40<01:50,  2.44it/s] 43%|████▎     | 201/470 [01:41<01:40,  2.66it/s] 43%|████▎     | 202/470 [01:41<01:33,  2.87it/s] 43%|████▎     | 203/470 [01:41<01:27,  3.03it/s] 43%|████▎     | 204/470 [01:42<01:24,  3.16it/s] 44%|████▎     | 205/470 [01:42<01:21,  3.25it/s] 44%|████▍     | 206/470 [01:42<01:19,  3.33it/s] 44%|████▍     | 207/470 [01:42<01:17,  3.38it/s] 44%|████▍     | 208/470 [01:43<01:16,  3.41it/s] 44%|████▍     | 209/470 [01:43<01:15,  3.44it/s] 45%|████▍     | 210/470 [01:43<01:15,  3.45it/s] 45%|████▍     | 211/470 [01:44<01:14,  3.47it/s] 45%|████▌     | 212/470 [01:44<01:14,  3.45it/s] 45%|████▌     | 213/470 [01:44<01:14,  3.46it/s] 46%|████▌     | 214/470 [01:44<01:13,  3.47it/s] 46%|████▌     | 215/470 [01:45<01:13,  3.48it/s] 46%|████▌     | 216/470 [01:45<01:12,  3.48it/s] 46%|████▌     | 217/470 [01:45<01:12,  3.49it/s] 46%|████▋     | 218/470 [01:46<01:12,  3.49it/s] 47%|████▋     | 219/470 [01:46<01:11,  3.50it/s] 47%|████▋     | 220/470 [01:46<01:11,  3.49it/s] 47%|████▋     | 221/470 [01:46<01:11,  3.49it/s] 47%|████▋     | 222/470 [01:47<01:10,  3.49it/s] 47%|████▋     | 223/470 [01:47<01:12,  3.42it/s] 48%|████▊     | 224/470 [01:47<01:11,  3.45it/s] 48%|████▊     | 225/470 [01:48<01:10,  3.46it/s] 48%|████▊     | 226/470 [01:48<01:10,  3.47it/s] 48%|████▊     | 227/470 [01:48<01:09,  3.47it/s] 49%|████▊     | 228/470 [01:48<01:09,  3.48it/s] 49%|████▊     | 229/470 [01:49<01:09,  3.48it/s] 49%|████▉     | 230/470 [01:49<01:08,  3.48it/s] 49%|████▉     | 231/470 [01:49<01:08,  3.48it/s] 49%|████▉     | 232/470 [01:50<01:15,  3.15it/s] 50%|████▉     | 233/470 [01:50<01:13,  3.24it/s] 50%|████▉     | 234/470 [01:50<01:11,  3.29it/s] 50%|█████     | 235/470 [01:51<01:10,  3.35it/s] 50%|█████     | 236/470 [01:51<01:09,  3.39it/s] 50%|█████     | 237/470 [01:51<01:08,  3.42it/s] 51%|█████     | 238/470 [01:51<01:07,  3.44it/s] 51%|█████     | 239/470 [01:52<01:06,  3.46it/s] 51%|█████     | 240/470 [01:52<01:06,  3.47it/s] 51%|█████▏    | 241/470 [01:52<01:05,  3.47it/s] 51%|█████▏    | 242/470 [01:53<01:05,  3.48it/s] 52%|█████▏    | 243/470 [01:53<01:05,  3.49it/s] 52%|█████▏    | 244/470 [01:53<01:04,  3.49it/s] 52%|█████▏    | 245/470 [01:53<01:04,  3.49it/s] 52%|█████▏    | 246/470 [01:54<01:04,  3.49it/s] 53%|█████▎    | 247/470 [01:54<01:03,  3.49it/s] 53%|█████▎    | 248/470 [01:54<01:03,  3.49it/s] 53%|█████▎    | 249/470 [01:55<01:03,  3.49it/s] 53%|█████▎    | 250/470 [01:55<01:03,  3.49it/s] 53%|█████▎    | 251/470 [01:55<01:02,  3.49it/s] 54%|█████▎    | 252/470 [01:55<01:02,  3.49it/s] 54%|█████▍    | 253/470 [01:56<01:02,  3.48it/s] 54%|█████▍    | 254/470 [01:56<01:02,  3.48it/s] 54%|█████▍    | 255/470 [01:56<01:01,  3.49it/s] 54%|█████▍    | 256/470 [01:57<01:01,  3.48it/s] 55%|█████▍    | 257/470 [01:57<01:01,  3.49it/s] 55%|█████▍    | 258/470 [01:57<01:00,  3.49it/s] 55%|█████▌    | 259/470 [01:57<01:00,  3.49it/s] 55%|█████▌    | 260/470 [01:58<01:00,  3.49it/s] 56%|█████▌    | 261/470 [01:58<00:59,  3.49it/s] 56%|█████▌    | 262/470 [01:58<00:59,  3.49it/s] 56%|█████▌    | 263/470 [01:59<00:59,  3.49it/s] 56%|█████▌    | 264/470 [01:59<00:59,  3.48it/s] 56%|█████▋    | 265/470 [01:59<00:58,  3.49it/s] 57%|█████▋    | 266/470 [01:59<00:58,  3.49it/s] 57%|█████▋    | 267/470 [02:00<00:58,  3.49it/s] 57%|█████▋    | 268/470 [02:00<00:57,  3.49it/s] 57%|█████▋    | 269/470 [02:00<00:57,  3.49it/s] 57%|█████▋    | 270/470 [02:01<00:57,  3.49it/s] 58%|█████▊    | 271/470 [02:01<00:57,  3.49it/s] 58%|█████▊    | 272/470 [02:01<00:56,  3.49it/s] 58%|█████▊    | 273/470 [02:01<00:56,  3.49it/s] 58%|█████▊    | 274/470 [02:02<00:56,  3.49it/s] 59%|█████▊    | 275/470 [02:02<00:56,  3.43it/s] 59%|█████▊    | 276/470 [02:02<00:56,  3.45it/s] 59%|█████▉    | 277/470 [02:03<00:55,  3.46it/s] 59%|█████▉    | 278/470 [02:03<00:55,  3.47it/s] 59%|█████▉    | 279/470 [02:03<00:54,  3.48it/s] 60%|█████▉    | 280/470 [02:03<00:54,  3.48it/s] 60%|█████▉    | 281/470 [02:04<00:54,  3.48it/s] 60%|██████    | 282/470 [02:04<00:50,  3.72it/s][INFO|trainer.py:2140] 2023-08-28 22:52:39,328 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:52:39,328 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 22:52:39,328 >>   Batch size = 8
{'eval_loss': 1.1925888061523438, 'eval_runtime': 9.2438, 'eval_samples_per_second': 377.765, 'eval_steps_per_second': 47.275, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.79it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.05it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.38it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.73it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.28it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.97it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.57it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.40it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.36it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.36it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.20it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.33it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.37it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.39it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.26it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.10it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.02it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.09it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.18it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.22it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.88it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.23it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.24it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.26it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.17it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.05it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.05it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.13it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.22it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.20it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.12it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.30it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.29it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.20it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.17it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.14it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.11it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.20it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.22it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.14it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.16it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.28it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.23it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.14it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.16it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.11it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.19it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.17it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.22it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.09it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.19it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.17it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.14it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.08it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.13it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.15it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.20it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.21it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.30it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.22it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.17it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.24it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.18it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.14it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.14it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.13it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.10it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.20it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.22it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.19it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.10it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.14it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.11it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.09it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.12it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.10it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.16it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.14it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.23it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.18it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.12it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.13it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.07it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.08it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.19it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.22it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.22it/s][A 60%|██████    | 282/470 [02:13<00:50,  3.72it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:52:48,620 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-282
[INFO|configuration_utils.py:351] 2023-08-28 22:52:48,677 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-282/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:52:52,679 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-282/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:52:52,695 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-282/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:52:52,708 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-282/special_tokens_map.json
 60%|██████    | 283/470 [02:24<19:37,  6.30s/it] 60%|██████    | 284/470 [02:25<13:55,  4.49s/it] 61%|██████    | 285/470 [02:25<09:57,  3.23s/it] 61%|██████    | 286/470 [02:25<07:11,  2.35s/it] 61%|██████    | 287/470 [02:25<05:16,  1.73s/it] 61%|██████▏   | 288/470 [02:26<03:55,  1.30s/it] 61%|██████▏   | 289/470 [02:26<02:59,  1.01it/s] 62%|██████▏   | 290/470 [02:26<02:20,  1.28it/s] 62%|██████▏   | 291/470 [02:27<01:53,  1.58it/s] 62%|██████▏   | 292/470 [02:27<01:34,  1.89it/s] 62%|██████▏   | 293/470 [02:27<01:20,  2.19it/s] 63%|██████▎   | 294/470 [02:27<01:11,  2.47it/s] 63%|██████▎   | 295/470 [02:28<01:04,  2.71it/s] 63%|██████▎   | 296/470 [02:28<00:59,  2.90it/s] 63%|██████▎   | 297/470 [02:28<00:56,  3.06it/s] 63%|██████▎   | 298/470 [02:29<00:54,  3.18it/s] 64%|██████▎   | 299/470 [02:29<00:52,  3.27it/s] 64%|██████▍   | 300/470 [02:29<00:50,  3.33it/s] 64%|██████▍   | 301/470 [02:29<00:50,  3.37it/s] 64%|██████▍   | 302/470 [02:30<00:49,  3.41it/s] 64%|██████▍   | 303/470 [02:30<00:48,  3.43it/s] 65%|██████▍   | 304/470 [02:30<00:48,  3.45it/s] 65%|██████▍   | 305/470 [02:31<00:47,  3.46it/s] 65%|██████▌   | 306/470 [02:31<00:47,  3.47it/s] 65%|██████▌   | 307/470 [02:31<00:46,  3.48it/s] 66%|██████▌   | 308/470 [02:31<00:46,  3.48it/s] 66%|██████▌   | 309/470 [02:32<00:46,  3.49it/s] 66%|██████▌   | 310/470 [02:32<00:45,  3.49it/s] 66%|██████▌   | 311/470 [02:32<00:45,  3.49it/s] 66%|██████▋   | 312/470 [02:33<00:45,  3.47it/s] 67%|██████▋   | 313/470 [02:33<00:45,  3.48it/s] 67%|██████▋   | 314/470 [02:33<00:44,  3.48it/s] 67%|██████▋   | 315/470 [02:33<00:44,  3.48it/s] 67%|██████▋   | 316/470 [02:34<00:44,  3.48it/s] 67%|██████▋   | 317/470 [02:34<00:43,  3.49it/s] 68%|██████▊   | 318/470 [02:34<00:43,  3.49it/s] 68%|██████▊   | 319/470 [02:35<00:43,  3.49it/s] 68%|██████▊   | 320/470 [02:35<00:42,  3.49it/s] 68%|██████▊   | 321/470 [02:35<00:42,  3.49it/s] 69%|██████▊   | 322/470 [02:35<00:42,  3.49it/s] 69%|██████▊   | 323/470 [02:36<00:42,  3.46it/s] 69%|██████▉   | 324/470 [02:36<00:42,  3.47it/s] 69%|██████▉   | 325/470 [02:36<00:41,  3.48it/s] 69%|██████▉   | 326/470 [02:37<00:41,  3.48it/s] 70%|██████▉   | 327/470 [02:37<00:41,  3.49it/s] 70%|██████▉   | 328/470 [02:37<00:40,  3.49it/s] 70%|███████   | 329/470 [02:38<00:40,  3.49it/s] 70%|███████   | 330/470 [02:38<00:40,  3.49it/s] 70%|███████   | 331/470 [02:38<00:39,  3.49it/s] 71%|███████   | 332/470 [02:38<00:39,  3.49it/s] 71%|███████   | 333/470 [02:39<00:39,  3.49it/s] 71%|███████   | 334/470 [02:39<00:39,  3.47it/s] 71%|███████▏  | 335/470 [02:39<00:38,  3.48it/s] 71%|███████▏  | 336/470 [02:40<00:38,  3.48it/s] 72%|███████▏  | 337/470 [02:40<00:38,  3.49it/s] 72%|███████▏  | 338/470 [02:40<00:37,  3.49it/s] 72%|███████▏  | 339/470 [02:40<00:37,  3.49it/s] 72%|███████▏  | 340/470 [02:41<00:37,  3.49it/s] 73%|███████▎  | 341/470 [02:41<00:36,  3.49it/s] 73%|███████▎  | 342/470 [02:41<00:36,  3.49it/s] 73%|███████▎  | 343/470 [02:42<00:36,  3.49it/s] 73%|███████▎  | 344/470 [02:42<00:36,  3.49it/s] 73%|███████▎  | 345/470 [02:42<00:36,  3.45it/s] 74%|███████▎  | 346/470 [02:42<00:35,  3.46it/s] 74%|███████▍  | 347/470 [02:43<00:35,  3.47it/s] 74%|███████▍  | 348/470 [02:43<00:35,  3.48it/s] 74%|███████▍  | 349/470 [02:43<00:34,  3.48it/s] 74%|███████▍  | 350/470 [02:44<00:34,  3.49it/s] 75%|███████▍  | 351/470 [02:44<00:34,  3.49it/s] 75%|███████▍  | 352/470 [02:44<00:33,  3.49it/s] 75%|███████▌  | 353/470 [02:44<00:33,  3.49it/s] 75%|███████▌  | 354/470 [02:45<00:33,  3.49it/s] 76%|███████▌  | 355/470 [02:45<00:32,  3.49it/s] 76%|███████▌  | 356/470 [02:45<00:32,  3.46it/s] 76%|███████▌  | 357/470 [02:46<00:32,  3.47it/s] 76%|███████▌  | 358/470 [02:46<00:32,  3.48it/s] 76%|███████▋  | 359/470 [02:46<00:31,  3.48it/s] 77%|███████▋  | 360/470 [02:46<00:31,  3.49it/s] 77%|███████▋  | 361/470 [02:47<00:31,  3.49it/s] 77%|███████▋  | 362/470 [02:47<00:30,  3.49it/s] 77%|███████▋  | 363/470 [02:47<00:30,  3.49it/s] 77%|███████▋  | 364/470 [02:48<00:30,  3.49it/s] 78%|███████▊  | 365/470 [02:48<00:30,  3.49it/s] 78%|███████▊  | 366/470 [02:48<00:29,  3.49it/s] 78%|███████▊  | 367/470 [02:48<00:31,  3.30it/s] 78%|███████▊  | 368/470 [02:49<00:30,  3.35it/s] 79%|███████▊  | 369/470 [02:49<00:30,  3.33it/s] 79%|███████▊  | 370/470 [02:49<00:29,  3.37it/s] 79%|███████▉  | 371/470 [02:50<00:29,  3.33it/s] 79%|███████▉  | 372/470 [02:50<00:29,  3.37it/s] 79%|███████▉  | 373/470 [02:50<00:28,  3.41it/s] 80%|███████▉  | 374/470 [02:51<00:27,  3.44it/s] 80%|███████▉  | 375/470 [02:51<00:27,  3.45it/s] 80%|████████  | 376/470 [02:51<00:25,  3.70it/s][INFO|trainer.py:2140] 2023-08-28 22:53:26,403 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:53:26,404 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 22:53:26,404 >>   Batch size = 8
{'eval_loss': 1.2098230123519897, 'eval_runtime': 9.2562, 'eval_samples_per_second': 377.259, 'eval_steps_per_second': 47.211, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.29it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.03it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.26it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.60it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.08it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.71it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.55it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.34it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.27it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.30it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.20it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.20it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.98it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.14it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.09it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.12it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.06it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.10it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.11it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.26it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.09it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.21it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.21it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.28it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.14it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.20it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.16it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.09it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.12it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.27it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.17it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.08it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.17it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.11it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.10it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.18it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.15it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.11it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.14it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.05it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.07it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.02it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.15it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.04it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.06it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.10it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.11it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.06it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.13it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.20it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.18it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.17it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.99it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.06it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.10it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.11it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.10it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.05it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.14it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.15it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.15it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.17it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.16it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.02it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.11it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.09it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.10it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.10it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.10it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.14it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.14it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.25it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.21it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.04it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.09it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.09it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.07it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.13it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.11it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.17it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.15it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.25it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.08it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.06it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.11it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.11it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.11it/s][A 80%|████████  | 376/470 [03:00<00:25,  3.70it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:53:35,701 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-376
[INFO|configuration_utils.py:351] 2023-08-28 22:53:35,767 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-376/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:53:39,149 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-376/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:53:39,165 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-376/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:53:39,175 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-376/special_tokens_map.json
 80%|████████  | 377/470 [03:11<09:27,  6.11s/it] 80%|████████  | 378/470 [03:11<06:41,  4.36s/it] 81%|████████  | 379/470 [03:11<04:45,  3.14s/it] 81%|████████  | 380/470 [03:12<03:25,  2.28s/it] 81%|████████  | 381/470 [03:12<02:29,  1.68s/it] 81%|████████▏ | 382/470 [03:12<01:51,  1.27s/it] 81%|████████▏ | 383/470 [03:12<01:24,  1.03it/s] 82%|████████▏ | 384/470 [03:13<01:05,  1.31it/s] 82%|████████▏ | 385/470 [03:13<00:52,  1.61it/s] 82%|████████▏ | 386/470 [03:13<00:43,  1.92it/s] 82%|████████▏ | 387/470 [03:14<00:37,  2.22it/s] 83%|████████▎ | 388/470 [03:14<00:32,  2.49it/s] 83%|████████▎ | 389/470 [03:14<00:29,  2.71it/s] 83%|████████▎ | 390/470 [03:14<00:27,  2.91it/s] 83%|████████▎ | 391/470 [03:15<00:25,  3.06it/s] 83%|████████▎ | 392/470 [03:15<00:24,  3.18it/s] 84%|████████▎ | 393/470 [03:15<00:23,  3.27it/s] 84%|████████▍ | 394/470 [03:16<00:22,  3.33it/s] 84%|████████▍ | 395/470 [03:16<00:22,  3.38it/s] 84%|████████▍ | 396/470 [03:16<00:21,  3.41it/s] 84%|████████▍ | 397/470 [03:16<00:21,  3.44it/s] 85%|████████▍ | 398/470 [03:17<00:20,  3.45it/s] 85%|████████▍ | 399/470 [03:17<00:20,  3.47it/s] 85%|████████▌ | 400/470 [03:17<00:20,  3.46it/s] 85%|████████▌ | 401/470 [03:18<00:19,  3.47it/s] 86%|████████▌ | 402/470 [03:18<00:19,  3.47it/s] 86%|████████▌ | 403/470 [03:18<00:19,  3.48it/s] 86%|████████▌ | 404/470 [03:19<00:18,  3.48it/s] 86%|████████▌ | 405/470 [03:19<00:18,  3.49it/s] 86%|████████▋ | 406/470 [03:19<00:18,  3.49it/s] 87%|████████▋ | 407/470 [03:19<00:18,  3.49it/s] 87%|████████▋ | 408/470 [03:20<00:17,  3.49it/s] 87%|████████▋ | 409/470 [03:20<00:17,  3.49it/s] 87%|████████▋ | 410/470 [03:20<00:17,  3.49it/s] 87%|████████▋ | 411/470 [03:21<00:16,  3.48it/s] 88%|████████▊ | 412/470 [03:21<00:16,  3.48it/s] 88%|████████▊ | 413/470 [03:21<00:16,  3.49it/s] 88%|████████▊ | 414/470 [03:21<00:16,  3.49it/s] 88%|████████▊ | 415/470 [03:22<00:15,  3.49it/s] 89%|████████▊ | 416/470 [03:22<00:15,  3.49it/s] 89%|████████▊ | 417/470 [03:22<00:15,  3.49it/s] 89%|████████▉ | 418/470 [03:23<00:14,  3.49it/s] 89%|████████▉ | 419/470 [03:23<00:14,  3.49it/s] 89%|████████▉ | 420/470 [03:23<00:14,  3.49it/s] 90%|████████▉ | 421/470 [03:23<00:14,  3.49it/s] 90%|████████▉ | 422/470 [03:24<00:13,  3.46it/s] 90%|█████████ | 423/470 [03:24<00:13,  3.47it/s] 90%|█████████ | 424/470 [03:24<00:13,  3.47it/s] 90%|█████████ | 425/470 [03:25<00:12,  3.47it/s] 91%|█████████ | 426/470 [03:25<00:12,  3.48it/s] 91%|█████████ | 427/470 [03:25<00:12,  3.48it/s] 91%|█████████ | 428/470 [03:25<00:12,  3.48it/s] 91%|█████████▏| 429/470 [03:26<00:11,  3.48it/s] 91%|█████████▏| 430/470 [03:26<00:11,  3.49it/s] 92%|█████████▏| 431/470 [03:26<00:11,  3.49it/s] 92%|█████████▏| 432/470 [03:27<00:10,  3.49it/s] 92%|█████████▏| 433/470 [03:27<00:10,  3.49it/s] 92%|█████████▏| 434/470 [03:27<00:10,  3.49it/s] 93%|█████████▎| 435/470 [03:27<00:10,  3.49it/s] 93%|█████████▎| 436/470 [03:28<00:09,  3.45it/s] 93%|█████████▎| 437/470 [03:28<00:09,  3.46it/s] 93%|█████████▎| 438/470 [03:28<00:09,  3.47it/s] 93%|█████████▎| 439/470 [03:29<00:08,  3.48it/s] 94%|█████████▎| 440/470 [03:29<00:08,  3.48it/s] 94%|█████████▍| 441/470 [03:29<00:08,  3.48it/s] 94%|█████████▍| 442/470 [03:29<00:08,  3.48it/s] 94%|█████████▍| 443/470 [03:30<00:07,  3.49it/s] 94%|█████████▍| 444/470 [03:30<00:07,  3.49it/s] 95%|█████████▍| 445/470 [03:30<00:07,  3.49it/s] 95%|█████████▍| 446/470 [03:31<00:06,  3.49it/s] 95%|█████████▌| 447/470 [03:31<00:06,  3.47it/s] 95%|█████████▌| 448/470 [03:31<00:06,  3.48it/s] 96%|█████████▌| 449/470 [03:31<00:06,  3.48it/s] 96%|█████████▌| 450/470 [03:32<00:05,  3.48it/s] 96%|█████████▌| 451/470 [03:32<00:05,  3.48it/s] 96%|█████████▌| 452/470 [03:32<00:05,  3.49it/s] 96%|█████████▋| 453/470 [03:33<00:04,  3.48it/s] 97%|█████████▋| 454/470 [03:33<00:04,  3.49it/s] 97%|█████████▋| 455/470 [03:33<00:04,  3.48it/s] 97%|█████████▋| 456/470 [03:33<00:04,  3.49it/s] 97%|█████████▋| 457/470 [03:34<00:03,  3.49it/s] 97%|█████████▋| 458/470 [03:34<00:03,  3.45it/s] 98%|█████████▊| 459/470 [03:34<00:03,  3.46it/s] 98%|█████████▊| 460/470 [03:35<00:02,  3.47it/s] 98%|█████████▊| 461/470 [03:35<00:02,  3.47it/s] 98%|█████████▊| 462/470 [03:35<00:02,  3.48it/s] 99%|█████████▊| 463/470 [03:35<00:02,  3.48it/s] 99%|█████████▊| 464/470 [03:36<00:01,  3.48it/s] 99%|█████████▉| 465/470 [03:36<00:01,  3.49it/s] 99%|█████████▉| 466/470 [03:36<00:01,  3.49it/s] 99%|█████████▉| 467/470 [03:37<00:00,  3.49it/s]100%|█████████▉| 468/470 [03:37<00:00,  3.49it/s]100%|█████████▉| 469/470 [03:37<00:00,  3.47it/s]100%|██████████| 470/470 [03:37<00:00,  3.72it/s][INFO|trainer.py:2140] 2023-08-28 22:54:12,775 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:54:12,775 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 22:54:12,775 >>   Batch size = 8
{'eval_loss': 1.2205318212509155, 'eval_runtime': 9.2667, 'eval_samples_per_second': 376.833, 'eval_steps_per_second': 47.158, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.09it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.07it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.36it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.54it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.13it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.76it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.58it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.32it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.27it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.30it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.25it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.26it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.25it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.22it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.11it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.14it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.08it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.10it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.17it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.22it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.14it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.15it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.15it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.17it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.11it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.09it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.06it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.07it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.22it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.22it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.12it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.17it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.15it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.14it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.06it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.13it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.12it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.11it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.25it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.15it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.11it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.18it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.16it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.11it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.21it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.04it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.10it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.10it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.23it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.11it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.13it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.11it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 45.35it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 45.93it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.34it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.47it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.77it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.83it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.89it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.98it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.93it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.88it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.09it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.11it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.02it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.19it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.15it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.08it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.13it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.08it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.02it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.07it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.15it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.11it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.15it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.17it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.23it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.15it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.20it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.15it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.06it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.98it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.09it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.12it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.07it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.14it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.14it/s][A100%|██████████| 470/470 [03:47<00:00,  3.72it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:54:22,070 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-470
[INFO|configuration_utils.py:351] 2023-08-28 22:54:22,083 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-470/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:54:26,047 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:54:26,078 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:54:26,105 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-470/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 22:54:35,812 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 22:54:35,817 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94 (score: 1.175108551979065).
                                                 100%|██████████| 470/470 [04:05<00:00,  3.72it/s]100%|██████████| 470/470 [04:05<00:00,  1.92it/s]
[INFO|trainer.py:1894] 2023-08-28 22:54:39,889 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 22:54:39,922 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:54:43,375 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:54:43,392 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:54:43,400 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:54:43,576 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:43,576 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:43,576 >>   train_loss               =     0.3682
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:43,576 >>   train_runtime            = 0:04:05.00
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:43,576 >>   train_samples            =       6000
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:43,576 >>   train_samples_per_second =    122.445
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:43,576 >>   train_steps_per_second   =      1.918
{'eval_loss': 1.2285836935043335, 'eval_runtime': 9.2788, 'eval_samples_per_second': 376.341, 'eval_steps_per_second': 47.097, 'epoch': 5.0}
{'train_runtime': 245.0087, 'train_samples_per_second': 122.445, 'train_steps_per_second': 1.918, 'train_loss': 0.36823740208402594, 'epoch': 5.0}
08/28/2023 22:54:43 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 22:54:43,614 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:54:43,614 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 22:54:43,614 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 59.48it/s]  3%|▎         | 12/437 [00:00<00:08, 51.88it/s]  4%|▍         | 18/437 [00:00<00:08, 49.84it/s]  5%|▌         | 24/437 [00:00<00:08, 48.97it/s]  7%|▋         | 29/437 [00:00<00:08, 48.59it/s]  8%|▊         | 34/437 [00:00<00:08, 48.33it/s]  9%|▉         | 39/437 [00:00<00:08, 48.08it/s] 10%|█         | 44/437 [00:00<00:08, 47.97it/s] 11%|█         | 49/437 [00:01<00:08, 47.72it/s] 12%|█▏        | 54/437 [00:01<00:08, 47.69it/s] 14%|█▎        | 59/437 [00:01<00:07, 47.54it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.66it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.62it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.57it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.57it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.65it/s] 20%|██        | 89/437 [00:01<00:07, 47.58it/s] 22%|██▏       | 94/437 [00:01<00:07, 47.62it/s] 23%|██▎       | 99/437 [00:02<00:07, 47.60it/s] 24%|██▍       | 104/437 [00:02<00:07, 47.57it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.57it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.47it/s] 27%|██▋       | 119/437 [00:02<00:06, 47.53it/s] 28%|██▊       | 124/437 [00:02<00:06, 47.45it/s] 30%|██▉       | 129/437 [00:02<00:06, 47.50it/s] 31%|███       | 134/437 [00:02<00:06, 47.51it/s] 32%|███▏      | 139/437 [00:02<00:06, 47.39it/s] 33%|███▎      | 144/437 [00:03<00:06, 47.35it/s] 34%|███▍      | 149/437 [00:03<00:06, 47.42it/s] 35%|███▌      | 154/437 [00:03<00:05, 47.47it/s] 36%|███▋      | 159/437 [00:03<00:05, 47.37it/s] 38%|███▊      | 164/437 [00:03<00:05, 47.36it/s] 39%|███▊      | 169/437 [00:03<00:05, 47.36it/s] 40%|███▉      | 174/437 [00:03<00:05, 47.47it/s] 41%|████      | 179/437 [00:03<00:05, 47.55it/s] 42%|████▏     | 184/437 [00:03<00:05, 47.55it/s] 43%|████▎     | 189/437 [00:03<00:05, 47.56it/s] 44%|████▍     | 194/437 [00:04<00:05, 47.47it/s] 46%|████▌     | 199/437 [00:04<00:05, 47.44it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.43it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.48it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.55it/s] 50%|█████     | 219/437 [00:04<00:04, 47.50it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.44it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.45it/s] 54%|█████▎    | 234/437 [00:04<00:04, 47.48it/s] 55%|█████▍    | 239/437 [00:05<00:04, 47.41it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.32it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.37it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.39it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.38it/s] 60%|██████    | 264/437 [00:05<00:03, 47.40it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.45it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.33it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.37it/s] 65%|██████▍   | 284/437 [00:05<00:03, 47.41it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.34it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.34it/s] 68%|██████▊   | 299/437 [00:06<00:02, 47.32it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.38it/s] 71%|███████   | 309/437 [00:06<00:02, 47.40it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.37it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.38it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.20it/s] 75%|███████▌  | 329/437 [00:06<00:02, 47.22it/s] 76%|███████▋  | 334/437 [00:07<00:02, 47.27it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.21it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.24it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.29it/s] 81%|████████  | 354/437 [00:07<00:01, 47.28it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.35it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.37it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.31it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.29it/s] 87%|████████▋ | 379/437 [00:07<00:01, 47.41it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.41it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.37it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.27it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.29it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.30it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.35it/s] 95%|█████████▍| 414/437 [00:08<00:00, 47.39it/s] 96%|█████████▌| 419/437 [00:08<00:00, 47.32it/s] 97%|█████████▋| 424/437 [00:08<00:00, 47.35it/s] 98%|█████████▊| 429/437 [00:09<00:00, 47.40it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.38it/s]100%|██████████| 437/437 [00:09<00:00, 47.55it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:54:52,829 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:52,829 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:52,829 >>   eval_loss               =     1.1751
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:52,829 >>   eval_runtime            = 0:00:09.21
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:52,829 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:52,829 >>   eval_samples_per_second =    378.971
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:52,829 >>   eval_steps_per_second   =     47.426
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:54:52,829 >>   perplexity              =     3.2385
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:00,078 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:00,083 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:00,083 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:00,083 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:00,083 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:55:00,422 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:55:00,423 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:55:00,704 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:55:01,741 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:55:01,741 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:03,489 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:03,491 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:03,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:03,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:03,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:55:03,834 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:55:03,835 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:55:04,107 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:55:04,252 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:55:04,256 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-282
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-376
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-470
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.48it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.47it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:19,  1.55it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.53it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.52it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.53it/s]Extractor Predicting: 40it [00:26,  1.44it/s]Extractor Predicting: 41it [00:27,  1.46it/s]Extractor Predicting: 42it [00:27,  1.46it/s]Extractor Predicting: 43it [00:28,  1.49it/s]Extractor Predicting: 44it [00:29,  1.49it/s]Extractor Predicting: 45it [00:29,  1.48it/s]Extractor Predicting: 46it [00:30,  1.48it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:32,  1.50it/s]Extractor Predicting: 50it [00:33,  1.52it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:34,  1.47it/s]Extractor Predicting: 53it [00:35,  1.47it/s]Extractor Predicting: 54it [00:35,  1.48it/s]Extractor Predicting: 55it [00:36,  1.49it/s]Extractor Predicting: 56it [00:37,  1.50it/s]Extractor Predicting: 57it [00:37,  1.50it/s]Extractor Predicting: 58it [00:38,  1.52it/s]Extractor Predicting: 59it [00:39,  1.51it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.47it/s]Extractor Predicting: 62it [00:41,  1.47it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:42,  1.45it/s]Extractor Predicting: 65it [00:43,  1.46it/s]Extractor Predicting: 66it [00:43,  1.44it/s]Extractor Predicting: 67it [00:44,  1.44it/s]Extractor Predicting: 68it [00:45,  1.43it/s]Extractor Predicting: 69it [00:46,  1.43it/s]Extractor Predicting: 70it [00:46,  1.43it/s]Extractor Predicting: 71it [00:47,  1.43it/s]Extractor Predicting: 72it [00:48,  1.41it/s]Extractor Predicting: 73it [00:48,  1.43it/s]Extractor Predicting: 74it [00:49,  1.40it/s]Extractor Predicting: 75it [00:50,  1.44it/s]Extractor Predicting: 76it [00:50,  1.44it/s]Extractor Predicting: 77it [00:51,  1.43it/s]Extractor Predicting: 78it [00:52,  1.45it/s]Extractor Predicting: 79it [00:53,  1.43it/s]Extractor Predicting: 80it [00:53,  1.41it/s]Extractor Predicting: 81it [00:54,  1.41it/s]Extractor Predicting: 82it [00:55,  1.44it/s]Extractor Predicting: 83it [00:55,  1.45it/s]Extractor Predicting: 84it [00:56,  1.47it/s]Extractor Predicting: 85it [00:57,  1.47it/s]Extractor Predicting: 86it [00:57,  1.43it/s]Extractor Predicting: 87it [00:58,  1.45it/s]Extractor Predicting: 88it [00:59,  1.49it/s]Extractor Predicting: 89it [00:59,  1.54it/s]Extractor Predicting: 90it [01:00,  1.53it/s]Extractor Predicting: 91it [01:01,  1.58it/s]Extractor Predicting: 92it [01:01,  1.64it/s]Extractor Predicting: 93it [01:02,  1.67it/s]Extractor Predicting: 94it [01:02,  1.70it/s]Extractor Predicting: 95it [01:03,  1.65it/s]Extractor Predicting: 96it [01:03,  1.68it/s]Extractor Predicting: 97it [01:04,  1.63it/s]Extractor Predicting: 98it [01:05,  1.62it/s]Extractor Predicting: 99it [01:05,  1.67it/s]Extractor Predicting: 100it [01:06,  1.68it/s]Extractor Predicting: 101it [01:07,  1.65it/s]Extractor Predicting: 102it [01:07,  1.60it/s]Extractor Predicting: 103it [01:08,  1.60it/s]Extractor Predicting: 104it [01:08,  1.56it/s]Extractor Predicting: 105it [01:09,  1.57it/s]Extractor Predicting: 106it [01:10,  1.58it/s]Extractor Predicting: 107it [01:10,  1.55it/s]Extractor Predicting: 108it [01:11,  1.59it/s]Extractor Predicting: 109it [01:12,  1.61it/s]Extractor Predicting: 110it [01:12,  1.62it/s]Extractor Predicting: 111it [01:13,  1.63it/s]Extractor Predicting: 112it [01:13,  1.64it/s]Extractor Predicting: 113it [01:14,  1.67it/s]Extractor Predicting: 114it [01:15,  1.63it/s]Extractor Predicting: 115it [01:15,  1.65it/s]Extractor Predicting: 116it [01:16,  1.61it/s]Extractor Predicting: 117it [01:17,  1.57it/s]Extractor Predicting: 118it [01:17,  1.57it/s]Extractor Predicting: 119it [01:18,  1.55it/s]Extractor Predicting: 120it [01:19,  1.52it/s]Extractor Predicting: 121it [01:19,  1.52it/s]Extractor Predicting: 122it [01:20,  1.40it/s]Extractor Predicting: 123it [01:21,  1.41it/s]Extractor Predicting: 124it [01:21,  1.41it/s]Extractor Predicting: 125it [01:22,  1.43it/s]Extractor Predicting: 126it [01:23,  1.43it/s]Extractor Predicting: 127it [01:24,  1.43it/s]Extractor Predicting: 128it [01:24,  1.44it/s]Extractor Predicting: 129it [01:25,  1.46it/s]Extractor Predicting: 130it [01:25,  1.50it/s]Extractor Predicting: 131it [01:26,  1.46it/s]Extractor Predicting: 132it [01:27,  1.45it/s]Extractor Predicting: 133it [01:28,  1.45it/s]Extractor Predicting: 134it [01:28,  1.46it/s]Extractor Predicting: 135it [01:29,  1.47it/s]Extractor Predicting: 136it [01:30,  1.46it/s]Extractor Predicting: 137it [01:30,  1.46it/s]Extractor Predicting: 138it [01:31,  1.49it/s]Extractor Predicting: 139it [01:32,  1.49it/s]Extractor Predicting: 140it [01:32,  1.51it/s]Extractor Predicting: 141it [01:33,  1.48it/s]Extractor Predicting: 142it [01:34,  1.47it/s]Extractor Predicting: 143it [01:34,  1.48it/s]Extractor Predicting: 144it [01:35,  1.51it/s]Extractor Predicting: 144it [01:35,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:56:47,269 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:56:47,275 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:56:47,275 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:56:47,275 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:56:47,275 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:56:47,642 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:56:47,643 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:56:48,327 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:56:49,372 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:56:49,372 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:56:51,150 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:56:51,157 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:56:51,157 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:56:51,157 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:56:51,157 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:56:51,506 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:56:51,507 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:56:51,793 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:56:51,963 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:56:51,963 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.40152963671128106,
  "recall": 0.18041237113402062,
  "score": 0.24896265560165975,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.42it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.50it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.50it/s]Extractor Predicting: 29it [00:19,  1.48it/s]Extractor Predicting: 30it [00:19,  1.45it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:21,  1.47it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.51it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.58it/s]Extractor Predicting: 38it [00:25,  1.58it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:26,  1.58it/s]Extractor Predicting: 41it [00:26,  1.58it/s]Extractor Predicting: 42it [00:27,  1.57it/s]Extractor Predicting: 43it [00:28,  1.56it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:30,  1.56it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:31,  1.56it/s]Extractor Predicting: 49it [00:32,  1.55it/s]Extractor Predicting: 50it [00:32,  1.54it/s]Extractor Predicting: 51it [00:33,  1.55it/s]Extractor Predicting: 52it [00:34,  1.54it/s]Extractor Predicting: 53it [00:34,  1.54it/s]Extractor Predicting: 54it [00:35,  1.55it/s]Extractor Predicting: 55it [00:36,  1.53it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:37,  1.57it/s]Extractor Predicting: 58it [00:37,  1.59it/s]Extractor Predicting: 59it [00:38,  1.62it/s]Extractor Predicting: 60it [00:39,  1.60it/s]Extractor Predicting: 61it [00:39,  1.58it/s]Extractor Predicting: 62it [00:40,  1.58it/s]Extractor Predicting: 63it [00:41,  1.59it/s]Extractor Predicting: 64it [00:41,  1.55it/s]Extractor Predicting: 65it [00:42,  1.55it/s]Extractor Predicting: 66it [00:43,  1.55it/s]Extractor Predicting: 67it [00:43,  1.56it/s]Extractor Predicting: 68it [00:44,  1.62it/s]Extractor Predicting: 69it [00:44,  1.59it/s]Extractor Predicting: 70it [00:45,  1.59it/s]Extractor Predicting: 71it [00:46,  1.58it/s]Extractor Predicting: 72it [00:46,  1.58it/s]Extractor Predicting: 73it [00:47,  1.54it/s]Extractor Predicting: 74it [00:48,  1.54it/s]Extractor Predicting: 75it [00:48,  1.55it/s]Extractor Predicting: 76it [00:49,  1.56it/s]Extractor Predicting: 77it [00:50,  1.53it/s]Extractor Predicting: 78it [00:50,  1.54it/s]Extractor Predicting: 79it [00:51,  1.39it/s]Extractor Predicting: 80it [00:52,  1.43it/s]Extractor Predicting: 81it [00:52,  1.45it/s]Extractor Predicting: 82it [00:53,  1.46it/s]Extractor Predicting: 83it [00:54,  1.48it/s]Extractor Predicting: 84it [00:54,  1.50it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:56,  1.52it/s]Extractor Predicting: 87it [00:56,  1.51it/s]Extractor Predicting: 88it [00:57,  1.51it/s]Extractor Predicting: 89it [00:58,  1.53it/s]Extractor Predicting: 90it [00:58,  1.53it/s]Extractor Predicting: 91it [00:59,  1.53it/s]Extractor Predicting: 92it [01:00,  1.49it/s]Extractor Predicting: 93it [01:00,  1.54it/s]Extractor Predicting: 94it [01:01,  1.51it/s]Extractor Predicting: 95it [01:02,  1.51it/s]Extractor Predicting: 96it [01:02,  1.52it/s]Extractor Predicting: 97it [01:03,  1.54it/s]Extractor Predicting: 98it [01:04,  1.54it/s]Extractor Predicting: 99it [01:04,  1.55it/s]Extractor Predicting: 100it [01:05,  1.56it/s]Extractor Predicting: 101it [01:05,  1.57it/s]Extractor Predicting: 102it [01:06,  1.57it/s]Extractor Predicting: 103it [01:07,  1.53it/s]Extractor Predicting: 104it [01:07,  1.51it/s]Extractor Predicting: 105it [01:08,  1.51it/s]Extractor Predicting: 106it [01:09,  1.51it/s]Extractor Predicting: 107it [01:09,  1.51it/s]Extractor Predicting: 108it [01:10,  1.52it/s]Extractor Predicting: 109it [01:11,  1.51it/s]Extractor Predicting: 110it [01:11,  1.52it/s]Extractor Predicting: 111it [01:12,  1.50it/s]Extractor Predicting: 112it [01:13,  1.49it/s]Extractor Predicting: 113it [01:13,  1.48it/s]Extractor Predicting: 114it [01:14,  1.48it/s]Extractor Predicting: 115it [01:15,  1.49it/s]Extractor Predicting: 116it [01:15,  1.53it/s]Extractor Predicting: 117it [01:16,  1.54it/s]Extractor Predicting: 118it [01:17,  1.55it/s]Extractor Predicting: 119it [01:17,  1.54it/s]Extractor Predicting: 120it [01:18,  1.54it/s]Extractor Predicting: 121it [01:19,  1.56it/s]Extractor Predicting: 122it [01:19,  1.57it/s]Extractor Predicting: 123it [01:20,  1.59it/s]Extractor Predicting: 124it [01:20,  1.60it/s]Extractor Predicting: 125it [01:21,  1.59it/s]Extractor Predicting: 126it [01:22,  1.59it/s]Extractor Predicting: 127it [01:22,  1.60it/s]Extractor Predicting: 128it [01:23,  1.58it/s]Extractor Predicting: 129it [01:24,  1.55it/s]Extractor Predicting: 130it [01:24,  1.53it/s]Extractor Predicting: 131it [01:25,  1.57it/s]Extractor Predicting: 132it [01:26,  1.57it/s]Extractor Predicting: 133it [01:26,  1.58it/s]Extractor Predicting: 134it [01:27,  1.59it/s]Extractor Predicting: 135it [01:27,  1.60it/s]Extractor Predicting: 136it [01:28,  1.57it/s]Extractor Predicting: 137it [01:29,  1.58it/s]Extractor Predicting: 138it [01:29,  1.55it/s]Extractor Predicting: 139it [01:30,  1.56it/s]Extractor Predicting: 140it [01:31,  1.61it/s]Extractor Predicting: 141it [01:31,  1.61it/s]Extractor Predicting: 142it [01:32,  1.59it/s]Extractor Predicting: 143it [01:33,  1.56it/s]Extractor Predicting: 144it [01:33,  1.57it/s]Extractor Predicting: 145it [01:34,  1.58it/s]Extractor Predicting: 146it [01:34,  1.60it/s]Extractor Predicting: 147it [01:35,  1.61it/s]Extractor Predicting: 148it [01:36,  1.55it/s]Extractor Predicting: 149it [01:36,  1.58it/s]Extractor Predicting: 150it [01:37,  1.57it/s]Extractor Predicting: 151it [01:38,  1.57it/s]Extractor Predicting: 152it [01:38,  1.57it/s]Extractor Predicting: 153it [01:39,  1.54it/s]Extractor Predicting: 154it [01:40,  1.57it/s]Extractor Predicting: 155it [01:40,  1.58it/s]Extractor Predicting: 156it [01:41,  1.60it/s]Extractor Predicting: 157it [01:41,  1.61it/s]Extractor Predicting: 158it [01:42,  1.66it/s]Extractor Predicting: 159it [01:43,  1.62it/s]Extractor Predicting: 160it [01:43,  1.58it/s]Extractor Predicting: 161it [01:44,  1.41it/s]Extractor Predicting: 162it [01:45,  1.44it/s]Extractor Predicting: 163it [01:45,  1.48it/s]Extractor Predicting: 164it [01:46,  1.50it/s]Extractor Predicting: 165it [01:47,  1.55it/s]Extractor Predicting: 166it [01:47,  1.58it/s]Extractor Predicting: 167it [01:48,  1.57it/s]Extractor Predicting: 168it [01:49,  1.53it/s]Extractor Predicting: 169it [01:49,  1.54it/s]Extractor Predicting: 170it [01:50,  1.54it/s]Extractor Predicting: 171it [01:51,  1.54it/s]Extractor Predicting: 172it [01:51,  1.55it/s]Extractor Predicting: 173it [01:52,  1.54it/s]Extractor Predicting: 174it [01:52,  1.54it/s]Extractor Predicting: 175it [01:53,  1.51it/s]Extractor Predicting: 176it [01:54,  1.49it/s]Extractor Predicting: 177it [01:54,  1.53it/s]Extractor Predicting: 178it [01:55,  1.53it/s]Extractor Predicting: 179it [01:56,  1.55it/s]Extractor Predicting: 180it [01:56,  1.58it/s]Extractor Predicting: 181it [01:57,  1.54it/s]Extractor Predicting: 182it [01:58,  1.55it/s]Extractor Predicting: 183it [01:58,  1.54it/s]Extractor Predicting: 184it [01:59,  1.53it/s]Extractor Predicting: 185it [02:00,  1.52it/s]Extractor Predicting: 186it [02:00,  1.52it/s]Extractor Predicting: 187it [02:01,  1.52it/s]Extractor Predicting: 188it [02:02,  1.50it/s]Extractor Predicting: 189it [02:02,  1.50it/s]Extractor Predicting: 190it [02:03,  1.51it/s]Extractor Predicting: 191it [02:04,  1.54it/s]Extractor Predicting: 192it [02:04,  1.58it/s]Extractor Predicting: 193it [02:05,  1.51it/s]Extractor Predicting: 194it [02:06,  1.50it/s]Extractor Predicting: 195it [02:06,  1.54it/s]Extractor Predicting: 196it [02:07,  1.55it/s]Extractor Predicting: 197it [02:08,  1.55it/s]Extractor Predicting: 198it [02:08,  1.53it/s]Extractor Predicting: 199it [02:09,  1.56it/s]Extractor Predicting: 200it [02:09,  1.61it/s]Extractor Predicting: 201it [02:10,  1.63it/s]Extractor Predicting: 202it [02:11,  1.62it/s]Extractor Predicting: 203it [02:11,  1.59it/s]Extractor Predicting: 204it [02:12,  1.58it/s]Extractor Predicting: 205it [02:13,  1.59it/s]Extractor Predicting: 206it [02:13,  1.57it/s]Extractor Predicting: 207it [02:14,  1.57it/s]Extractor Predicting: 208it [02:14,  1.54it/s]Extractor Predicting: 209it [02:15,  1.56it/s]Extractor Predicting: 210it [02:16,  1.55it/s]Extractor Predicting: 211it [02:16,  1.55it/s]Extractor Predicting: 212it [02:17,  1.56it/s]Extractor Predicting: 213it [02:18,  1.55it/s]Extractor Predicting: 214it [02:18,  1.53it/s]Extractor Predicting: 215it [02:19,  1.53it/s]Extractor Predicting: 216it [02:20,  1.55it/s]Extractor Predicting: 217it [02:20,  1.56it/s]Extractor Predicting: 218it [02:21,  1.55it/s]Extractor Predicting: 219it [02:22,  1.53it/s]Extractor Predicting: 220it [02:22,  1.55it/s]Extractor Predicting: 221it [02:23,  1.55it/s]Extractor Predicting: 222it [02:24,  1.54it/s]Extractor Predicting: 223it [02:24,  1.57it/s]Extractor Predicting: 224it [02:25,  1.57it/s]Extractor Predicting: 225it [02:25,  1.54it/s]Extractor Predicting: 226it [02:26,  1.53it/s]Extractor Predicting: 227it [02:27,  1.54it/s]Extractor Predicting: 228it [02:27,  1.55it/s]Extractor Predicting: 229it [02:28,  1.57it/s]Extractor Predicting: 230it [02:29,  1.58it/s]Extractor Predicting: 231it [02:29,  1.56it/s]Extractor Predicting: 232it [02:30,  1.56it/s]Extractor Predicting: 233it [02:31,  1.56it/s]Extractor Predicting: 234it [02:31,  1.57it/s]Extractor Predicting: 235it [02:32,  1.57it/s]Extractor Predicting: 236it [02:32,  1.57it/s]Extractor Predicting: 237it [02:33,  1.56it/s]Extractor Predicting: 238it [02:34,  1.56it/s]Extractor Predicting: 239it [02:34,  1.55it/s]Extractor Predicting: 240it [02:35,  1.58it/s]Extractor Predicting: 241it [02:36,  1.58it/s]Extractor Predicting: 242it [02:36,  1.60it/s]Extractor Predicting: 243it [02:37,  1.44it/s]Extractor Predicting: 244it [02:38,  1.47it/s]Extractor Predicting: 245it [02:38,  1.49it/s]Extractor Predicting: 246it [02:39,  1.51it/s]Extractor Predicting: 247it [02:40,  1.54it/s]Extractor Predicting: 248it [02:40,  1.54it/s]Extractor Predicting: 249it [02:41,  1.56it/s]Extractor Predicting: 250it [02:42,  1.58it/s]Extractor Predicting: 251it [02:42,  1.55it/s]Extractor Predicting: 252it [02:43,  1.54it/s]Extractor Predicting: 253it [02:43,  1.60it/s]Extractor Predicting: 254it [02:44,  1.58it/s]Extractor Predicting: 255it [02:45,  1.55it/s]Extractor Predicting: 256it [02:45,  1.55it/s]Extractor Predicting: 257it [02:46,  1.56it/s]Extractor Predicting: 258it [02:47,  1.59it/s]Extractor Predicting: 259it [02:47,  1.60it/s]Extractor Predicting: 260it [02:48,  1.60it/s]Extractor Predicting: 261it [02:49,  1.57it/s]Extractor Predicting: 262it [02:49,  1.58it/s]Extractor Predicting: 263it [02:50,  1.58it/s]Extractor Predicting: 264it [02:50,  1.60it/s]Extractor Predicting: 265it [02:51,  1.61it/s]Extractor Predicting: 266it [02:52,  1.64it/s]Extractor Predicting: 267it [02:52,  1.58it/s]Extractor Predicting: 268it [02:53,  1.58it/s]Extractor Predicting: 269it [02:54,  1.60it/s]Extractor Predicting: 270it [02:54,  1.60it/s]Extractor Predicting: 271it [02:55,  1.63it/s]Extractor Predicting: 272it [02:55,  1.66it/s]Extractor Predicting: 273it [02:56,  1.63it/s]Extractor Predicting: 274it [02:57,  1.63it/s]Extractor Predicting: 275it [02:57,  1.61it/s]Extractor Predicting: 276it [02:58,  1.61it/s]Extractor Predicting: 277it [02:59,  1.60it/s]Extractor Predicting: 278it [02:59,  1.60it/s]Extractor Predicting: 279it [03:00,  1.58it/s]Extractor Predicting: 280it [03:00,  1.59it/s]Extractor Predicting: 281it [03:01,  1.56it/s]Extractor Predicting: 281it [03:01,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:00:01,259 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:00:01,274 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:00:01,275 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:00:01,275 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:00:01,275 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:00:02,043 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:00:02,044 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:00:02,743 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:00:03,783 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:00:03,784 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:00:07,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:00:07,511 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:00:07,512 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:00:07,512 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:00:07,512 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:00:08,175 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:00:08,176 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:00:08,752 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:00:08,910 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:00:08,910 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4190740007914523,
  "recall": 0.3141035147560433,
  "score": 0.3590743409341358,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.42it/s]Extractor Predicting: 4it [00:02,  1.42it/s]Extractor Predicting: 5it [00:03,  1.43it/s]Extractor Predicting: 6it [00:03,  1.87it/s]Extractor Predicting: 6it [00:03,  1.60it/s]
[INFO|configuration_utils.py:515] 2023-08-28 23:00:13,185 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:00:13,185 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:00:13,188 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:00:13,189 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 23:00:13,192 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:00:17,538 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 23:00:17,551 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 23:00:17,635 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:00:17,636 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:00:17,699 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:00:17,712 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:00:17,712 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:00:17,713 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:00:17,713 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:00:17,713 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:00:17,713 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2736842105263158,
  "recall": 0.10116731517509728,
  "score": 0.1477272727272727,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 23:00:17,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:18,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:19,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:20,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:20,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:21,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:22,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:22,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:23,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:24,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:25,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:25,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:26,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:27,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:28,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:28,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:29,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:30,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:31,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:31,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:25, 14.71s/it][WARNING|generation_utils.py:914] 2023-08-28 23:00:32,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:33,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:34,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:35,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:35,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:36,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:37,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:38,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:38,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:39,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:40,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:41,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:42,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:43,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:44,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:44,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:45,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:46,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:47,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:48,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:23, 15.68s/it][WARNING|generation_utils.py:914] 2023-08-28 23:00:49,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:49,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:50,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:51,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:51,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:52,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:53,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:53,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:54,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:55,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:56,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:57,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:58,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:58,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:59,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:00,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:01,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:01,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:02,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:03,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:03,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:46<03:07, 15.63s/it][WARNING|generation_utils.py:914] 2023-08-28 23:01:04,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:05,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:05,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:06,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:07,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:07,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:08,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:08,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:09,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:10,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:10,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:11,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:12,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:12,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:13,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:14,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:14,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:15,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:16,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:16,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:17,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:00<02:42, 14.76s/it][WARNING|generation_utils.py:914] 2023-08-28 23:01:18,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:18,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:19,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:19,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:20,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:21,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:21,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:22,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:23,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:23,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:24,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:25,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:25,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:26,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:27,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:27,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:28,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:29,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:29,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:30,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:31,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:14<02:25, 14.59s/it][WARNING|generation_utils.py:914] 2023-08-28 23:01:32,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:32,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:33,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:34,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:34,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:35,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:35,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:36,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:37,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:38,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:38,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:39,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:39,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:40,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:41,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:41,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:42,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:43,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:43,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:44,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:45,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:45,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:28<02:09, 14.39s/it][WARNING|generation_utils.py:914] 2023-08-28 23:01:46,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:47,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:47,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:48,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:49,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:49,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:50,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:51,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:51,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:52,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:53,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:53,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:54,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:55,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:55,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:56,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:57,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:58,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:58,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:59,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:00,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:43<01:56, 14.58s/it][WARNING|generation_utils.py:914] 2023-08-28 23:02:01,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:01,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:02,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:03,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:03,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:04,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:05,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:05,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:06,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:07,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:07,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:08,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:09,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:09,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:10,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:11,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:11,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:12,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:13,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:13,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:14,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:57<01:41, 14.48s/it][WARNING|generation_utils.py:914] 2023-08-28 23:02:15,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:16,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:17,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:17,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:18,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:19,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:19,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:20,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:21,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:22,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:22,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:23,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:24,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:25,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:26,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:26,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:27,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:28,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:29,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:30,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:31,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:13<01:30, 15.04s/it][WARNING|generation_utils.py:914] 2023-08-28 23:02:31,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:32,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:32,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:33,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:34,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:34,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:35,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:35,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:36,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:36,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:37,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:37,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:38,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:38,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:39,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:40,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:40,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:41,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:41,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:42,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:25<01:09, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-28 23:02:42,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:43,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:44,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:45,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:45,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:46,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:47,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:47,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:48,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:49,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:49,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:50,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:51,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:51,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:52,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:53,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:53,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:54,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:55,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:55,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:38<00:55, 13.78s/it][WARNING|generation_utils.py:914] 2023-08-28 23:02:56,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:57,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:57,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:58,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:59,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:02:59,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:00,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:01,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:02,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:03,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:03,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:04,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:05,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:05,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:06,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:07,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:07,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:08,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:09,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:09,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:52<00:41, 13.86s/it][WARNING|generation_utils.py:914] 2023-08-28 23:03:10,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:11,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:11,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:12,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:13,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:14,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:14,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:15,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:16,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:16,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:17,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:18,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:18,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:19,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:20,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:20,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:21,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:21,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:22,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:23,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:24,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:06<00:27, 13.97s/it][WARNING|generation_utils.py:914] 2023-08-28 23:03:24,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:25,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:26,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:26,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:27,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:28,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:28,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:29,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:29,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:30,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:31,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:31,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:32,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:32,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:33,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:34,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:34,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:35,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:35,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:36,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:37,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:19<00:13, 13.68s/it][WARNING|generation_utils.py:914] 2023-08-28 23:03:37,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:38,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:39,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:40,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:40,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:41,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:42,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:42,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:43,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:44,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:45,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:45,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:46,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:47,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:47,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:48,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:49,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:49,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:50,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:03:51,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:34<00:00, 13.82s/it]Generating: 100%|██████████| 15/15 [03:34<00:00, 14.27s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:04:03,184 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:04:03,189 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:04:03,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:04:03,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:04:03,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:04:04,376 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:04:04,378 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:04:05,604 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:04:06,700 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:04:06,710 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:04:10,694 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:04:10,696 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:04:10,696 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:04:10,696 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:04:10,696 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:04:11,790 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:04:11,791 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:04:14,237 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:04:14,403 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:04:14,403 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 409, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 471, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 533, 'raw': 544}
{'target': 600, 'success': 565, 'raw': 576}
{'target': 600, 'success': 597, 'raw': 608}
{'target': 600, 'success': 629, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.9828125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : main subject .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : participant in .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.9166666666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : competition class .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : location .', 'success_rate': 0.8973214285714286, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8928571428571429, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : position held .', 'success_rate': 0.8973214285714286, 'errors': {''}}
['Relation : position played on team / speciality . Context : On 31 March 2014 , the Brazilian national team selected him as captain of the national team in the 2014 FIFA World Cup . Head Entity : Brazil national team , Tail Entity : captain .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : religion .', 'success_rate': 0.95625, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 7274
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7374, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.49it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:02,  1.49it/s]Extractor Estimating: 4it [00:02,  1.51it/s]Extractor Estimating: 5it [00:03,  1.60it/s]Extractor Estimating: 6it [00:03,  1.60it/s]Extractor Estimating: 7it [00:04,  1.62it/s]Extractor Estimating: 8it [00:05,  1.62it/s]Extractor Estimating: 9it [00:05,  1.65it/s]Extractor Estimating: 10it [00:06,  1.60it/s]Extractor Estimating: 11it [00:06,  1.56it/s]Extractor Estimating: 12it [00:07,  1.58it/s]Extractor Estimating: 13it [00:08,  1.58it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.54it/s]Extractor Estimating: 16it [00:10,  1.55it/s]Extractor Estimating: 17it [00:10,  1.61it/s]Extractor Estimating: 18it [00:11,  1.63it/s]Extractor Estimating: 19it [00:11,  1.61it/s]Extractor Estimating: 20it [00:12,  1.58it/s]Extractor Estimating: 21it [00:13,  1.55it/s]Extractor Estimating: 22it [00:13,  1.54it/s]Extractor Estimating: 23it [00:14,  1.59it/s]Extractor Estimating: 24it [00:15,  1.54it/s]Extractor Estimating: 25it [00:15,  1.49it/s]Extractor Estimating: 26it [00:16,  1.50it/s]Extractor Estimating: 27it [00:17,  1.44it/s]Extractor Estimating: 28it [00:18,  1.50it/s]Extractor Estimating: 29it [00:18,  1.46it/s]Extractor Estimating: 30it [00:19,  1.52it/s]Extractor Estimating: 31it [00:19,  1.54it/s]Extractor Estimating: 32it [00:20,  1.58it/s]Extractor Estimating: 33it [00:21,  1.55it/s]Extractor Estimating: 34it [00:21,  1.59it/s]Extractor Estimating: 35it [00:22,  1.54it/s]Extractor Estimating: 36it [00:23,  1.53it/s]Extractor Estimating: 37it [00:23,  1.45it/s]Extractor Estimating: 38it [00:24,  1.51it/s]Extractor Estimating: 39it [00:25,  1.48it/s]Extractor Estimating: 40it [00:25,  1.54it/s]Extractor Estimating: 41it [00:26,  1.50it/s]Extractor Estimating: 42it [00:27,  1.51it/s]Extractor Estimating: 43it [00:27,  1.47it/s]Extractor Estimating: 44it [00:28,  1.50it/s]Extractor Estimating: 45it [00:29,  1.50it/s]Extractor Estimating: 46it [00:29,  1.53it/s]Extractor Estimating: 47it [00:30,  1.56it/s]Extractor Estimating: 48it [00:31,  1.59it/s]Extractor Estimating: 49it [00:31,  1.61it/s]Extractor Estimating: 50it [00:32,  1.58it/s]Extractor Estimating: 51it [00:32,  1.63it/s]Extractor Estimating: 52it [00:33,  1.60it/s]Extractor Estimating: 53it [00:34,  1.60it/s]Extractor Estimating: 54it [00:34,  1.59it/s]Extractor Estimating: 55it [00:35,  1.61it/s]Extractor Estimating: 56it [00:35,  1.66it/s]Extractor Estimating: 57it [00:36,  1.67it/s]Extractor Estimating: 58it [00:37,  1.63it/s]Extractor Estimating: 59it [00:37,  1.66it/s]Extractor Estimating: 60it [00:38,  1.63it/s]Extractor Estimating: 61it [00:39,  1.64it/s]Extractor Estimating: 62it [00:39,  1.63it/s]Extractor Estimating: 63it [00:40,  1.62it/s]Extractor Estimating: 64it [00:40,  1.63it/s]Extractor Estimating: 65it [00:41,  1.57it/s]Extractor Estimating: 66it [00:42,  1.60it/s]Extractor Estimating: 67it [00:42,  1.62it/s]Extractor Estimating: 68it [00:43,  1.61it/s]Extractor Estimating: 69it [00:44,  1.60it/s]Extractor Estimating: 70it [00:44,  1.60it/s]Extractor Estimating: 71it [00:45,  1.62it/s]Extractor Estimating: 72it [00:45,  1.64it/s]Extractor Estimating: 73it [00:46,  1.65it/s]Extractor Estimating: 74it [00:47,  1.65it/s]Extractor Estimating: 75it [00:47,  1.66it/s]Extractor Estimating: 76it [00:48,  1.68it/s]Extractor Estimating: 77it [00:48,  1.71it/s]Extractor Estimating: 78it [00:49,  1.75it/s]Extractor Estimating: 79it [00:49,  1.67it/s]Extractor Estimating: 80it [00:50,  1.74it/s]Extractor Estimating: 81it [00:51,  1.71it/s]Extractor Estimating: 82it [00:51,  1.70it/s]Extractor Estimating: 83it [00:52,  1.70it/s]Extractor Estimating: 84it [00:52,  1.77it/s]Extractor Estimating: 85it [00:53,  1.75it/s]Extractor Estimating: 86it [00:53,  1.76it/s]Extractor Estimating: 87it [00:54,  1.73it/s]Extractor Estimating: 88it [00:55,  1.77it/s]Extractor Estimating: 89it [00:55,  1.73it/s]Extractor Estimating: 90it [00:56,  1.72it/s]Extractor Estimating: 91it [00:56,  1.72it/s]Extractor Estimating: 92it [00:57,  1.75it/s]Extractor Estimating: 93it [00:58,  1.75it/s]Extractor Estimating: 94it [00:58,  1.76it/s]Extractor Estimating: 95it [00:59,  1.72it/s]Extractor Estimating: 96it [00:59,  1.71it/s]Extractor Estimating: 97it [01:00,  1.67it/s]Extractor Estimating: 98it [01:01,  1.64it/s]Extractor Estimating: 99it [01:01,  1.67it/s]Extractor Estimating: 100it [01:02,  1.65it/s]Extractor Estimating: 101it [01:02,  1.74it/s]Extractor Estimating: 102it [01:03,  1.85it/s]Extractor Estimating: 103it [01:03,  1.81it/s]Extractor Estimating: 104it [01:04,  1.89it/s]Extractor Estimating: 105it [01:04,  1.87it/s]Extractor Estimating: 106it [01:05,  1.95it/s]Extractor Estimating: 107it [01:05,  2.04it/s]Extractor Estimating: 108it [01:06,  2.00it/s]Extractor Estimating: 109it [01:06,  1.98it/s]Extractor Estimating: 110it [01:07,  1.91it/s]Extractor Estimating: 111it [01:07,  1.95it/s]Extractor Estimating: 112it [01:08,  1.95it/s]Extractor Estimating: 113it [01:08,  1.99it/s]Extractor Estimating: 114it [01:09,  1.92it/s]Extractor Estimating: 115it [01:09,  1.90it/s]Extractor Estimating: 116it [01:10,  1.90it/s]Extractor Estimating: 117it [01:10,  1.96it/s]Extractor Estimating: 118it [01:11,  1.94it/s]Extractor Estimating: 119it [01:11,  1.96it/s]Extractor Estimating: 120it [01:12,  1.86it/s]Extractor Estimating: 121it [01:13,  1.88it/s]Extractor Estimating: 122it [01:13,  1.87it/s]Extractor Estimating: 123it [01:14,  1.87it/s]Extractor Estimating: 124it [01:14,  1.92it/s]Extractor Estimating: 125it [01:15,  1.85it/s]Extractor Estimating: 126it [01:15,  1.89it/s]Extractor Estimating: 127it [01:16,  1.97it/s]Extractor Estimating: 128it [01:16,  1.96it/s]Extractor Estimating: 129it [01:17,  1.94it/s]Extractor Estimating: 130it [01:17,  1.95it/s]Extractor Estimating: 131it [01:18,  1.97it/s]Extractor Estimating: 132it [01:18,  1.81it/s]Extractor Estimating: 133it [01:19,  1.89it/s]Extractor Estimating: 134it [01:19,  1.95it/s]Extractor Estimating: 135it [01:20,  1.93it/s]Extractor Estimating: 136it [01:20,  1.95it/s]Extractor Estimating: 137it [01:21,  1.96it/s]Extractor Estimating: 138it [01:21,  1.94it/s]Extractor Estimating: 139it [01:22,  1.98it/s]Extractor Estimating: 140it [01:22,  1.96it/s]Extractor Estimating: 141it [01:23,  1.94it/s]Extractor Estimating: 142it [01:23,  1.99it/s]Extractor Estimating: 143it [01:24,  2.01it/s]Extractor Estimating: 144it [01:24,  2.00it/s]Extractor Estimating: 145it [01:25,  2.04it/s]Extractor Estimating: 146it [01:25,  2.04it/s]Extractor Estimating: 147it [01:26,  1.95it/s]Extractor Estimating: 148it [01:26,  1.95it/s]Extractor Estimating: 149it [01:27,  1.97it/s]Extractor Estimating: 150it [01:27,  2.00it/s]Extractor Estimating: 151it [01:28,  1.81it/s]Extractor Estimating: 152it [01:29,  1.71it/s]Extractor Estimating: 153it [01:29,  1.68it/s]Extractor Estimating: 154it [01:30,  1.61it/s]Extractor Estimating: 155it [01:31,  1.58it/s]Extractor Estimating: 156it [01:31,  1.62it/s]Extractor Estimating: 157it [01:32,  1.64it/s]Extractor Estimating: 158it [01:32,  1.66it/s]Extractor Estimating: 159it [01:33,  1.68it/s]Extractor Estimating: 160it [01:34,  1.66it/s]Extractor Estimating: 161it [01:34,  1.69it/s]Extractor Estimating: 162it [01:35,  1.65it/s]Extractor Estimating: 163it [01:35,  1.69it/s]Extractor Estimating: 164it [01:36,  1.69it/s]Extractor Estimating: 165it [01:37,  1.66it/s]Extractor Estimating: 166it [01:37,  1.65it/s]Extractor Estimating: 167it [01:38,  1.65it/s]Extractor Estimating: 168it [01:38,  1.67it/s]Extractor Estimating: 169it [01:39,  1.66it/s]Extractor Estimating: 170it [01:40,  1.67it/s]Extractor Estimating: 171it [01:40,  1.65it/s]Extractor Estimating: 172it [01:41,  1.57it/s]Extractor Estimating: 173it [01:42,  1.56it/s]Extractor Estimating: 174it [01:42,  1.59it/s]Extractor Estimating: 175it [01:43,  1.60it/s]Extractor Estimating: 176it [01:43,  1.62it/s]Extractor Estimating: 177it [01:44,  1.66it/s]Extractor Estimating: 178it [01:45,  1.69it/s]Extractor Estimating: 179it [01:45,  1.65it/s]Extractor Estimating: 180it [01:46,  1.63it/s]Extractor Estimating: 181it [01:46,  1.62it/s]Extractor Estimating: 182it [01:47,  1.71it/s]Extractor Estimating: 183it [01:48,  1.71it/s]Extractor Estimating: 184it [01:48,  1.72it/s]Extractor Estimating: 185it [01:49,  1.68it/s]Extractor Estimating: 186it [01:49,  1.65it/s]Extractor Estimating: 187it [01:50,  1.70it/s]Extractor Estimating: 188it [01:50,  1.74it/s]Extractor Estimating: 189it [01:51,  1.71it/s]Extractor Estimating: 190it [01:52,  1.75it/s]Extractor Estimating: 191it [01:52,  1.73it/s]Extractor Estimating: 192it [01:53,  1.71it/s]Extractor Estimating: 193it [01:53,  1.74it/s]Extractor Estimating: 194it [01:54,  1.76it/s]Extractor Estimating: 195it [01:55,  1.71it/s]Extractor Estimating: 196it [01:55,  1.69it/s]Extractor Estimating: 197it [01:56,  1.72it/s]Extractor Estimating: 198it [01:56,  1.73it/s]Extractor Estimating: 199it [01:57,  1.56it/s]Extractor Estimating: 200it [01:58,  1.55it/s]Extractor Estimating: 201it [01:59,  1.42it/s]Extractor Estimating: 202it [01:59,  1.37it/s]Extractor Estimating: 203it [02:00,  1.37it/s]Extractor Estimating: 204it [02:01,  1.39it/s]Extractor Estimating: 205it [02:01,  1.43it/s]Extractor Estimating: 206it [02:02,  1.40it/s]Extractor Estimating: 207it [02:03,  1.42it/s]Extractor Estimating: 208it [02:04,  1.43it/s]Extractor Estimating: 209it [02:04,  1.41it/s]Extractor Estimating: 210it [02:05,  1.40it/s]Extractor Estimating: 211it [02:06,  1.34it/s]Extractor Estimating: 212it [02:07,  1.33it/s]Extractor Estimating: 213it [02:07,  1.36it/s]Extractor Estimating: 214it [02:08,  1.36it/s]Extractor Estimating: 215it [02:09,  1.34it/s]Extractor Estimating: 216it [02:10,  1.34it/s]Extractor Estimating: 217it [02:10,  1.37it/s]Extractor Estimating: 218it [02:11,  1.36it/s]Extractor Estimating: 219it [02:12,  1.41it/s]Extractor Estimating: 220it [02:13,  1.31it/s]Extractor Estimating: 221it [02:13,  1.31it/s]Extractor Estimating: 222it [02:14,  1.34it/s]Extractor Estimating: 223it [02:15,  1.35it/s]Extractor Estimating: 224it [02:15,  1.32it/s]Extractor Estimating: 225it [02:16,  1.40it/s]Extractor Estimating: 226it [02:17,  1.59it/s]Extractor Estimating: 227it [02:17,  1.69it/s]Extractor Estimating: 228it [02:18,  1.81it/s]Extractor Estimating: 229it [02:18,  1.88it/s]Extractor Estimating: 230it [02:18,  1.96it/s]Extractor Estimating: 231it [02:19,  2.02it/s]Extractor Estimating: 232it [02:19,  1.98it/s]Extractor Estimating: 233it [02:20,  2.07it/s]Extractor Estimating: 234it [02:20,  2.13it/s]Extractor Estimating: 235it [02:21,  2.08it/s]Extractor Estimating: 236it [02:21,  2.09it/s]Extractor Estimating: 237it [02:22,  2.15it/s]Extractor Estimating: 238it [02:22,  2.09it/s]Extractor Estimating: 239it [02:23,  2.09it/s]Extractor Estimating: 240it [02:23,  2.19it/s]Extractor Estimating: 241it [02:24,  2.14it/s]Extractor Estimating: 242it [02:24,  2.10it/s]Extractor Estimating: 243it [02:25,  1.99it/s]Extractor Estimating: 244it [02:25,  1.94it/s]Extractor Estimating: 245it [02:26,  1.96it/s]Extractor Estimating: 246it [02:26,  2.03it/s]Extractor Estimating: 247it [02:27,  2.06it/s]Extractor Estimating: 248it [02:27,  2.04it/s]Extractor Estimating: 249it [02:28,  2.11it/s]Extractor Estimating: 250it [02:28,  2.08it/s]Extractor Estimating: 251it [02:29,  1.96it/s]Extractor Estimating: 252it [02:29,  1.89it/s]Extractor Estimating: 253it [02:30,  1.82it/s]Extractor Estimating: 254it [02:30,  1.79it/s]Extractor Estimating: 255it [02:31,  1.83it/s]Extractor Estimating: 256it [02:32,  1.75it/s]Extractor Estimating: 257it [02:32,  1.80it/s]Extractor Estimating: 258it [02:33,  1.78it/s]Extractor Estimating: 259it [02:33,  1.73it/s]Extractor Estimating: 260it [02:34,  1.72it/s]Extractor Estimating: 261it [02:34,  1.68it/s]Extractor Estimating: 262it [02:35,  1.74it/s]Extractor Estimating: 263it [02:36,  1.70it/s]Extractor Estimating: 264it [02:36,  1.74it/s]Extractor Estimating: 265it [02:37,  1.71it/s]Extractor Estimating: 266it [02:37,  1.75it/s]Extractor Estimating: 267it [02:38,  1.72it/s]Extractor Estimating: 268it [02:39,  1.68it/s]Extractor Estimating: 269it [02:39,  1.61it/s]Extractor Estimating: 270it [02:40,  1.63it/s]Extractor Estimating: 271it [02:40,  1.70it/s]Extractor Estimating: 272it [02:41,  1.70it/s]Extractor Estimating: 273it [02:42,  1.71it/s]Extractor Estimating: 274it [02:42,  1.67it/s]Extractor Estimating: 275it [02:43,  1.68it/s]Extractor Estimating: 276it [02:43,  1.72it/s]Extractor Estimating: 277it [02:44,  1.77it/s]Extractor Estimating: 278it [02:44,  1.80it/s]Extractor Estimating: 279it [02:45,  1.82it/s]Extractor Estimating: 280it [02:46,  1.73it/s]Extractor Estimating: 281it [02:46,  1.79it/s]Extractor Estimating: 282it [02:47,  1.68it/s]Extractor Estimating: 283it [02:47,  1.70it/s]Extractor Estimating: 284it [02:48,  1.72it/s]Extractor Estimating: 285it [02:49,  1.55it/s]Extractor Estimating: 286it [02:49,  1.60it/s]Extractor Estimating: 287it [02:50,  1.60it/s]Extractor Estimating: 288it [02:50,  1.70it/s]Extractor Estimating: 289it [02:51,  1.74it/s]Extractor Estimating: 290it [02:51,  1.82it/s]Extractor Estimating: 291it [02:52,  1.84it/s]Extractor Estimating: 292it [02:52,  1.88it/s]Extractor Estimating: 293it [02:53,  1.86it/s]Extractor Estimating: 294it [02:54,  1.81it/s]Extractor Estimating: 295it [02:54,  1.82it/s]Extractor Estimating: 296it [02:55,  1.83it/s]Extractor Estimating: 297it [02:55,  1.81it/s]Extractor Estimating: 298it [02:56,  1.83it/s]Extractor Estimating: 299it [02:56,  1.87it/s]Extractor Estimating: 300it [02:57,  1.80it/s]Extractor Estimating: 301it [02:57,  1.84it/s]Extractor Estimating: 302it [02:58,  1.82it/s]Extractor Estimating: 303it [02:59,  1.74it/s]Extractor Estimating: 304it [02:59,  1.74it/s]Extractor Estimating: 305it [03:00,  1.72it/s]Extractor Estimating: 306it [03:00,  1.73it/s]Extractor Estimating: 307it [03:01,  1.71it/s]Extractor Estimating: 308it [03:01,  1.74it/s]Extractor Estimating: 309it [03:02,  1.72it/s]Extractor Estimating: 310it [03:03,  1.73it/s]Extractor Estimating: 311it [03:03,  1.71it/s]Extractor Estimating: 312it [03:04,  1.70it/s]Extractor Estimating: 313it [03:04,  1.70it/s]Extractor Estimating: 314it [03:05,  1.68it/s]Extractor Estimating: 315it [03:06,  1.73it/s]Extractor Estimating: 316it [03:06,  1.69it/s]Extractor Estimating: 317it [03:07,  1.80it/s]Extractor Estimating: 318it [03:07,  1.76it/s]Extractor Estimating: 319it [03:08,  1.76it/s]Extractor Estimating: 320it [03:08,  1.79it/s]Extractor Estimating: 321it [03:09,  1.77it/s]Extractor Estimating: 322it [03:09,  1.81it/s]Extractor Estimating: 323it [03:10,  1.85it/s]Extractor Estimating: 324it [03:11,  1.77it/s]Extractor Estimating: 325it [03:11,  1.75it/s]Extractor Estimating: 326it [03:12,  1.80it/s]Extractor Estimating: 327it [03:12,  1.82it/s]Extractor Estimating: 328it [03:13,  1.85it/s]Extractor Estimating: 329it [03:13,  1.87it/s]Extractor Estimating: 330it [03:14,  1.92it/s]Extractor Estimating: 331it [03:14,  1.95it/s]Extractor Estimating: 332it [03:15,  1.93it/s]Extractor Estimating: 333it [03:15,  1.91it/s]Extractor Estimating: 334it [03:16,  1.93it/s]Extractor Estimating: 335it [03:16,  1.98it/s]Extractor Estimating: 336it [03:17,  1.94it/s]Extractor Estimating: 337it [03:17,  1.96it/s]Extractor Estimating: 338it [03:18,  1.98it/s]Extractor Estimating: 339it [03:18,  1.99it/s]Extractor Estimating: 340it [03:19,  1.99it/s]Extractor Estimating: 341it [03:19,  1.94it/s]Extractor Estimating: 342it [03:20,  1.93it/s]Extractor Estimating: 343it [03:20,  2.00it/s]Extractor Estimating: 344it [03:21,  1.98it/s]Extractor Estimating: 345it [03:21,  1.97it/s]Extractor Estimating: 346it [03:22,  1.97it/s]Extractor Estimating: 347it [03:22,  1.94it/s]Extractor Estimating: 348it [03:23,  1.94it/s]Extractor Estimating: 349it [03:23,  1.95it/s]Extractor Estimating: 350it [03:24,  1.88it/s]Extractor Estimating: 351it [03:25,  1.86it/s]Extractor Estimating: 352it [03:25,  1.79it/s]Extractor Estimating: 353it [03:26,  1.67it/s]Extractor Estimating: 354it [03:27,  1.62it/s]Extractor Estimating: 355it [03:27,  1.64it/s]Extractor Estimating: 356it [03:28,  1.66it/s]Extractor Estimating: 357it [03:28,  1.70it/s]Extractor Estimating: 358it [03:29,  1.65it/s]Extractor Estimating: 359it [03:30,  1.65it/s]Extractor Estimating: 360it [03:30,  1.65it/s]Extractor Estimating: 361it [03:31,  1.68it/s]Extractor Estimating: 362it [03:31,  1.67it/s]Extractor Estimating: 363it [03:32,  1.65it/s]Extractor Estimating: 364it [03:33,  1.62it/s]Extractor Estimating: 365it [03:33,  1.66it/s]Extractor Estimating: 366it [03:34,  1.66it/s]Extractor Estimating: 367it [03:34,  1.69it/s]Extractor Estimating: 368it [03:35,  1.72it/s]Extractor Estimating: 369it [03:35,  1.77it/s]Extractor Estimating: 370it [03:36,  1.83it/s]Extractor Estimating: 371it [03:37,  1.78it/s]Extractor Estimating: 372it [03:37,  1.84it/s]Extractor Estimating: 373it [03:38,  1.68it/s]Extractor Estimating: 374it [03:38,  1.69it/s]Extractor Estimating: 375it [03:39,  1.71it/s]Extractor Estimating: 375it [03:39,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:08:04,955 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:08:04,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:08:04,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:08:04,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:08:04,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:08:05,271 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:08:05,273 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:08:05,955 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:08:06,999 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:08:06,999 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:08:08,334 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:08:08,341 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:08:08,341 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:08:08,341 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:08:08,341 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:08:09,115 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:08:09,116 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:08:09,809 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:08:09,974 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:08:09,974 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 01:26:44,109 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 01:26:44,135 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7493 mean pseudo reward: 0.9659629144352923
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 15145
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15245, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15245, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.058, loss:405.5378
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.069, loss:356.5650
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.090, loss:394.2660
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.080, loss:342.3958
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.062, loss:322.3220
>> valid entity prec:0.5542, rec:0.4999, f1:0.5257
>> valid relation prec:0.3060, rec:0.1636, f1:0.2132
>> valid relation with NER prec:0.3060, rec:0.1636, f1:0.2132
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.402, loss:348.6468
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.073, loss:311.3583
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.065, loss:320.1739
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.060, loss:349.7816
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.063, loss:333.9214
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5043, rec:0.5018, f1:0.5030
>> valid relation prec:0.2950, rec:0.1582, f1:0.2059
>> valid relation with NER prec:0.2950, rec:0.1582, f1:0.2059
g_step 1100, step 161, avg_time 2.414, loss:314.0042
g_step 1200, step 261, avg_time 1.066, loss:332.7087
g_step 1300, step 48, avg_time 1.080, loss:306.9918
g_step 1400, step 148, avg_time 1.065, loss:303.8533
g_step 1500, step 248, avg_time 1.067, loss:306.1302
>> valid entity prec:0.5562, rec:0.5047, f1:0.5292
>> valid relation prec:0.3172, rec:0.1619, f1:0.2144
>> valid relation with NER prec:0.3172, rec:0.1619, f1:0.2144
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.413, loss:303.5964
g_step 1700, step 135, avg_time 1.079, loss:302.3921
g_step 1800, step 235, avg_time 1.067, loss:297.1349
g_step 1900, step 22, avg_time 1.078, loss:287.5114
g_step 2000, step 122, avg_time 1.068, loss:263.2430
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5368, rec:0.5139, f1:0.5251
>> valid relation prec:0.2961, rec:0.1587, f1:0.2067
>> valid relation with NER prec:0.2961, rec:0.1587, f1:0.2067
g_step 2100, step 222, avg_time 2.416, loss:297.8687
g_step 2200, step 9, avg_time 1.069, loss:270.1986
g_step 2300, step 109, avg_time 1.072, loss:260.3275
g_step 2400, step 209, avg_time 1.060, loss:270.5055
g_step 2500, step 309, avg_time 1.084, loss:275.5140
>> valid entity prec:0.5597, rec:0.5237, f1:0.5411
>> valid relation prec:0.3126, rec:0.1544, f1:0.2068
>> valid relation with NER prec:0.3126, rec:0.1544, f1:0.2068
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 96, avg_time 2.411, loss:224.0334
g_step 2700, step 196, avg_time 1.077, loss:247.8678
g_step 2800, step 296, avg_time 1.067, loss:273.8917
g_step 2900, step 83, avg_time 1.077, loss:229.1145
g_step 3000, step 183, avg_time 1.069, loss:243.5061
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5369, rec:0.5132, f1:0.5248
>> valid relation prec:0.2767, rec:0.1433, f1:0.1888
>> valid relation with NER prec:0.2767, rec:0.1433, f1:0.1888
g_step 3100, step 283, avg_time 2.429, loss:248.7141
g_step 3200, step 70, avg_time 1.051, loss:232.5270
g_step 3300, step 170, avg_time 1.068, loss:236.7632
g_step 3400, step 270, avg_time 1.068, loss:233.5999
g_step 3500, step 57, avg_time 1.069, loss:221.2738
>> valid entity prec:0.5623, rec:0.4939, f1:0.5259
>> valid relation prec:0.3161, rec:0.1521, f1:0.2054
>> valid relation with NER prec:0.3161, rec:0.1521, f1:0.2054
g_step 3600, step 157, avg_time 2.398, loss:225.0556
g_step 3700, step 257, avg_time 1.085, loss:228.6476
g_step 3800, step 44, avg_time 1.081, loss:221.2561
g_step 3900, step 144, avg_time 1.079, loss:207.5661
g_step 4000, step 244, avg_time 1.077, loss:221.5572
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5204, rec:0.4983, f1:0.5091
>> valid relation prec:0.2755, rec:0.1438, f1:0.1890
>> valid relation with NER prec:0.2755, rec:0.1438, f1:0.1890
g_step 4100, step 31, avg_time 2.394, loss:208.0948
g_step 4200, step 131, avg_time 1.053, loss:200.5593
g_step 4300, step 231, avg_time 1.086, loss:212.0851
g_step 4400, step 18, avg_time 1.070, loss:197.9011
g_step 4500, step 118, avg_time 1.065, loss:193.3888
>> valid entity prec:0.5601, rec:0.4658, f1:0.5086
>> valid relation prec:0.2883, rec:0.1450, f1:0.1929
>> valid relation with NER prec:0.2883, rec:0.1450, f1:0.1929
g_step 4600, step 218, avg_time 2.417, loss:186.9847
g_step 4700, step 5, avg_time 1.063, loss:201.2182
g_step 4800, step 105, avg_time 1.067, loss:189.7750
g_step 4900, step 205, avg_time 1.091, loss:175.7015
g_step 5000, step 305, avg_time 1.052, loss:202.2696
learning rate was adjusted to 0.0008
>> valid entity prec:0.5141, rec:0.5218, f1:0.5179
>> valid relation prec:0.2880, rec:0.1567, f1:0.2030
>> valid relation with NER prec:0.2880, rec:0.1567, f1:0.2030
g_step 5100, step 92, avg_time 2.406, loss:168.7206
g_step 5200, step 192, avg_time 1.049, loss:186.0343
g_step 5300, step 292, avg_time 1.102, loss:186.7264
g_step 5400, step 79, avg_time 1.068, loss:169.5557
g_step 5500, step 179, avg_time 1.060, loss:175.5510
>> valid entity prec:0.5477, rec:0.5011, f1:0.5234
>> valid relation prec:0.3161, rec:0.1542, f1:0.2072
>> valid relation with NER prec:0.3161, rec:0.1542, f1:0.2072
g_step 5600, step 279, avg_time 2.406, loss:179.9563
g_step 5700, step 66, avg_time 1.099, loss:174.1564
g_step 5800, step 166, avg_time 1.060, loss:167.0361
g_step 5900, step 266, avg_time 1.080, loss:175.3854
g_step 6000, step 53, avg_time 1.072, loss:171.5661
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5444, rec:0.4800, f1:0.5102
>> valid relation prec:0.2989, rec:0.1524, f1:0.2019
>> valid relation with NER prec:0.2989, rec:0.1524, f1:0.2019
g_step 6100, step 153, avg_time 2.417, loss:163.2230
g_step 6200, step 253, avg_time 1.075, loss:165.4090
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 01:26:44 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 01:26:44 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_01-26-44_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 01:26:45 - WARNING - datasets.builder -   Using custom data configuration default-0c76152416c6a202
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-0c76152416c6a202/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 01:26:45,426 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:26:45,428 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:26:45,428 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:26:45,429 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:26:45,440 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:26:45,447 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:26:45,448 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:26:45,448 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:26:45,448 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:26:45,448 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:26:45,448 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 01:26:45,585 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:26:48,784 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 01:26:48,786 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-0c76152416c6a202/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.37ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.20ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.54ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.73ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.85ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.95ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  5.01ba/s]100%|██████████| 8/8 [00:01<00:00,  5.95ba/s]100%|██████████| 8/8 [00:01<00:00,  5.07ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.11ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.37ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.45ba/s]100%|██████████| 4/4 [00:00<00:00,  5.56ba/s]100%|██████████| 4/4 [00:00<00:00,  5.04ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.05ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.64ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.92ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  9.92ba/s]100%|██████████| 8/8 [00:00<00:00, 11.26ba/s]100%|██████████| 8/8 [00:00<00:00, 10.51ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.30ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.71ba/s]100%|██████████| 4/4 [00:00<00:00, 11.07ba/s]
[INFO|trainer.py:414] 2023-08-29 01:26:52,673 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 01:26:52,685 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 01:26:52,685 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 01:26:52,685 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 01:26:52,685 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 01:26:52,685 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 01:26:52,685 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 01:26:52,685 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:53,  3.36it/s]  0%|          | 2/585 [00:00<02:49,  3.45it/s]  1%|          | 3/585 [00:00<02:47,  3.48it/s]  1%|          | 4/585 [00:01<02:46,  3.48it/s]  1%|          | 5/585 [00:01<02:46,  3.49it/s]  1%|          | 6/585 [00:01<02:45,  3.49it/s]  1%|          | 7/585 [00:02<02:45,  3.50it/s]  1%|▏         | 8/585 [00:02<02:44,  3.50it/s]  2%|▏         | 9/585 [00:02<02:46,  3.46it/s]  2%|▏         | 10/585 [00:02<02:45,  3.47it/s]  2%|▏         | 11/585 [00:03<02:44,  3.48it/s]  2%|▏         | 12/585 [00:03<02:44,  3.49it/s]  2%|▏         | 13/585 [00:03<02:43,  3.50it/s]  2%|▏         | 14/585 [00:04<02:43,  3.50it/s]  3%|▎         | 15/585 [00:04<02:42,  3.50it/s]  3%|▎         | 16/585 [00:04<02:42,  3.50it/s]  3%|▎         | 17/585 [00:04<02:42,  3.50it/s]  3%|▎         | 18/585 [00:05<02:41,  3.50it/s]  3%|▎         | 19/585 [00:05<02:41,  3.50it/s]  3%|▎         | 20/585 [00:05<02:42,  3.47it/s]  4%|▎         | 21/585 [00:06<02:42,  3.48it/s]  4%|▍         | 22/585 [00:06<02:41,  3.49it/s]  4%|▍         | 23/585 [00:06<02:40,  3.49it/s]  4%|▍         | 24/585 [00:06<02:40,  3.49it/s]  4%|▍         | 25/585 [00:07<02:40,  3.50it/s]  4%|▍         | 26/585 [00:07<02:39,  3.50it/s]  5%|▍         | 27/585 [00:07<02:39,  3.50it/s]  5%|▍         | 28/585 [00:08<02:39,  3.50it/s]  5%|▍         | 29/585 [00:08<02:38,  3.50it/s]  5%|▌         | 30/585 [00:08<02:38,  3.50it/s]  5%|▌         | 31/585 [00:08<02:38,  3.50it/s]  5%|▌         | 32/585 [00:09<02:37,  3.50it/s]  6%|▌         | 33/585 [00:09<02:37,  3.50it/s]  6%|▌         | 34/585 [00:09<02:37,  3.50it/s]  6%|▌         | 35/585 [00:10<02:38,  3.47it/s]  6%|▌         | 36/585 [00:10<02:37,  3.48it/s]  6%|▋         | 37/585 [00:10<02:37,  3.48it/s]  6%|▋         | 38/585 [00:10<02:36,  3.49it/s]  7%|▋         | 39/585 [00:11<02:36,  3.49it/s]  7%|▋         | 40/585 [00:11<02:36,  3.49it/s]  7%|▋         | 41/585 [00:11<02:35,  3.49it/s]  7%|▋         | 42/585 [00:12<02:35,  3.50it/s]  7%|▋         | 43/585 [00:12<02:35,  3.50it/s]  8%|▊         | 44/585 [00:12<02:34,  3.50it/s]  8%|▊         | 45/585 [00:12<02:34,  3.50it/s]  8%|▊         | 46/585 [00:13<02:33,  3.50it/s]  8%|▊         | 47/585 [00:13<02:33,  3.50it/s]  8%|▊         | 48/585 [00:13<02:33,  3.50it/s]  8%|▊         | 49/585 [00:14<02:33,  3.50it/s]  9%|▊         | 50/585 [00:14<02:32,  3.50it/s]  9%|▊         | 51/585 [00:14<02:32,  3.50it/s]  9%|▉         | 52/585 [00:14<02:32,  3.50it/s]  9%|▉         | 53/585 [00:15<02:33,  3.47it/s]  9%|▉         | 54/585 [00:15<02:32,  3.48it/s]  9%|▉         | 55/585 [00:15<02:32,  3.48it/s] 10%|▉         | 56/585 [00:16<02:31,  3.49it/s] 10%|▉         | 57/585 [00:16<02:31,  3.49it/s] 10%|▉         | 58/585 [00:16<02:30,  3.50it/s] 10%|█         | 59/585 [00:16<02:30,  3.50it/s] 10%|█         | 60/585 [00:17<02:30,  3.49it/s] 10%|█         | 61/585 [00:17<02:29,  3.50it/s] 11%|█         | 62/585 [00:17<02:29,  3.50it/s] 11%|█         | 63/585 [00:18<02:29,  3.49it/s] 11%|█         | 64/585 [00:18<02:29,  3.49it/s] 11%|█         | 65/585 [00:18<02:28,  3.49it/s] 11%|█▏        | 66/585 [00:18<02:28,  3.49it/s] 11%|█▏        | 67/585 [00:19<02:28,  3.49it/s] 12%|█▏        | 68/585 [00:19<02:27,  3.49it/s] 12%|█▏        | 69/585 [00:19<02:27,  3.49it/s] 12%|█▏        | 70/585 [00:20<02:27,  3.49it/s] 12%|█▏        | 71/585 [00:20<02:27,  3.48it/s] 12%|█▏        | 72/585 [00:20<02:27,  3.48it/s] 12%|█▏        | 73/585 [00:20<02:26,  3.49it/s] 13%|█▎        | 74/585 [00:21<02:26,  3.49it/s] 13%|█▎        | 75/585 [00:21<02:26,  3.49it/s] 13%|█▎        | 76/585 [00:21<02:25,  3.49it/s] 13%|█▎        | 77/585 [00:22<02:25,  3.49it/s] 13%|█▎        | 78/585 [00:22<02:25,  3.49it/s] 14%|█▎        | 79/585 [00:22<02:24,  3.49it/s] 14%|█▎        | 80/585 [00:22<02:24,  3.49it/s] 14%|█▍        | 81/585 [00:23<02:24,  3.49it/s] 14%|█▍        | 82/585 [00:23<02:24,  3.49it/s] 14%|█▍        | 83/585 [00:23<02:23,  3.49it/s] 14%|█▍        | 84/585 [00:24<02:23,  3.49it/s] 15%|█▍        | 85/585 [00:24<02:23,  3.49it/s] 15%|█▍        | 86/585 [00:24<02:22,  3.49it/s] 15%|█▍        | 87/585 [00:24<02:22,  3.49it/s] 15%|█▌        | 88/585 [00:25<02:22,  3.49it/s] 15%|█▌        | 89/585 [00:25<02:22,  3.48it/s] 15%|█▌        | 90/585 [00:25<02:22,  3.48it/s] 16%|█▌        | 91/585 [00:26<02:21,  3.49it/s] 16%|█▌        | 92/585 [00:26<02:21,  3.49it/s] 16%|█▌        | 93/585 [00:26<02:21,  3.49it/s] 16%|█▌        | 94/585 [00:26<02:20,  3.49it/s] 16%|█▌        | 95/585 [00:27<02:20,  3.49it/s] 16%|█▋        | 96/585 [00:27<02:19,  3.49it/s] 17%|█▋        | 97/585 [00:27<02:19,  3.49it/s] 17%|█▋        | 98/585 [00:28<02:19,  3.49it/s] 17%|█▋        | 99/585 [00:28<02:19,  3.49it/s] 17%|█▋        | 100/585 [00:28<02:18,  3.49it/s] 17%|█▋        | 101/585 [00:28<02:18,  3.50it/s] 17%|█▋        | 102/585 [00:29<02:18,  3.49it/s] 18%|█▊        | 103/585 [00:29<02:18,  3.49it/s] 18%|█▊        | 104/585 [00:29<02:17,  3.49it/s] 18%|█▊        | 105/585 [00:30<02:17,  3.49it/s] 18%|█▊        | 106/585 [00:30<02:17,  3.49it/s] 18%|█▊        | 107/585 [00:30<02:17,  3.48it/s] 18%|█▊        | 108/585 [00:30<02:16,  3.48it/s] 19%|█▊        | 109/585 [00:31<02:16,  3.48it/s] 19%|█▉        | 110/585 [00:31<02:16,  3.48it/s] 19%|█▉        | 111/585 [00:31<02:15,  3.49it/s] 19%|█▉        | 112/585 [00:32<02:15,  3.49it/s] 19%|█▉        | 113/585 [00:32<02:15,  3.49it/s] 19%|█▉        | 114/585 [00:32<02:14,  3.49it/s] 20%|█▉        | 115/585 [00:32<02:14,  3.49it/s] 20%|█▉        | 116/585 [00:33<02:14,  3.49it/s] 20%|██        | 117/585 [00:33<02:14,  3.49it/s][INFO|trainer.py:2140] 2023-08-29 01:27:26,249 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:27:26,249 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 01:27:26,249 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.84it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.06it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.40it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.69it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.26it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.09it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.74it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.46it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.46it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.43it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.52it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.49it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.53it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.40it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.41it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.38it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.19it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.20it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.29it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.39it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.33it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.34it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.39it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.37it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.36it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.20it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.15it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.16it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.21it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.31it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.40it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.31it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.34it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.38it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.38it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.30it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.22it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.29it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.22it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.33it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.37it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.35it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.27it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.39it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.40it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.30it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.20it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.27it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.26it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.21it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.34it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.41it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.30it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.35it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.38it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.32it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.20it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.25it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.20it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.28it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.30it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.33it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.31it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.28it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.28it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.27it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.30it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.25it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.26it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.28it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.33it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.35it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.29it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.32it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.25it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.30it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.32it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.29it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.25it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.26it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.28it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.32it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.31it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.23it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.21it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.30it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.30it/s][A 20%|██        | 117/585 [00:42<02:14,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:27:35,507 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 01:27:35,530 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:27:37,842 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:27:37,855 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:27:37,874 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:50<41:02,  5.27s/it] 20%|██        | 119/585 [00:50<29:20,  3.78s/it] 21%|██        | 120/585 [00:51<21:09,  2.73s/it] 21%|██        | 121/585 [00:51<15:26,  2.00s/it] 21%|██        | 122/585 [00:51<11:29,  1.49s/it] 21%|██        | 123/585 [00:51<08:41,  1.13s/it] 21%|██        | 124/585 [00:52<06:44,  1.14it/s] 21%|██▏       | 125/585 [00:52<05:21,  1.43it/s] 22%|██▏       | 126/585 [00:52<04:24,  1.74it/s] 22%|██▏       | 127/585 [00:53<03:43,  2.05it/s] 22%|██▏       | 128/585 [00:53<03:15,  2.34it/s] 22%|██▏       | 129/585 [00:53<02:55,  2.59it/s] 22%|██▏       | 130/585 [00:53<02:42,  2.81it/s] 22%|██▏       | 131/585 [00:54<02:32,  2.98it/s] 23%|██▎       | 132/585 [00:54<02:25,  3.12it/s] 23%|██▎       | 133/585 [00:54<02:20,  3.22it/s] 23%|██▎       | 134/585 [00:55<02:16,  3.30it/s] 23%|██▎       | 135/585 [00:55<02:14,  3.36it/s] 23%|██▎       | 136/585 [00:55<02:12,  3.39it/s] 23%|██▎       | 137/585 [00:55<02:10,  3.42it/s] 24%|██▎       | 138/585 [00:56<02:09,  3.44it/s] 24%|██▍       | 139/585 [00:56<02:08,  3.46it/s] 24%|██▍       | 140/585 [00:56<02:08,  3.47it/s] 24%|██▍       | 141/585 [00:57<02:08,  3.46it/s] 24%|██▍       | 142/585 [00:57<02:07,  3.47it/s] 24%|██▍       | 143/585 [00:57<02:07,  3.48it/s] 25%|██▍       | 144/585 [00:57<02:06,  3.48it/s] 25%|██▍       | 145/585 [00:58<02:06,  3.49it/s] 25%|██▍       | 146/585 [00:58<02:05,  3.49it/s] 25%|██▌       | 147/585 [00:58<02:05,  3.49it/s] 25%|██▌       | 148/585 [00:59<02:05,  3.49it/s] 25%|██▌       | 149/585 [00:59<02:04,  3.49it/s] 26%|██▌       | 150/585 [00:59<02:04,  3.49it/s] 26%|██▌       | 151/585 [00:59<02:04,  3.49it/s] 26%|██▌       | 152/585 [01:00<02:04,  3.47it/s] 26%|██▌       | 153/585 [01:00<02:04,  3.48it/s] 26%|██▋       | 154/585 [01:00<02:03,  3.48it/s] 26%|██▋       | 155/585 [01:01<02:03,  3.48it/s] 27%|██▋       | 156/585 [01:01<02:02,  3.49it/s] 27%|██▋       | 157/585 [01:01<02:02,  3.49it/s] 27%|██▋       | 158/585 [01:01<02:02,  3.49it/s] 27%|██▋       | 159/585 [01:02<02:02,  3.49it/s] 27%|██▋       | 160/585 [01:02<02:01,  3.49it/s] 28%|██▊       | 161/585 [01:02<02:01,  3.49it/s] 28%|██▊       | 162/585 [01:03<02:01,  3.49it/s] 28%|██▊       | 163/585 [01:03<02:01,  3.48it/s] 28%|██▊       | 164/585 [01:03<02:00,  3.48it/s] 28%|██▊       | 165/585 [01:03<02:00,  3.48it/s] 28%|██▊       | 166/585 [01:04<02:00,  3.48it/s] 29%|██▊       | 167/585 [01:04<02:00,  3.48it/s] 29%|██▊       | 168/585 [01:04<01:59,  3.48it/s] 29%|██▉       | 169/585 [01:05<01:59,  3.49it/s] 29%|██▉       | 170/585 [01:05<01:59,  3.49it/s] 29%|██▉       | 171/585 [01:05<01:58,  3.49it/s] 29%|██▉       | 172/585 [01:05<01:58,  3.49it/s] 30%|██▉       | 173/585 [01:06<01:58,  3.49it/s] 30%|██▉       | 174/585 [01:06<01:58,  3.48it/s] 30%|██▉       | 175/585 [01:06<01:57,  3.49it/s] 30%|███       | 176/585 [01:07<01:57,  3.49it/s] 30%|███       | 177/585 [01:07<01:57,  3.49it/s] 30%|███       | 178/585 [01:07<01:56,  3.49it/s] 31%|███       | 179/585 [01:07<01:56,  3.49it/s] 31%|███       | 180/585 [01:08<01:56,  3.49it/s] 31%|███       | 181/585 [01:08<01:55,  3.49it/s] 31%|███       | 182/585 [01:08<01:55,  3.49it/s] 31%|███▏      | 183/585 [01:09<01:55,  3.49it/s] 31%|███▏      | 184/585 [01:09<01:54,  3.49it/s] 32%|███▏      | 185/585 [01:09<01:54,  3.48it/s] 32%|███▏      | 186/585 [01:09<01:54,  3.48it/s] 32%|███▏      | 187/585 [01:10<01:54,  3.48it/s] 32%|███▏      | 188/585 [01:10<01:53,  3.49it/s] 32%|███▏      | 189/585 [01:10<01:53,  3.49it/s] 32%|███▏      | 190/585 [01:11<01:53,  3.49it/s] 33%|███▎      | 191/585 [01:11<01:52,  3.49it/s] 33%|███▎      | 192/585 [01:11<01:52,  3.49it/s] 33%|███▎      | 193/585 [01:11<01:52,  3.49it/s] 33%|███▎      | 194/585 [01:12<01:51,  3.49it/s] 33%|███▎      | 195/585 [01:12<01:51,  3.49it/s] 34%|███▎      | 196/585 [01:12<01:51,  3.47it/s] 34%|███▎      | 197/585 [01:13<01:51,  3.48it/s] 34%|███▍      | 198/585 [01:13<01:51,  3.48it/s] 34%|███▍      | 199/585 [01:13<01:50,  3.48it/s] 34%|███▍      | 200/585 [01:13<01:50,  3.48it/s] 34%|███▍      | 201/585 [01:14<01:50,  3.48it/s] 35%|███▍      | 202/585 [01:14<01:49,  3.49it/s] 35%|███▍      | 203/585 [01:14<01:49,  3.48it/s] 35%|███▍      | 204/585 [01:15<01:49,  3.49it/s] 35%|███▌      | 205/585 [01:15<01:49,  3.49it/s] 35%|███▌      | 206/585 [01:15<01:48,  3.48it/s] 35%|███▌      | 207/585 [01:15<01:48,  3.48it/s] 36%|███▌      | 208/585 [01:16<01:48,  3.48it/s] 36%|███▌      | 209/585 [01:16<01:48,  3.48it/s] 36%|███▌      | 210/585 [01:16<01:47,  3.48it/s] 36%|███▌      | 211/585 [01:17<01:47,  3.48it/s] 36%|███▌      | 212/585 [01:17<01:47,  3.48it/s] 36%|███▋      | 213/585 [01:17<01:46,  3.48it/s] 37%|███▋      | 214/585 [01:17<01:46,  3.48it/s] 37%|███▋      | 215/585 [01:18<01:46,  3.48it/s] 37%|███▋      | 216/585 [01:18<01:46,  3.48it/s] 37%|███▋      | 217/585 [01:18<01:45,  3.48it/s] 37%|███▋      | 218/585 [01:19<01:45,  3.48it/s] 37%|███▋      | 219/585 [01:19<01:45,  3.48it/s] 38%|███▊      | 220/585 [01:19<01:45,  3.47it/s] 38%|███▊      | 221/585 [01:20<01:44,  3.47it/s] 38%|███▊      | 222/585 [01:20<01:44,  3.47it/s] 38%|███▊      | 223/585 [01:20<01:44,  3.48it/s] 38%|███▊      | 224/585 [01:20<01:43,  3.47it/s] 38%|███▊      | 225/585 [01:21<01:43,  3.47it/s] 39%|███▊      | 226/585 [01:21<01:43,  3.48it/s] 39%|███▉      | 227/585 [01:21<01:42,  3.48it/s] 39%|███▉      | 228/585 [01:22<01:42,  3.48it/s] 39%|███▉      | 229/585 [01:22<01:42,  3.48it/s] 39%|███▉      | 230/585 [01:22<01:41,  3.48it/s] 39%|███▉      | 231/585 [01:22<01:42,  3.47it/s] 40%|███▉      | 232/585 [01:23<01:41,  3.47it/s] 40%|███▉      | 233/585 [01:23<01:41,  3.47it/s] 40%|████      | 234/585 [01:23<01:41,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 01:28:16,474 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:28:16,474 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 01:28:16,474 >>   Batch size = 8
{'eval_loss': 1.2101424932479858, 'eval_runtime': 9.2432, 'eval_samples_per_second': 377.789, 'eval_steps_per_second': 47.278, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.52it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.94it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.23it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.49it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.05it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.80it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.49it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.21it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.22it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.24it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.17it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.24it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.21it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.12it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.14it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.09it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.92it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.01it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.04it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.08it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.10it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.14it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.07it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.04it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.01it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.85it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.87it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.00it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.03it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.02it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.06it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.84it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.13it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.15it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.98it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.89it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.04it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.98it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.03it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.04it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.06it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.97it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.05it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.99it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.91it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.90it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.99it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.00it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.10it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.06it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.02it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.06it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.05it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.92it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.94it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.90it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.93it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.97it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.07it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.12it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.98it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.03it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.01it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.93it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.00it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.81it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.88it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.06it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.07it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.03it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.00it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.95it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.78it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.91it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.85it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.85it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.91it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.06it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.03it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.98it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.97it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.83it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.93it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.84it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.81it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.94it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.94it/s][A 40%|████      | 234/585 [01:33<01:41,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:28:25,798 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 01:28:25,818 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:28:28,105 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:28:28,117 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:28:28,127 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:40<30:59,  5.31s/it] 40%|████      | 236/585 [01:41<22:07,  3.80s/it] 41%|████      | 237/585 [01:41<15:56,  2.75s/it] 41%|████      | 238/585 [01:41<11:37,  2.01s/it] 41%|████      | 239/585 [01:41<08:36,  1.49s/it] 41%|████      | 240/585 [01:42<06:30,  1.13s/it] 41%|████      | 241/585 [01:42<05:02,  1.14it/s] 41%|████▏     | 242/585 [01:42<04:00,  1.43it/s] 42%|████▏     | 243/585 [01:43<03:17,  1.73it/s] 42%|████▏     | 244/585 [01:43<02:47,  2.04it/s] 42%|████▏     | 245/585 [01:43<02:25,  2.33it/s] 42%|████▏     | 246/585 [01:43<02:10,  2.59it/s] 42%|████▏     | 247/585 [01:44<02:00,  2.80it/s] 42%|████▏     | 248/585 [01:44<01:53,  2.98it/s] 43%|████▎     | 249/585 [01:44<01:47,  3.12it/s] 43%|████▎     | 250/585 [01:45<01:44,  3.22it/s] 43%|████▎     | 251/585 [01:45<01:41,  3.30it/s] 43%|████▎     | 252/585 [01:45<01:39,  3.35it/s] 43%|████▎     | 253/585 [01:45<01:37,  3.39it/s] 43%|████▎     | 254/585 [01:46<01:36,  3.42it/s] 44%|████▎     | 255/585 [01:46<01:35,  3.44it/s] 44%|████▍     | 256/585 [01:46<01:35,  3.46it/s] 44%|████▍     | 257/585 [01:47<01:34,  3.46it/s] 44%|████▍     | 258/585 [01:47<01:35,  3.43it/s] 44%|████▍     | 259/585 [01:47<01:34,  3.45it/s] 44%|████▍     | 260/585 [01:47<01:34,  3.46it/s] 45%|████▍     | 261/585 [01:48<01:33,  3.46it/s] 45%|████▍     | 262/585 [01:48<01:33,  3.47it/s] 45%|████▍     | 263/585 [01:48<01:32,  3.48it/s] 45%|████▌     | 264/585 [01:49<01:32,  3.48it/s] 45%|████▌     | 265/585 [01:49<01:31,  3.48it/s] 45%|████▌     | 266/585 [01:49<01:31,  3.48it/s] 46%|████▌     | 267/585 [01:49<01:31,  3.48it/s] 46%|████▌     | 268/585 [01:50<01:30,  3.49it/s] 46%|████▌     | 269/585 [01:50<01:31,  3.46it/s] 46%|████▌     | 270/585 [01:50<01:30,  3.47it/s] 46%|████▋     | 271/585 [01:51<01:30,  3.48it/s] 46%|████▋     | 272/585 [01:51<01:29,  3.48it/s] 47%|████▋     | 273/585 [01:51<01:31,  3.41it/s] 47%|████▋     | 274/585 [01:52<01:30,  3.43it/s] 47%|████▋     | 275/585 [01:52<01:29,  3.45it/s] 47%|████▋     | 276/585 [01:52<01:29,  3.46it/s] 47%|████▋     | 277/585 [01:52<01:28,  3.47it/s] 48%|████▊     | 278/585 [01:53<01:28,  3.48it/s] 48%|████▊     | 279/585 [01:53<01:27,  3.48it/s] 48%|████▊     | 280/585 [01:53<01:28,  3.45it/s] 48%|████▊     | 281/585 [01:54<01:27,  3.46it/s] 48%|████▊     | 282/585 [01:54<01:27,  3.47it/s] 48%|████▊     | 283/585 [01:54<01:26,  3.47it/s] 49%|████▊     | 284/585 [01:54<01:26,  3.48it/s] 49%|████▊     | 285/585 [01:55<01:26,  3.48it/s] 49%|████▉     | 286/585 [01:55<01:25,  3.48it/s] 49%|████▉     | 287/585 [01:55<01:25,  3.48it/s] 49%|████▉     | 288/585 [01:56<01:25,  3.48it/s] 49%|████▉     | 289/585 [01:56<01:25,  3.48it/s] 50%|████▉     | 290/585 [01:56<01:24,  3.48it/s] 50%|████▉     | 291/585 [01:56<01:24,  3.47it/s] 50%|████▉     | 292/585 [01:57<01:24,  3.48it/s] 50%|█████     | 293/585 [01:57<01:23,  3.48it/s] 50%|█████     | 294/585 [01:57<01:23,  3.48it/s] 50%|█████     | 295/585 [01:58<01:23,  3.48it/s] 51%|█████     | 296/585 [01:58<01:23,  3.48it/s] 51%|█████     | 297/585 [01:58<01:22,  3.48it/s] 51%|█████     | 298/585 [01:58<01:22,  3.48it/s] 51%|█████     | 299/585 [01:59<01:22,  3.48it/s] 51%|█████▏    | 300/585 [01:59<01:21,  3.48it/s] 51%|█████▏    | 301/585 [01:59<01:21,  3.48it/s] 52%|█████▏    | 302/585 [02:00<01:21,  3.47it/s] 52%|█████▏    | 303/585 [02:00<01:21,  3.47it/s] 52%|█████▏    | 304/585 [02:00<01:20,  3.47it/s] 52%|█████▏    | 305/585 [02:00<01:20,  3.48it/s] 52%|█████▏    | 306/585 [02:01<01:20,  3.48it/s] 52%|█████▏    | 307/585 [02:01<01:19,  3.48it/s] 53%|█████▎    | 308/585 [02:01<01:19,  3.48it/s] 53%|█████▎    | 309/585 [02:02<01:19,  3.48it/s] 53%|█████▎    | 310/585 [02:02<01:19,  3.48it/s] 53%|█████▎    | 311/585 [02:02<01:18,  3.47it/s] 53%|█████▎    | 312/585 [02:02<01:18,  3.47it/s] 54%|█████▎    | 313/585 [02:03<01:18,  3.47it/s] 54%|█████▎    | 314/585 [02:03<01:17,  3.48it/s] 54%|█████▍    | 315/585 [02:03<01:17,  3.48it/s] 54%|█████▍    | 316/585 [02:04<01:17,  3.48it/s] 54%|█████▍    | 317/585 [02:04<01:17,  3.48it/s] 54%|█████▍    | 318/585 [02:04<01:16,  3.48it/s] 55%|█████▍    | 319/585 [02:04<01:16,  3.48it/s] 55%|█████▍    | 320/585 [02:05<01:16,  3.48it/s] 55%|█████▍    | 321/585 [02:05<01:15,  3.48it/s] 55%|█████▌    | 322/585 [02:05<01:16,  3.43it/s] 55%|█████▌    | 323/585 [02:06<01:16,  3.45it/s] 55%|█████▌    | 324/585 [02:06<01:15,  3.45it/s] 56%|█████▌    | 325/585 [02:06<01:15,  3.46it/s] 56%|█████▌    | 326/585 [02:06<01:14,  3.47it/s] 56%|█████▌    | 327/585 [02:07<01:14,  3.47it/s] 56%|█████▌    | 328/585 [02:07<01:14,  3.47it/s] 56%|█████▌    | 329/585 [02:07<01:13,  3.48it/s] 56%|█████▋    | 330/585 [02:08<01:13,  3.48it/s] 57%|█████▋    | 331/585 [02:08<01:12,  3.48it/s] 57%|█████▋    | 332/585 [02:08<01:12,  3.48it/s] 57%|█████▋    | 333/585 [02:08<01:12,  3.47it/s] 57%|█████▋    | 334/585 [02:09<01:12,  3.47it/s] 57%|█████▋    | 335/585 [02:09<01:12,  3.47it/s] 57%|█████▋    | 336/585 [02:09<01:11,  3.47it/s] 58%|█████▊    | 337/585 [02:10<01:11,  3.47it/s] 58%|█████▊    | 338/585 [02:10<01:11,  3.48it/s] 58%|█████▊    | 339/585 [02:10<01:10,  3.48it/s] 58%|█████▊    | 340/585 [02:10<01:10,  3.48it/s] 58%|█████▊    | 341/585 [02:11<01:10,  3.48it/s] 58%|█████▊    | 342/585 [02:11<01:09,  3.48it/s] 59%|█████▊    | 343/585 [02:11<01:09,  3.48it/s] 59%|█████▉    | 344/585 [02:12<01:09,  3.48it/s] 59%|█████▉    | 345/585 [02:12<01:09,  3.48it/s] 59%|█████▉    | 346/585 [02:12<01:08,  3.48it/s] 59%|█████▉    | 347/585 [02:13<01:08,  3.47it/s] 59%|█████▉    | 348/585 [02:13<01:08,  3.47it/s] 60%|█████▉    | 349/585 [02:13<01:07,  3.47it/s] 60%|█████▉    | 350/585 [02:13<01:07,  3.48it/s] 60%|██████    | 351/585 [02:14<01:07,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 01:29:06,891 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:29:06,891 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 01:29:06,891 >>   Batch size = 8
{'eval_loss': 1.2282401323318481, 'eval_runtime': 9.3028, 'eval_samples_per_second': 375.371, 'eval_steps_per_second': 46.975, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.91it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.02it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.25it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.57it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.07it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.77it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.47it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.14it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.00it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.91it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.03it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.04it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.14it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.17it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.17it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.24it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.97it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.92it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.87it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.00it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.03it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.02it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.13it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.11it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.03it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.94it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.88it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.94it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.94it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.91it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.99it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.06it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.08it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.97it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.95it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.91it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.89it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.93it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.93it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.83it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.99it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.93it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.90it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.90it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.86it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.72it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.92it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.98it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.98it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.10it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.09it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.97it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.99it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.02it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.93it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.98it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.96it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.01it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.05it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.08it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.93it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.03it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.94it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.89it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.92it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.01it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.86it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.99it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.06it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.97it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.01it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.91it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.90it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.89it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.99it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.00it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.93it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.02it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.01it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.94it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.00it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.85it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.85it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.98it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.01it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.00it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.00it/s][A 60%|██████    | 351/585 [02:23<01:07,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:29:16,210 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 01:29:16,244 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:29:18,629 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:29:18,648 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:29:18,662 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:30<20:21,  5.24s/it] 60%|██████    | 353/585 [02:31<14:31,  3.76s/it] 61%|██████    | 354/585 [02:31<10:27,  2.72s/it] 61%|██████    | 355/585 [02:31<07:36,  1.99s/it] 61%|██████    | 356/585 [02:32<05:38,  1.48s/it] 61%|██████    | 357/585 [02:32<04:15,  1.12s/it] 61%|██████    | 358/585 [02:32<03:17,  1.15it/s] 61%|██████▏   | 359/585 [02:32<02:37,  1.44it/s] 62%|██████▏   | 360/585 [02:33<02:08,  1.75it/s] 62%|██████▏   | 361/585 [02:33<01:49,  2.05it/s] 62%|██████▏   | 362/585 [02:33<01:35,  2.34it/s] 62%|██████▏   | 363/585 [02:34<01:25,  2.60it/s] 62%|██████▏   | 364/585 [02:34<01:18,  2.80it/s] 62%|██████▏   | 365/585 [02:34<01:13,  2.98it/s] 63%|██████▎   | 366/585 [02:34<01:10,  3.11it/s] 63%|██████▎   | 367/585 [02:35<01:07,  3.22it/s] 63%|██████▎   | 368/585 [02:35<01:05,  3.29it/s] 63%|██████▎   | 369/585 [02:35<01:04,  3.35it/s] 63%|██████▎   | 370/585 [02:36<01:03,  3.39it/s] 63%|██████▎   | 371/585 [02:36<01:02,  3.42it/s] 64%|██████▎   | 372/585 [02:36<01:01,  3.44it/s] 64%|██████▍   | 373/585 [02:36<01:01,  3.45it/s] 64%|██████▍   | 374/585 [02:37<01:00,  3.46it/s] 64%|██████▍   | 375/585 [02:37<01:00,  3.46it/s] 64%|██████▍   | 376/585 [02:37<01:00,  3.47it/s] 64%|██████▍   | 377/585 [02:38<00:59,  3.47it/s] 65%|██████▍   | 378/585 [02:38<00:59,  3.48it/s] 65%|██████▍   | 379/585 [02:38<00:59,  3.48it/s] 65%|██████▍   | 380/585 [02:39<00:58,  3.48it/s] 65%|██████▌   | 381/585 [02:39<00:58,  3.48it/s] 65%|██████▌   | 382/585 [02:39<00:58,  3.49it/s] 65%|██████▌   | 383/585 [02:39<00:57,  3.49it/s] 66%|██████▌   | 384/585 [02:40<00:57,  3.49it/s] 66%|██████▌   | 385/585 [02:40<00:57,  3.49it/s] 66%|██████▌   | 386/585 [02:40<00:57,  3.47it/s] 66%|██████▌   | 387/585 [02:41<00:56,  3.48it/s] 66%|██████▋   | 388/585 [02:41<00:56,  3.48it/s] 66%|██████▋   | 389/585 [02:41<00:56,  3.48it/s] 67%|██████▋   | 390/585 [02:41<00:55,  3.48it/s] 67%|██████▋   | 391/585 [02:42<00:55,  3.49it/s] 67%|██████▋   | 392/585 [02:42<00:55,  3.48it/s] 67%|██████▋   | 393/585 [02:42<00:55,  3.49it/s] 67%|██████▋   | 394/585 [02:43<00:54,  3.49it/s] 68%|██████▊   | 395/585 [02:43<00:54,  3.49it/s] 68%|██████▊   | 396/585 [02:43<00:54,  3.49it/s] 68%|██████▊   | 397/585 [02:43<00:54,  3.47it/s] 68%|██████▊   | 398/585 [02:44<00:53,  3.48it/s] 68%|██████▊   | 399/585 [02:44<00:53,  3.48it/s] 68%|██████▊   | 400/585 [02:44<00:53,  3.48it/s] 69%|██████▊   | 401/585 [02:45<00:52,  3.48it/s] 69%|██████▊   | 402/585 [02:45<00:52,  3.48it/s] 69%|██████▉   | 403/585 [02:45<00:52,  3.48it/s] 69%|██████▉   | 404/585 [02:45<00:51,  3.49it/s] 69%|██████▉   | 405/585 [02:46<00:51,  3.48it/s] 69%|██████▉   | 406/585 [02:46<00:51,  3.48it/s] 70%|██████▉   | 407/585 [02:46<00:51,  3.48it/s] 70%|██████▉   | 408/585 [02:47<00:51,  3.47it/s] 70%|██████▉   | 409/585 [02:47<00:50,  3.47it/s] 70%|███████   | 410/585 [02:47<00:50,  3.47it/s] 70%|███████   | 411/585 [02:47<00:50,  3.47it/s] 70%|███████   | 412/585 [02:48<00:49,  3.47it/s] 71%|███████   | 413/585 [02:48<00:49,  3.48it/s] 71%|███████   | 414/585 [02:48<00:49,  3.48it/s] 71%|███████   | 415/585 [02:49<00:48,  3.48it/s] 71%|███████   | 416/585 [02:49<00:48,  3.48it/s] 71%|███████▏  | 417/585 [02:49<00:48,  3.48it/s] 71%|███████▏  | 418/585 [02:49<00:47,  3.48it/s] 72%|███████▏  | 419/585 [02:50<00:47,  3.46it/s] 72%|███████▏  | 420/585 [02:50<00:47,  3.47it/s] 72%|███████▏  | 421/585 [02:50<00:47,  3.45it/s] 72%|███████▏  | 422/585 [02:51<00:47,  3.46it/s] 72%|███████▏  | 423/585 [02:51<00:46,  3.47it/s] 72%|███████▏  | 424/585 [02:51<00:46,  3.47it/s] 73%|███████▎  | 425/585 [02:51<00:47,  3.37it/s] 73%|███████▎  | 426/585 [02:52<00:46,  3.40it/s] 73%|███████▎  | 427/585 [02:52<00:46,  3.42it/s] 73%|███████▎  | 428/585 [02:52<00:45,  3.44it/s] 73%|███████▎  | 429/585 [02:53<00:45,  3.45it/s] 74%|███████▎  | 430/585 [02:53<00:44,  3.46it/s] 74%|███████▎  | 431/585 [02:53<00:44,  3.47it/s] 74%|███████▍  | 432/585 [02:53<00:44,  3.46it/s] 74%|███████▍  | 433/585 [02:54<00:43,  3.47it/s] 74%|███████▍  | 434/585 [02:54<00:43,  3.47it/s] 74%|███████▍  | 435/585 [02:54<00:43,  3.47it/s] 75%|███████▍  | 436/585 [02:55<00:42,  3.47it/s] 75%|███████▍  | 437/585 [02:55<00:42,  3.47it/s] 75%|███████▍  | 438/585 [02:55<00:42,  3.48it/s] 75%|███████▌  | 439/585 [02:55<00:41,  3.48it/s] 75%|███████▌  | 440/585 [02:56<00:41,  3.48it/s] 75%|███████▌  | 441/585 [02:56<00:41,  3.48it/s] 76%|███████▌  | 442/585 [02:56<00:41,  3.48it/s] 76%|███████▌  | 443/585 [02:57<00:41,  3.46it/s] 76%|███████▌  | 444/585 [02:57<00:40,  3.46it/s] 76%|███████▌  | 445/585 [02:57<00:40,  3.47it/s] 76%|███████▌  | 446/585 [02:58<00:40,  3.47it/s] 76%|███████▋  | 447/585 [02:58<00:39,  3.48it/s] 77%|███████▋  | 448/585 [02:58<00:39,  3.48it/s] 77%|███████▋  | 449/585 [02:58<00:39,  3.48it/s] 77%|███████▋  | 450/585 [02:59<00:38,  3.48it/s] 77%|███████▋  | 451/585 [02:59<00:38,  3.48it/s] 77%|███████▋  | 452/585 [02:59<00:38,  3.48it/s] 77%|███████▋  | 453/585 [03:00<00:37,  3.48it/s] 78%|███████▊  | 454/585 [03:00<00:37,  3.47it/s] 78%|███████▊  | 455/585 [03:00<00:37,  3.47it/s] 78%|███████▊  | 456/585 [03:00<00:37,  3.48it/s] 78%|███████▊  | 457/585 [03:01<00:36,  3.48it/s] 78%|███████▊  | 458/585 [03:01<00:36,  3.48it/s] 78%|███████▊  | 459/585 [03:01<00:36,  3.48it/s] 79%|███████▊  | 460/585 [03:02<00:35,  3.48it/s] 79%|███████▉  | 461/585 [03:02<00:35,  3.48it/s] 79%|███████▉  | 462/585 [03:02<00:35,  3.48it/s] 79%|███████▉  | 463/585 [03:02<00:35,  3.48it/s] 79%|███████▉  | 464/585 [03:03<00:34,  3.48it/s] 79%|███████▉  | 465/585 [03:03<00:34,  3.48it/s] 80%|███████▉  | 466/585 [03:03<00:34,  3.48it/s] 80%|███████▉  | 467/585 [03:04<00:33,  3.47it/s] 80%|████████  | 468/585 [03:04<00:33,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 01:29:57,070 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:29:57,070 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 01:29:57,070 >>   Batch size = 8
{'eval_loss': 1.2460665702819824, 'eval_runtime': 9.3066, 'eval_samples_per_second': 375.218, 'eval_steps_per_second': 46.956, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.66it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.08it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.11it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.48it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.08it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.70it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.41it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.05it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.11it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.14it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.19it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.21it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.10it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.18it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.19it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.17it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.10it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.92it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.00it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.10it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.11it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.07it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.97it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.06it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.04it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.99it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.93it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.80it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.98it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.10it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.10it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.93it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.06it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.00it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.96it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.94it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.88it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.83it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.99it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.03it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.91it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.99it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.00it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.87it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.88it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.74it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.77it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.91it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.01it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.98it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.08it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.08it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.96it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.95it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.94it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.91it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.95it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.92it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.99it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.04it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.02it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.97it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.96it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.95it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.83it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.90it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.01it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.95it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.01it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.04it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.92it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.94it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.97it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.91it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.92it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.93it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.04it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.95it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.00it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.99it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.94it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.97it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.95it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.81it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.00it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.91it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.91it/s][A 80%|████████  | 468/585 [03:13<00:33,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:30:06,406 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 01:30:06,462 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:30:08,672 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:30:08,691 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:30:08,704 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:21<10:05,  5.22s/it] 80%|████████  | 470/585 [03:21<07:09,  3.74s/it] 81%|████████  | 471/585 [03:21<05:08,  2.70s/it] 81%|████████  | 472/585 [03:21<03:43,  1.98s/it] 81%|████████  | 473/585 [03:22<02:44,  1.47s/it] 81%|████████  | 474/585 [03:22<02:03,  1.12s/it] 81%|████████  | 475/585 [03:22<01:35,  1.15it/s] 81%|████████▏ | 476/585 [03:23<01:15,  1.44it/s] 82%|████████▏ | 477/585 [03:23<01:01,  1.75it/s] 82%|████████▏ | 478/585 [03:23<00:51,  2.06it/s] 82%|████████▏ | 479/585 [03:23<00:45,  2.35it/s] 82%|████████▏ | 480/585 [03:24<00:40,  2.60it/s] 82%|████████▏ | 481/585 [03:24<00:37,  2.81it/s] 82%|████████▏ | 482/585 [03:24<00:34,  2.99it/s] 83%|████████▎ | 483/585 [03:25<00:32,  3.12it/s] 83%|████████▎ | 484/585 [03:25<00:31,  3.22it/s] 83%|████████▎ | 485/585 [03:25<00:30,  3.30it/s] 83%|████████▎ | 486/585 [03:25<00:29,  3.35it/s] 83%|████████▎ | 487/585 [03:26<00:28,  3.39it/s] 83%|████████▎ | 488/585 [03:26<00:28,  3.42it/s] 84%|████████▎ | 489/585 [03:26<00:27,  3.44it/s] 84%|████████▍ | 490/585 [03:27<00:27,  3.46it/s] 84%|████████▍ | 491/585 [03:27<00:27,  3.47it/s] 84%|████████▍ | 492/585 [03:27<00:26,  3.46it/s] 84%|████████▍ | 493/585 [03:27<00:26,  3.47it/s] 84%|████████▍ | 494/585 [03:28<00:26,  3.47it/s] 85%|████████▍ | 495/585 [03:28<00:25,  3.48it/s] 85%|████████▍ | 496/585 [03:28<00:25,  3.48it/s] 85%|████████▍ | 497/585 [03:29<00:25,  3.48it/s] 85%|████████▌ | 498/585 [03:29<00:24,  3.49it/s] 85%|████████▌ | 499/585 [03:29<00:24,  3.49it/s] 85%|████████▌ | 500/585 [03:29<00:24,  3.49it/s]                                                  85%|████████▌ | 500/585 [03:29<00:24,  3.49it/s] 86%|████████▌ | 501/585 [03:30<00:24,  3.48it/s] 86%|████████▌ | 502/585 [03:30<00:23,  3.48it/s] 86%|████████▌ | 503/585 [03:30<00:23,  3.48it/s] 86%|████████▌ | 504/585 [03:31<00:23,  3.48it/s] 86%|████████▋ | 505/585 [03:31<00:22,  3.48it/s] 86%|████████▋ | 506/585 [03:31<00:22,  3.48it/s] 87%|████████▋ | 507/585 [03:31<00:22,  3.49it/s] 87%|████████▋ | 508/585 [03:32<00:22,  3.49it/s] 87%|████████▋ | 509/585 [03:32<00:21,  3.49it/s] 87%|████████▋ | 510/585 [03:32<00:21,  3.49it/s] 87%|████████▋ | 511/585 [03:33<00:21,  3.49it/s] 88%|████████▊ | 512/585 [03:33<00:20,  3.49it/s] 88%|████████▊ | 513/585 [03:33<00:20,  3.49it/s] 88%|████████▊ | 514/585 [03:33<00:20,  3.47it/s] 88%|████████▊ | 515/585 [03:34<00:20,  3.47it/s] 88%|████████▊ | 516/585 [03:34<00:19,  3.48it/s] 88%|████████▊ | 517/585 [03:34<00:19,  3.48it/s] 89%|████████▊ | 518/585 [03:35<00:19,  3.48it/s] 89%|████████▊ | 519/585 [03:35<00:18,  3.48it/s] 89%|████████▉ | 520/585 [03:35<00:18,  3.48it/s] 89%|████████▉ | 521/585 [03:35<00:18,  3.48it/s] 89%|████████▉ | 522/585 [03:36<00:18,  3.49it/s] 89%|████████▉ | 523/585 [03:36<00:17,  3.49it/s] 90%|████████▉ | 524/585 [03:36<00:17,  3.49it/s] 90%|████████▉ | 525/585 [03:37<00:17,  3.47it/s] 90%|████████▉ | 526/585 [03:37<00:16,  3.48it/s] 90%|█████████ | 527/585 [03:37<00:16,  3.48it/s] 90%|█████████ | 528/585 [03:37<00:16,  3.48it/s] 90%|█████████ | 529/585 [03:38<00:16,  3.48it/s] 91%|█████████ | 530/585 [03:38<00:15,  3.48it/s] 91%|█████████ | 531/585 [03:38<00:15,  3.48it/s] 91%|█████████ | 532/585 [03:39<00:15,  3.48it/s] 91%|█████████ | 533/585 [03:39<00:14,  3.48it/s] 91%|█████████▏| 534/585 [03:39<00:14,  3.48it/s] 91%|█████████▏| 535/585 [03:40<00:14,  3.48it/s] 92%|█████████▏| 536/585 [03:40<00:14,  3.47it/s] 92%|█████████▏| 537/585 [03:40<00:13,  3.47it/s] 92%|█████████▏| 538/585 [03:40<00:13,  3.47it/s] 92%|█████████▏| 539/585 [03:41<00:13,  3.47it/s] 92%|█████████▏| 540/585 [03:41<00:12,  3.48it/s] 92%|█████████▏| 541/585 [03:41<00:12,  3.48it/s] 93%|█████████▎| 542/585 [03:42<00:12,  3.48it/s] 93%|█████████▎| 543/585 [03:42<00:12,  3.48it/s] 93%|█████████▎| 544/585 [03:42<00:11,  3.48it/s] 93%|█████████▎| 545/585 [03:42<00:11,  3.48it/s] 93%|█████████▎| 546/585 [03:43<00:11,  3.48it/s] 94%|█████████▎| 547/585 [03:43<00:10,  3.48it/s] 94%|█████████▎| 548/585 [03:43<00:10,  3.48it/s] 94%|█████████▍| 549/585 [03:44<00:10,  3.48it/s] 94%|█████████▍| 550/585 [03:44<00:10,  3.48it/s] 94%|█████████▍| 551/585 [03:44<00:09,  3.48it/s] 94%|█████████▍| 552/585 [03:44<00:09,  3.48it/s] 95%|█████████▍| 553/585 [03:45<00:09,  3.47it/s] 95%|█████████▍| 554/585 [03:45<00:08,  3.48it/s] 95%|█████████▍| 555/585 [03:45<00:08,  3.48it/s] 95%|█████████▌| 556/585 [03:46<00:08,  3.48it/s] 95%|█████████▌| 557/585 [03:46<00:08,  3.48it/s] 95%|█████████▌| 558/585 [03:46<00:07,  3.48it/s] 96%|█████████▌| 559/585 [03:46<00:07,  3.48it/s] 96%|█████████▌| 560/585 [03:47<00:07,  3.48it/s] 96%|█████████▌| 561/585 [03:47<00:06,  3.48it/s] 96%|█████████▌| 562/585 [03:47<00:06,  3.48it/s] 96%|█████████▌| 563/585 [03:48<00:06,  3.48it/s] 96%|█████████▋| 564/585 [03:48<00:06,  3.47it/s] 97%|█████████▋| 565/585 [03:48<00:05,  3.47it/s] 97%|█████████▋| 566/585 [03:48<00:05,  3.48it/s] 97%|█████████▋| 567/585 [03:49<00:05,  3.48it/s] 97%|█████████▋| 568/585 [03:49<00:04,  3.48it/s] 97%|█████████▋| 569/585 [03:49<00:04,  3.48it/s] 97%|█████████▋| 570/585 [03:50<00:04,  3.48it/s] 98%|█████████▊| 571/585 [03:50<00:04,  3.48it/s] 98%|█████████▊| 572/585 [03:50<00:03,  3.48it/s] 98%|█████████▊| 573/585 [03:50<00:03,  3.47it/s] 98%|█████████▊| 574/585 [03:51<00:03,  3.47it/s] 98%|█████████▊| 575/585 [03:51<00:02,  3.45it/s] 98%|█████████▊| 576/585 [03:51<00:02,  3.46it/s] 99%|█████████▊| 577/585 [03:52<00:02,  3.37it/s] 99%|█████████▉| 578/585 [03:52<00:02,  3.39it/s] 99%|█████████▉| 579/585 [03:52<00:01,  3.42it/s] 99%|█████████▉| 580/585 [03:52<00:01,  3.44it/s] 99%|█████████▉| 581/585 [03:53<00:01,  3.45it/s] 99%|█████████▉| 582/585 [03:53<00:00,  3.46it/s]100%|█████████▉| 583/585 [03:53<00:00,  3.47it/s]100%|█████████▉| 584/585 [03:54<00:00,  3.47it/s]100%|██████████| 585/585 [03:54<00:00,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 01:30:47,110 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:30:47,110 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 01:30:47,110 >>   Batch size = 8
{'eval_loss': 1.2579089403152466, 'eval_runtime': 9.3068, 'eval_samples_per_second': 375.208, 'eval_steps_per_second': 46.955, 'epoch': 4.0}
{'loss': 0.3587, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.80it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.11it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.27it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.55it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.04it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.68it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.41it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.18it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.02it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.12it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.13it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.04it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.08it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.12it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.99it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.95it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.85it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.82it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.89it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.02it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.03it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.96it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.99it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.01it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.86it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.94it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.83it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.83it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.97it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.82it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.88it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.05it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.07it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.02it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.88it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.86it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.84it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.95it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.88it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.96it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.05it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.14it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.01it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.04it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.93it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.85it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.96it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.87it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.89it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.00it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.02it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.99it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.97it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.94it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.90it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.89it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.87it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.97it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.91it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.02it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.93it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.96it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.93it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.90it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.87it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.91it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.92it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.98it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.00it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.89it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.84it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.92it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.89it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.91it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.94it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.85it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.94it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.97it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.94it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.91it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.84it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.89it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.85it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.99it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.00it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.86it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.86it/s][A100%|██████████| 585/585 [04:03<00:00,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:30:56,424 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 01:30:56,447 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:30:58,843 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:30:58,860 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:30:58,873 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 01:31:04,932 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 01:31:04,938 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117 (score: 1.2101424932479858).
                                                 100%|██████████| 585/585 [04:14<00:00,  3.47it/s]100%|██████████| 585/585 [04:14<00:00,  2.30it/s]
[INFO|trainer.py:1894] 2023-08-29 01:31:07,066 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 01:31:07,235 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:31:09,655 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:31:09,699 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:31:09,714 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:31:09,950 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:09,950 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:09,950 >>   train_loss               =     0.3562
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:09,950 >>   train_runtime            = 0:04:14.36
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:09,950 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:09,950 >>   train_samples_per_second =    147.426
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:09,950 >>   train_steps_per_second   =        2.3
{'eval_loss': 1.2627753019332886, 'eval_runtime': 9.3007, 'eval_samples_per_second': 375.454, 'eval_steps_per_second': 46.986, 'epoch': 5.0}
{'train_runtime': 254.3655, 'train_samples_per_second': 147.426, 'train_steps_per_second': 2.3, 'train_loss': 0.35618192558614614, 'epoch': 5.0}
08/29/2023 01:31:10 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 01:31:10,016 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:31:10,017 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 01:31:10,017 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.90it/s]  3%|▎         | 12/437 [00:00<00:08, 51.55it/s]  4%|▍         | 18/437 [00:00<00:08, 49.68it/s]  5%|▌         | 24/437 [00:00<00:08, 48.78it/s]  7%|▋         | 29/437 [00:00<00:08, 48.19it/s]  8%|▊         | 34/437 [00:00<00:08, 48.01it/s]  9%|▉         | 39/437 [00:00<00:08, 47.83it/s] 10%|█         | 44/437 [00:00<00:08, 47.74it/s] 11%|█         | 49/437 [00:01<00:08, 47.59it/s] 12%|█▏        | 54/437 [00:01<00:08, 47.48it/s] 14%|█▎        | 59/437 [00:01<00:07, 47.45it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.43it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.46it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.47it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.51it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.43it/s] 20%|██        | 89/437 [00:01<00:07, 47.35it/s] 22%|██▏       | 94/437 [00:01<00:07, 47.29it/s] 23%|██▎       | 99/437 [00:02<00:07, 47.31it/s] 24%|██▍       | 104/437 [00:02<00:07, 47.33it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.32it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.35it/s] 27%|██▋       | 119/437 [00:02<00:06, 47.36it/s] 28%|██▊       | 124/437 [00:02<00:06, 47.43it/s] 30%|██▉       | 129/437 [00:02<00:06, 47.43it/s] 31%|███       | 134/437 [00:02<00:06, 47.39it/s] 32%|███▏      | 139/437 [00:02<00:06, 47.39it/s] 33%|███▎      | 144/437 [00:03<00:06, 47.28it/s] 34%|███▍      | 149/437 [00:03<00:06, 47.25it/s] 35%|███▌      | 154/437 [00:03<00:05, 47.29it/s] 36%|███▋      | 159/437 [00:03<00:05, 47.30it/s] 38%|███▊      | 164/437 [00:03<00:05, 47.31it/s] 39%|███▊      | 169/437 [00:03<00:05, 47.36it/s] 40%|███▉      | 174/437 [00:03<00:05, 47.24it/s] 41%|████      | 179/437 [00:03<00:05, 47.32it/s] 42%|████▏     | 184/437 [00:03<00:05, 47.29it/s] 43%|████▎     | 189/437 [00:03<00:05, 47.31it/s] 44%|████▍     | 194/437 [00:04<00:05, 47.29it/s] 46%|████▌     | 199/437 [00:04<00:05, 47.27it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.25it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.16it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.27it/s] 50%|█████     | 219/437 [00:04<00:04, 47.34it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.34it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.40it/s] 54%|█████▎    | 234/437 [00:04<00:04, 47.38it/s] 55%|█████▍    | 239/437 [00:05<00:04, 47.31it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.30it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.31it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.28it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.30it/s] 60%|██████    | 264/437 [00:05<00:03, 47.32it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.33it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.32it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.30it/s] 65%|██████▍   | 284/437 [00:05<00:03, 47.32it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.30it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.28it/s] 68%|██████▊   | 299/437 [00:06<00:02, 47.25it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.24it/s] 71%|███████   | 309/437 [00:06<00:02, 47.24it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.23it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.22it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.36it/s] 75%|███████▌  | 329/437 [00:06<00:02, 47.32it/s] 76%|███████▋  | 334/437 [00:07<00:02, 47.32it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.35it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.35it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.26it/s] 81%|████████  | 354/437 [00:07<00:01, 47.26it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.25it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.25it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.30it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.26it/s] 87%|████████▋ | 379/437 [00:07<00:01, 47.33it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.27it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.26it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.36it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.33it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.21it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.24it/s] 95%|█████████▍| 414/437 [00:08<00:00, 47.29it/s] 96%|█████████▌| 419/437 [00:08<00:00, 47.24it/s] 97%|█████████▋| 424/437 [00:08<00:00, 47.17it/s] 98%|█████████▊| 429/437 [00:09<00:00, 47.31it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.37it/s]100%|██████████| 437/437 [00:09<00:00, 47.44it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:31:19,250 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:19,251 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:19,251 >>   eval_loss               =     1.2101
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:19,251 >>   eval_runtime            = 0:00:09.23
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:19,251 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:19,251 >>   eval_samples_per_second =    378.176
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:19,251 >>   eval_steps_per_second   =     47.326
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:31:19,251 >>   perplexity              =      3.354
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:31:25,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:31:25,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:31:25,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:31:25,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:31:25,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:31:26,566 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:31:26,567 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:31:27,149 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:31:28,171 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:31:28,171 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:31:31,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:31:31,021 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:31:31,021 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:31:31,021 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:31:31,021 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:31:31,673 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:31:31,674 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:31:32,245 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:31:32,401 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:31:32,401 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.48it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:11,  1.47it/s]Extractor Predicting: 18it [00:11,  1.46it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.53it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.43it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:23,  1.47it/s]Extractor Predicting: 37it [00:24,  1.48it/s]Extractor Predicting: 38it [00:25,  1.49it/s]Extractor Predicting: 39it [00:25,  1.50it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:27,  1.49it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:29,  1.49it/s]Extractor Predicting: 46it [00:30,  1.49it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:32,  1.49it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:33,  1.48it/s]Extractor Predicting: 52it [00:34,  1.46it/s]Extractor Predicting: 53it [00:35,  1.47it/s]Extractor Predicting: 54it [00:35,  1.47it/s]Extractor Predicting: 55it [00:36,  1.48it/s]Extractor Predicting: 56it [00:37,  1.49it/s]Extractor Predicting: 57it [00:37,  1.49it/s]Extractor Predicting: 58it [00:38,  1.51it/s]Extractor Predicting: 59it [00:39,  1.50it/s]Extractor Predicting: 60it [00:39,  1.48it/s]Extractor Predicting: 61it [00:40,  1.46it/s]Extractor Predicting: 62it [00:41,  1.46it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.44it/s]Extractor Predicting: 65it [00:43,  1.45it/s]Extractor Predicting: 66it [00:44,  1.44it/s]Extractor Predicting: 67it [00:44,  1.43it/s]Extractor Predicting: 68it [00:45,  1.42it/s]Extractor Predicting: 69it [00:46,  1.43it/s]Extractor Predicting: 70it [00:46,  1.42it/s]Extractor Predicting: 71it [00:47,  1.42it/s]Extractor Predicting: 72it [00:48,  1.41it/s]Extractor Predicting: 73it [00:49,  1.43it/s]Extractor Predicting: 74it [00:49,  1.40it/s]Extractor Predicting: 75it [00:50,  1.44it/s]Extractor Predicting: 76it [00:51,  1.45it/s]Extractor Predicting: 77it [00:51,  1.43it/s]Extractor Predicting: 78it [00:52,  1.46it/s]Extractor Predicting: 79it [00:53,  1.42it/s]Extractor Predicting: 80it [00:53,  1.41it/s]Extractor Predicting: 81it [00:54,  1.41it/s]Extractor Predicting: 82it [00:55,  1.44it/s]Extractor Predicting: 83it [00:55,  1.45it/s]Extractor Predicting: 84it [00:56,  1.47it/s]Extractor Predicting: 85it [00:57,  1.46it/s]Extractor Predicting: 86it [00:58,  1.43it/s]Extractor Predicting: 87it [00:58,  1.45it/s]Extractor Predicting: 88it [00:59,  1.49it/s]Extractor Predicting: 89it [00:59,  1.54it/s]Extractor Predicting: 90it [01:00,  1.53it/s]Extractor Predicting: 91it [01:01,  1.59it/s]Extractor Predicting: 92it [01:01,  1.64it/s]Extractor Predicting: 93it [01:02,  1.67it/s]Extractor Predicting: 94it [01:02,  1.70it/s]Extractor Predicting: 95it [01:03,  1.64it/s]Extractor Predicting: 96it [01:04,  1.67it/s]Extractor Predicting: 97it [01:04,  1.63it/s]Extractor Predicting: 98it [01:05,  1.62it/s]Extractor Predicting: 99it [01:05,  1.68it/s]Extractor Predicting: 100it [01:06,  1.68it/s]Extractor Predicting: 101it [01:07,  1.65it/s]Extractor Predicting: 102it [01:07,  1.60it/s]Extractor Predicting: 103it [01:08,  1.60it/s]Extractor Predicting: 104it [01:09,  1.56it/s]Extractor Predicting: 105it [01:09,  1.56it/s]Extractor Predicting: 106it [01:10,  1.58it/s]Extractor Predicting: 107it [01:11,  1.56it/s]Extractor Predicting: 108it [01:11,  1.59it/s]Extractor Predicting: 109it [01:12,  1.62it/s]Extractor Predicting: 110it [01:12,  1.63it/s]Extractor Predicting: 111it [01:13,  1.64it/s]Extractor Predicting: 112it [01:14,  1.65it/s]Extractor Predicting: 113it [01:14,  1.67it/s]Extractor Predicting: 114it [01:15,  1.63it/s]Extractor Predicting: 115it [01:15,  1.65it/s]Extractor Predicting: 116it [01:16,  1.61it/s]Extractor Predicting: 117it [01:17,  1.45it/s]Extractor Predicting: 118it [01:18,  1.47it/s]Extractor Predicting: 119it [01:18,  1.47it/s]Extractor Predicting: 120it [01:19,  1.46it/s]Extractor Predicting: 121it [01:20,  1.47it/s]Extractor Predicting: 122it [01:20,  1.48it/s]Extractor Predicting: 123it [01:21,  1.47it/s]Extractor Predicting: 124it [01:22,  1.46it/s]Extractor Predicting: 125it [01:22,  1.46it/s]Extractor Predicting: 126it [01:23,  1.44it/s]Extractor Predicting: 127it [01:24,  1.45it/s]Extractor Predicting: 128it [01:24,  1.45it/s]Extractor Predicting: 129it [01:25,  1.47it/s]Extractor Predicting: 130it [01:26,  1.50it/s]Extractor Predicting: 131it [01:26,  1.47it/s]Extractor Predicting: 132it [01:27,  1.46it/s]Extractor Predicting: 133it [01:28,  1.45it/s]Extractor Predicting: 134it [01:28,  1.45it/s]Extractor Predicting: 135it [01:29,  1.47it/s]Extractor Predicting: 136it [01:30,  1.45it/s]Extractor Predicting: 137it [01:31,  1.45it/s]Extractor Predicting: 138it [01:31,  1.48it/s]Extractor Predicting: 139it [01:32,  1.48it/s]Extractor Predicting: 140it [01:33,  1.51it/s]Extractor Predicting: 141it [01:33,  1.47it/s]Extractor Predicting: 142it [01:34,  1.46it/s]Extractor Predicting: 143it [01:35,  1.48it/s]Extractor Predicting: 144it [01:35,  1.50it/s]Extractor Predicting: 144it [01:35,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:15,221 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:15,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:15,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:15,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:15,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:33:15,549 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:33:15,550 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:33:16,263 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:33:17,297 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:33:17,298 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:20,804 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:20,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:20,810 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:20,810 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:33:20,810 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:33:21,554 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:33:21,555 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:33:22,239 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:33:22,393 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:33:22,393 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4644945697577276,
  "recall": 0.15922107674684993,
  "score": 0.23715077841757304,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.46it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.48it/s]Extractor Predicting: 30it [00:19,  1.45it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:21,  1.46it/s]Extractor Predicting: 33it [00:21,  1.48it/s]Extractor Predicting: 34it [00:22,  1.50it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.57it/s]Extractor Predicting: 38it [00:25,  1.57it/s]Extractor Predicting: 39it [00:25,  1.56it/s]Extractor Predicting: 40it [00:26,  1.57it/s]Extractor Predicting: 41it [00:27,  1.57it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:28,  1.55it/s]Extractor Predicting: 44it [00:29,  1.55it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:30,  1.55it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:31,  1.56it/s]Extractor Predicting: 49it [00:32,  1.55it/s]Extractor Predicting: 50it [00:32,  1.53it/s]Extractor Predicting: 51it [00:33,  1.54it/s]Extractor Predicting: 52it [00:34,  1.54it/s]Extractor Predicting: 53it [00:34,  1.54it/s]Extractor Predicting: 54it [00:35,  1.55it/s]Extractor Predicting: 55it [00:36,  1.52it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:37,  1.56it/s]Extractor Predicting: 58it [00:38,  1.59it/s]Extractor Predicting: 59it [00:38,  1.62it/s]Extractor Predicting: 60it [00:39,  1.60it/s]Extractor Predicting: 61it [00:39,  1.58it/s]Extractor Predicting: 62it [00:40,  1.58it/s]Extractor Predicting: 63it [00:41,  1.58it/s]Extractor Predicting: 64it [00:41,  1.54it/s]Extractor Predicting: 65it [00:42,  1.54it/s]Extractor Predicting: 66it [00:43,  1.54it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:44,  1.61it/s]Extractor Predicting: 69it [00:45,  1.58it/s]Extractor Predicting: 70it [00:45,  1.57it/s]Extractor Predicting: 71it [00:46,  1.57it/s]Extractor Predicting: 72it [00:46,  1.57it/s]Extractor Predicting: 73it [00:47,  1.53it/s]Extractor Predicting: 74it [00:48,  1.53it/s]Extractor Predicting: 75it [00:48,  1.53it/s]Extractor Predicting: 76it [00:49,  1.55it/s]Extractor Predicting: 77it [00:50,  1.53it/s]Extractor Predicting: 78it [00:50,  1.54it/s]Extractor Predicting: 79it [00:51,  1.53it/s]Extractor Predicting: 80it [00:52,  1.54it/s]Extractor Predicting: 81it [00:52,  1.53it/s]Extractor Predicting: 82it [00:53,  1.52it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:54,  1.53it/s]Extractor Predicting: 85it [00:55,  1.56it/s]Extractor Predicting: 86it [00:56,  1.54it/s]Extractor Predicting: 87it [00:56,  1.53it/s]Extractor Predicting: 88it [00:57,  1.52it/s]Extractor Predicting: 89it [00:58,  1.55it/s]Extractor Predicting: 90it [00:58,  1.55it/s]Extractor Predicting: 91it [00:59,  1.54it/s]Extractor Predicting: 92it [01:00,  1.51it/s]Extractor Predicting: 93it [01:00,  1.55it/s]Extractor Predicting: 94it [01:01,  1.52it/s]Extractor Predicting: 95it [01:02,  1.52it/s]Extractor Predicting: 96it [01:02,  1.53it/s]Extractor Predicting: 97it [01:03,  1.54it/s]Extractor Predicting: 98it [01:03,  1.55it/s]Extractor Predicting: 99it [01:04,  1.56it/s]Extractor Predicting: 100it [01:05,  1.58it/s]Extractor Predicting: 101it [01:05,  1.58it/s]Extractor Predicting: 102it [01:06,  1.58it/s]Extractor Predicting: 103it [01:07,  1.53it/s]Extractor Predicting: 104it [01:07,  1.52it/s]Extractor Predicting: 105it [01:08,  1.52it/s]Extractor Predicting: 106it [01:09,  1.52it/s]Extractor Predicting: 107it [01:09,  1.52it/s]Extractor Predicting: 108it [01:10,  1.53it/s]Extractor Predicting: 109it [01:11,  1.52it/s]Extractor Predicting: 110it [01:11,  1.39it/s]Extractor Predicting: 111it [01:12,  1.41it/s]Extractor Predicting: 112it [01:13,  1.42it/s]Extractor Predicting: 113it [01:14,  1.43it/s]Extractor Predicting: 114it [01:14,  1.44it/s]Extractor Predicting: 115it [01:15,  1.46it/s]Extractor Predicting: 116it [01:15,  1.51it/s]Extractor Predicting: 117it [01:16,  1.52it/s]Extractor Predicting: 118it [01:17,  1.53it/s]Extractor Predicting: 119it [01:17,  1.53it/s]Extractor Predicting: 120it [01:18,  1.52it/s]Extractor Predicting: 121it [01:19,  1.54it/s]Extractor Predicting: 122it [01:19,  1.55it/s]Extractor Predicting: 123it [01:20,  1.57it/s]Extractor Predicting: 124it [01:21,  1.58it/s]Extractor Predicting: 125it [01:21,  1.58it/s]Extractor Predicting: 126it [01:22,  1.57it/s]Extractor Predicting: 127it [01:23,  1.58it/s]Extractor Predicting: 128it [01:23,  1.56it/s]Extractor Predicting: 129it [01:24,  1.53it/s]Extractor Predicting: 130it [01:25,  1.52it/s]Extractor Predicting: 131it [01:25,  1.55it/s]Extractor Predicting: 132it [01:26,  1.55it/s]Extractor Predicting: 133it [01:26,  1.56it/s]Extractor Predicting: 134it [01:27,  1.57it/s]Extractor Predicting: 135it [01:28,  1.57it/s]Extractor Predicting: 136it [01:28,  1.56it/s]Extractor Predicting: 137it [01:29,  1.57it/s]Extractor Predicting: 138it [01:30,  1.54it/s]Extractor Predicting: 139it [01:30,  1.55it/s]Extractor Predicting: 140it [01:31,  1.60it/s]Extractor Predicting: 141it [01:31,  1.61it/s]Extractor Predicting: 142it [01:32,  1.58it/s]Extractor Predicting: 143it [01:33,  1.56it/s]Extractor Predicting: 144it [01:33,  1.56it/s]Extractor Predicting: 145it [01:34,  1.57it/s]Extractor Predicting: 146it [01:35,  1.59it/s]Extractor Predicting: 147it [01:35,  1.60it/s]Extractor Predicting: 148it [01:36,  1.55it/s]Extractor Predicting: 149it [01:37,  1.57it/s]Extractor Predicting: 150it [01:37,  1.57it/s]Extractor Predicting: 151it [01:38,  1.55it/s]Extractor Predicting: 152it [01:38,  1.56it/s]Extractor Predicting: 153it [01:39,  1.53it/s]Extractor Predicting: 154it [01:40,  1.55it/s]Extractor Predicting: 155it [01:40,  1.56it/s]Extractor Predicting: 156it [01:41,  1.59it/s]Extractor Predicting: 157it [01:42,  1.60it/s]Extractor Predicting: 158it [01:42,  1.65it/s]Extractor Predicting: 159it [01:43,  1.61it/s]Extractor Predicting: 160it [01:44,  1.58it/s]Extractor Predicting: 161it [01:44,  1.56it/s]Extractor Predicting: 162it [01:45,  1.54it/s]Extractor Predicting: 163it [01:45,  1.56it/s]Extractor Predicting: 164it [01:46,  1.57it/s]Extractor Predicting: 165it [01:47,  1.60it/s]Extractor Predicting: 166it [01:47,  1.62it/s]Extractor Predicting: 167it [01:48,  1.60it/s]Extractor Predicting: 168it [01:49,  1.56it/s]Extractor Predicting: 169it [01:49,  1.56it/s]Extractor Predicting: 170it [01:50,  1.56it/s]Extractor Predicting: 171it [01:51,  1.56it/s]Extractor Predicting: 172it [01:51,  1.57it/s]Extractor Predicting: 173it [01:52,  1.55it/s]Extractor Predicting: 174it [01:52,  1.56it/s]Extractor Predicting: 175it [01:53,  1.53it/s]Extractor Predicting: 176it [01:54,  1.50it/s]Extractor Predicting: 177it [01:54,  1.54it/s]Extractor Predicting: 178it [01:55,  1.54it/s]Extractor Predicting: 179it [01:56,  1.57it/s]Extractor Predicting: 180it [01:56,  1.59it/s]Extractor Predicting: 181it [01:57,  1.56it/s]Extractor Predicting: 182it [01:58,  1.56it/s]Extractor Predicting: 183it [01:58,  1.55it/s]Extractor Predicting: 184it [01:59,  1.54it/s]Extractor Predicting: 185it [02:00,  1.53it/s]Extractor Predicting: 186it [02:00,  1.52it/s]Extractor Predicting: 187it [02:01,  1.53it/s]Extractor Predicting: 188it [02:02,  1.51it/s]Extractor Predicting: 189it [02:02,  1.50it/s]Extractor Predicting: 190it [02:03,  1.52it/s]Extractor Predicting: 191it [02:04,  1.55it/s]Extractor Predicting: 192it [02:04,  1.59it/s]Extractor Predicting: 193it [02:05,  1.52it/s]Extractor Predicting: 194it [02:06,  1.51it/s]Extractor Predicting: 195it [02:06,  1.55it/s]Extractor Predicting: 196it [02:07,  1.55it/s]Extractor Predicting: 197it [02:07,  1.56it/s]Extractor Predicting: 198it [02:08,  1.54it/s]Extractor Predicting: 199it [02:09,  1.56it/s]Extractor Predicting: 200it [02:10,  1.45it/s]Extractor Predicting: 201it [02:10,  1.51it/s]Extractor Predicting: 202it [02:11,  1.53it/s]Extractor Predicting: 203it [02:11,  1.53it/s]Extractor Predicting: 204it [02:12,  1.53it/s]Extractor Predicting: 205it [02:13,  1.55it/s]Extractor Predicting: 206it [02:13,  1.54it/s]Extractor Predicting: 207it [02:14,  1.55it/s]Extractor Predicting: 208it [02:15,  1.52it/s]Extractor Predicting: 209it [02:15,  1.54it/s]Extractor Predicting: 210it [02:16,  1.54it/s]Extractor Predicting: 211it [02:17,  1.54it/s]Extractor Predicting: 212it [02:17,  1.55it/s]Extractor Predicting: 213it [02:18,  1.55it/s]Extractor Predicting: 214it [02:19,  1.52it/s]Extractor Predicting: 215it [02:19,  1.52it/s]Extractor Predicting: 216it [02:20,  1.55it/s]Extractor Predicting: 217it [02:20,  1.56it/s]Extractor Predicting: 218it [02:21,  1.54it/s]Extractor Predicting: 219it [02:22,  1.52it/s]Extractor Predicting: 220it [02:22,  1.54it/s]Extractor Predicting: 221it [02:23,  1.54it/s]Extractor Predicting: 222it [02:24,  1.53it/s]Extractor Predicting: 223it [02:24,  1.55it/s]Extractor Predicting: 224it [02:25,  1.55it/s]Extractor Predicting: 225it [02:26,  1.53it/s]Extractor Predicting: 226it [02:26,  1.52it/s]Extractor Predicting: 227it [02:27,  1.53it/s]Extractor Predicting: 228it [02:28,  1.54it/s]Extractor Predicting: 229it [02:28,  1.55it/s]Extractor Predicting: 230it [02:29,  1.56it/s]Extractor Predicting: 231it [02:30,  1.55it/s]Extractor Predicting: 232it [02:30,  1.55it/s]Extractor Predicting: 233it [02:31,  1.54it/s]Extractor Predicting: 234it [02:32,  1.55it/s]Extractor Predicting: 235it [02:32,  1.56it/s]Extractor Predicting: 236it [02:33,  1.57it/s]Extractor Predicting: 237it [02:33,  1.56it/s]Extractor Predicting: 238it [02:34,  1.55it/s]Extractor Predicting: 239it [02:35,  1.54it/s]Extractor Predicting: 240it [02:35,  1.57it/s]Extractor Predicting: 241it [02:36,  1.57it/s]Extractor Predicting: 242it [02:37,  1.60it/s]Extractor Predicting: 243it [02:37,  1.58it/s]Extractor Predicting: 244it [02:38,  1.58it/s]Extractor Predicting: 245it [02:39,  1.56it/s]Extractor Predicting: 246it [02:39,  1.56it/s]Extractor Predicting: 247it [02:40,  1.58it/s]Extractor Predicting: 248it [02:40,  1.58it/s]Extractor Predicting: 249it [02:41,  1.59it/s]Extractor Predicting: 250it [02:42,  1.61it/s]Extractor Predicting: 251it [02:42,  1.56it/s]Extractor Predicting: 252it [02:43,  1.55it/s]Extractor Predicting: 253it [02:44,  1.61it/s]Extractor Predicting: 254it [02:44,  1.59it/s]Extractor Predicting: 255it [02:45,  1.56it/s]Extractor Predicting: 256it [02:46,  1.55it/s]Extractor Predicting: 257it [02:46,  1.57it/s]Extractor Predicting: 258it [02:47,  1.59it/s]Extractor Predicting: 259it [02:47,  1.61it/s]Extractor Predicting: 260it [02:48,  1.61it/s]Extractor Predicting: 261it [02:49,  1.58it/s]Extractor Predicting: 262it [02:49,  1.60it/s]Extractor Predicting: 263it [02:50,  1.59it/s]Extractor Predicting: 264it [02:50,  1.61it/s]Extractor Predicting: 265it [02:51,  1.61it/s]Extractor Predicting: 266it [02:52,  1.65it/s]Extractor Predicting: 267it [02:52,  1.60it/s]Extractor Predicting: 268it [02:53,  1.60it/s]Extractor Predicting: 269it [02:54,  1.61it/s]Extractor Predicting: 270it [02:54,  1.61it/s]Extractor Predicting: 271it [02:55,  1.63it/s]Extractor Predicting: 272it [02:55,  1.67it/s]Extractor Predicting: 273it [02:56,  1.63it/s]Extractor Predicting: 274it [02:57,  1.63it/s]Extractor Predicting: 275it [02:57,  1.61it/s]Extractor Predicting: 276it [02:58,  1.61it/s]Extractor Predicting: 277it [02:59,  1.61it/s]Extractor Predicting: 278it [02:59,  1.60it/s]Extractor Predicting: 279it [03:00,  1.57it/s]Extractor Predicting: 280it [03:00,  1.59it/s]Extractor Predicting: 281it [03:01,  1.55it/s]Extractor Predicting: 281it [03:01,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:36:31,996 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:36:32,001 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:36:32,002 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:36:32,002 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:36:32,002 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:36:32,619 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:36:32,621 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:36:33,195 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:36:34,212 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:36:34,212 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:36:37,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:36:37,088 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:36:37,088 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:36:37,088 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:36:37,088 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:36:37,736 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:36:37,737 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:36:38,307 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:36:38,458 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:36:38,458 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4294534644408873,
  "recall": 0.2785110484947353,
  "score": 0.33789132781576103,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.36it/s]Extractor Predicting: 3it [00:02,  1.42it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:03,  1.88it/s]Extractor Predicting: 6it [00:03,  1.60it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.21686746987951808,
  "recall": 0.07003891050583658,
  "score": 0.10588235294117647,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
