Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_4', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:16<05:18, 16.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:34<05:08, 17.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:46<04:17, 15.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:02<04:06, 15.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:17<03:47, 15.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:33<03:37, 15.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:48<03:20, 15.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:05<03:10, 15.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:23<03:00, 16.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:37<02:37, 15.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:52<02:19, 15.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:07<02:03, 15.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:22<01:45, 15.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:37<01:30, 15.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:52<01:16, 15.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:09<01:02, 15.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:25<00:47, 15.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:39<00:30, 15.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:58<00:16, 16.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:14<00:00, 16.31s/it]Generating: 100%|██████████| 20/20 [05:14<00:00, 15.74s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7877604166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : military rank . Context : On 31 March 1859 , his unit , called " The Three Knights " , reached the Rhine , having previously crossed the Danube in May . Head Entity : Three Knights , Tail Entity : cavalry .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 137, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 201, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 247, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 336, 'raw': 480}
{'target': 600, 'success': 361, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 436, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 479, 'raw': 672}
{'target': 600, 'success': 502, 'raw': 704}
{'target': 600, 'success': 522, 'raw': 736}
{'target': 600, 'success': 545, 'raw': 768}
{'target': 600, 'success': 567, 'raw': 800}
{'target': 600, 'success': 589, 'raw': 832}
{'target': 600, 'success': 612, 'raw': 864}
{'prompt': 'Relation : military rank .', 'success_rate': 0.7083333333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('Tamerlane', 'military rank', '', 'During his time on active duty in the British navy , Tamerlane won a bronze medal in the 1st Squadron of the Royal Navy , in World War II .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8678977272727273, 'errors': {'', "('American Airlines', 'operating system', '', 'The company was founded in 1969 by American Airlines engineer Neil Armstrong , who had retired from service in 1967 after retiring from the USAF as early as 1958 .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : record label . Context : Later in 2008 , the band became a major draw for their debut solo album entitled " The Perfect Day " , released on June 30 , 2008 . Head Entity : The Perfect Day , Tail Entity : single label .\n']
['Relation : record label . Context : Later in 2008 , the band became a major draw for their debut solo album entitled " The Perfect Day " , released on June 30 , 2008 . Head Entity : The Perfect Day , Tail Entity : single label .\n', 'Relation : record label . Context : After the death of The Church and the release of the album , Sommers began publishing his own book on Sommers . Head Entity : The Church and the album , Tail Entity : Sommers .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 554, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 604, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.7864583333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : tributary .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : architect .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : constellation . Context : Later in the year ( 1143–1230 ) , at the conclusion of a series of conflicts , the Kingdom of France lost control in the battle between the Ottoman Empire and the Greek Republic . Head Entity : Kingdom of France , Tail Entity : constellation .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : constellation .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 443, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 514, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 554, 'raw': 768}
{'target': 600, 'success': 577, 'raw': 800}
{'target': 600, 'success': 596, 'raw': 832}
{'target': 600, 'success': 616, 'raw': 864}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7129629629629629, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Munich\', \'country of origin\', \'\', \'" All of Us " is the first song the band have released since they released " A Very Happy Birthday " at the 2009 International Music Festival in Munich .\')', '(\'The Encyclopedia of Mammals and Reptiles , Vol .\', \'country of origin\', \'\', \'Gipsy was awarded the Distinguished Award in 2009 by the International Committee for the Study of the Nature of Species for his work in " The Encyclopedia of Mammals and Reptiles , Vol. 1 " .\')'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 556, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 610, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.7942708333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : league . Context : On 31 occasions in all competitions in the 1974 FIFA World Cup , he represented Australia at the 1958 FIFA Confederations Cup , 1966 FIFA World Cup , 1960 FIFA World Cup , 1971 FIFA World Cup . Head Entity : 1958 FIFA World Cup , Tail Entity : Argentina .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : league .', 'success_rate': 0.76875, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of . Context : Later in the year ( October 1887 ) , a deal was announced that the band would perform in front of many of their members ; in spite of this , the band was unable to perform . Head Entity : band , Tail Entity : Sigmund Kaempfer .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 505, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 576, 'raw': 736}
{'target': 600, 'success': 601, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7825520833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('election of 1937', 'member of political party', '', 'It was defeated in the election of 1937 with 17 .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 499, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 604, 'raw': 768}
{'prompt': 'Relation : notable work .', 'success_rate': 0.7864583333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : operator . Context : Later in the year , the department built a substation to serve the town of Baskerville near Baskerville Station to serve the area outside the city , and then used Baskerville Airfield as a landing base . Head Entity : Baskerville Airfield , Tail Entity : Baskerville Group .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : position held . Context : On 31 March 2014 , the Armenian government appointed him a Vice President of the Armenian Community . Head Entity : Armenian President , Tail Entity : Vice President .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 56, 'raw': 96}
{'target': 600, 'success': 72, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 111, 'raw': 192}
{'target': 600, 'success': 133, 'raw': 224}
{'target': 600, 'success': 153, 'raw': 256}
{'target': 600, 'success': 173, 'raw': 288}
{'target': 600, 'success': 196, 'raw': 320}
{'target': 600, 'success': 212, 'raw': 352}
{'target': 600, 'success': 232, 'raw': 384}
{'target': 600, 'success': 249, 'raw': 416}
{'target': 600, 'success': 266, 'raw': 448}
{'target': 600, 'success': 286, 'raw': 480}
{'target': 600, 'success': 309, 'raw': 512}
{'target': 600, 'success': 330, 'raw': 544}
{'target': 600, 'success': 348, 'raw': 576}
{'target': 600, 'success': 369, 'raw': 608}
{'target': 600, 'success': 390, 'raw': 640}
{'target': 600, 'success': 411, 'raw': 672}
{'target': 600, 'success': 426, 'raw': 704}
{'target': 600, 'success': 451, 'raw': 736}
{'target': 600, 'success': 471, 'raw': 768}
{'target': 600, 'success': 491, 'raw': 800}
{'target': 600, 'success': 511, 'raw': 832}
{'target': 600, 'success': 532, 'raw': 864}
{'target': 600, 'success': 554, 'raw': 896}
{'target': 600, 'success': 577, 'raw': 928}
{'target': 600, 'success': 594, 'raw': 960}
{'target': 600, 'success': 616, 'raw': 992}
{'prompt': 'Relation : position held .', 'success_rate': 0.6209677419354839, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 16923
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17023, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.01s/it]Extractor Estimating: 2it [00:17,  7.67s/it]Extractor Estimating: 3it [00:18,  4.48s/it]Extractor Estimating: 4it [00:19,  2.96s/it]Extractor Estimating: 5it [00:20,  2.34s/it]Extractor Estimating: 6it [00:21,  1.76s/it]Extractor Estimating: 7it [00:21,  1.41s/it]Extractor Estimating: 8it [00:24,  1.89s/it]Extractor Estimating: 9it [00:25,  1.53s/it]Extractor Estimating: 10it [00:26,  1.25s/it]Extractor Estimating: 11it [00:27,  1.27s/it]Extractor Estimating: 12it [00:28,  1.09s/it]Extractor Estimating: 13it [00:28,  1.05it/s]Extractor Estimating: 14it [00:29,  1.17it/s]Extractor Estimating: 15it [00:29,  1.25it/s]Extractor Estimating: 16it [00:30,  1.33it/s]Extractor Estimating: 17it [00:31,  1.40it/s]Extractor Estimating: 18it [00:33,  1.08s/it]Extractor Estimating: 19it [00:33,  1.05it/s]Extractor Estimating: 20it [00:34,  1.14it/s]Extractor Estimating: 21it [00:35,  1.21it/s]Extractor Estimating: 22it [00:35,  1.27it/s]Extractor Estimating: 23it [00:36,  1.38it/s]Extractor Estimating: 24it [00:37,  1.44it/s]Extractor Estimating: 25it [00:37,  1.47it/s]Extractor Estimating: 26it [00:38,  1.53it/s]Extractor Estimating: 27it [00:38,  1.54it/s]Extractor Estimating: 28it [00:39,  1.59it/s]Extractor Estimating: 29it [00:40,  1.62it/s]Extractor Estimating: 30it [00:40,  1.64it/s]Extractor Estimating: 31it [00:41,  1.58it/s]Extractor Estimating: 32it [00:42,  1.59it/s]Extractor Estimating: 33it [00:42,  1.58it/s]Extractor Estimating: 34it [00:43,  1.59it/s]Extractor Estimating: 35it [00:43,  1.56it/s]Extractor Estimating: 36it [00:44,  1.58it/s]Extractor Estimating: 37it [00:45,  1.60it/s]Extractor Estimating: 38it [00:45,  1.64it/s]Extractor Estimating: 39it [00:46,  1.62it/s]Extractor Estimating: 40it [00:47,  1.60it/s]Extractor Estimating: 41it [00:47,  1.63it/s]Extractor Estimating: 42it [00:48,  1.61it/s]Extractor Estimating: 43it [00:48,  1.59it/s]Extractor Estimating: 44it [00:49,  1.61it/s]Extractor Estimating: 45it [00:50,  1.65it/s]Extractor Estimating: 46it [00:50,  1.63it/s]Extractor Estimating: 47it [00:51,  1.60it/s]Extractor Estimating: 48it [00:51,  1.67it/s]Extractor Estimating: 49it [00:52,  1.66it/s]Extractor Estimating: 50it [00:53,  1.65it/s]Extractor Estimating: 51it [00:53,  1.66it/s]Extractor Estimating: 52it [00:54,  1.71it/s]Extractor Estimating: 53it [00:54,  1.76it/s]Extractor Estimating: 54it [00:55,  1.68it/s]Extractor Estimating: 55it [00:56,  1.73it/s]Extractor Estimating: 56it [00:56,  1.63it/s]Extractor Estimating: 57it [00:57,  1.66it/s]Extractor Estimating: 58it [00:57,  1.63it/s]Extractor Estimating: 59it [00:58,  1.67it/s]Extractor Estimating: 60it [00:59,  1.69it/s]Extractor Estimating: 61it [00:59,  1.72it/s]Extractor Estimating: 62it [01:00,  1.75it/s]Extractor Estimating: 63it [01:00,  1.75it/s]Extractor Estimating: 64it [01:01,  1.70it/s]Extractor Estimating: 65it [01:02,  1.56it/s]Extractor Estimating: 66it [01:02,  1.60it/s]Extractor Estimating: 67it [01:03,  1.59it/s]Extractor Estimating: 68it [01:03,  1.65it/s]Extractor Estimating: 69it [01:04,  1.71it/s]Extractor Estimating: 70it [01:05,  1.73it/s]Extractor Estimating: 71it [01:05,  1.70it/s]Extractor Estimating: 72it [01:06,  1.71it/s]Extractor Estimating: 73it [01:06,  1.64it/s]Extractor Estimating: 74it [01:07,  1.63it/s]Extractor Estimating: 75it [01:08,  1.66it/s]Extractor Estimating: 76it [01:08,  1.64it/s]Extractor Estimating: 77it [01:09,  1.63it/s]Extractor Estimating: 78it [01:09,  1.63it/s]Extractor Estimating: 79it [01:10,  1.64it/s]Extractor Estimating: 80it [01:11,  1.67it/s]Extractor Estimating: 81it [01:11,  1.64it/s]Extractor Estimating: 82it [01:12,  1.58it/s]Extractor Estimating: 83it [01:13,  1.61it/s]Extractor Estimating: 84it [01:13,  1.58it/s]Extractor Estimating: 85it [01:14,  1.50it/s]Extractor Estimating: 86it [01:15,  1.49it/s]Extractor Estimating: 87it [01:15,  1.48it/s]Extractor Estimating: 88it [01:16,  1.47it/s]Extractor Estimating: 89it [01:17,  1.44it/s]Extractor Estimating: 90it [01:17,  1.48it/s]Extractor Estimating: 91it [01:18,  1.48it/s]Extractor Estimating: 92it [01:19,  1.45it/s]Extractor Estimating: 93it [01:19,  1.46it/s]Extractor Estimating: 94it [01:20,  1.51it/s]Extractor Estimating: 95it [01:21,  1.50it/s]Extractor Estimating: 96it [01:21,  1.51it/s]Extractor Estimating: 97it [01:22,  1.49it/s]Extractor Estimating: 98it [01:23,  1.47it/s]Extractor Estimating: 99it [01:23,  1.47it/s]Extractor Estimating: 100it [01:24,  1.52it/s]Extractor Estimating: 101it [01:25,  1.55it/s]Extractor Estimating: 102it [01:25,  1.60it/s]Extractor Estimating: 103it [01:26,  1.65it/s]Extractor Estimating: 104it [01:26,  1.64it/s]Extractor Estimating: 105it [01:27,  1.61it/s]Extractor Estimating: 106it [01:28,  1.63it/s]Extractor Estimating: 107it [01:28,  1.47it/s]Extractor Estimating: 108it [01:29,  1.55it/s]Extractor Estimating: 109it [01:30,  1.59it/s]Extractor Estimating: 110it [01:30,  1.63it/s]Extractor Estimating: 111it [01:31,  1.63it/s]Extractor Estimating: 112it [01:31,  1.63it/s]Extractor Estimating: 113it [01:32,  1.59it/s]Extractor Estimating: 114it [01:33,  1.58it/s]Extractor Estimating: 115it [01:33,  1.61it/s]Extractor Estimating: 116it [01:34,  1.60it/s]Extractor Estimating: 117it [01:35,  1.60it/s]Extractor Estimating: 118it [01:35,  1.64it/s]Extractor Estimating: 119it [01:36,  1.66it/s]Extractor Estimating: 120it [01:36,  1.69it/s]Extractor Estimating: 121it [01:37,  1.67it/s]Extractor Estimating: 122it [01:38,  1.64it/s]Extractor Estimating: 123it [01:38,  1.62it/s]Extractor Estimating: 124it [01:39,  1.61it/s]Extractor Estimating: 125it [01:39,  1.67it/s]Extractor Estimating: 126it [01:40,  1.66it/s]Extractor Estimating: 127it [01:41,  1.64it/s]Extractor Estimating: 128it [01:41,  1.69it/s]Extractor Estimating: 129it [01:42,  1.65it/s]Extractor Estimating: 130it [01:42,  1.68it/s]Extractor Estimating: 131it [01:43,  1.61it/s]Extractor Estimating: 132it [01:44,  1.64it/s]Extractor Estimating: 133it [01:44,  1.59it/s]Extractor Estimating: 134it [01:45,  1.59it/s]Extractor Estimating: 135it [01:46,  1.58it/s]Extractor Estimating: 136it [01:46,  1.59it/s]Extractor Estimating: 137it [01:47,  1.59it/s]Extractor Estimating: 138it [01:47,  1.60it/s]Extractor Estimating: 139it [01:48,  1.57it/s]Extractor Estimating: 140it [01:49,  1.56it/s]Extractor Estimating: 141it [01:49,  1.58it/s]Extractor Estimating: 142it [01:50,  1.52it/s]Extractor Estimating: 143it [01:51,  1.56it/s]Extractor Estimating: 144it [01:51,  1.57it/s]Extractor Estimating: 145it [01:52,  1.64it/s]Extractor Estimating: 146it [01:53,  1.57it/s]Extractor Estimating: 147it [01:53,  1.66it/s]Extractor Estimating: 148it [01:54,  1.67it/s]Extractor Estimating: 149it [01:54,  1.68it/s]Extractor Estimating: 150it [01:55,  1.66it/s]Extractor Estimating: 151it [01:56,  1.56it/s]Extractor Estimating: 152it [01:56,  1.59it/s]Extractor Estimating: 153it [01:57,  1.60it/s]Extractor Estimating: 154it [01:57,  1.64it/s]Extractor Estimating: 155it [01:58,  1.65it/s]Extractor Estimating: 156it [01:59,  1.65it/s]Extractor Estimating: 157it [01:59,  1.69it/s]Extractor Estimating: 158it [02:00,  1.67it/s]Extractor Estimating: 159it [02:00,  1.67it/s]Extractor Estimating: 160it [02:01,  1.63it/s]Extractor Estimating: 161it [02:02,  1.58it/s]Extractor Estimating: 162it [02:02,  1.57it/s]Extractor Estimating: 163it [02:03,  1.62it/s]Extractor Estimating: 164it [02:04,  1.62it/s]Extractor Estimating: 165it [02:04,  1.65it/s]Extractor Estimating: 166it [02:05,  1.65it/s]Extractor Estimating: 167it [02:05,  1.68it/s]Extractor Estimating: 168it [02:06,  1.66it/s]Extractor Estimating: 169it [02:07,  1.63it/s]Extractor Estimating: 170it [02:07,  1.58it/s]Extractor Estimating: 171it [02:08,  1.60it/s]Extractor Estimating: 172it [02:08,  1.62it/s]Extractor Estimating: 173it [02:09,  1.61it/s]Extractor Estimating: 174it [02:10,  1.58it/s]Extractor Estimating: 175it [02:10,  1.60it/s]Extractor Estimating: 176it [02:11,  1.62it/s]Extractor Estimating: 177it [02:12,  1.62it/s]Extractor Estimating: 178it [02:12,  1.67it/s]Extractor Estimating: 179it [02:13,  1.66it/s]Extractor Estimating: 180it [02:13,  1.65it/s]Extractor Estimating: 181it [02:14,  1.67it/s]Extractor Estimating: 182it [02:15,  1.64it/s]Extractor Estimating: 183it [02:15,  1.60it/s]Extractor Estimating: 184it [02:16,  1.60it/s]Extractor Estimating: 185it [02:16,  1.61it/s]Extractor Estimating: 186it [02:17,  1.60it/s]Extractor Estimating: 187it [02:18,  1.48it/s]Extractor Estimating: 188it [02:18,  1.57it/s]Extractor Estimating: 189it [02:19,  1.58it/s]Extractor Estimating: 190it [02:20,  1.63it/s]Extractor Estimating: 191it [02:20,  1.60it/s]Extractor Estimating: 192it [02:21,  1.59it/s]Extractor Estimating: 193it [02:22,  1.62it/s]Extractor Estimating: 194it [02:22,  1.62it/s]Extractor Estimating: 195it [02:23,  1.63it/s]Extractor Estimating: 196it [02:23,  1.60it/s]Extractor Estimating: 197it [02:24,  1.64it/s]Extractor Estimating: 198it [02:25,  1.64it/s]Extractor Estimating: 199it [02:25,  1.68it/s]Extractor Estimating: 200it [02:26,  1.68it/s]Extractor Estimating: 201it [02:26,  1.71it/s]Extractor Estimating: 202it [02:27,  1.68it/s]Extractor Estimating: 203it [02:27,  1.70it/s]Extractor Estimating: 204it [02:28,  1.68it/s]Extractor Estimating: 205it [02:29,  1.67it/s]Extractor Estimating: 206it [02:29,  1.57it/s]Extractor Estimating: 207it [02:30,  1.60it/s]Extractor Estimating: 208it [02:31,  1.59it/s]Extractor Estimating: 209it [02:31,  1.59it/s]Extractor Estimating: 210it [02:32,  1.62it/s]Extractor Estimating: 211it [02:32,  1.65it/s]Extractor Estimating: 212it [02:33,  1.66it/s]Extractor Estimating: 213it [02:34,  1.70it/s]Extractor Estimating: 214it [02:34,  1.74it/s]Extractor Estimating: 215it [02:35,  1.71it/s]Extractor Estimating: 216it [02:35,  1.69it/s]Extractor Estimating: 217it [02:36,  1.64it/s]Extractor Estimating: 218it [02:37,  1.66it/s]Extractor Estimating: 219it [02:37,  1.68it/s]Extractor Estimating: 220it [02:38,  1.68it/s]Extractor Estimating: 221it [02:38,  1.65it/s]Extractor Estimating: 222it [02:39,  1.62it/s]Extractor Estimating: 223it [02:40,  1.60it/s]Extractor Estimating: 224it [02:40,  1.56it/s]Extractor Estimating: 225it [02:41,  1.59it/s]Extractor Estimating: 226it [02:42,  1.61it/s]Extractor Estimating: 227it [02:42,  1.61it/s]Extractor Estimating: 228it [02:43,  1.61it/s]Extractor Estimating: 229it [02:43,  1.62it/s]Extractor Estimating: 230it [02:44,  1.66it/s]Extractor Estimating: 231it [02:45,  1.63it/s]Extractor Estimating: 232it [02:45,  1.62it/s]Extractor Estimating: 233it [02:46,  1.65it/s]Extractor Estimating: 234it [02:47,  1.60it/s]Extractor Estimating: 235it [02:47,  1.56it/s]Extractor Estimating: 236it [02:48,  1.60it/s]Extractor Estimating: 237it [02:48,  1.58it/s]Extractor Estimating: 238it [02:49,  1.61it/s]Extractor Estimating: 239it [02:50,  1.52it/s]Extractor Estimating: 240it [02:50,  1.50it/s]Extractor Estimating: 241it [02:51,  1.57it/s]Extractor Estimating: 242it [02:52,  1.57it/s]Extractor Estimating: 243it [02:52,  1.60it/s]Extractor Estimating: 244it [02:53,  1.63it/s]Extractor Estimating: 245it [02:53,  1.60it/s]Extractor Estimating: 246it [02:54,  1.65it/s]Extractor Estimating: 247it [02:55,  1.59it/s]Extractor Estimating: 248it [02:55,  1.56it/s]Extractor Estimating: 249it [02:56,  1.56it/s]Extractor Estimating: 250it [02:57,  1.54it/s]Extractor Estimating: 251it [02:57,  1.54it/s]Extractor Estimating: 252it [02:58,  1.53it/s]Extractor Estimating: 253it [02:59,  1.52it/s]Extractor Estimating: 254it [03:00,  1.42it/s]Extractor Estimating: 255it [03:00,  1.44it/s]Extractor Estimating: 256it [03:01,  1.46it/s]Extractor Estimating: 257it [03:02,  1.47it/s]Extractor Estimating: 258it [03:02,  1.44it/s]Extractor Estimating: 259it [03:03,  1.43it/s]Extractor Estimating: 260it [03:04,  1.45it/s]Extractor Estimating: 261it [03:04,  1.48it/s]Extractor Estimating: 262it [03:05,  1.56it/s]Extractor Estimating: 263it [03:05,  1.55it/s]Extractor Estimating: 264it [03:06,  1.54it/s]Extractor Estimating: 265it [03:07,  1.53it/s]Extractor Estimating: 266it [03:07,  1.55it/s]Extractor Estimating: 267it [03:08,  1.47it/s]Extractor Estimating: 268it [03:09,  1.35it/s]Extractor Estimating: 269it [03:10,  1.43it/s]Extractor Estimating: 270it [03:10,  1.48it/s]Extractor Estimating: 271it [03:11,  1.53it/s]Extractor Estimating: 272it [03:12,  1.52it/s]Extractor Estimating: 273it [03:12,  1.55it/s]Extractor Estimating: 274it [03:13,  1.55it/s]Extractor Estimating: 275it [03:13,  1.58it/s]Extractor Estimating: 276it [03:14,  1.64it/s]Extractor Estimating: 277it [03:15,  1.63it/s]Extractor Estimating: 278it [03:15,  1.66it/s]Extractor Estimating: 279it [03:16,  1.66it/s]Extractor Estimating: 280it [03:16,  1.68it/s]Extractor Estimating: 281it [03:17,  1.65it/s]Extractor Estimating: 282it [03:18,  1.69it/s]Extractor Estimating: 283it [03:18,  1.70it/s]Extractor Estimating: 284it [03:19,  1.68it/s]Extractor Estimating: 285it [03:19,  1.68it/s]Extractor Estimating: 286it [03:20,  1.64it/s]Extractor Estimating: 287it [03:21,  1.65it/s]Extractor Estimating: 288it [03:21,  1.66it/s]Extractor Estimating: 289it [03:22,  1.64it/s]Extractor Estimating: 290it [03:22,  1.65it/s]Extractor Estimating: 291it [03:23,  1.66it/s]Extractor Estimating: 292it [03:24,  1.62it/s]Extractor Estimating: 293it [03:24,  1.62it/s]Extractor Estimating: 294it [03:25,  1.61it/s]Extractor Estimating: 295it [03:25,  1.63it/s]Extractor Estimating: 296it [03:26,  1.58it/s]Extractor Estimating: 297it [03:27,  1.59it/s]Extractor Estimating: 298it [03:27,  1.60it/s]Extractor Estimating: 299it [03:28,  1.65it/s]Extractor Estimating: 300it [03:29,  1.65it/s]Extractor Estimating: 301it [03:29,  1.70it/s]Extractor Estimating: 302it [03:30,  1.74it/s]Extractor Estimating: 303it [03:30,  1.79it/s]Extractor Estimating: 304it [03:31,  1.80it/s]Extractor Estimating: 305it [03:31,  1.82it/s]Extractor Estimating: 306it [03:32,  1.83it/s]Extractor Estimating: 307it [03:32,  1.79it/s]Extractor Estimating: 308it [03:33,  1.79it/s]Extractor Estimating: 309it [03:34,  1.73it/s]Extractor Estimating: 310it [03:34,  1.75it/s]Extractor Estimating: 311it [03:35,  1.79it/s]Extractor Estimating: 312it [03:35,  1.82it/s]Extractor Estimating: 313it [03:36,  1.83it/s]Extractor Estimating: 314it [03:36,  1.80it/s]Extractor Estimating: 315it [03:37,  1.84it/s]Extractor Estimating: 316it [03:37,  1.85it/s]Extractor Estimating: 317it [03:38,  1.83it/s]Extractor Estimating: 318it [03:38,  1.80it/s]Extractor Estimating: 319it [03:39,  1.81it/s]Extractor Estimating: 320it [03:40,  1.84it/s]Extractor Estimating: 321it [03:40,  1.77it/s]Extractor Estimating: 322it [03:41,  1.82it/s]Extractor Estimating: 323it [03:41,  1.77it/s]Extractor Estimating: 324it [03:42,  1.74it/s]Extractor Estimating: 325it [03:42,  1.72it/s]Extractor Estimating: 326it [03:43,  1.62it/s]Extractor Estimating: 327it [03:44,  1.65it/s]Extractor Estimating: 328it [03:44,  1.64it/s]Extractor Estimating: 329it [03:45,  1.63it/s]Extractor Estimating: 330it [03:46,  1.65it/s]Extractor Estimating: 331it [03:46,  1.67it/s]Extractor Estimating: 332it [03:47,  1.58it/s]Extractor Estimating: 333it [03:47,  1.58it/s]Extractor Estimating: 334it [03:48,  1.57it/s]Extractor Estimating: 335it [03:49,  1.49it/s]Extractor Estimating: 336it [03:50,  1.52it/s]Extractor Estimating: 337it [03:50,  1.60it/s]Extractor Estimating: 338it [03:51,  1.56it/s]Extractor Estimating: 339it [03:51,  1.57it/s]Extractor Estimating: 340it [03:52,  1.57it/s]Extractor Estimating: 341it [03:53,  1.57it/s]Extractor Estimating: 342it [03:53,  1.56it/s]Extractor Estimating: 343it [03:54,  1.57it/s]Extractor Estimating: 344it [03:55,  1.60it/s]Extractor Estimating: 345it [03:55,  1.62it/s]Extractor Estimating: 346it [03:56,  1.62it/s]Extractor Estimating: 347it [03:56,  1.56it/s]Extractor Estimating: 348it [03:57,  1.55it/s]Extractor Estimating: 349it [03:58,  1.59it/s]Extractor Estimating: 350it [03:58,  1.60it/s]Extractor Estimating: 351it [03:59,  1.58it/s]Extractor Estimating: 352it [04:00,  1.64it/s]Extractor Estimating: 353it [04:00,  1.67it/s]Extractor Estimating: 354it [04:01,  1.75it/s]Extractor Estimating: 355it [04:01,  1.74it/s]Extractor Estimating: 356it [04:02,  1.67it/s]Extractor Estimating: 357it [04:02,  1.75it/s]Extractor Estimating: 358it [04:03,  1.72it/s]Extractor Estimating: 359it [04:04,  1.55it/s]Extractor Estimating: 360it [04:04,  1.57it/s]Extractor Estimating: 361it [04:05,  1.56it/s]Extractor Estimating: 362it [04:06,  1.59it/s]Extractor Estimating: 363it [04:06,  1.59it/s]Extractor Estimating: 364it [04:07,  1.64it/s]Extractor Estimating: 365it [04:07,  1.68it/s]Extractor Estimating: 366it [04:08,  1.70it/s]Extractor Estimating: 367it [04:09,  1.62it/s]Extractor Estimating: 368it [04:09,  1.63it/s]Extractor Estimating: 369it [04:10,  1.61it/s]Extractor Estimating: 370it [04:10,  1.64it/s]Extractor Estimating: 371it [04:11,  1.59it/s]Extractor Estimating: 372it [04:12,  1.56it/s]Extractor Estimating: 373it [04:12,  1.57it/s]Extractor Estimating: 374it [04:13,  1.59it/s]Extractor Estimating: 375it [04:14,  1.56it/s]Extractor Estimating: 376it [04:14,  1.53it/s]Extractor Estimating: 377it [04:15,  1.49it/s]Extractor Estimating: 378it [04:16,  1.51it/s]Extractor Estimating: 379it [04:16,  1.53it/s]Extractor Estimating: 380it [04:17,  1.50it/s]Extractor Estimating: 381it [04:18,  1.50it/s]Extractor Estimating: 382it [04:18,  1.50it/s]Extractor Estimating: 383it [04:19,  1.52it/s]Extractor Estimating: 384it [04:20,  1.52it/s]Extractor Estimating: 385it [04:20,  1.53it/s]Extractor Estimating: 386it [04:21,  1.51it/s]Extractor Estimating: 387it [04:22,  1.47it/s]Extractor Estimating: 388it [04:22,  1.48it/s]Extractor Estimating: 389it [04:23,  1.40it/s]Extractor Estimating: 390it [04:24,  1.40it/s]Extractor Estimating: 391it [04:25,  1.48it/s]Extractor Estimating: 392it [04:25,  1.46it/s]Extractor Estimating: 393it [04:26,  1.42it/s]Extractor Estimating: 394it [04:27,  1.39it/s]Extractor Estimating: 395it [04:28,  1.33it/s]Extractor Estimating: 396it [04:28,  1.44it/s]Extractor Estimating: 397it [04:29,  1.48it/s]Extractor Estimating: 398it [04:29,  1.46it/s]Extractor Estimating: 399it [04:30,  1.44it/s]Extractor Estimating: 400it [04:31,  1.48it/s]Extractor Estimating: 401it [04:31,  1.53it/s]Extractor Estimating: 402it [04:32,  1.52it/s]Extractor Estimating: 403it [04:33,  1.48it/s]Extractor Estimating: 404it [04:33,  1.50it/s]Extractor Estimating: 405it [04:34,  1.46it/s]Extractor Estimating: 406it [04:35,  1.52it/s]Extractor Estimating: 407it [04:35,  1.56it/s]Extractor Estimating: 408it [04:36,  1.58it/s]Extractor Estimating: 409it [04:37,  1.58it/s]Extractor Estimating: 410it [04:37,  1.62it/s]Extractor Estimating: 411it [04:38,  1.58it/s]Extractor Estimating: 412it [04:38,  1.59it/s]Extractor Estimating: 413it [04:39,  1.59it/s]Extractor Estimating: 414it [04:40,  1.57it/s]Extractor Estimating: 415it [04:40,  1.54it/s]Extractor Estimating: 416it [04:41,  1.54it/s]Extractor Estimating: 417it [04:42,  1.59it/s]Extractor Estimating: 418it [04:42,  1.56it/s]Extractor Estimating: 419it [04:43,  1.61it/s]Extractor Estimating: 420it [04:43,  1.65it/s]Extractor Estimating: 421it [04:44,  1.67it/s]Extractor Estimating: 422it [04:45,  1.66it/s]Extractor Estimating: 423it [04:45,  1.60it/s]Extractor Estimating: 424it [04:46,  1.59it/s]Extractor Estimating: 425it [04:47,  1.58it/s]Extractor Estimating: 426it [04:47,  1.48it/s]Extractor Estimating: 427it [04:48,  1.54it/s]Extractor Estimating: 428it [04:49,  1.57it/s]Extractor Estimating: 429it [04:49,  1.55it/s]Extractor Estimating: 430it [04:50,  1.56it/s]Extractor Estimating: 431it [04:51,  1.58it/s]Extractor Estimating: 432it [04:51,  1.58it/s]Extractor Estimating: 433it [04:52,  1.60it/s]Extractor Estimating: 434it [04:52,  1.60it/s]Extractor Estimating: 435it [04:53,  1.45it/s]Extractor Estimating: 436it [04:54,  1.49it/s]Extractor Estimating: 437it [04:54,  1.54it/s]Extractor Estimating: 438it [04:55,  1.52it/s]Extractor Estimating: 439it [04:56,  1.54it/s]Extractor Estimating: 440it [04:56,  1.59it/s]Extractor Estimating: 441it [04:57,  1.61it/s]Extractor Estimating: 442it [04:58,  1.57it/s]Extractor Estimating: 443it [04:58,  1.53it/s]Extractor Estimating: 444it [04:59,  1.49it/s]Extractor Estimating: 445it [05:00,  1.54it/s]Extractor Estimating: 446it [05:00,  1.54it/s]Extractor Estimating: 447it [05:01,  1.61it/s]Extractor Estimating: 448it [05:01,  1.59it/s]Extractor Estimating: 449it [05:02,  1.56it/s]Extractor Estimating: 450it [05:03,  1.58it/s]Extractor Estimating: 451it [05:03,  1.60it/s]Extractor Estimating: 452it [05:04,  1.57it/s]Extractor Estimating: 453it [05:05,  1.62it/s]Extractor Estimating: 454it [05:05,  1.65it/s]Extractor Estimating: 455it [05:06,  1.68it/s]Extractor Estimating: 456it [05:06,  1.68it/s]Extractor Estimating: 457it [05:07,  1.69it/s]Extractor Estimating: 458it [05:08,  1.63it/s]Extractor Estimating: 459it [05:08,  1.62it/s]Extractor Estimating: 460it [05:09,  1.62it/s]Extractor Estimating: 461it [05:09,  1.60it/s]Extractor Estimating: 462it [05:10,  1.62it/s]Extractor Estimating: 463it [05:11,  1.56it/s]Extractor Estimating: 464it [05:11,  1.58it/s]Extractor Estimating: 465it [05:12,  1.58it/s]Extractor Estimating: 466it [05:13,  1.59it/s]Extractor Estimating: 467it [05:13,  1.59it/s]Extractor Estimating: 468it [05:14,  1.63it/s]Extractor Estimating: 469it [05:14,  1.64it/s]Extractor Estimating: 470it [05:15,  1.69it/s]Extractor Estimating: 471it [05:16,  1.68it/s]Extractor Estimating: 472it [05:16,  1.74it/s]Extractor Estimating: 473it [05:17,  1.73it/s]Extractor Estimating: 474it [05:17,  1.72it/s]Extractor Estimating: 475it [05:18,  1.68it/s]Extractor Estimating: 476it [05:18,  1.71it/s]Extractor Estimating: 477it [05:19,  1.68it/s]Extractor Estimating: 478it [05:20,  1.65it/s]Extractor Estimating: 479it [05:20,  1.63it/s]Extractor Estimating: 480it [05:21,  1.63it/s]Extractor Estimating: 481it [05:22,  1.60it/s]Extractor Estimating: 482it [05:22,  1.57it/s]Extractor Estimating: 483it [05:23,  1.57it/s]Extractor Estimating: 484it [05:23,  1.63it/s]Extractor Estimating: 485it [05:24,  1.60it/s]Extractor Estimating: 486it [05:25,  1.60it/s]Extractor Estimating: 487it [05:25,  1.58it/s]Extractor Estimating: 488it [05:26,  1.61it/s]Extractor Estimating: 489it [05:27,  1.64it/s]Extractor Estimating: 490it [05:27,  1.62it/s]Extractor Estimating: 491it [05:28,  1.57it/s]Extractor Estimating: 492it [05:29,  1.56it/s]Extractor Estimating: 493it [05:29,  1.56it/s]Extractor Estimating: 494it [05:30,  1.59it/s]Extractor Estimating: 495it [05:31,  1.50it/s]Extractor Estimating: 496it [05:31,  1.54it/s]Extractor Estimating: 497it [05:32,  1.57it/s]Extractor Estimating: 498it [05:32,  1.65it/s]Extractor Estimating: 499it [05:33,  1.64it/s]Extractor Estimating: 500it [05:33,  1.67it/s]Extractor Estimating: 500it [05:33,  1.50it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 10036 mean pseudo reward: 0.9670724210574336
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 30002
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 30102, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=30102, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.228, loss:3569.5229
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.001, loss:2395.6250
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.975, loss:1917.9978
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.961, loss:1696.6733
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 81, avg_time 0.971, loss:1546.9385
>> valid entity prec:0.6024, rec:0.6239, f1:0.6129
>> valid relation prec:0.2923, rec:0.0758, f1:0.1203
>> valid relation with NER prec:0.2923, rec:0.0758, f1:0.1203
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 181, avg_time 2.199, loss:1479.6961
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 281, avg_time 0.968, loss:1373.9578
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 381, avg_time 0.968, loss:1383.0554
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 62, avg_time 0.972, loss:1224.7914
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 162, avg_time 0.975, loss:1148.0380
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6155, rec:0.5822, f1:0.5984
>> valid relation prec:0.2181, rec:0.0679, f1:0.1036
>> valid relation with NER prec:0.2181, rec:0.0679, f1:0.1036
g_step 1100, step 262, avg_time 2.180, loss:1124.1321
g_step 1200, step 362, avg_time 0.973, loss:1129.8945
g_step 1300, step 43, avg_time 0.954, loss:1059.0605
g_step 1400, step 143, avg_time 0.972, loss:997.6636
g_step 1500, step 243, avg_time 0.973, loss:1029.6259
>> valid entity prec:0.5753, rec:0.5667, f1:0.5710
>> valid relation prec:0.2221, rec:0.0642, f1:0.0995
>> valid relation with NER prec:0.2221, rec:0.0642, f1:0.0995
g_step 1600, step 343, avg_time 2.196, loss:994.8160
g_step 1700, step 24, avg_time 0.968, loss:995.3056
g_step 1800, step 124, avg_time 0.966, loss:967.7789
g_step 1900, step 224, avg_time 0.970, loss:922.2601
g_step 2000, step 324, avg_time 0.978, loss:891.0542
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6112, rec:0.5399, f1:0.5733
>> valid relation prec:0.2036, rec:0.0746, f1:0.1092
>> valid relation with NER prec:0.2036, rec:0.0746, f1:0.1092
g_step 2100, step 5, avg_time 2.188, loss:917.4767
g_step 2200, step 105, avg_time 0.976, loss:849.8710
g_step 2300, step 205, avg_time 0.969, loss:881.1018
g_step 2400, step 305, avg_time 0.980, loss:877.2307
g_step 2500, step 405, avg_time 0.972, loss:871.6682
>> valid entity prec:0.5658, rec:0.5827, f1:0.5741
>> valid relation prec:0.2063, rec:0.0737, f1:0.1086
>> valid relation with NER prec:0.2063, rec:0.0737, f1:0.1086
g_step 2600, step 86, avg_time 2.192, loss:826.6659
g_step 2700, step 186, avg_time 0.975, loss:806.1649
g_step 2800, step 286, avg_time 0.977, loss:810.0560
g_step 2900, step 386, avg_time 0.978, loss:841.6693
g_step 3000, step 67, avg_time 0.969, loss:775.0999
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6190, rec:0.5515, f1:0.5833
>> valid relation prec:0.2144, rec:0.0827, f1:0.1194
>> valid relation with NER prec:0.2144, rec:0.0827, f1:0.1194
g_step 3100, step 167, avg_time 2.196, loss:761.1180
g_step 3200, step 267, avg_time 0.970, loss:801.7525
g_step 3300, step 367, avg_time 0.972, loss:801.8038
g_step 3400, step 48, avg_time 0.972, loss:767.0091
g_step 3500, step 148, avg_time 0.974, loss:751.5255
>> valid entity prec:0.6061, rec:0.6201, f1:0.6130
>> valid relation prec:0.2082, rec:0.0987, f1:0.1339
>> valid relation with NER prec:0.2082, rec:0.0987, f1:0.1339
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 248, avg_time 2.209, loss:764.9517
g_step 3700, step 348, avg_time 0.968, loss:753.5380
g_step 3800, step 29, avg_time 0.963, loss:745.2985
g_step 3900, step 129, avg_time 0.972, loss:717.1187
g_step 4000, step 229, avg_time 0.984, loss:697.2402
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6229, rec:0.5350, f1:0.5756
>> valid relation prec:0.1688, rec:0.0604, f1:0.0889
>> valid relation with NER prec:0.1688, rec:0.0604, f1:0.0889
g_step 4100, step 329, avg_time 2.198, loss:745.4655
g_step 4200, step 10, avg_time 0.980, loss:736.4481
g_step 4300, step 110, avg_time 0.980, loss:700.5407
g_step 4400, step 210, avg_time 0.970, loss:689.6057
g_step 4500, step 310, avg_time 0.985, loss:689.8985
>> valid entity prec:0.6070, rec:0.6198, f1:0.6133
>> valid relation prec:0.1882, rec:0.0961, f1:0.1272
>> valid relation with NER prec:0.1882, rec:0.0961, f1:0.1272
new max entity f1 on valid!
g_step 4600, step 410, avg_time 2.217, loss:700.0613
g_step 4700, step 91, avg_time 0.992, loss:663.9192
g_step 4800, step 191, avg_time 0.981, loss:666.7970
g_step 4900, step 291, avg_time 0.975, loss:683.5018
g_step 5000, step 391, avg_time 0.968, loss:661.1186
learning rate was adjusted to 0.0008
>> valid entity prec:0.5849, rec:0.6161, f1:0.6001
>> valid relation prec:0.1843, rec:0.0772, f1:0.1088
>> valid relation with NER prec:0.1843, rec:0.0772, f1:0.1088
g_step 5100, step 72, avg_time 2.207, loss:644.9801
g_step 5200, step 172, avg_time 0.976, loss:644.0120
g_step 5300, step 272, avg_time 0.973, loss:655.8044
g_step 5400, step 372, avg_time 0.982, loss:631.5834
g_step 5500, step 53, avg_time 0.971, loss:607.0715
>> valid entity prec:0.5956, rec:0.5629, f1:0.5788
>> valid relation prec:0.1754, rec:0.0810, f1:0.1108
>> valid relation with NER prec:0.1754, rec:0.0810, f1:0.1108
g_step 5600, step 153, avg_time 2.204, loss:620.4125
g_step 5700, step 253, avg_time 0.972, loss:623.8723
g_step 5800, step 353, avg_time 0.978, loss:617.9464
g_step 5900, step 34, avg_time 0.976, loss:603.7407
g_step 6000, step 134, avg_time 0.977, loss:602.5076
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5855, rec:0.5245, f1:0.5533
>> valid relation prec:0.1738, rec:0.0697, f1:0.0995
>> valid relation with NER prec:0.1738, rec:0.0697, f1:0.0995
g_step 6100, step 234, avg_time 2.205, loss:597.5776
g_step 6200, step 334, avg_time 0.981, loss:595.8111
g_step 6300, step 15, avg_time 0.960, loss:596.7172
g_step 6400, step 115, avg_time 0.976, loss:566.9470
g_step 6500, step 215, avg_time 0.982, loss:573.5598
>> valid entity prec:0.6090, rec:0.5098, f1:0.5550
>> valid relation prec:0.2195, rec:0.0909, f1:0.1285
>> valid relation with NER prec:0.2195, rec:0.0909, f1:0.1285
g_step 6600, step 315, avg_time 2.202, loss:582.1213
g_step 6700, step 415, avg_time 0.981, loss:590.9548
g_step 6800, step 96, avg_time 0.970, loss:536.6419
g_step 6900, step 196, avg_time 0.982, loss:568.4065
g_step 7000, step 296, avg_time 0.978, loss:582.0695
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5782, rec:0.5482, f1:0.5628
>> valid relation prec:0.2229, rec:0.0900, f1:0.1282
>> valid relation with NER prec:0.2229, rec:0.0900, f1:0.1282
g_step 7100, step 396, avg_time 2.203, loss:557.4251
g_step 7200, step 77, avg_time 0.956, loss:522.1186
g_step 7300, step 177, avg_time 0.969, loss:542.6640
g_step 7400, step 277, avg_time 0.977, loss:531.5362
g_step 7500, step 377, avg_time 0.999, loss:554.5996
>> valid entity prec:0.5814, rec:0.5665, f1:0.5739
>> valid relation prec:0.1713, rec:0.0763, f1:0.1056
>> valid relation with NER prec:0.1713, rec:0.0763, f1:0.1056
g_step 7600, step 58, avg_time 2.214, loss:519.3194
g_step 7700, step 158, avg_time 0.970, loss:526.4993
g_step 7800, step 258, avg_time 0.980, loss:537.6731
g_step 7900, step 358, avg_time 0.976, loss:527.2740
g_step 8000, step 39, avg_time 0.975, loss:501.7037
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5758, rec:0.5602, f1:0.5679
>> valid relation prec:0.1576, rec:0.0708, f1:0.0977
>> valid relation with NER prec:0.1576, rec:0.0708, f1:0.0977
g_step 8100, step 139, avg_time 2.212, loss:474.3567
g_step 8200, step 239, avg_time 0.977, loss:516.8937
g_step 8300, step 339, avg_time 0.993, loss:526.4330
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:48:51 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:48:51 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-48-51_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:48:52 - WARNING - datasets.builder -   Using custom data configuration default-6710335ea94527dc
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6710335ea94527dc/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:48:52,569 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:48:52,570 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:48:52,571 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:48:52,572 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:48:52,592 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:48:52,598 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:48:52,598 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:48:52,598 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:48:52,598 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:48:52,598 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:48:52,598 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:48:52,738 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:48:55,780 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:48:55,785 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6710335ea94527dc/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 13:48:55 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x150edcea6050> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.86ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.72ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.03ba/s] 36%|███▋      | 4/11 [00:01<00:02,  3.44ba/s] 45%|████▌     | 5/11 [00:01<00:01,  3.77ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  3.98ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.13ba/s] 73%|███████▎  | 8/11 [00:02<00:00,  4.26ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.33ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.40ba/s]100%|██████████| 11/11 [00:02<00:00,  4.40ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.26ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.43ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.46ba/s]100%|██████████| 4/4 [00:00<00:00,  5.68ba/s]100%|██████████| 4/4 [00:00<00:00,  5.14ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  9.20ba/s] 27%|██▋       | 3/11 [00:00<00:00, 10.46ba/s] 45%|████▌     | 5/11 [00:00<00:00, 10.34ba/s] 64%|██████▎   | 7/11 [00:00<00:00, 10.53ba/s] 82%|████████▏ | 9/11 [00:00<00:00, 10.65ba/s]100%|██████████| 11/11 [00:00<00:00, 12.69ba/s]100%|██████████| 11/11 [00:00<00:00, 11.49ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.22ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.52ba/s]100%|██████████| 4/4 [00:00<00:00, 12.01ba/s]
[INFO|trainer.py:414] 2023-08-28 13:49:00,777 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:49:00,790 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:49:00,791 >>   Num examples = 10090
[INFO|trainer.py:1149] 2023-08-28 13:49:00,791 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:49:00,791 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:49:00,791 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:49:00,791 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:49:00,791 >>   Total optimization steps = 790
  0%|          | 0/790 [00:00<?, ?it/s]  0%|          | 1/790 [00:00<03:56,  3.33it/s]  0%|          | 2/790 [00:00<03:52,  3.39it/s]  0%|          | 3/790 [00:00<03:51,  3.40it/s]  1%|          | 4/790 [00:01<03:51,  3.40it/s]  1%|          | 5/790 [00:01<03:51,  3.39it/s]  1%|          | 6/790 [00:01<03:50,  3.40it/s]  1%|          | 7/790 [00:02<03:50,  3.40it/s]  1%|          | 8/790 [00:02<03:49,  3.40it/s]  1%|          | 9/790 [00:02<03:50,  3.39it/s]  1%|▏         | 10/790 [00:02<03:50,  3.39it/s]  1%|▏         | 11/790 [00:03<03:50,  3.38it/s]  2%|▏         | 12/790 [00:03<03:49,  3.38it/s]  2%|▏         | 13/790 [00:03<03:48,  3.40it/s]  2%|▏         | 14/790 [00:04<03:48,  3.40it/s]  2%|▏         | 15/790 [00:04<03:47,  3.40it/s]  2%|▏         | 16/790 [00:04<03:47,  3.41it/s]  2%|▏         | 17/790 [00:05<03:47,  3.40it/s]  2%|▏         | 18/790 [00:05<03:46,  3.41it/s]  2%|▏         | 19/790 [00:05<03:46,  3.41it/s]  3%|▎         | 20/790 [00:05<03:46,  3.40it/s]  3%|▎         | 21/790 [00:06<03:45,  3.40it/s]  3%|▎         | 22/790 [00:06<03:45,  3.41it/s]  3%|▎         | 23/790 [00:06<03:44,  3.41it/s]  3%|▎         | 24/790 [00:07<03:44,  3.41it/s]  3%|▎         | 25/790 [00:07<03:44,  3.41it/s]  3%|▎         | 26/790 [00:07<03:43,  3.41it/s]  3%|▎         | 27/790 [00:07<03:43,  3.41it/s]  4%|▎         | 28/790 [00:08<03:43,  3.41it/s]  4%|▎         | 29/790 [00:08<03:42,  3.41it/s]  4%|▍         | 30/790 [00:08<03:42,  3.41it/s]  4%|▍         | 31/790 [00:09<03:42,  3.41it/s]  4%|▍         | 32/790 [00:09<03:42,  3.41it/s]  4%|▍         | 33/790 [00:09<03:41,  3.41it/s]  4%|▍         | 34/790 [00:09<03:41,  3.42it/s]  4%|▍         | 35/790 [00:10<03:41,  3.41it/s]  5%|▍         | 36/790 [00:10<03:41,  3.41it/s]  5%|▍         | 37/790 [00:10<03:40,  3.41it/s]  5%|▍         | 38/790 [00:11<03:40,  3.41it/s]  5%|▍         | 39/790 [00:11<03:40,  3.41it/s]  5%|▌         | 40/790 [00:11<03:39,  3.41it/s]  5%|▌         | 41/790 [00:12<03:39,  3.41it/s]  5%|▌         | 42/790 [00:12<03:39,  3.41it/s]  5%|▌         | 43/790 [00:12<03:38,  3.41it/s]  6%|▌         | 44/790 [00:12<03:38,  3.41it/s]  6%|▌         | 45/790 [00:13<03:38,  3.41it/s]  6%|▌         | 46/790 [00:13<03:38,  3.40it/s]  6%|▌         | 47/790 [00:13<03:38,  3.40it/s]  6%|▌         | 48/790 [00:14<03:38,  3.40it/s]  6%|▌         | 49/790 [00:14<03:37,  3.41it/s]  6%|▋         | 50/790 [00:14<03:37,  3.41it/s]  6%|▋         | 51/790 [00:14<03:36,  3.41it/s]  7%|▋         | 52/790 [00:15<03:36,  3.41it/s]  7%|▋         | 53/790 [00:15<03:36,  3.41it/s]  7%|▋         | 54/790 [00:15<03:35,  3.41it/s]  7%|▋         | 55/790 [00:16<03:35,  3.41it/s]  7%|▋         | 56/790 [00:16<03:35,  3.41it/s]  7%|▋         | 57/790 [00:16<03:35,  3.41it/s]  7%|▋         | 58/790 [00:17<03:34,  3.41it/s]  7%|▋         | 59/790 [00:17<03:34,  3.41it/s]  8%|▊         | 60/790 [00:17<03:34,  3.41it/s]  8%|▊         | 61/790 [00:17<03:34,  3.41it/s]  8%|▊         | 62/790 [00:18<03:33,  3.41it/s]  8%|▊         | 63/790 [00:18<03:33,  3.41it/s]  8%|▊         | 64/790 [00:18<03:32,  3.41it/s]  8%|▊         | 65/790 [00:19<03:32,  3.41it/s]  8%|▊         | 66/790 [00:19<03:32,  3.40it/s]  8%|▊         | 67/790 [00:19<03:32,  3.40it/s]  9%|▊         | 68/790 [00:19<03:31,  3.41it/s]  9%|▊         | 69/790 [00:20<03:31,  3.41it/s]  9%|▉         | 70/790 [00:20<03:31,  3.41it/s]  9%|▉         | 71/790 [00:20<03:31,  3.41it/s]  9%|▉         | 72/790 [00:21<03:30,  3.41it/s]  9%|▉         | 73/790 [00:21<03:30,  3.41it/s]  9%|▉         | 74/790 [00:21<03:29,  3.41it/s]  9%|▉         | 75/790 [00:22<03:29,  3.41it/s] 10%|▉         | 76/790 [00:22<03:29,  3.41it/s] 10%|▉         | 77/790 [00:22<03:29,  3.41it/s] 10%|▉         | 78/790 [00:22<03:28,  3.41it/s] 10%|█         | 79/790 [00:23<03:28,  3.41it/s] 10%|█         | 80/790 [00:23<03:28,  3.41it/s] 10%|█         | 81/790 [00:23<03:28,  3.41it/s] 10%|█         | 82/790 [00:24<03:27,  3.41it/s] 11%|█         | 83/790 [00:24<03:27,  3.41it/s] 11%|█         | 84/790 [00:24<03:27,  3.41it/s] 11%|█         | 85/790 [00:24<03:26,  3.41it/s] 11%|█         | 86/790 [00:25<03:26,  3.41it/s] 11%|█         | 87/790 [00:25<03:26,  3.41it/s] 11%|█         | 88/790 [00:25<03:26,  3.41it/s] 11%|█▏        | 89/790 [00:26<03:25,  3.41it/s] 11%|█▏        | 90/790 [00:26<03:25,  3.41it/s] 12%|█▏        | 91/790 [00:26<03:25,  3.40it/s] 12%|█▏        | 92/790 [00:27<03:24,  3.41it/s] 12%|█▏        | 93/790 [00:27<03:24,  3.41it/s] 12%|█▏        | 94/790 [00:27<03:24,  3.40it/s] 12%|█▏        | 95/790 [00:27<03:24,  3.40it/s] 12%|█▏        | 96/790 [00:28<03:23,  3.40it/s] 12%|█▏        | 97/790 [00:28<03:23,  3.40it/s] 12%|█▏        | 98/790 [00:28<03:23,  3.40it/s] 13%|█▎        | 99/790 [00:29<03:22,  3.41it/s] 13%|█▎        | 100/790 [00:29<03:22,  3.41it/s] 13%|█▎        | 101/790 [00:29<03:22,  3.41it/s] 13%|█▎        | 102/790 [00:29<03:21,  3.41it/s] 13%|█▎        | 103/790 [00:30<03:21,  3.41it/s] 13%|█▎        | 104/790 [00:30<03:21,  3.41it/s] 13%|█▎        | 105/790 [00:30<03:21,  3.41it/s] 13%|█▎        | 106/790 [00:31<03:20,  3.41it/s] 14%|█▎        | 107/790 [00:31<03:20,  3.41it/s] 14%|█▎        | 108/790 [00:31<03:20,  3.41it/s] 14%|█▍        | 109/790 [00:31<03:19,  3.41it/s] 14%|█▍        | 110/790 [00:32<03:19,  3.40it/s] 14%|█▍        | 111/790 [00:32<03:19,  3.40it/s] 14%|█▍        | 112/790 [00:32<03:19,  3.41it/s] 14%|█▍        | 113/790 [00:33<03:18,  3.40it/s] 14%|█▍        | 114/790 [00:33<03:18,  3.40it/s] 15%|█▍        | 115/790 [00:33<03:18,  3.40it/s] 15%|█▍        | 116/790 [00:34<03:18,  3.40it/s] 15%|█▍        | 117/790 [00:34<03:17,  3.40it/s] 15%|█▍        | 118/790 [00:34<03:17,  3.40it/s] 15%|█▌        | 119/790 [00:34<03:17,  3.40it/s] 15%|█▌        | 120/790 [00:35<03:17,  3.40it/s] 15%|█▌        | 121/790 [00:35<03:16,  3.40it/s] 15%|█▌        | 122/790 [00:35<03:16,  3.40it/s] 16%|█▌        | 123/790 [00:36<03:16,  3.40it/s] 16%|█▌        | 124/790 [00:36<03:15,  3.40it/s] 16%|█▌        | 125/790 [00:36<03:15,  3.40it/s] 16%|█▌        | 126/790 [00:36<03:14,  3.41it/s] 16%|█▌        | 127/790 [00:37<03:14,  3.40it/s] 16%|█▌        | 128/790 [00:37<03:14,  3.40it/s] 16%|█▋        | 129/790 [00:37<03:14,  3.40it/s] 16%|█▋        | 130/790 [00:38<03:14,  3.40it/s] 17%|█▋        | 131/790 [00:38<03:13,  3.40it/s] 17%|█▋        | 132/790 [00:38<03:13,  3.41it/s] 17%|█▋        | 133/790 [00:39<03:12,  3.40it/s] 17%|█▋        | 134/790 [00:39<03:12,  3.40it/s] 17%|█▋        | 135/790 [00:39<03:12,  3.40it/s] 17%|█▋        | 136/790 [00:39<03:12,  3.40it/s] 17%|█▋        | 137/790 [00:40<03:11,  3.40it/s] 17%|█▋        | 138/790 [00:40<03:11,  3.40it/s] 18%|█▊        | 139/790 [00:40<03:11,  3.41it/s] 18%|█▊        | 140/790 [00:41<03:10,  3.40it/s] 18%|█▊        | 141/790 [00:41<03:10,  3.40it/s] 18%|█▊        | 142/790 [00:41<03:10,  3.40it/s] 18%|█▊        | 143/790 [00:41<03:10,  3.40it/s] 18%|█▊        | 144/790 [00:42<03:09,  3.40it/s] 18%|█▊        | 145/790 [00:42<03:09,  3.41it/s] 18%|█▊        | 146/790 [00:42<03:09,  3.40it/s] 19%|█▊        | 147/790 [00:43<03:08,  3.40it/s] 19%|█▊        | 148/790 [00:43<03:08,  3.40it/s] 19%|█▉        | 149/790 [00:43<03:08,  3.40it/s] 19%|█▉        | 150/790 [00:44<03:08,  3.40it/s] 19%|█▉        | 151/790 [00:44<03:07,  3.40it/s] 19%|█▉        | 152/790 [00:44<03:07,  3.40it/s] 19%|█▉        | 153/790 [00:44<03:07,  3.40it/s] 19%|█▉        | 154/790 [00:45<03:07,  3.40it/s] 20%|█▉        | 155/790 [00:45<03:06,  3.40it/s] 20%|█▉        | 156/790 [00:45<03:06,  3.40it/s] 20%|█▉        | 157/790 [00:46<03:05,  3.40it/s] 20%|██        | 158/790 [00:46<02:49,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 13:49:47,108 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:49:47,108 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 13:49:47,108 >>   Batch size = 8

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 54.24it/s][A
  3%|▎         | 12/431 [00:00<00:08, 46.57it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.65it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.65it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.19it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.74it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.49it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.46it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.60it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.66it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.63it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.43it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.25it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.23it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.22it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.15it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.19it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.35it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.48it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.47it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.37it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.16it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.22it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.10it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.16it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.20it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.32it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.44it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.46it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.37it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.34it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.25it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.26it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.19it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.37it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.47it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 41.42it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 42.10it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 42.52it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 42.79it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 42.88it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 42.94it/s][A
 50%|█████     | 217/431 [00:05<00:04, 43.11it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.20it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.11it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.31it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.38it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.41it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.34it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.28it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.26it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.29it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.28it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.27it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.50it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.48it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.38it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.34it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.19it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.23it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.28it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.22it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.32it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.36it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.33it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.39it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.24it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.03it/s][A
 81%|████████  | 347/431 [00:08<00:01, 43.11it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.13it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.29it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.32it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.35it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.34it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.33it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.11it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.18it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.24it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.42it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.35it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.43it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.36it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.39it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.19it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.16it/s][A                                                 
                                                 [A 20%|██        | 158/790 [00:56<02:49,  3.73it/s]
100%|██████████| 431/431 [00:09<00:00, 43.16it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:49:57,103 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-158
[INFO|configuration_utils.py:351] 2023-08-28 13:49:57,120 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-158/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:49:59,048 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-158/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:49:59,058 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-158/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:49:59,067 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-158/special_tokens_map.json
 20%|██        | 159/790 [01:02<52:21,  4.98s/it] 20%|██        | 160/790 [01:02<37:31,  3.57s/it] 20%|██        | 161/790 [01:02<27:09,  2.59s/it] 21%|██        | 162/790 [01:03<19:55,  1.90s/it] 21%|██        | 163/790 [01:03<14:51,  1.42s/it] 21%|██        | 164/790 [01:03<11:18,  1.08s/it] 21%|██        | 165/790 [01:04<08:50,  1.18it/s] 21%|██        | 166/790 [01:04<07:06,  1.46it/s] 21%|██        | 167/790 [01:04<05:53,  1.76it/s] 21%|██▏       | 168/790 [01:04<05:02,  2.05it/s] 21%|██▏       | 169/790 [01:05<04:27,  2.32it/s] 22%|██▏       | 170/790 [01:05<04:02,  2.56it/s] 22%|██▏       | 171/790 [01:05<03:44,  2.76it/s] 22%|██▏       | 172/790 [01:06<03:32,  2.91it/s] 22%|██▏       | 173/790 [01:06<03:23,  3.03it/s] 22%|██▏       | 174/790 [01:06<03:17,  3.13it/s] 22%|██▏       | 175/790 [01:07<03:12,  3.19it/s] 22%|██▏       | 176/790 [01:07<03:09,  3.24it/s] 22%|██▏       | 177/790 [01:07<03:07,  3.27it/s] 23%|██▎       | 178/790 [01:07<03:05,  3.30it/s] 23%|██▎       | 179/790 [01:08<03:05,  3.29it/s] 23%|██▎       | 180/790 [01:08<03:04,  3.31it/s] 23%|██▎       | 181/790 [01:08<03:03,  3.32it/s] 23%|██▎       | 182/790 [01:09<03:02,  3.33it/s] 23%|██▎       | 183/790 [01:09<03:01,  3.34it/s] 23%|██▎       | 184/790 [01:09<03:01,  3.34it/s] 23%|██▎       | 185/790 [01:10<03:00,  3.35it/s] 24%|██▎       | 186/790 [01:10<03:00,  3.35it/s] 24%|██▎       | 187/790 [01:10<02:59,  3.35it/s] 24%|██▍       | 188/790 [01:10<02:59,  3.36it/s] 24%|██▍       | 189/790 [01:11<02:59,  3.34it/s] 24%|██▍       | 190/790 [01:11<02:59,  3.34it/s] 24%|██▍       | 191/790 [01:11<02:59,  3.34it/s] 24%|██▍       | 192/790 [01:12<02:58,  3.34it/s] 24%|██▍       | 193/790 [01:12<02:58,  3.35it/s] 25%|██▍       | 194/790 [01:12<02:57,  3.35it/s] 25%|██▍       | 195/790 [01:13<02:57,  3.35it/s] 25%|██▍       | 196/790 [01:13<02:57,  3.35it/s] 25%|██▍       | 197/790 [01:13<02:56,  3.36it/s] 25%|██▌       | 198/790 [01:13<02:56,  3.36it/s] 25%|██▌       | 199/790 [01:14<02:55,  3.36it/s] 25%|██▌       | 200/790 [01:14<02:55,  3.36it/s] 25%|██▌       | 201/790 [01:14<02:55,  3.36it/s] 26%|██▌       | 202/790 [01:15<02:55,  3.36it/s] 26%|██▌       | 203/790 [01:15<02:54,  3.36it/s] 26%|██▌       | 204/790 [01:15<02:54,  3.36it/s] 26%|██▌       | 205/790 [01:16<02:54,  3.35it/s] 26%|██▌       | 206/790 [01:16<02:54,  3.35it/s] 26%|██▌       | 207/790 [01:16<02:54,  3.35it/s] 26%|██▋       | 208/790 [01:16<02:53,  3.35it/s] 26%|██▋       | 209/790 [01:17<02:53,  3.35it/s] 27%|██▋       | 210/790 [01:17<02:52,  3.35it/s] 27%|██▋       | 211/790 [01:17<02:53,  3.34it/s] 27%|██▋       | 212/790 [01:18<02:52,  3.35it/s] 27%|██▋       | 213/790 [01:18<02:52,  3.35it/s] 27%|██▋       | 214/790 [01:18<02:51,  3.35it/s] 27%|██▋       | 215/790 [01:18<02:51,  3.36it/s] 27%|██▋       | 216/790 [01:19<02:51,  3.35it/s] 27%|██▋       | 217/790 [01:19<02:50,  3.35it/s] 28%|██▊       | 218/790 [01:19<02:50,  3.35it/s] 28%|██▊       | 219/790 [01:20<02:50,  3.35it/s] 28%|██▊       | 220/790 [01:20<02:50,  3.35it/s] 28%|██▊       | 221/790 [01:20<02:49,  3.35it/s] 28%|██▊       | 222/790 [01:21<02:50,  3.33it/s] 28%|██▊       | 223/790 [01:21<02:49,  3.34it/s] 28%|██▊       | 224/790 [01:21<02:49,  3.34it/s] 28%|██▊       | 225/790 [01:21<02:48,  3.34it/s] 29%|██▊       | 226/790 [01:22<02:48,  3.35it/s] 29%|██▊       | 227/790 [01:22<02:48,  3.35it/s] 29%|██▉       | 228/790 [01:22<02:47,  3.35it/s] 29%|██▉       | 229/790 [01:23<02:47,  3.36it/s] 29%|██▉       | 230/790 [01:23<02:46,  3.37it/s] 29%|██▉       | 231/790 [01:23<02:45,  3.38it/s] 29%|██▉       | 232/790 [01:24<02:44,  3.39it/s] 29%|██▉       | 233/790 [01:24<02:44,  3.39it/s] 30%|██▉       | 234/790 [01:24<02:43,  3.39it/s] 30%|██▉       | 235/790 [01:24<02:43,  3.40it/s] 30%|██▉       | 236/790 [01:25<02:43,  3.40it/s] 30%|███       | 237/790 [01:25<02:42,  3.40it/s] 30%|███       | 238/790 [01:25<02:42,  3.40it/s] 30%|███       | 239/790 [01:26<02:42,  3.40it/s] 30%|███       | 240/790 [01:26<02:41,  3.40it/s] 31%|███       | 241/790 [01:26<02:41,  3.40it/s] 31%|███       | 242/790 [01:26<02:41,  3.40it/s] 31%|███       | 243/790 [01:27<02:40,  3.40it/s] 31%|███       | 244/790 [01:27<02:40,  3.39it/s] 31%|███       | 245/790 [01:27<02:40,  3.39it/s] 31%|███       | 246/790 [01:28<02:40,  3.40it/s] 31%|███▏      | 247/790 [01:28<02:39,  3.40it/s] 31%|███▏      | 248/790 [01:28<02:39,  3.39it/s] 32%|███▏      | 249/790 [01:29<02:39,  3.40it/s] 32%|███▏      | 250/790 [01:29<02:38,  3.40it/s] 32%|███▏      | 251/790 [01:29<02:38,  3.40it/s] 32%|███▏      | 252/790 [01:29<02:38,  3.40it/s] 32%|███▏      | 253/790 [01:30<02:37,  3.40it/s] 32%|███▏      | 254/790 [01:30<02:37,  3.40it/s] 32%|███▏      | 255/790 [01:30<02:38,  3.39it/s] 32%|███▏      | 256/790 [01:31<02:37,  3.39it/s] 33%|███▎      | 257/790 [01:31<02:36,  3.40it/s] 33%|███▎      | 258/790 [01:31<02:36,  3.40it/s] 33%|███▎      | 259/790 [01:31<02:36,  3.39it/s] 33%|███▎      | 260/790 [01:32<02:36,  3.39it/s] 33%|███▎      | 261/790 [01:32<02:35,  3.40it/s] 33%|███▎      | 262/790 [01:32<02:35,  3.40it/s] 33%|███▎      | 263/790 [01:33<02:35,  3.40it/s] 33%|███▎      | 264/790 [01:33<02:34,  3.40it/s] 34%|███▎      | 265/790 [01:33<02:34,  3.40it/s] 34%|███▎      | 266/790 [01:34<02:34,  3.40it/s] 34%|███▍      | 267/790 [01:34<02:33,  3.40it/s] 34%|███▍      | 268/790 [01:34<02:33,  3.40it/s] 34%|███▍      | 269/790 [01:34<02:33,  3.40it/s] 34%|███▍      | 270/790 [01:35<02:32,  3.40it/s] 34%|███▍      | 271/790 [01:35<02:32,  3.40it/s] 34%|███▍      | 272/790 [01:35<02:32,  3.40it/s] 35%|███▍      | 273/790 [01:36<02:32,  3.39it/s] 35%|███▍      | 274/790 [01:36<02:32,  3.38it/s] 35%|███▍      | 275/790 [01:36<02:32,  3.37it/s] 35%|███▍      | 276/790 [01:37<02:32,  3.36it/s] 35%|███▌      | 277/790 [01:37<02:33,  3.35it/s] 35%|███▌      | 278/790 [01:37<02:32,  3.35it/s] 35%|███▌      | 279/790 [01:37<02:32,  3.35it/s] 35%|███▌      | 280/790 [01:38<02:32,  3.35it/s] 36%|███▌      | 281/790 [01:38<02:31,  3.35it/s] 36%|███▌      | 282/790 [01:38<02:31,  3.36it/s] 36%|███▌      | 283/790 [01:39<02:31,  3.36it/s] 36%|███▌      | 284/790 [01:39<02:30,  3.35it/s] 36%|███▌      | 285/790 [01:39<02:30,  3.35it/s] 36%|███▌      | 286/790 [01:39<02:30,  3.35it/s] 36%|███▋      | 287/790 [01:40<02:29,  3.35it/s] 36%|███▋      | 288/790 [01:40<02:30,  3.35it/s] 37%|███▋      | 289/790 [01:40<02:29,  3.35it/s] 37%|███▋      | 290/790 [01:41<02:29,  3.35it/s] 37%|███▋      | 291/790 [01:41<02:28,  3.35it/s] 37%|███▋      | 292/790 [01:41<02:28,  3.35it/s] 37%|███▋      | 293/790 [01:42<02:28,  3.35it/s] 37%|███▋      | 294/790 [01:42<02:27,  3.35it/s] 37%|███▋      | 295/790 [01:42<02:27,  3.35it/s] 37%|███▋      | 296/790 [01:42<02:27,  3.36it/s] 38%|███▊      | 297/790 [01:43<02:26,  3.35it/s] 38%|███▊      | 298/790 [01:43<02:26,  3.35it/s] 38%|███▊      | 299/790 [01:43<02:26,  3.34it/s] 38%|███▊      | 300/790 [01:44<02:26,  3.35it/s] 38%|███▊      | 301/790 [01:44<02:25,  3.35it/s] 38%|███▊      | 302/790 [01:44<02:25,  3.35it/s] 38%|███▊      | 303/790 [01:45<02:25,  3.35it/s] 38%|███▊      | 304/790 [01:45<02:25,  3.35it/s] 39%|███▊      | 305/790 [01:45<02:24,  3.35it/s] 39%|███▊      | 306/790 [01:45<02:23,  3.37it/s] 39%|███▉      | 307/790 [01:46<02:22,  3.38it/s] 39%|███▉      | 308/790 [01:46<02:22,  3.38it/s] 39%|███▉      | 309/790 [01:46<02:21,  3.39it/s] 39%|███▉      | 310/790 [01:47<02:21,  3.39it/s] 39%|███▉      | 311/790 [01:47<02:21,  3.39it/s] 39%|███▉      | 312/790 [01:47<02:20,  3.40it/s] 40%|███▉      | 313/790 [01:48<02:20,  3.40it/s] 40%|███▉      | 314/790 [01:48<02:20,  3.40it/s] 40%|███▉      | 315/790 [01:48<02:20,  3.38it/s] 40%|████      | 316/790 [01:48<02:07,  3.71it/s][INFO|trainer.py:2140] 2023-08-28 13:50:49,615 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:50:49,615 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 13:50:49,615 >>   Batch size = 8
{'eval_loss': 0.9453633427619934, 'eval_runtime': 9.9654, 'eval_samples_per_second': 345.796, 'eval_steps_per_second': 43.25, 'epoch': 1.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 55.60it/s][A
  3%|▎         | 12/431 [00:00<00:08, 47.29it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.35it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.36it/s][A
  6%|▋         | 27/431 [00:00<00:09, 43.99it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.76it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.56it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.39it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.43it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.67it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.48it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.38it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.26it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.24it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 41.25it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 42.19it/s][A
 20%|██        | 87/431 [00:01<00:08, 42.55it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 42.86it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.23it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.06it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.22it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.21it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 42.98it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.05it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.22it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.41it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.56it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.41it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.25it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.29it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.17it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.09it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.16it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.21it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.49it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.50it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.39it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.32it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.27it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.11it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.08it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.25it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.51it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.48it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.35it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.38it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.24it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.14it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.19it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.13it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.33it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.44it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.40it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.40it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.40it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.25it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.19it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.13it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.31it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.31it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.31it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.35it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.42it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.36it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.17it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.09it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.19it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.34it/s][A
 81%|████████  | 347/431 [00:08<00:01, 43.32it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.42it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.42it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.33it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.31it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.20it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.11it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.33it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.39it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.27it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.38it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.40it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.40it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.27it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.16it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.25it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.33it/s][A                                                 
                                                 [A 40%|████      | 316/790 [01:58<02:07,  3.71it/s]
100%|██████████| 431/431 [00:09<00:00, 43.33it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:50:59,603 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-316
[INFO|configuration_utils.py:351] 2023-08-28 13:50:59,622 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-316/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:51:01,480 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-316/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:51:01,493 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-316/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:51:01,506 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-316/special_tokens_map.json
 40%|████      | 317/790 [02:04<39:12,  4.97s/it] 40%|████      | 318/790 [02:05<28:05,  3.57s/it] 40%|████      | 319/790 [02:05<20:19,  2.59s/it] 41%|████      | 320/790 [02:05<14:53,  1.90s/it] 41%|████      | 321/790 [02:05<11:06,  1.42s/it] 41%|████      | 322/790 [02:06<08:26,  1.08s/it] 41%|████      | 323/790 [02:06<06:34,  1.18it/s] 41%|████      | 324/790 [02:06<05:16,  1.47it/s] 41%|████      | 325/790 [02:07<04:22,  1.77it/s] 41%|████▏     | 326/790 [02:07<03:43,  2.07it/s] 41%|████▏     | 327/790 [02:07<03:17,  2.35it/s] 42%|████▏     | 328/790 [02:08<02:58,  2.59it/s] 42%|████▏     | 329/790 [02:08<02:45,  2.79it/s] 42%|████▏     | 330/790 [02:08<02:36,  2.95it/s] 42%|████▏     | 331/790 [02:08<02:29,  3.07it/s] 42%|████▏     | 332/790 [02:09<02:24,  3.16it/s] 42%|████▏     | 333/790 [02:09<02:21,  3.24it/s] 42%|████▏     | 334/790 [02:09<02:18,  3.29it/s] 42%|████▏     | 335/790 [02:10<02:16,  3.32it/s] 43%|████▎     | 336/790 [02:10<02:15,  3.35it/s] 43%|████▎     | 337/790 [02:10<02:14,  3.36it/s] 43%|████▎     | 338/790 [02:10<02:13,  3.38it/s] 43%|████▎     | 339/790 [02:11<02:13,  3.38it/s] 43%|████▎     | 340/790 [02:11<02:13,  3.38it/s] 43%|████▎     | 341/790 [02:11<02:12,  3.38it/s] 43%|████▎     | 342/790 [02:12<02:12,  3.39it/s] 43%|████▎     | 343/790 [02:12<02:11,  3.39it/s] 44%|████▎     | 344/790 [02:12<02:11,  3.40it/s] 44%|████▎     | 345/790 [02:13<02:10,  3.40it/s] 44%|████▍     | 346/790 [02:13<02:10,  3.40it/s] 44%|████▍     | 347/790 [02:13<02:10,  3.40it/s] 44%|████▍     | 348/790 [02:13<02:09,  3.40it/s] 44%|████▍     | 349/790 [02:14<02:09,  3.41it/s] 44%|████▍     | 350/790 [02:14<02:09,  3.40it/s] 44%|████▍     | 351/790 [02:14<02:09,  3.40it/s] 45%|████▍     | 352/790 [02:15<02:08,  3.40it/s] 45%|████▍     | 353/790 [02:15<02:08,  3.40it/s] 45%|████▍     | 354/790 [02:15<02:08,  3.40it/s] 45%|████▍     | 355/790 [02:15<02:07,  3.40it/s] 45%|████▌     | 356/790 [02:16<02:07,  3.41it/s] 45%|████▌     | 357/790 [02:16<02:07,  3.40it/s] 45%|████▌     | 358/790 [02:16<02:06,  3.41it/s] 45%|████▌     | 359/790 [02:17<02:06,  3.40it/s] 46%|████▌     | 360/790 [02:17<02:06,  3.40it/s] 46%|████▌     | 361/790 [02:17<02:06,  3.40it/s] 46%|████▌     | 362/790 [02:18<02:05,  3.41it/s] 46%|████▌     | 363/790 [02:18<02:05,  3.40it/s] 46%|████▌     | 364/790 [02:18<02:05,  3.40it/s] 46%|████▌     | 365/790 [02:18<02:04,  3.40it/s] 46%|████▋     | 366/790 [02:19<02:05,  3.39it/s] 46%|████▋     | 367/790 [02:19<02:04,  3.39it/s] 47%|████▋     | 368/790 [02:19<02:04,  3.40it/s] 47%|████▋     | 369/790 [02:20<02:03,  3.40it/s] 47%|████▋     | 370/790 [02:20<02:03,  3.40it/s] 47%|████▋     | 371/790 [02:20<02:03,  3.40it/s] 47%|████▋     | 372/790 [02:20<02:02,  3.40it/s] 47%|████▋     | 373/790 [02:21<02:02,  3.40it/s] 47%|████▋     | 374/790 [02:21<02:02,  3.40it/s] 47%|████▋     | 375/790 [02:21<02:01,  3.40it/s] 48%|████▊     | 376/790 [02:22<02:01,  3.40it/s] 48%|████▊     | 377/790 [02:22<02:01,  3.39it/s] 48%|████▊     | 378/790 [02:22<02:01,  3.39it/s] 48%|████▊     | 379/790 [02:23<02:00,  3.40it/s] 48%|████▊     | 380/790 [02:23<02:00,  3.40it/s] 48%|████▊     | 381/790 [02:23<02:00,  3.39it/s] 48%|████▊     | 382/790 [02:23<02:00,  3.38it/s] 48%|████▊     | 383/790 [02:24<02:00,  3.37it/s] 49%|████▊     | 384/790 [02:24<02:00,  3.37it/s] 49%|████▊     | 385/790 [02:24<02:00,  3.37it/s] 49%|████▉     | 386/790 [02:25<02:00,  3.36it/s] 49%|████▉     | 387/790 [02:25<01:59,  3.36it/s] 49%|████▉     | 388/790 [02:25<01:59,  3.35it/s] 49%|████▉     | 389/790 [02:25<01:59,  3.35it/s] 49%|████▉     | 390/790 [02:26<01:59,  3.36it/s] 49%|████▉     | 391/790 [02:26<01:58,  3.36it/s] 50%|████▉     | 392/790 [02:26<01:58,  3.36it/s] 50%|████▉     | 393/790 [02:27<01:58,  3.35it/s] 50%|████▉     | 394/790 [02:27<01:58,  3.35it/s] 50%|█████     | 395/790 [02:27<01:57,  3.36it/s] 50%|█████     | 396/790 [02:28<01:57,  3.36it/s] 50%|█████     | 397/790 [02:28<01:57,  3.36it/s] 50%|█████     | 398/790 [02:28<01:56,  3.36it/s] 51%|█████     | 399/790 [02:28<01:56,  3.35it/s] 51%|█████     | 400/790 [02:29<01:56,  3.36it/s] 51%|█████     | 401/790 [02:29<01:56,  3.35it/s] 51%|█████     | 402/790 [02:29<01:55,  3.35it/s] 51%|█████     | 403/790 [02:30<01:55,  3.35it/s] 51%|█████     | 404/790 [02:30<01:55,  3.34it/s] 51%|█████▏    | 405/790 [02:30<01:55,  3.35it/s] 51%|█████▏    | 406/790 [02:31<01:54,  3.35it/s] 52%|█████▏    | 407/790 [02:31<01:54,  3.35it/s] 52%|█████▏    | 408/790 [02:31<01:53,  3.35it/s] 52%|█████▏    | 409/790 [02:31<01:53,  3.36it/s] 52%|█████▏    | 410/790 [02:32<01:53,  3.34it/s] 52%|█████▏    | 411/790 [02:32<01:53,  3.35it/s] 52%|█████▏    | 412/790 [02:32<01:52,  3.35it/s] 52%|█████▏    | 413/790 [02:33<01:52,  3.35it/s] 52%|█████▏    | 414/790 [02:33<01:52,  3.35it/s] 53%|█████▎    | 415/790 [02:33<01:51,  3.35it/s] 53%|█████▎    | 416/790 [02:34<01:51,  3.35it/s] 53%|█████▎    | 417/790 [02:34<01:51,  3.35it/s] 53%|█████▎    | 418/790 [02:34<01:50,  3.35it/s] 53%|█████▎    | 419/790 [02:34<01:50,  3.35it/s] 53%|█████▎    | 420/790 [02:35<01:50,  3.35it/s] 53%|█████▎    | 421/790 [02:35<01:50,  3.35it/s] 53%|█████▎    | 422/790 [02:35<01:49,  3.35it/s] 54%|█████▎    | 423/790 [02:36<01:49,  3.35it/s] 54%|█████▎    | 424/790 [02:36<01:49,  3.35it/s] 54%|█████▍    | 425/790 [02:36<01:48,  3.36it/s] 54%|█████▍    | 426/790 [02:37<01:48,  3.35it/s] 54%|█████▍    | 427/790 [02:37<01:48,  3.35it/s] 54%|█████▍    | 428/790 [02:37<01:48,  3.35it/s] 54%|█████▍    | 429/790 [02:37<01:47,  3.35it/s] 54%|█████▍    | 430/790 [02:38<01:47,  3.35it/s] 55%|█████▍    | 431/790 [02:38<01:47,  3.35it/s] 55%|█████▍    | 432/790 [02:38<01:47,  3.34it/s] 55%|█████▍    | 433/790 [02:39<01:46,  3.34it/s] 55%|█████▍    | 434/790 [02:39<01:46,  3.35it/s] 55%|█████▌    | 435/790 [02:39<01:45,  3.35it/s] 55%|█████▌    | 436/790 [02:40<01:45,  3.36it/s] 55%|█████▌    | 437/790 [02:40<01:45,  3.35it/s] 55%|█████▌    | 438/790 [02:40<01:44,  3.36it/s] 56%|█████▌    | 439/790 [02:40<01:44,  3.36it/s] 56%|█████▌    | 440/790 [02:41<01:44,  3.36it/s] 56%|█████▌    | 441/790 [02:41<01:43,  3.36it/s] 56%|█████▌    | 442/790 [02:41<01:43,  3.36it/s] 56%|█████▌    | 443/790 [02:42<01:43,  3.34it/s] 56%|█████▌    | 444/790 [02:42<01:43,  3.34it/s] 56%|█████▋    | 445/790 [02:42<01:43,  3.35it/s] 56%|█████▋    | 446/790 [02:42<01:42,  3.35it/s] 57%|█████▋    | 447/790 [02:43<01:42,  3.35it/s] 57%|█████▋    | 448/790 [02:43<01:42,  3.34it/s] 57%|█████▋    | 449/790 [02:43<01:41,  3.35it/s] 57%|█████▋    | 450/790 [02:44<01:41,  3.35it/s] 57%|█████▋    | 451/790 [02:44<01:41,  3.35it/s] 57%|█████▋    | 452/790 [02:44<01:40,  3.35it/s] 57%|█████▋    | 453/790 [02:45<01:40,  3.36it/s] 57%|█████▋    | 454/790 [02:45<01:40,  3.35it/s] 58%|█████▊    | 455/790 [02:45<01:39,  3.35it/s] 58%|█████▊    | 456/790 [02:45<01:39,  3.35it/s] 58%|█████▊    | 457/790 [02:46<01:39,  3.35it/s] 58%|█████▊    | 458/790 [02:46<01:38,  3.35it/s] 58%|█████▊    | 459/790 [02:46<01:38,  3.35it/s] 58%|█████▊    | 460/790 [02:47<01:38,  3.35it/s] 58%|█████▊    | 461/790 [02:47<01:38,  3.35it/s] 58%|█████▊    | 462/790 [02:47<01:37,  3.36it/s] 59%|█████▊    | 463/790 [02:48<01:37,  3.36it/s] 59%|█████▊    | 464/790 [02:48<01:37,  3.36it/s] 59%|█████▉    | 465/790 [02:48<01:36,  3.35it/s] 59%|█████▉    | 466/790 [02:48<01:36,  3.35it/s] 59%|█████▉    | 467/790 [02:49<01:36,  3.33it/s] 59%|█████▉    | 468/790 [02:49<01:36,  3.34it/s] 59%|█████▉    | 469/790 [02:49<01:36,  3.34it/s] 59%|█████▉    | 470/790 [02:50<01:36,  3.33it/s] 60%|█████▉    | 471/790 [02:50<01:35,  3.34it/s] 60%|█████▉    | 472/790 [02:50<01:37,  3.28it/s] 60%|█████▉    | 473/790 [02:51<01:35,  3.30it/s] 60%|██████    | 474/790 [02:51<01:27,  3.63it/s][INFO|trainer.py:2140] 2023-08-28 13:51:52,079 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:51:52,079 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 13:51:52,079 >>   Batch size = 8
{'eval_loss': 0.944346010684967, 'eval_runtime': 9.9647, 'eval_samples_per_second': 345.819, 'eval_steps_per_second': 43.253, 'epoch': 2.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 55.09it/s][A
  3%|▎         | 12/431 [00:00<00:08, 47.61it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.86it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.36it/s][A
  6%|▋         | 27/431 [00:00<00:09, 43.74it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.57it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.55it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.45it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.42it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.46it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.68it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.55it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.40it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.19it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.13it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.29it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.30it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.29it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.44it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.51it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.45it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.27it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.15it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.22it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.14it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.29it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.40it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.43it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.45it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.37it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.24it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.22it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.16it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.18it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.19it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.28it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.48it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.52it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.34it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.35it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.28it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.18it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.23it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.21it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.25it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.37it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.49it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.48it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.39it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.17it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.29it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.29it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.31it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.30it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.27it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.49it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.44it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.25it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.31it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.24it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.26it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.29it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.24it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.50it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.45it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.29it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.19it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.31it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.40it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.18it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.18it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.41it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.46it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.43it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.25it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.21it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.35it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.41it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.27it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.17it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.38it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.44it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.31it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.24it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.23it/s][A                                                 
                                                 [A 60%|██████    | 474/790 [03:01<01:27,  3.63it/s]
100%|██████████| 431/431 [00:09<00:00, 43.23it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:52:02,043 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-474
[INFO|configuration_utils.py:351] 2023-08-28 13:52:02,065 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-474/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:52:03,962 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-474/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:52:03,979 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-474/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:52:03,990 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-474/special_tokens_map.json
 60%|██████    | 475/790 [03:07<26:20,  5.02s/it] 60%|██████    | 476/790 [03:07<18:51,  3.60s/it] 60%|██████    | 477/790 [03:07<13:37,  2.61s/it] 61%|██████    | 478/790 [03:08<09:58,  1.92s/it] 61%|██████    | 479/790 [03:08<07:25,  1.43s/it] 61%|██████    | 480/790 [03:08<05:38,  1.09s/it] 61%|██████    | 481/790 [03:09<04:23,  1.17it/s] 61%|██████    | 482/790 [03:09<03:31,  1.46it/s] 61%|██████    | 483/790 [03:09<02:54,  1.76it/s] 61%|██████▏   | 484/790 [03:10<02:29,  2.05it/s] 61%|██████▏   | 485/790 [03:10<02:11,  2.32it/s] 62%|██████▏   | 486/790 [03:10<01:58,  2.56it/s] 62%|██████▏   | 487/790 [03:10<01:50,  2.75it/s] 62%|██████▏   | 488/790 [03:11<01:43,  2.91it/s] 62%|██████▏   | 489/790 [03:11<01:39,  3.03it/s] 62%|██████▏   | 490/790 [03:11<01:36,  3.12it/s] 62%|██████▏   | 491/790 [03:12<01:33,  3.19it/s] 62%|██████▏   | 492/790 [03:12<01:32,  3.24it/s] 62%|██████▏   | 493/790 [03:12<01:30,  3.27it/s] 63%|██████▎   | 494/790 [03:13<01:29,  3.30it/s] 63%|██████▎   | 495/790 [03:13<01:28,  3.32it/s] 63%|██████▎   | 496/790 [03:13<01:28,  3.33it/s] 63%|██████▎   | 497/790 [03:13<01:27,  3.34it/s] 63%|██████▎   | 498/790 [03:14<01:28,  3.28it/s] 63%|██████▎   | 499/790 [03:14<01:27,  3.31it/s] 63%|██████▎   | 500/790 [03:14<01:27,  3.33it/s]                                                  63%|██████▎   | 500/790 [03:14<01:27,  3.33it/s] 63%|██████▎   | 501/790 [03:15<01:26,  3.34it/s] 64%|██████▎   | 502/790 [03:15<01:26,  3.34it/s] 64%|██████▎   | 503/790 [03:15<01:25,  3.35it/s] 64%|██████▍   | 504/790 [03:16<01:25,  3.35it/s] 64%|██████▍   | 505/790 [03:16<01:24,  3.35it/s] 64%|██████▍   | 506/790 [03:16<01:24,  3.36it/s] 64%|██████▍   | 507/790 [03:16<01:24,  3.36it/s] 64%|██████▍   | 508/790 [03:17<01:24,  3.36it/s] 64%|██████▍   | 509/790 [03:17<01:24,  3.33it/s] 65%|██████▍   | 510/790 [03:17<01:23,  3.34it/s] 65%|██████▍   | 511/790 [03:18<01:23,  3.35it/s] 65%|██████▍   | 512/790 [03:18<01:22,  3.35it/s] 65%|██████▍   | 513/790 [03:18<01:22,  3.35it/s] 65%|██████▌   | 514/790 [03:19<01:22,  3.36it/s] 65%|██████▌   | 515/790 [03:19<01:21,  3.35it/s] 65%|██████▌   | 516/790 [03:19<01:21,  3.35it/s] 65%|██████▌   | 517/790 [03:19<01:21,  3.35it/s] 66%|██████▌   | 518/790 [03:20<01:21,  3.36it/s] 66%|██████▌   | 519/790 [03:20<01:20,  3.36it/s] 66%|██████▌   | 520/790 [03:20<01:20,  3.35it/s] 66%|██████▌   | 521/790 [03:21<01:20,  3.35it/s] 66%|██████▌   | 522/790 [03:21<01:19,  3.35it/s] 66%|██████▌   | 523/790 [03:21<01:19,  3.36it/s] 66%|██████▋   | 524/790 [03:21<01:18,  3.37it/s] 66%|██████▋   | 525/790 [03:22<01:18,  3.38it/s] 67%|██████▋   | 526/790 [03:22<01:17,  3.38it/s] 67%|██████▋   | 527/790 [03:22<01:17,  3.39it/s] 67%|██████▋   | 528/790 [03:23<01:17,  3.39it/s] 67%|██████▋   | 529/790 [03:23<01:16,  3.40it/s] 67%|██████▋   | 530/790 [03:23<01:16,  3.40it/s] 67%|██████▋   | 531/790 [03:24<01:16,  3.39it/s] 67%|██████▋   | 532/790 [03:24<01:16,  3.39it/s] 67%|██████▋   | 533/790 [03:24<01:15,  3.39it/s] 68%|██████▊   | 534/790 [03:24<01:15,  3.40it/s] 68%|██████▊   | 535/790 [03:25<01:14,  3.40it/s] 68%|██████▊   | 536/790 [03:25<01:14,  3.40it/s] 68%|██████▊   | 537/790 [03:25<01:14,  3.40it/s] 68%|██████▊   | 538/790 [03:26<01:14,  3.41it/s] 68%|██████▊   | 539/790 [03:26<01:13,  3.41it/s] 68%|██████▊   | 540/790 [03:26<01:13,  3.41it/s] 68%|██████▊   | 541/790 [03:26<01:13,  3.41it/s] 69%|██████▊   | 542/790 [03:27<01:13,  3.38it/s] 69%|██████▊   | 543/790 [03:27<01:12,  3.39it/s] 69%|██████▉   | 544/790 [03:27<01:12,  3.40it/s] 69%|██████▉   | 545/790 [03:28<01:12,  3.40it/s] 69%|██████▉   | 546/790 [03:28<01:11,  3.40it/s] 69%|██████▉   | 547/790 [03:28<01:11,  3.40it/s] 69%|██████▉   | 548/790 [03:29<01:11,  3.41it/s] 69%|██████▉   | 549/790 [03:29<01:10,  3.40it/s] 70%|██████▉   | 550/790 [03:29<01:10,  3.40it/s] 70%|██████▉   | 551/790 [03:29<01:10,  3.41it/s] 70%|██████▉   | 552/790 [03:30<01:09,  3.41it/s] 70%|███████   | 553/790 [03:30<01:09,  3.40it/s] 70%|███████   | 554/790 [03:30<01:09,  3.40it/s] 70%|███████   | 555/790 [03:31<01:09,  3.40it/s] 70%|███████   | 556/790 [03:31<01:08,  3.40it/s] 71%|███████   | 557/790 [03:31<01:08,  3.40it/s] 71%|███████   | 558/790 [03:31<01:08,  3.40it/s] 71%|███████   | 559/790 [03:32<01:07,  3.40it/s] 71%|███████   | 560/790 [03:32<01:07,  3.39it/s] 71%|███████   | 561/790 [03:32<01:07,  3.40it/s] 71%|███████   | 562/790 [03:33<01:07,  3.40it/s] 71%|███████▏  | 563/790 [03:33<01:06,  3.40it/s] 71%|███████▏  | 564/790 [03:33<01:06,  3.40it/s] 72%|███████▏  | 565/790 [03:34<01:06,  3.40it/s] 72%|███████▏  | 566/790 [03:34<01:05,  3.40it/s] 72%|███████▏  | 567/790 [03:34<01:05,  3.40it/s] 72%|███████▏  | 568/790 [03:34<01:05,  3.40it/s] 72%|███████▏  | 569/790 [03:35<01:04,  3.40it/s] 72%|███████▏  | 570/790 [03:35<01:04,  3.40it/s] 72%|███████▏  | 571/790 [03:35<01:04,  3.40it/s] 72%|███████▏  | 572/790 [03:36<01:04,  3.40it/s] 73%|███████▎  | 573/790 [03:36<01:03,  3.40it/s] 73%|███████▎  | 574/790 [03:36<01:03,  3.40it/s] 73%|███████▎  | 575/790 [03:36<01:03,  3.39it/s] 73%|███████▎  | 576/790 [03:37<01:02,  3.40it/s] 73%|███████▎  | 577/790 [03:37<01:02,  3.40it/s] 73%|███████▎  | 578/790 [03:37<01:02,  3.40it/s] 73%|███████▎  | 579/790 [03:38<01:02,  3.40it/s] 73%|███████▎  | 580/790 [03:38<01:01,  3.40it/s] 74%|███████▎  | 581/790 [03:38<01:01,  3.40it/s] 74%|███████▎  | 582/790 [03:39<01:01,  3.40it/s] 74%|███████▍  | 583/790 [03:39<01:00,  3.40it/s] 74%|███████▍  | 584/790 [03:39<01:00,  3.40it/s] 74%|███████▍  | 585/790 [03:39<01:00,  3.40it/s] 74%|███████▍  | 586/790 [03:40<01:00,  3.39it/s] 74%|███████▍  | 587/790 [03:40<00:59,  3.39it/s] 74%|███████▍  | 588/790 [03:40<00:59,  3.40it/s] 75%|███████▍  | 589/790 [03:41<00:59,  3.40it/s] 75%|███████▍  | 590/790 [03:41<00:58,  3.40it/s] 75%|███████▍  | 591/790 [03:41<00:58,  3.40it/s] 75%|███████▍  | 592/790 [03:41<00:58,  3.40it/s] 75%|███████▌  | 593/790 [03:42<00:57,  3.40it/s] 75%|███████▌  | 594/790 [03:42<00:57,  3.40it/s] 75%|███████▌  | 595/790 [03:42<00:57,  3.40it/s] 75%|███████▌  | 596/790 [03:43<00:56,  3.40it/s] 76%|███████▌  | 597/790 [03:43<00:56,  3.39it/s] 76%|███████▌  | 598/790 [03:43<00:56,  3.40it/s] 76%|███████▌  | 599/790 [03:44<00:56,  3.40it/s] 76%|███████▌  | 600/790 [03:44<00:55,  3.40it/s] 76%|███████▌  | 601/790 [03:44<00:55,  3.40it/s] 76%|███████▌  | 602/790 [03:44<00:55,  3.40it/s] 76%|███████▋  | 603/790 [03:45<00:54,  3.40it/s] 76%|███████▋  | 604/790 [03:45<00:54,  3.40it/s] 77%|███████▋  | 605/790 [03:45<00:54,  3.40it/s] 77%|███████▋  | 606/790 [03:46<00:54,  3.40it/s] 77%|███████▋  | 607/790 [03:46<00:53,  3.40it/s] 77%|███████▋  | 608/790 [03:46<00:53,  3.39it/s] 77%|███████▋  | 609/790 [03:46<00:53,  3.39it/s] 77%|███████▋  | 610/790 [03:47<00:52,  3.40it/s] 77%|███████▋  | 611/790 [03:47<00:52,  3.40it/s] 77%|███████▋  | 612/790 [03:47<00:52,  3.40it/s] 78%|███████▊  | 613/790 [03:48<00:52,  3.40it/s] 78%|███████▊  | 614/790 [03:48<00:51,  3.40it/s] 78%|███████▊  | 615/790 [03:48<00:51,  3.40it/s] 78%|███████▊  | 616/790 [03:49<00:51,  3.40it/s] 78%|███████▊  | 617/790 [03:49<00:51,  3.38it/s] 78%|███████▊  | 618/790 [03:49<00:50,  3.39it/s] 78%|███████▊  | 619/790 [03:49<00:50,  3.39it/s] 78%|███████▊  | 620/790 [03:50<00:50,  3.39it/s] 79%|███████▊  | 621/790 [03:50<00:49,  3.40it/s] 79%|███████▊  | 622/790 [03:50<00:50,  3.32it/s] 79%|███████▉  | 623/790 [03:51<00:49,  3.35it/s] 79%|███████▉  | 624/790 [03:51<00:49,  3.35it/s] 79%|███████▉  | 625/790 [03:51<00:48,  3.37it/s] 79%|███████▉  | 626/790 [03:52<00:48,  3.38it/s] 79%|███████▉  | 627/790 [03:52<00:48,  3.38it/s] 79%|███████▉  | 628/790 [03:52<00:47,  3.39it/s] 80%|███████▉  | 629/790 [03:52<00:47,  3.40it/s] 80%|███████▉  | 630/790 [03:53<00:47,  3.40it/s] 80%|███████▉  | 631/790 [03:53<00:46,  3.40it/s] 80%|████████  | 632/790 [03:53<00:42,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 13:52:54,487 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:52:54,487 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 13:52:54,487 >>   Batch size = 8
{'eval_loss': 0.9505304098129272, 'eval_runtime': 9.9471, 'eval_samples_per_second': 346.432, 'eval_steps_per_second': 43.329, 'epoch': 3.0}
{'loss': 0.8148, 'learning_rate': 1.3765822784810127e-05, 'epoch': 3.16}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 54.27it/s][A
  3%|▎         | 12/431 [00:00<00:08, 46.78it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.24it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.51it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.04it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.71it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.43it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.50it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.56it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.52it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.47it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.46it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.40it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.22it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.26it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.23it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.35it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.39it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.45it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.46it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.24it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.30it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.22it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.30it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.31it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.36it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.34it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.40it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.34it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.26it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.18it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.27it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.18it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.32it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.36it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.41it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.38it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.27it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.26it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.22it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.21it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.27it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.31it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.25it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.49it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.40it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.35it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.25it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.11it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.21it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.30it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.32it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.39it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.43it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.28it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.27it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.29it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.33it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.21it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.40it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.39it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.38it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.42it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.29it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.20it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.29it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.28it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.29it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.38it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.30it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.51it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.37it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.31it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.31it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.29it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.32it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.39it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.41it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.45it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.37it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.30it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.32it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.26it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.19it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.28it/s][A                                                 
                                                 [A 80%|████████  | 632/790 [04:03<00:42,  3.73it/s]
100%|██████████| 431/431 [00:09<00:00, 43.28it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:53:04,457 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-632
[INFO|configuration_utils.py:351] 2023-08-28 13:53:04,481 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-632/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:53:06,459 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-632/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:53:06,471 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-632/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:53:06,485 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-632/special_tokens_map.json
 80%|████████  | 633/790 [04:09<13:04,  4.99s/it] 80%|████████  | 634/790 [04:10<09:19,  3.59s/it] 80%|████████  | 635/790 [04:10<06:42,  2.60s/it] 81%|████████  | 636/790 [04:10<04:53,  1.91s/it] 81%|████████  | 637/790 [04:10<03:37,  1.42s/it] 81%|████████  | 638/790 [04:11<02:45,  1.09s/it] 81%|████████  | 639/790 [04:11<02:08,  1.18it/s] 81%|████████  | 640/790 [04:11<01:42,  1.46it/s] 81%|████████  | 641/790 [04:12<01:24,  1.76it/s] 81%|████████▏ | 642/790 [04:12<01:12,  2.06it/s] 81%|████████▏ | 643/790 [04:12<01:03,  2.33it/s] 82%|████████▏ | 644/790 [04:12<00:56,  2.56it/s] 82%|████████▏ | 645/790 [04:13<00:52,  2.75it/s] 82%|████████▏ | 646/790 [04:13<00:49,  2.91it/s] 82%|████████▏ | 647/790 [04:13<00:47,  3.04it/s] 82%|████████▏ | 648/790 [04:14<00:45,  3.13it/s] 82%|████████▏ | 649/790 [04:14<00:44,  3.19it/s] 82%|████████▏ | 650/790 [04:14<00:43,  3.24it/s] 82%|████████▏ | 651/790 [04:15<00:42,  3.28it/s] 83%|████████▎ | 652/790 [04:15<00:41,  3.30it/s] 83%|████████▎ | 653/790 [04:15<00:41,  3.32it/s] 83%|████████▎ | 654/790 [04:15<00:40,  3.34it/s] 83%|████████▎ | 655/790 [04:16<00:40,  3.35it/s] 83%|████████▎ | 656/790 [04:16<00:40,  3.34it/s] 83%|████████▎ | 657/790 [04:16<00:39,  3.35it/s] 83%|████████▎ | 658/790 [04:17<00:39,  3.35it/s] 83%|████████▎ | 659/790 [04:17<00:39,  3.35it/s] 84%|████████▎ | 660/790 [04:17<00:38,  3.35it/s] 84%|████████▎ | 661/790 [04:18<00:38,  3.35it/s] 84%|████████▍ | 662/790 [04:18<00:38,  3.35it/s] 84%|████████▍ | 663/790 [04:18<00:37,  3.36it/s] 84%|████████▍ | 664/790 [04:18<00:37,  3.36it/s] 84%|████████▍ | 665/790 [04:19<00:37,  3.36it/s] 84%|████████▍ | 666/790 [04:19<00:36,  3.36it/s] 84%|████████▍ | 667/790 [04:19<00:36,  3.36it/s] 85%|████████▍ | 668/790 [04:20<00:36,  3.36it/s] 85%|████████▍ | 669/790 [04:20<00:36,  3.36it/s] 85%|████████▍ | 670/790 [04:20<00:35,  3.36it/s] 85%|████████▍ | 671/790 [04:21<00:35,  3.36it/s] 85%|████████▌ | 672/790 [04:21<00:35,  3.36it/s] 85%|████████▌ | 673/790 [04:21<00:34,  3.36it/s] 85%|████████▌ | 674/790 [04:21<00:34,  3.36it/s] 85%|████████▌ | 675/790 [04:22<00:34,  3.34it/s] 86%|████████▌ | 676/790 [04:22<00:34,  3.35it/s] 86%|████████▌ | 677/790 [04:22<00:33,  3.35it/s] 86%|████████▌ | 678/790 [04:23<00:33,  3.36it/s] 86%|████████▌ | 679/790 [04:23<00:33,  3.36it/s] 86%|████████▌ | 680/790 [04:23<00:32,  3.36it/s] 86%|████████▌ | 681/790 [04:24<00:32,  3.36it/s] 86%|████████▋ | 682/790 [04:24<00:32,  3.36it/s] 86%|████████▋ | 683/790 [04:24<00:31,  3.36it/s] 87%|████████▋ | 684/790 [04:24<00:31,  3.36it/s] 87%|████████▋ | 685/790 [04:25<00:31,  3.36it/s] 87%|████████▋ | 686/790 [04:25<00:31,  3.35it/s] 87%|████████▋ | 687/790 [04:25<00:30,  3.36it/s] 87%|████████▋ | 688/790 [04:26<00:30,  3.36it/s] 87%|████████▋ | 689/790 [04:26<00:30,  3.36it/s] 87%|████████▋ | 690/790 [04:26<00:29,  3.36it/s] 87%|████████▋ | 691/790 [04:26<00:29,  3.36it/s] 88%|████████▊ | 692/790 [04:27<00:29,  3.36it/s] 88%|████████▊ | 693/790 [04:27<00:28,  3.36it/s] 88%|████████▊ | 694/790 [04:27<00:28,  3.36it/s] 88%|████████▊ | 695/790 [04:28<00:28,  3.36it/s] 88%|████████▊ | 696/790 [04:28<00:27,  3.36it/s] 88%|████████▊ | 697/790 [04:28<00:27,  3.34it/s] 88%|████████▊ | 698/790 [04:29<00:27,  3.35it/s] 88%|████████▊ | 699/790 [04:29<00:27,  3.35it/s] 89%|████████▊ | 700/790 [04:29<00:26,  3.35it/s] 89%|████████▊ | 701/790 [04:29<00:26,  3.35it/s] 89%|████████▉ | 702/790 [04:30<00:26,  3.36it/s] 89%|████████▉ | 703/790 [04:30<00:25,  3.36it/s] 89%|████████▉ | 704/790 [04:30<00:25,  3.36it/s] 89%|████████▉ | 705/790 [04:31<00:25,  3.36it/s] 89%|████████▉ | 706/790 [04:31<00:24,  3.36it/s] 89%|████████▉ | 707/790 [04:31<00:24,  3.36it/s] 90%|████████▉ | 708/790 [04:32<00:24,  3.35it/s] 90%|████████▉ | 709/790 [04:32<00:24,  3.36it/s] 90%|████████▉ | 710/790 [04:32<00:23,  3.35it/s] 90%|█████████ | 711/790 [04:32<00:23,  3.36it/s] 90%|█████████ | 712/790 [04:33<00:23,  3.36it/s] 90%|█████████ | 713/790 [04:33<00:22,  3.36it/s] 90%|█████████ | 714/790 [04:33<00:22,  3.36it/s] 91%|█████████ | 715/790 [04:34<00:22,  3.36it/s] 91%|█████████ | 716/790 [04:34<00:22,  3.36it/s] 91%|█████████ | 717/790 [04:34<00:21,  3.37it/s] 91%|█████████ | 718/790 [04:35<00:21,  3.38it/s] 91%|█████████ | 719/790 [04:35<00:21,  3.38it/s] 91%|█████████ | 720/790 [04:35<00:20,  3.39it/s] 91%|█████████▏| 721/790 [04:35<00:20,  3.39it/s] 91%|█████████▏| 722/790 [04:36<00:20,  3.40it/s] 92%|█████████▏| 723/790 [04:36<00:19,  3.40it/s] 92%|█████████▏| 724/790 [04:36<00:19,  3.40it/s] 92%|█████████▏| 725/790 [04:37<00:19,  3.40it/s] 92%|█████████▏| 726/790 [04:37<00:18,  3.41it/s] 92%|█████████▏| 727/790 [04:37<00:18,  3.40it/s] 92%|█████████▏| 728/790 [04:37<00:18,  3.40it/s] 92%|█████████▏| 729/790 [04:38<00:17,  3.41it/s] 92%|█████████▏| 730/790 [04:38<00:17,  3.39it/s] 93%|█████████▎| 731/790 [04:38<00:17,  3.39it/s] 93%|█████████▎| 732/790 [04:39<00:17,  3.40it/s] 93%|█████████▎| 733/790 [04:39<00:16,  3.40it/s] 93%|█████████▎| 734/790 [04:39<00:16,  3.40it/s] 93%|█████████▎| 735/790 [04:40<00:16,  3.40it/s] 93%|█████████▎| 736/790 [04:40<00:15,  3.40it/s] 93%|█████████▎| 737/790 [04:40<00:15,  3.40it/s] 93%|█████████▎| 738/790 [04:40<00:15,  3.40it/s] 94%|█████████▎| 739/790 [04:41<00:14,  3.40it/s] 94%|█████████▎| 740/790 [04:41<00:14,  3.40it/s] 94%|█████████▍| 741/790 [04:41<00:14,  3.39it/s] 94%|█████████▍| 742/790 [04:42<00:14,  3.40it/s] 94%|█████████▍| 743/790 [04:42<00:13,  3.40it/s] 94%|█████████▍| 744/790 [04:42<00:13,  3.40it/s] 94%|█████████▍| 745/790 [04:42<00:13,  3.40it/s] 94%|█████████▍| 746/790 [04:43<00:12,  3.40it/s] 95%|█████████▍| 747/790 [04:43<00:12,  3.41it/s] 95%|█████████▍| 748/790 [04:43<00:12,  3.40it/s] 95%|█████████▍| 749/790 [04:44<00:12,  3.40it/s] 95%|█████████▍| 750/790 [04:44<00:11,  3.40it/s] 95%|█████████▌| 751/790 [04:44<00:11,  3.40it/s] 95%|█████████▌| 752/790 [04:45<00:11,  3.39it/s] 95%|█████████▌| 753/790 [04:45<00:10,  3.40it/s] 95%|█████████▌| 754/790 [04:45<00:10,  3.40it/s] 96%|█████████▌| 755/790 [04:45<00:10,  3.40it/s] 96%|█████████▌| 756/790 [04:46<00:09,  3.40it/s] 96%|█████████▌| 757/790 [04:46<00:09,  3.40it/s] 96%|█████████▌| 758/790 [04:46<00:09,  3.40it/s] 96%|█████████▌| 759/790 [04:47<00:09,  3.40it/s] 96%|█████████▌| 760/790 [04:47<00:08,  3.40it/s] 96%|█████████▋| 761/790 [04:47<00:08,  3.40it/s] 96%|█████████▋| 762/790 [04:47<00:08,  3.40it/s] 97%|█████████▋| 763/790 [04:48<00:07,  3.39it/s] 97%|█████████▋| 764/790 [04:48<00:07,  3.39it/s] 97%|█████████▋| 765/790 [04:48<00:07,  3.40it/s] 97%|█████████▋| 766/790 [04:49<00:07,  3.40it/s] 97%|█████████▋| 767/790 [04:49<00:06,  3.40it/s] 97%|█████████▋| 768/790 [04:49<00:06,  3.40it/s] 97%|█████████▋| 769/790 [04:50<00:06,  3.40it/s] 97%|█████████▋| 770/790 [04:50<00:05,  3.40it/s] 98%|█████████▊| 771/790 [04:50<00:05,  3.40it/s] 98%|█████████▊| 772/790 [04:50<00:05,  3.33it/s] 98%|█████████▊| 773/790 [04:51<00:05,  3.36it/s] 98%|█████████▊| 774/790 [04:51<00:04,  3.37it/s] 98%|█████████▊| 775/790 [04:51<00:04,  3.38it/s] 98%|█████████▊| 776/790 [04:52<00:04,  3.39it/s] 98%|█████████▊| 777/790 [04:52<00:03,  3.39it/s] 98%|█████████▊| 778/790 [04:52<00:03,  3.40it/s] 99%|█████████▊| 779/790 [04:52<00:03,  3.38it/s] 99%|█████████▊| 780/790 [04:53<00:02,  3.39it/s] 99%|█████████▉| 781/790 [04:53<00:02,  3.39it/s] 99%|█████████▉| 782/790 [04:53<00:02,  3.40it/s] 99%|█████████▉| 783/790 [04:54<00:02,  3.40it/s] 99%|█████████▉| 784/790 [04:54<00:01,  3.40it/s] 99%|█████████▉| 785/790 [04:54<00:01,  3.40it/s] 99%|█████████▉| 786/790 [04:55<00:01,  3.40it/s]100%|█████████▉| 787/790 [04:55<00:00,  3.40it/s]100%|█████████▉| 788/790 [04:55<00:00,  3.40it/s]100%|█████████▉| 789/790 [04:55<00:00,  3.40it/s]100%|██████████| 790/790 [04:56<00:00,  3.71it/s][INFO|trainer.py:2140] 2023-08-28 13:53:56,923 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:53:56,923 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 13:53:56,923 >>   Batch size = 8
{'eval_loss': 0.9551032185554504, 'eval_runtime': 9.9462, 'eval_samples_per_second': 346.464, 'eval_steps_per_second': 43.333, 'epoch': 4.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 54.77it/s][A
  3%|▎         | 12/431 [00:00<00:08, 46.97it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.50it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.66it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.15it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.76it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.54it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.43it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.50it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.50it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.55it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.54it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.34it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.35it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.21it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.19it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.29it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.22it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.41it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.48it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.45it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.22it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.17it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.26it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.33it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.24it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.40it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.37it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.47it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.36it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.31it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.28it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.32it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.21it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.31it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.29it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.43it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.37it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.31it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.24it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.20it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.34it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.26it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.35it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.40it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.38it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.40it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.25it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.19it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.27it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.23it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.26it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.30it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.37it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.40it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.40it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.27it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.28it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.30it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.30it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.38it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.33it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.35it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.45it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.32it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.32it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.27it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.28it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.35it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.37it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.32it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.36it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.36it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.32it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.31it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.37it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.39it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.36it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.36it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.38it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.31it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.40it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.23it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.15it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.27it/s][A                                                 
                                                 [A100%|██████████| 790/790 [05:06<00:00,  3.71it/s]
100%|██████████| 431/431 [00:09<00:00, 43.27it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:54:06,896 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-790
[INFO|configuration_utils.py:351] 2023-08-28 13:54:06,917 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-790/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:54:08,851 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-790/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:54:08,876 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-790/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:54:08,885 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-790/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:54:12,692 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:54:12,695 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-316 (score: 0.944346010684967).
                                                 100%|██████████| 790/790 [05:13<00:00,  3.71it/s]100%|██████████| 790/790 [05:13<00:00,  2.52it/s]
[INFO|trainer.py:1894] 2023-08-28 13:54:14,438 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 13:54:14,454 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:54:16,824 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:54:16,842 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:54:16,852 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:54:17,047 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:17,048 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:17,048 >>   train_loss               =     0.8007
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:17,048 >>   train_runtime            = 0:05:13.64
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:17,048 >>   train_samples            =      10090
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:17,048 >>   train_samples_per_second =    160.852
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:17,048 >>   train_steps_per_second   =      2.519
{'eval_loss': 0.9569603204727173, 'eval_runtime': 9.943, 'eval_samples_per_second': 346.575, 'eval_steps_per_second': 43.347, 'epoch': 5.0}
{'train_runtime': 313.6424, 'train_samples_per_second': 160.852, 'train_steps_per_second': 2.519, 'train_loss': 0.8006822223904767, 'epoch': 5.0}
08/28/2023 13:54:17 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:54:17,087 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:54:17,088 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 13:54:17,088 >>   Batch size = 8
  0%|          | 0/431 [00:00<?, ?it/s]  1%|▏         | 6/431 [00:00<00:07, 55.57it/s]  3%|▎         | 12/431 [00:00<00:08, 48.10it/s]  4%|▍         | 17/431 [00:00<00:08, 46.22it/s]  5%|▌         | 22/431 [00:00<00:08, 45.55it/s]  6%|▋         | 27/431 [00:00<00:08, 45.04it/s]  7%|▋         | 32/431 [00:00<00:08, 44.59it/s]  9%|▊         | 37/431 [00:00<00:08, 44.35it/s] 10%|▉         | 42/431 [00:00<00:08, 43.96it/s] 11%|█         | 47/431 [00:01<00:08, 43.37it/s] 12%|█▏        | 52/431 [00:01<00:08, 43.35it/s] 13%|█▎        | 57/431 [00:01<00:08, 43.34it/s] 14%|█▍        | 62/431 [00:01<00:08, 43.60it/s] 16%|█▌        | 67/431 [00:01<00:08, 43.76it/s] 17%|█▋        | 72/431 [00:01<00:08, 43.78it/s] 18%|█▊        | 77/431 [00:01<00:08, 43.84it/s] 19%|█▉        | 82/431 [00:01<00:07, 43.75it/s] 20%|██        | 87/431 [00:01<00:07, 43.41it/s] 21%|██▏       | 92/431 [00:02<00:07, 43.26it/s] 23%|██▎       | 97/431 [00:02<00:07, 43.20it/s] 24%|██▎       | 102/431 [00:02<00:07, 43.35it/s] 25%|██▍       | 107/431 [00:02<00:07, 43.56it/s] 26%|██▌       | 112/431 [00:02<00:07, 43.65it/s] 27%|██▋       | 117/431 [00:02<00:07, 43.77it/s] 28%|██▊       | 122/431 [00:02<00:07, 43.77it/s] 29%|██▉       | 127/431 [00:02<00:06, 43.59it/s] 31%|███       | 132/431 [00:03<00:06, 43.29it/s] 32%|███▏      | 137/431 [00:03<00:06, 43.30it/s] 33%|███▎      | 142/431 [00:03<00:06, 43.33it/s] 34%|███▍      | 147/431 [00:03<00:06, 43.44it/s] 35%|███▌      | 152/431 [00:03<00:06, 43.52it/s] 36%|███▋      | 157/431 [00:03<00:06, 43.67it/s] 38%|███▊      | 162/431 [00:03<00:06, 43.82it/s] 39%|███▊      | 167/431 [00:03<00:06, 43.76it/s] 40%|███▉      | 172/431 [00:03<00:05, 43.44it/s] 41%|████      | 177/431 [00:04<00:05, 43.40it/s] 42%|████▏     | 182/431 [00:04<00:05, 43.27it/s] 43%|████▎     | 187/431 [00:04<00:05, 43.38it/s] 45%|████▍     | 192/431 [00:04<00:05, 43.43it/s] 46%|████▌     | 197/431 [00:04<00:05, 43.50it/s] 47%|████▋     | 202/431 [00:04<00:05, 43.72it/s] 48%|████▊     | 207/431 [00:04<00:05, 43.68it/s] 49%|████▉     | 212/431 [00:04<00:05, 43.63it/s] 50%|█████     | 217/431 [00:04<00:04, 43.52it/s] 52%|█████▏    | 222/431 [00:05<00:04, 43.41it/s] 53%|█████▎    | 227/431 [00:05<00:04, 43.35it/s] 54%|█████▍    | 232/431 [00:05<00:04, 43.36it/s] 55%|█████▍    | 237/431 [00:05<00:04, 43.43it/s] 56%|█████▌    | 242/431 [00:05<00:04, 43.51it/s] 57%|█████▋    | 247/431 [00:05<00:04, 43.63it/s] 58%|█████▊    | 252/431 [00:05<00:04, 43.73it/s] 60%|█████▉    | 257/431 [00:05<00:03, 43.57it/s] 61%|██████    | 262/431 [00:05<00:03, 43.46it/s] 62%|██████▏   | 267/431 [00:06<00:03, 43.36it/s] 63%|██████▎   | 272/431 [00:06<00:03, 43.38it/s] 64%|██████▍   | 277/431 [00:06<00:03, 43.49it/s] 65%|██████▌   | 282/431 [00:06<00:03, 43.52it/s] 67%|██████▋   | 287/431 [00:06<00:03, 43.50it/s] 68%|██████▊   | 292/431 [00:06<00:03, 43.67it/s] 69%|██████▉   | 297/431 [00:06<00:03, 43.62it/s] 70%|███████   | 302/431 [00:06<00:02, 43.59it/s] 71%|███████   | 307/431 [00:07<00:02, 43.36it/s] 72%|███████▏  | 312/431 [00:07<00:02, 43.40it/s] 74%|███████▎  | 317/431 [00:07<00:02, 43.39it/s] 75%|███████▍  | 322/431 [00:07<00:02, 43.45it/s] 76%|███████▌  | 327/431 [00:07<00:02, 43.52it/s] 77%|███████▋  | 332/431 [00:07<00:02, 43.46it/s] 78%|███████▊  | 337/431 [00:07<00:02, 43.59it/s] 79%|███████▉  | 342/431 [00:07<00:02, 43.58it/s] 81%|████████  | 347/431 [00:07<00:01, 43.52it/s] 82%|████████▏ | 352/431 [00:08<00:01, 43.46it/s] 83%|████████▎ | 357/431 [00:08<00:01, 43.50it/s] 84%|████████▍ | 362/431 [00:08<00:01, 43.40it/s] 85%|████████▌ | 367/431 [00:08<00:01, 43.44it/s] 86%|████████▋ | 372/431 [00:08<00:01, 43.42it/s] 87%|████████▋ | 377/431 [00:08<00:01, 43.53it/s] 89%|████████▊ | 382/431 [00:08<00:01, 43.54it/s] 90%|████████▉ | 387/431 [00:08<00:01, 43.55it/s] 91%|█████████ | 392/431 [00:08<00:00, 43.42it/s] 92%|█████████▏| 397/431 [00:09<00:00, 43.44it/s] 93%|█████████▎| 402/431 [00:09<00:00, 43.48it/s] 94%|█████████▍| 407/431 [00:09<00:00, 43.44it/s] 96%|█████████▌| 412/431 [00:09<00:00, 43.49it/s] 97%|█████████▋| 417/431 [00:09<00:00, 43.46it/s] 98%|█████████▊| 422/431 [00:09<00:00, 43.53it/s] 99%|█████████▉| 427/431 [00:09<00:00, 43.59it/s]100%|██████████| 431/431 [00:09<00:00, 43.62it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:54:26,987 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:26,987 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:26,987 >>   eval_loss               =     0.9443
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:26,987 >>   eval_runtime            = 0:00:09.89
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:26,988 >>   eval_samples            =       3446
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:26,988 >>   eval_samples_per_second =    348.093
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:26,988 >>   eval_steps_per_second   =     43.537
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:54:26,988 >>   perplexity              =     2.5711
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:33,627 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:33,633 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:33,633 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:33,633 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:33,633 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:54:34,256 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:54:34,256 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:54:34,860 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:54:35,938 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:54:35,938 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:38,745 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:38,749 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:38,750 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:38,750 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:54:38,750 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:54:39,379 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:54:39,380 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:54:39,953 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:54:40,139 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:54:40,139 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-474
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-790
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-632
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-158
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-316
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:05,  1.45it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.50it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.56it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:17,  1.47it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.48it/s]Extractor Predicting: 32it [00:21,  1.49it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:22,  1.56it/s]Extractor Predicting: 35it [00:23,  1.58it/s]Extractor Predicting: 36it [00:23,  1.60it/s]Extractor Predicting: 37it [00:24,  1.61it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:25,  1.60it/s]Extractor Predicting: 40it [00:26,  1.59it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:27,  1.61it/s]Extractor Predicting: 43it [00:28,  1.58it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:29,  1.59it/s]Extractor Predicting: 46it [00:29,  1.60it/s]Extractor Predicting: 47it [00:30,  1.64it/s]Extractor Predicting: 48it [00:31,  1.63it/s]Extractor Predicting: 49it [00:31,  1.64it/s]Extractor Predicting: 50it [00:32,  1.61it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.57it/s]Extractor Predicting: 53it [00:34,  1.56it/s]Extractor Predicting: 54it [00:34,  1.59it/s]Extractor Predicting: 55it [00:35,  1.59it/s]Extractor Predicting: 56it [00:36,  1.61it/s]Extractor Predicting: 57it [00:36,  1.64it/s]Extractor Predicting: 58it [00:37,  1.61it/s]Extractor Predicting: 59it [00:37,  1.66it/s]Extractor Predicting: 60it [00:38,  1.62it/s]Extractor Predicting: 61it [00:39,  1.63it/s]Extractor Predicting: 62it [00:39,  1.68it/s]Extractor Predicting: 63it [00:40,  1.68it/s]Extractor Predicting: 64it [00:40,  1.71it/s]Extractor Predicting: 65it [00:41,  1.71it/s]Extractor Predicting: 66it [00:42,  1.73it/s]Extractor Predicting: 67it [00:42,  1.72it/s]Extractor Predicting: 68it [00:43,  1.72it/s]Extractor Predicting: 69it [00:43,  1.74it/s]Extractor Predicting: 70it [00:44,  1.72it/s]Extractor Predicting: 71it [00:44,  1.69it/s]Extractor Predicting: 72it [00:45,  1.72it/s]Extractor Predicting: 73it [00:46,  1.66it/s]Extractor Predicting: 74it [00:46,  1.66it/s]Extractor Predicting: 75it [00:47,  1.70it/s]Extractor Predicting: 76it [00:47,  1.71it/s]Extractor Predicting: 77it [00:48,  1.71it/s]Extractor Predicting: 78it [00:49,  1.73it/s]Extractor Predicting: 79it [00:49,  1.73it/s]Extractor Predicting: 80it [00:50,  1.71it/s]Extractor Predicting: 81it [00:50,  1.67it/s]Extractor Predicting: 82it [00:51,  1.63it/s]Extractor Predicting: 83it [00:52,  1.65it/s]Extractor Predicting: 84it [00:52,  1.65it/s]Extractor Predicting: 85it [00:53,  1.60it/s]Extractor Predicting: 86it [00:54,  1.59it/s]Extractor Predicting: 87it [00:54,  1.60it/s]Extractor Predicting: 88it [00:55,  1.57it/s]Extractor Predicting: 89it [00:56,  1.43it/s]Extractor Predicting: 90it [00:56,  1.47it/s]Extractor Predicting: 91it [00:57,  1.51it/s]Extractor Predicting: 92it [00:58,  1.51it/s]Extractor Predicting: 93it [00:58,  1.51it/s]Extractor Predicting: 94it [00:59,  1.54it/s]Extractor Predicting: 95it [01:00,  1.53it/s]Extractor Predicting: 96it [01:00,  1.56it/s]Extractor Predicting: 97it [01:01,  1.55it/s]Extractor Predicting: 98it [01:01,  1.55it/s]Extractor Predicting: 99it [01:02,  1.57it/s]Extractor Predicting: 100it [01:03,  1.58it/s]Extractor Predicting: 101it [01:03,  1.57it/s]Extractor Predicting: 102it [01:04,  1.59it/s]Extractor Predicting: 103it [01:05,  1.59it/s]Extractor Predicting: 104it [01:05,  1.61it/s]Extractor Predicting: 105it [01:06,  1.63it/s]Extractor Predicting: 106it [01:06,  1.61it/s]Extractor Predicting: 107it [01:07,  1.59it/s]Extractor Predicting: 108it [01:08,  1.59it/s]Extractor Predicting: 109it [01:08,  1.58it/s]Extractor Predicting: 110it [01:09,  1.56it/s]Extractor Predicting: 111it [01:10,  1.56it/s]Extractor Predicting: 112it [01:10,  1.57it/s]Extractor Predicting: 113it [01:11,  1.58it/s]Extractor Predicting: 114it [01:11,  1.58it/s]Extractor Predicting: 115it [01:12,  1.58it/s]Extractor Predicting: 116it [01:13,  1.61it/s]Extractor Predicting: 117it [01:13,  1.61it/s]Extractor Predicting: 118it [01:14,  1.59it/s]Extractor Predicting: 119it [01:15,  1.60it/s]Extractor Predicting: 120it [01:15,  1.59it/s]Extractor Predicting: 121it [01:16,  1.58it/s]Extractor Predicting: 122it [01:17,  1.56it/s]Extractor Predicting: 123it [01:17,  1.56it/s]Extractor Predicting: 124it [01:18,  1.54it/s]Extractor Predicting: 125it [01:18,  1.56it/s]Extractor Predicting: 126it [01:19,  1.56it/s]Extractor Predicting: 127it [01:20,  1.55it/s]Extractor Predicting: 128it [01:20,  1.52it/s]Extractor Predicting: 129it [01:21,  1.54it/s]Extractor Predicting: 130it [01:22,  1.57it/s]Extractor Predicting: 131it [01:22,  1.55it/s]Extractor Predicting: 132it [01:23,  1.57it/s]Extractor Predicting: 133it [01:24,  1.59it/s]Extractor Predicting: 134it [01:24,  1.57it/s]Extractor Predicting: 135it [01:25,  1.55it/s]Extractor Predicting: 136it [01:26,  1.56it/s]Extractor Predicting: 137it [01:26,  1.58it/s]Extractor Predicting: 138it [01:27,  1.60it/s]Extractor Predicting: 139it [01:27,  1.60it/s]Extractor Predicting: 140it [01:28,  1.61it/s]Extractor Predicting: 141it [01:29,  1.67it/s]Extractor Predicting: 141it [01:29,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:56:18,217 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:56:18,222 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:56:18,222 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:56:18,222 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:56:18,222 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:56:18,839 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:56:18,840 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:56:19,409 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:56:20,464 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:56:20,465 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:56:23,317 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:56:23,321 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:56:23,321 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:56:23,321 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:56:23,321 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:56:23,958 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:56:23,959 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:56:24,528 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:56:24,702 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:56:24,702 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3032721468475658,
  "recall": 0.11027278003482298,
  "score": 0.16173653968929558,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.70it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.61it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.62it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:14,  1.59it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:15,  1.62it/s]Extractor Predicting: 27it [00:16,  1.60it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:17,  1.58it/s]Extractor Predicting: 30it [00:18,  1.54it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:19,  1.52it/s]Extractor Predicting: 33it [00:20,  1.53it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:21,  1.56it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.62it/s]Extractor Predicting: 39it [00:24,  1.61it/s]Extractor Predicting: 40it [00:24,  1.62it/s]Extractor Predicting: 41it [00:25,  1.61it/s]Extractor Predicting: 42it [00:26,  1.60it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:27,  1.59it/s]Extractor Predicting: 45it [00:28,  1.56it/s]Extractor Predicting: 46it [00:28,  1.60it/s]Extractor Predicting: 47it [00:29,  1.58it/s]Extractor Predicting: 48it [00:29,  1.59it/s]Extractor Predicting: 49it [00:30,  1.58it/s]Extractor Predicting: 50it [00:31,  1.57it/s]Extractor Predicting: 51it [00:31,  1.57it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:33,  1.58it/s]Extractor Predicting: 54it [00:33,  1.59it/s]Extractor Predicting: 55it [00:34,  1.55it/s]Extractor Predicting: 56it [00:34,  1.57it/s]Extractor Predicting: 57it [00:35,  1.60it/s]Extractor Predicting: 58it [00:36,  1.62it/s]Extractor Predicting: 59it [00:36,  1.62it/s]Extractor Predicting: 60it [00:37,  1.63it/s]Extractor Predicting: 61it [00:38,  1.61it/s]Extractor Predicting: 62it [00:38,  1.61it/s]Extractor Predicting: 63it [00:39,  1.57it/s]Extractor Predicting: 64it [00:39,  1.59it/s]Extractor Predicting: 65it [00:40,  1.58it/s]Extractor Predicting: 66it [00:41,  1.55it/s]Extractor Predicting: 67it [00:42,  1.45it/s]Extractor Predicting: 68it [00:42,  1.50it/s]Extractor Predicting: 69it [00:43,  1.56it/s]Extractor Predicting: 70it [00:43,  1.57it/s]Extractor Predicting: 71it [00:44,  1.56it/s]Extractor Predicting: 72it [00:45,  1.59it/s]Extractor Predicting: 73it [00:45,  1.55it/s]Extractor Predicting: 74it [00:46,  1.56it/s]Extractor Predicting: 75it [00:47,  1.57it/s]Extractor Predicting: 76it [00:47,  1.59it/s]Extractor Predicting: 77it [00:48,  1.55it/s]Extractor Predicting: 78it [00:49,  1.55it/s]Extractor Predicting: 79it [00:49,  1.57it/s]Extractor Predicting: 80it [00:50,  1.56it/s]Extractor Predicting: 81it [00:50,  1.57it/s]Extractor Predicting: 82it [00:51,  1.56it/s]Extractor Predicting: 83it [00:52,  1.54it/s]Extractor Predicting: 84it [00:52,  1.56it/s]Extractor Predicting: 85it [00:53,  1.57it/s]Extractor Predicting: 86it [00:54,  1.56it/s]Extractor Predicting: 87it [00:54,  1.55it/s]Extractor Predicting: 88it [00:55,  1.56it/s]Extractor Predicting: 89it [00:56,  1.54it/s]Extractor Predicting: 90it [00:56,  1.54it/s]Extractor Predicting: 91it [00:57,  1.55it/s]Extractor Predicting: 92it [00:58,  1.55it/s]Extractor Predicting: 93it [00:58,  1.54it/s]Extractor Predicting: 94it [00:59,  1.51it/s]Extractor Predicting: 95it [01:00,  1.50it/s]Extractor Predicting: 96it [01:00,  1.52it/s]Extractor Predicting: 97it [01:01,  1.52it/s]Extractor Predicting: 98it [01:01,  1.54it/s]Extractor Predicting: 99it [01:02,  1.52it/s]Extractor Predicting: 100it [01:03,  1.52it/s]Extractor Predicting: 101it [01:03,  1.55it/s]Extractor Predicting: 102it [01:04,  1.55it/s]Extractor Predicting: 103it [01:05,  1.54it/s]Extractor Predicting: 104it [01:05,  1.53it/s]Extractor Predicting: 105it [01:06,  1.53it/s]Extractor Predicting: 106it [01:07,  1.54it/s]Extractor Predicting: 107it [01:07,  1.50it/s]Extractor Predicting: 108it [01:08,  1.51it/s]Extractor Predicting: 109it [01:09,  1.52it/s]Extractor Predicting: 110it [01:09,  1.50it/s]Extractor Predicting: 111it [01:10,  1.50it/s]Extractor Predicting: 112it [01:11,  1.51it/s]Extractor Predicting: 113it [01:11,  1.55it/s]Extractor Predicting: 114it [01:12,  1.57it/s]Extractor Predicting: 115it [01:13,  1.55it/s]Extractor Predicting: 116it [01:13,  1.56it/s]Extractor Predicting: 117it [01:14,  1.60it/s]Extractor Predicting: 118it [01:14,  1.58it/s]Extractor Predicting: 119it [01:15,  1.60it/s]Extractor Predicting: 120it [01:16,  1.60it/s]Extractor Predicting: 121it [01:16,  1.60it/s]Extractor Predicting: 122it [01:17,  1.64it/s]Extractor Predicting: 123it [01:17,  1.63it/s]Extractor Predicting: 124it [01:18,  1.62it/s]Extractor Predicting: 125it [01:19,  1.60it/s]Extractor Predicting: 126it [01:19,  1.58it/s]Extractor Predicting: 127it [01:20,  1.59it/s]Extractor Predicting: 128it [01:21,  1.63it/s]Extractor Predicting: 129it [01:21,  1.58it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:23,  1.57it/s]Extractor Predicting: 132it [01:23,  1.58it/s]Extractor Predicting: 133it [01:24,  1.58it/s]Extractor Predicting: 134it [01:24,  1.57it/s]Extractor Predicting: 135it [01:25,  1.59it/s]Extractor Predicting: 136it [01:26,  1.59it/s]Extractor Predicting: 137it [01:26,  1.59it/s]Extractor Predicting: 138it [01:27,  1.58it/s]Extractor Predicting: 139it [01:28,  1.61it/s]Extractor Predicting: 140it [01:28,  1.60it/s]Extractor Predicting: 141it [01:29,  1.58it/s]Extractor Predicting: 142it [01:29,  1.61it/s]Extractor Predicting: 143it [01:30,  1.61it/s]Extractor Predicting: 144it [01:31,  1.58it/s]Extractor Predicting: 145it [01:31,  1.60it/s]Extractor Predicting: 146it [01:32,  1.59it/s]Extractor Predicting: 147it [01:33,  1.57it/s]Extractor Predicting: 148it [01:33,  1.56it/s]Extractor Predicting: 149it [01:34,  1.56it/s]Extractor Predicting: 150it [01:35,  1.57it/s]Extractor Predicting: 151it [01:35,  1.60it/s]Extractor Predicting: 152it [01:36,  1.61it/s]Extractor Predicting: 153it [01:37,  1.42it/s]Extractor Predicting: 154it [01:37,  1.45it/s]Extractor Predicting: 155it [01:38,  1.48it/s]Extractor Predicting: 156it [01:39,  1.50it/s]Extractor Predicting: 157it [01:39,  1.51it/s]Extractor Predicting: 158it [01:40,  1.50it/s]Extractor Predicting: 159it [01:41,  1.52it/s]Extractor Predicting: 160it [01:41,  1.53it/s]Extractor Predicting: 161it [01:42,  1.56it/s]Extractor Predicting: 162it [01:42,  1.58it/s]Extractor Predicting: 163it [01:43,  1.58it/s]Extractor Predicting: 164it [01:44,  1.58it/s]Extractor Predicting: 165it [01:44,  1.56it/s]Extractor Predicting: 166it [01:45,  1.57it/s]Extractor Predicting: 167it [01:46,  1.59it/s]Extractor Predicting: 168it [01:46,  1.58it/s]Extractor Predicting: 169it [01:47,  1.57it/s]Extractor Predicting: 170it [01:48,  1.56it/s]Extractor Predicting: 171it [01:48,  1.53it/s]Extractor Predicting: 172it [01:49,  1.52it/s]Extractor Predicting: 173it [01:50,  1.50it/s]Extractor Predicting: 174it [01:50,  1.47it/s]Extractor Predicting: 175it [01:51,  1.45it/s]Extractor Predicting: 176it [01:52,  1.48it/s]Extractor Predicting: 177it [01:52,  1.50it/s]Extractor Predicting: 178it [01:53,  1.52it/s]Extractor Predicting: 179it [01:54,  1.56it/s]Extractor Predicting: 180it [01:54,  1.54it/s]Extractor Predicting: 181it [01:55,  1.56it/s]Extractor Predicting: 182it [01:55,  1.57it/s]Extractor Predicting: 183it [01:56,  1.57it/s]Extractor Predicting: 184it [01:57,  1.59it/s]Extractor Predicting: 185it [01:57,  1.61it/s]Extractor Predicting: 186it [01:58,  1.63it/s]Extractor Predicting: 187it [01:59,  1.62it/s]Extractor Predicting: 188it [01:59,  1.63it/s]Extractor Predicting: 189it [02:00,  1.63it/s]Extractor Predicting: 190it [02:00,  1.63it/s]Extractor Predicting: 191it [02:01,  1.58it/s]Extractor Predicting: 192it [02:02,  1.56it/s]Extractor Predicting: 193it [02:02,  1.57it/s]Extractor Predicting: 194it [02:03,  1.61it/s]Extractor Predicting: 195it [02:04,  1.61it/s]Extractor Predicting: 196it [02:04,  1.61it/s]Extractor Predicting: 197it [02:05,  1.62it/s]Extractor Predicting: 198it [02:05,  1.62it/s]Extractor Predicting: 199it [02:06,  1.60it/s]Extractor Predicting: 200it [02:07,  1.58it/s]Extractor Predicting: 201it [02:07,  1.59it/s]Extractor Predicting: 202it [02:08,  1.61it/s]Extractor Predicting: 203it [02:08,  1.64it/s]Extractor Predicting: 204it [02:09,  1.64it/s]Extractor Predicting: 205it [02:10,  1.62it/s]Extractor Predicting: 206it [02:10,  1.61it/s]Extractor Predicting: 207it [02:11,  1.58it/s]Extractor Predicting: 208it [02:12,  1.58it/s]Extractor Predicting: 209it [02:12,  1.59it/s]Extractor Predicting: 210it [02:13,  1.59it/s]Extractor Predicting: 211it [02:14,  1.60it/s]Extractor Predicting: 212it [02:14,  1.58it/s]Extractor Predicting: 213it [02:15,  1.61it/s]Extractor Predicting: 214it [02:15,  1.60it/s]Extractor Predicting: 215it [02:16,  1.62it/s]Extractor Predicting: 216it [02:17,  1.62it/s]Extractor Predicting: 217it [02:17,  1.56it/s]Extractor Predicting: 218it [02:18,  1.58it/s]Extractor Predicting: 219it [02:19,  1.61it/s]Extractor Predicting: 220it [02:19,  1.62it/s]Extractor Predicting: 221it [02:20,  1.61it/s]Extractor Predicting: 222it [02:20,  1.62it/s]Extractor Predicting: 223it [02:21,  1.64it/s]Extractor Predicting: 224it [02:22,  1.63it/s]Extractor Predicting: 225it [02:22,  1.61it/s]Extractor Predicting: 226it [02:23,  1.57it/s]Extractor Predicting: 227it [02:24,  1.57it/s]Extractor Predicting: 228it [02:24,  1.53it/s]Extractor Predicting: 229it [02:25,  1.53it/s]Extractor Predicting: 230it [02:25,  1.56it/s]Extractor Predicting: 231it [02:26,  1.54it/s]Extractor Predicting: 232it [02:27,  1.55it/s]Extractor Predicting: 233it [02:27,  1.57it/s]Extractor Predicting: 234it [02:28,  1.57it/s]Extractor Predicting: 235it [02:29,  1.54it/s]Extractor Predicting: 236it [02:29,  1.57it/s]Extractor Predicting: 237it [02:30,  1.61it/s]Extractor Predicting: 238it [02:31,  1.58it/s]Extractor Predicting: 239it [02:31,  1.55it/s]Extractor Predicting: 240it [02:32,  1.53it/s]Extractor Predicting: 241it [02:33,  1.54it/s]Extractor Predicting: 242it [02:33,  1.57it/s]Extractor Predicting: 243it [02:34,  1.57it/s]Extractor Predicting: 244it [02:34,  1.54it/s]Extractor Predicting: 245it [02:35,  1.56it/s]Extractor Predicting: 246it [02:36,  1.57it/s]Extractor Predicting: 247it [02:36,  1.59it/s]Extractor Predicting: 248it [02:37,  1.53it/s]Extractor Predicting: 249it [02:38,  1.55it/s]Extractor Predicting: 250it [02:38,  1.55it/s]Extractor Predicting: 251it [02:39,  1.53it/s]Extractor Predicting: 252it [02:40,  1.52it/s]Extractor Predicting: 253it [02:40,  1.52it/s]Extractor Predicting: 254it [02:41,  1.56it/s]Extractor Predicting: 255it [02:42,  1.57it/s]Extractor Predicting: 256it [02:42,  1.60it/s]Extractor Predicting: 257it [02:43,  1.63it/s]Extractor Predicting: 258it [02:43,  1.60it/s]Extractor Predicting: 259it [02:44,  1.41it/s]Extractor Predicting: 260it [02:45,  1.44it/s]Extractor Predicting: 261it [02:46,  1.47it/s]Extractor Predicting: 262it [02:46,  1.50it/s]Extractor Predicting: 263it [02:47,  1.47it/s]Extractor Predicting: 264it [02:48,  1.50it/s]Extractor Predicting: 265it [02:48,  1.52it/s]Extractor Predicting: 266it [02:49,  1.54it/s]Extractor Predicting: 267it [02:49,  1.55it/s]Extractor Predicting: 268it [02:50,  1.58it/s]Extractor Predicting: 269it [02:51,  1.61it/s]Extractor Predicting: 270it [02:51,  1.63it/s]Extractor Predicting: 271it [02:52,  1.60it/s]Extractor Predicting: 272it [02:53,  1.62it/s]Extractor Predicting: 273it [02:53,  1.60it/s]Extractor Predicting: 274it [02:54,  1.60it/s]Extractor Predicting: 275it [02:54,  1.60it/s]Extractor Predicting: 276it [02:55,  1.59it/s]Extractor Predicting: 277it [02:56,  1.57it/s]Extractor Predicting: 278it [02:56,  1.61it/s]Extractor Predicting: 279it [02:57,  1.56it/s]Extractor Predicting: 280it [02:58,  1.56it/s]Extractor Predicting: 281it [02:58,  1.57it/s]Extractor Predicting: 282it [02:59,  1.58it/s]Extractor Predicting: 283it [02:59,  1.59it/s]Extractor Predicting: 284it [03:00,  1.56it/s]Extractor Predicting: 285it [03:01,  1.56it/s]Extractor Predicting: 286it [03:01,  1.58it/s]Extractor Predicting: 287it [03:02,  1.60it/s]Extractor Predicting: 288it [03:03,  1.59it/s]Extractor Predicting: 289it [03:03,  1.54it/s]Extractor Predicting: 290it [03:04,  1.60it/s]Extractor Predicting: 291it [03:05,  1.58it/s]Extractor Predicting: 292it [03:05,  1.57it/s]Extractor Predicting: 293it [03:06,  1.54it/s]Extractor Predicting: 294it [03:07,  1.51it/s]Extractor Predicting: 295it [03:07,  1.55it/s]Extractor Predicting: 296it [03:08,  1.53it/s]Extractor Predicting: 297it [03:08,  1.56it/s]Extractor Predicting: 298it [03:09,  1.51it/s]Extractor Predicting: 299it [03:10,  1.51it/s]Extractor Predicting: 300it [03:10,  1.53it/s]Extractor Predicting: 301it [03:11,  1.54it/s]Extractor Predicting: 302it [03:12,  1.53it/s]Extractor Predicting: 303it [03:12,  1.51it/s]Extractor Predicting: 304it [03:13,  1.49it/s]Extractor Predicting: 305it [03:14,  1.51it/s]Extractor Predicting: 306it [03:15,  1.49it/s]Extractor Predicting: 307it [03:15,  1.48it/s]Extractor Predicting: 308it [03:16,  1.51it/s]Extractor Predicting: 309it [03:16,  1.51it/s]Extractor Predicting: 310it [03:17,  1.48it/s]Extractor Predicting: 311it [03:18,  1.47it/s]Extractor Predicting: 312it [03:19,  1.48it/s]Extractor Predicting: 313it [03:19,  1.49it/s]Extractor Predicting: 314it [03:20,  1.47it/s]Extractor Predicting: 315it [03:21,  1.52it/s]Extractor Predicting: 316it [03:21,  1.50it/s]Extractor Predicting: 317it [03:22,  1.48it/s]Extractor Predicting: 318it [03:23,  1.49it/s]Extractor Predicting: 319it [03:23,  1.50it/s]Extractor Predicting: 320it [03:24,  1.59it/s]Extractor Predicting: 321it [03:24,  1.62it/s]Extractor Predicting: 322it [03:25,  1.70it/s]Extractor Predicting: 323it [03:25,  1.74it/s]Extractor Predicting: 324it [03:26,  1.75it/s]Extractor Predicting: 325it [03:27,  1.77it/s]Extractor Predicting: 326it [03:27,  1.77it/s]Extractor Predicting: 327it [03:28,  1.76it/s]Extractor Predicting: 328it [03:28,  1.78it/s]Extractor Predicting: 329it [03:29,  1.81it/s]Extractor Predicting: 330it [03:29,  1.81it/s]Extractor Predicting: 331it [03:30,  1.85it/s]Extractor Predicting: 332it [03:30,  1.82it/s]Extractor Predicting: 333it [03:31,  1.85it/s]Extractor Predicting: 334it [03:31,  1.84it/s]Extractor Predicting: 335it [03:32,  1.83it/s]Extractor Predicting: 336it [03:33,  1.80it/s]Extractor Predicting: 337it [03:33,  1.85it/s]Extractor Predicting: 338it [03:34,  1.79it/s]Extractor Predicting: 339it [03:34,  1.78it/s]Extractor Predicting: 340it [03:35,  1.84it/s]Extractor Predicting: 341it [03:35,  1.81it/s]Extractor Predicting: 342it [03:36,  1.82it/s]Extractor Predicting: 343it [03:36,  1.82it/s]Extractor Predicting: 344it [03:37,  1.78it/s]Extractor Predicting: 345it [03:38,  1.81it/s]Extractor Predicting: 346it [03:38,  1.80it/s]Extractor Predicting: 347it [03:39,  1.78it/s]Extractor Predicting: 348it [03:39,  1.72it/s]Extractor Predicting: 349it [03:40,  1.67it/s]Extractor Predicting: 350it [03:41,  1.63it/s]Extractor Predicting: 351it [03:41,  1.60it/s]Extractor Predicting: 352it [03:42,  1.60it/s]Extractor Predicting: 353it [03:42,  1.62it/s]Extractor Predicting: 354it [03:43,  1.60it/s]Extractor Predicting: 355it [03:44,  1.59it/s]Extractor Predicting: 356it [03:44,  1.55it/s]Extractor Predicting: 357it [03:45,  1.52it/s]Extractor Predicting: 358it [03:46,  1.36it/s]Extractor Predicting: 359it [03:47,  1.42it/s]Extractor Predicting: 360it [03:47,  1.48it/s]Extractor Predicting: 361it [03:48,  1.50it/s]Extractor Predicting: 362it [03:49,  1.55it/s]Extractor Predicting: 363it [03:49,  1.54it/s]Extractor Predicting: 364it [03:50,  1.54it/s]Extractor Predicting: 365it [03:50,  1.54it/s]Extractor Predicting: 366it [03:51,  1.55it/s]Extractor Predicting: 367it [03:52,  1.55it/s]Extractor Predicting: 368it [03:52,  1.56it/s]Extractor Predicting: 369it [03:53,  1.56it/s]Extractor Predicting: 370it [03:54,  1.55it/s]Extractor Predicting: 371it [03:54,  1.55it/s]Extractor Predicting: 372it [03:55,  1.56it/s]Extractor Predicting: 373it [03:56,  1.54it/s]Extractor Predicting: 374it [03:56,  1.56it/s]Extractor Predicting: 375it [03:57,  1.57it/s]Extractor Predicting: 376it [03:58,  1.55it/s]Extractor Predicting: 377it [03:58,  1.56it/s]Extractor Predicting: 378it [03:59,  1.53it/s]Extractor Predicting: 379it [04:00,  1.53it/s]Extractor Predicting: 380it [04:00,  1.53it/s]Extractor Predicting: 381it [04:01,  1.51it/s]Extractor Predicting: 382it [04:02,  1.50it/s]Extractor Predicting: 383it [04:02,  1.51it/s]Extractor Predicting: 384it [04:03,  1.50it/s]Extractor Predicting: 385it [04:04,  1.50it/s]Extractor Predicting: 386it [04:04,  1.52it/s]Extractor Predicting: 387it [04:05,  1.51it/s]Extractor Predicting: 388it [04:06,  1.50it/s]Extractor Predicting: 389it [04:06,  1.53it/s]Extractor Predicting: 390it [04:07,  1.54it/s]Extractor Predicting: 391it [04:07,  1.54it/s]Extractor Predicting: 392it [04:08,  1.50it/s]Extractor Predicting: 393it [04:09,  1.54it/s]Extractor Predicting: 394it [04:09,  1.53it/s]Extractor Predicting: 395it [04:10,  1.51it/s]Extractor Predicting: 396it [04:11,  1.51it/s]Extractor Predicting: 397it [04:11,  1.48it/s]Extractor Predicting: 398it [04:12,  1.49it/s]Extractor Predicting: 399it [04:13,  1.46it/s]Extractor Predicting: 400it [04:14,  1.46it/s]Extractor Predicting: 401it [04:14,  1.47it/s]Extractor Predicting: 402it [04:15,  1.49it/s]Extractor Predicting: 403it [04:16,  1.49it/s]Extractor Predicting: 404it [04:16,  1.54it/s]Extractor Predicting: 405it [04:17,  1.53it/s]Extractor Predicting: 406it [04:17,  1.57it/s]Extractor Predicting: 407it [04:18,  1.56it/s]Extractor Predicting: 408it [04:19,  1.54it/s]Extractor Predicting: 409it [04:19,  1.58it/s]Extractor Predicting: 410it [04:20,  1.57it/s]Extractor Predicting: 411it [04:21,  1.59it/s]Extractor Predicting: 412it [04:21,  1.56it/s]Extractor Predicting: 413it [04:22,  1.60it/s]Extractor Predicting: 414it [04:22,  1.59it/s]Extractor Predicting: 415it [04:23,  1.58it/s]Extractor Predicting: 416it [04:24,  1.56it/s]Extractor Predicting: 417it [04:24,  1.58it/s]Extractor Predicting: 418it [04:25,  1.57it/s]Extractor Predicting: 419it [04:26,  1.55it/s]Extractor Predicting: 420it [04:26,  1.56it/s]Extractor Predicting: 421it [04:27,  1.56it/s]Extractor Predicting: 422it [04:28,  1.54it/s]Extractor Predicting: 423it [04:28,  1.55it/s]Extractor Predicting: 424it [04:29,  1.52it/s]Extractor Predicting: 425it [04:30,  1.54it/s]Extractor Predicting: 426it [04:30,  1.52it/s]Extractor Predicting: 427it [04:31,  1.55it/s]Extractor Predicting: 428it [04:32,  1.54it/s]Extractor Predicting: 429it [04:32,  1.55it/s]Extractor Predicting: 430it [04:32,  1.93it/s]Extractor Predicting: 430it [04:32,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:06,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:06,319 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:06,319 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:06,319 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:06,319 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:01:06,928 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:01:06,929 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:01:07,505 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:01:08,580 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:01:08,580 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:11,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:11,586 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:11,586 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:11,586 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:01:11,586 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:01:12,212 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:01:12,213 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:01:12,761 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:01:12,934 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:01:12,934 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3545521835677276,
  "recall": 0.09302777238298698,
  "score": 0.1473846153846154,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.46it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.32it/s]Extractor Predicting: 5it [00:03,  1.38it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:01:16,983 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:01:16,984 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:01:16,991 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:01:16,992 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:01:16,995 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:01:20,033 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:01:20,036 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:01:20,054 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:01:20,055 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:01:20,062 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:01:20,066 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:01:20,066 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:01:20,066 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:01:20,066 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:01:20,066 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:01:20,066 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.41935483870967744,
  "recall": 0.06435643564356436,
  "score": 0.11158798283261803,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:01:20,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:20,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:21,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:22,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:22,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:23,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:24,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:24,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:25,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:25,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:26,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:27,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:28,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:28,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:29,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:30,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:31,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:31,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:32,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:32,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:33,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:34,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:34,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:35,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<04:59, 15.78s/it][WARNING|generation_utils.py:914] 2023-08-28 14:01:36,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:36,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:37,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:37,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:38,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:39,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:39,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:40,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:40,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:41,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:42,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:42,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:43,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:43,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:44,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:45,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:45,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:46,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:47,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:47,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:48,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:49,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:29<04:20, 14.49s/it][WARNING|generation_utils.py:914] 2023-08-28 14:01:49,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:50,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:50,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:51,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:52,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:52,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:53,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:53,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:54,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:54,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:55,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:56,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:56,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:57,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:58,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:58,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:59,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:01:59,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:00,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:00,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:01,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:02,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:42<03:54, 13.80s/it][WARNING|generation_utils.py:914] 2023-08-28 14:02:02,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:03,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:03,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:04,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:05,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:05,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:06,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:06,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:07,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:08,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:08,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:09,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:09,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:10,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:11,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:11,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:12,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:13,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:13,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:14,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:14,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:55<03:33, 13.37s/it][WARNING|generation_utils.py:914] 2023-08-28 14:02:15,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:16,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:16,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:17,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:17,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:18,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:19,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:19,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:20,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:20,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:21,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:22,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:23,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:23,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:24,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:25,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:25,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:26,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:27,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:27,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:28,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:08<03:21, 13.44s/it][WARNING|generation_utils.py:914] 2023-08-28 14:02:28,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:29,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:30,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:30,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:31,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:32,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:32,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:33,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:34,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:34,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:35,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:35,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:36,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:37,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:37,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:38,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:39,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:39,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:40,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:41,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:41,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:42,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:22<03:11, 13.71s/it][WARNING|generation_utils.py:914] 2023-08-28 14:02:43,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:43,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:44,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:44,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:45,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:46,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:46,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:47,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:47,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:48,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:49,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:49,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:50,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:50,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:51,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:52,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:52,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:53,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:53,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:54,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:55,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:55,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:35<02:55, 13.51s/it][WARNING|generation_utils.py:914] 2023-08-28 14:02:56,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:56,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:57,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:58,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:58,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:59,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:02:59,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:00,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:01,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:01,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:02,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:03,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:03,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:04,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:05,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:05,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:06,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:07,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:08,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:08,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:09,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:10,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:50<02:45, 13.81s/it][WARNING|generation_utils.py:914] 2023-08-28 14:03:10,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:11,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:12,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:12,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:13,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:13,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:14,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:15,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:15,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:16,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:17,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:17,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:18,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:19,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:19,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:20,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:21,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:22,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:22,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:23,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:24,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:24,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:25,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:26,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:26,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:27,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:07<02:43, 14.90s/it][WARNING|generation_utils.py:914] 2023-08-28 14:03:28,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:28,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:29,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:29,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:30,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:30,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:31,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:32,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:32,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:33,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:33,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:34,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:34,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:35,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:36,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:36,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:37,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:38,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:38,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:39,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:39,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:40,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:41,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:21<02:25, 14.58s/it][WARNING|generation_utils.py:914] 2023-08-28 14:03:41,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:42,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:43,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:43,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:44,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:45,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:45,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:46,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:47,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:47,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:48,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:49,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:49,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:50,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:50,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:51,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:52,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:53,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:53,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:54,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:54,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:55,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:35<02:10, 14.50s/it][WARNING|generation_utils.py:914] 2023-08-28 14:03:56,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:56,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:57,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:58,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:59,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:03:59,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:00,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:00,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:01,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:02,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:02,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:03,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:03,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:04,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:04,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:05,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:05,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:06,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:07,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:07,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:08,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:09,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:09,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:10,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:50<01:56, 14.59s/it][WARNING|generation_utils.py:914] 2023-08-28 14:04:10,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:11,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:12,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:12,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:13,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:13,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:14,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:15,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:15,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:16,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:16,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:17,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:18,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:18,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:19,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:20,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:20,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:21,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:22,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:22,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:23,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:03<01:39, 14.15s/it][WARNING|generation_utils.py:914] 2023-08-28 14:04:24,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:24,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:25,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:26,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:26,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:27,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:28,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:28,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:29,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:30,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:30,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:31,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:32,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:32,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:33,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:34,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:34,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:35,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:36,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:36,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:37,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:18<01:25, 14.17s/it][WARNING|generation_utils.py:914] 2023-08-28 14:04:38,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:39,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:39,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:40,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:40,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:41,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:42,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:42,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:43,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:43,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:44,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:45,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:45,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:46,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:47,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:47,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:48,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:48,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:49,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:49,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:50,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:51,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:52,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:32<01:11, 14.24s/it][WARNING|generation_utils.py:914] 2023-08-28 14:04:52,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:53,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:54,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:54,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:55,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:56,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:57,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:57,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:58,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:59,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:04:59,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:00,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:00,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:01,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:02,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:03,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:03,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:04,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:05,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:05,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:06,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:07,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:47<00:57, 14.50s/it][WARNING|generation_utils.py:914] 2023-08-28 14:05:07,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:08,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:09,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:09,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:10,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:10,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:11,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:12,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:12,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:13,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:14,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:14,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:15,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:16,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:16,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:17,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:18,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:18,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:19,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:20,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:20,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:21,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:02<00:43, 14.50s/it][WARNING|generation_utils.py:914] 2023-08-28 14:05:22,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:23,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:23,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:24,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:24,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:25,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:26,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:26,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:27,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:28,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:28,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:29,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:29,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:30,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:30,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:31,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:32,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:32,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:33,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:34,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:34,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:35,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:15<00:28, 14.27s/it][WARNING|generation_utils.py:914] 2023-08-28 14:05:36,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:36,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:37,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:37,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:38,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:39,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:39,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:40,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:41,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:41,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:42,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:42,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:43,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:44,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:44,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:45,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:45,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:46,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:46,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:47,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:48,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:48,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:49,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:49,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:50,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:51,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:51,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:52,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:53,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:53,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:54,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:34<00:15, 15.62s/it][WARNING|generation_utils.py:914] 2023-08-28 14:05:54,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:55,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:56,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:56,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:57,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:58,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:58,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:05:59,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:00,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:00,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:01,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:02,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:02,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:03,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:04,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:05,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:05,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:06,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:07,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:07,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:08,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:09,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:06:09,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:50<00:00, 15.61s/it]Generating: 100%|██████████| 20/20 [04:50<00:00, 14.51s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:17,861 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:17,867 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:17,867 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:17,867 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:17,867 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:06:18,477 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:06:18,477 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:06:19,068 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:06:20,161 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:06:20,161 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:22,865 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:22,869 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:22,869 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:22,869 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:22,869 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:06:23,202 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:06:23,203 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:06:23,464 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:06:23,640 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:06:23,640 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : followed by . Context : Later in the year ( the year before ) , a battle between the French and Portuguese forces in Iberian Peninsula raged from June until the death of Emperor Lu Bu in six months . Head Entity : battle , Tail Entity : 6 months .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : followed by .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Sondheim und Halle\', \'followed by\', \'\', \'" Sondheim und Halle " , The " Sondheim - und Hofach " , Günenmaus , 1819 , pp .\')', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : military rank .', 'success_rate': 0.859375, 'errors': {'', "('Japanese Army', 'military rank', '', 'In 1944 , the Japanese Army invaded and occupied Okinawa , and occupied Pearl Harbor , Hawaii .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.8973214285714286, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : tributary .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : architect .', 'success_rate': 0.859375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : constellation .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 331, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 408, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 482, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 528, 'raw': 704}
{'target': 600, 'success': 548, 'raw': 736}
{'target': 600, 'success': 571, 'raw': 768}
{'target': 600, 'success': 591, 'raw': 800}
{'target': 600, 'success': 614, 'raw': 832}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7379807692307693, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : developer .', 'success_rate': 0.8396739130434783, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : follows .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : league .', 'success_rate': 0.8059895833333334, 'errors': {'', "('Atlanta Braves', 'league', '', 'That same year , he played for the Atlanta Braves .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8369565217391305, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : notable work .', 'success_rate': 0.8849431818181818, 'errors': {'', "('Samuel Pepys', 'notable work', '', 'In 1848 , he received a grant from Samuel Pepys to write a collection of his works .')", '(\'The Secret Life of Henri Poincaré\', \'notable work\', \'\', \'The first three novels of the series were adapted from the play " The Secret Life of Henri Poincaré " .\')'}}
['Relation : operator . Context : Later in the year , the company built a high - speed commuter railway crossing the Bishkek River between Bishkek and Minsk . Head Entity : commuter railway , Tail Entity : RIB .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 142, 'raw': 224}
{'target': 600, 'success': 160, 'raw': 256}
{'target': 600, 'success': 180, 'raw': 288}
{'target': 600, 'success': 201, 'raw': 320}
{'target': 600, 'success': 215, 'raw': 352}
{'target': 600, 'success': 232, 'raw': 384}
{'target': 600, 'success': 248, 'raw': 416}
{'target': 600, 'success': 267, 'raw': 448}
{'target': 600, 'success': 286, 'raw': 480}
{'target': 600, 'success': 307, 'raw': 512}
{'target': 600, 'success': 327, 'raw': 544}
{'target': 600, 'success': 348, 'raw': 576}
{'target': 600, 'success': 370, 'raw': 608}
{'target': 600, 'success': 392, 'raw': 640}
{'target': 600, 'success': 413, 'raw': 672}
{'target': 600, 'success': 435, 'raw': 704}
{'target': 600, 'success': 454, 'raw': 736}
{'target': 600, 'success': 473, 'raw': 768}
{'target': 600, 'success': 494, 'raw': 800}
{'target': 600, 'success': 518, 'raw': 832}
{'target': 600, 'success': 535, 'raw': 864}
{'target': 600, 'success': 553, 'raw': 896}
{'target': 600, 'success': 572, 'raw': 928}
{'target': 600, 'success': 590, 'raw': 960}
{'target': 600, 'success': 608, 'raw': 992}
{'prompt': 'Relation : position held .', 'success_rate': 0.6129032258064516, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8519021739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 14989
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15089, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.50it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:01,  1.51it/s]Extractor Estimating: 4it [00:02,  1.36it/s]Extractor Estimating: 5it [00:03,  1.44it/s]Extractor Estimating: 6it [00:04,  1.45it/s]Extractor Estimating: 7it [00:04,  1.45it/s]Extractor Estimating: 8it [00:05,  1.53it/s]Extractor Estimating: 9it [00:06,  1.50it/s]Extractor Estimating: 10it [00:06,  1.53it/s]Extractor Estimating: 11it [00:07,  1.55it/s]Extractor Estimating: 12it [00:08,  1.54it/s]Extractor Estimating: 13it [00:08,  1.49it/s]Extractor Estimating: 14it [00:09,  1.50it/s]Extractor Estimating: 15it [00:09,  1.56it/s]Extractor Estimating: 16it [00:10,  1.53it/s]Extractor Estimating: 17it [00:11,  1.51it/s]Extractor Estimating: 18it [00:11,  1.57it/s]Extractor Estimating: 19it [00:12,  1.53it/s]Extractor Estimating: 20it [00:13,  1.54it/s]Extractor Estimating: 21it [00:13,  1.54it/s]Extractor Estimating: 22it [00:14,  1.44it/s]Extractor Estimating: 23it [00:15,  1.48it/s]Extractor Estimating: 24it [00:15,  1.57it/s]Extractor Estimating: 25it [00:16,  1.53it/s]Extractor Estimating: 26it [00:17,  1.60it/s]Extractor Estimating: 27it [00:17,  1.61it/s]Extractor Estimating: 28it [00:18,  1.66it/s]Extractor Estimating: 29it [00:18,  1.69it/s]Extractor Estimating: 30it [00:19,  1.73it/s]Extractor Estimating: 31it [00:20,  1.72it/s]Extractor Estimating: 32it [00:20,  1.70it/s]Extractor Estimating: 33it [00:21,  1.67it/s]Extractor Estimating: 34it [00:21,  1.70it/s]Extractor Estimating: 35it [00:22,  1.74it/s]Extractor Estimating: 36it [00:22,  1.71it/s]Extractor Estimating: 37it [00:23,  1.70it/s]Extractor Estimating: 38it [00:24,  1.68it/s]Extractor Estimating: 39it [00:24,  1.66it/s]Extractor Estimating: 40it [00:25,  1.68it/s]Extractor Estimating: 41it [00:26,  1.62it/s]Extractor Estimating: 42it [00:26,  1.66it/s]Extractor Estimating: 43it [00:27,  1.69it/s]Extractor Estimating: 44it [00:27,  1.67it/s]Extractor Estimating: 45it [00:28,  1.69it/s]Extractor Estimating: 46it [00:28,  1.69it/s]Extractor Estimating: 47it [00:29,  1.73it/s]Extractor Estimating: 48it [00:30,  1.73it/s]Extractor Estimating: 49it [00:30,  1.77it/s]Extractor Estimating: 50it [00:31,  1.76it/s]Extractor Estimating: 51it [00:31,  1.66it/s]Extractor Estimating: 52it [00:32,  1.68it/s]Extractor Estimating: 53it [00:33,  1.68it/s]Extractor Estimating: 54it [00:33,  1.74it/s]Extractor Estimating: 55it [00:34,  1.76it/s]Extractor Estimating: 56it [00:34,  1.80it/s]Extractor Estimating: 57it [00:35,  1.76it/s]Extractor Estimating: 58it [00:35,  1.75it/s]Extractor Estimating: 59it [00:36,  1.82it/s]Extractor Estimating: 60it [00:36,  1.86it/s]Extractor Estimating: 61it [00:37,  1.79it/s]Extractor Estimating: 62it [00:38,  1.77it/s]Extractor Estimating: 63it [00:38,  1.66it/s]Extractor Estimating: 64it [00:39,  1.74it/s]Extractor Estimating: 65it [00:39,  1.64it/s]Extractor Estimating: 66it [00:40,  1.61it/s]Extractor Estimating: 67it [00:41,  1.62it/s]Extractor Estimating: 68it [00:41,  1.63it/s]Extractor Estimating: 69it [00:42,  1.68it/s]Extractor Estimating: 70it [00:42,  1.72it/s]Extractor Estimating: 71it [00:43,  1.72it/s]Extractor Estimating: 72it [00:44,  1.72it/s]Extractor Estimating: 73it [00:44,  1.69it/s]Extractor Estimating: 74it [00:45,  1.74it/s]Extractor Estimating: 75it [00:45,  1.78it/s]Extractor Estimating: 76it [00:46,  1.72it/s]Extractor Estimating: 77it [00:46,  1.69it/s]Extractor Estimating: 78it [00:47,  1.60it/s]Extractor Estimating: 79it [00:48,  1.65it/s]Extractor Estimating: 80it [00:48,  1.65it/s]Extractor Estimating: 81it [00:49,  1.71it/s]Extractor Estimating: 82it [00:50,  1.60it/s]Extractor Estimating: 83it [00:50,  1.61it/s]Extractor Estimating: 84it [00:51,  1.63it/s]Extractor Estimating: 85it [00:51,  1.62it/s]Extractor Estimating: 86it [00:52,  1.64it/s]Extractor Estimating: 87it [00:53,  1.66it/s]Extractor Estimating: 88it [00:53,  1.67it/s]Extractor Estimating: 89it [00:54,  1.67it/s]Extractor Estimating: 90it [00:54,  1.63it/s]Extractor Estimating: 91it [00:55,  1.66it/s]Extractor Estimating: 92it [00:56,  1.64it/s]Extractor Estimating: 93it [00:56,  1.65it/s]Extractor Estimating: 94it [00:57,  1.63it/s]Extractor Estimating: 95it [00:58,  1.58it/s]Extractor Estimating: 96it [00:58,  1.55it/s]Extractor Estimating: 97it [00:59,  1.59it/s]Extractor Estimating: 98it [00:59,  1.58it/s]Extractor Estimating: 99it [01:00,  1.61it/s]Extractor Estimating: 100it [01:01,  1.63it/s]Extractor Estimating: 101it [01:01,  1.69it/s]Extractor Estimating: 102it [01:02,  1.72it/s]Extractor Estimating: 103it [01:02,  1.62it/s]Extractor Estimating: 104it [01:03,  1.71it/s]Extractor Estimating: 105it [01:03,  1.75it/s]Extractor Estimating: 106it [01:04,  1.79it/s]Extractor Estimating: 107it [01:05,  1.82it/s]Extractor Estimating: 108it [01:05,  1.84it/s]Extractor Estimating: 109it [01:06,  1.91it/s]Extractor Estimating: 110it [01:06,  1.87it/s]Extractor Estimating: 111it [01:07,  1.90it/s]Extractor Estimating: 112it [01:07,  1.86it/s]Extractor Estimating: 113it [01:08,  1.78it/s]Extractor Estimating: 114it [01:08,  1.77it/s]Extractor Estimating: 115it [01:09,  1.72it/s]Extractor Estimating: 116it [01:10,  1.74it/s]Extractor Estimating: 117it [01:10,  1.83it/s]Extractor Estimating: 118it [01:11,  1.76it/s]Extractor Estimating: 119it [01:11,  1.80it/s]Extractor Estimating: 120it [01:12,  1.76it/s]Extractor Estimating: 121it [01:12,  1.73it/s]Extractor Estimating: 122it [01:13,  1.69it/s]Extractor Estimating: 123it [01:14,  1.73it/s]Extractor Estimating: 124it [01:14,  1.75it/s]Extractor Estimating: 125it [01:15,  1.80it/s]Extractor Estimating: 126it [01:15,  1.66it/s]Extractor Estimating: 127it [01:16,  1.68it/s]Extractor Estimating: 128it [01:16,  1.74it/s]Extractor Estimating: 129it [01:17,  1.72it/s]Extractor Estimating: 130it [01:18,  1.70it/s]Extractor Estimating: 131it [01:18,  1.74it/s]Extractor Estimating: 132it [01:19,  1.69it/s]Extractor Estimating: 133it [01:19,  1.65it/s]Extractor Estimating: 134it [01:20,  1.73it/s]Extractor Estimating: 135it [01:21,  1.73it/s]Extractor Estimating: 136it [01:21,  1.74it/s]Extractor Estimating: 137it [01:22,  1.75it/s]Extractor Estimating: 138it [01:22,  1.69it/s]Extractor Estimating: 139it [01:23,  1.74it/s]Extractor Estimating: 140it [01:23,  1.74it/s]Extractor Estimating: 141it [01:24,  1.77it/s]Extractor Estimating: 142it [01:25,  1.74it/s]Extractor Estimating: 143it [01:25,  1.71it/s]Extractor Estimating: 144it [01:26,  1.75it/s]Extractor Estimating: 145it [01:26,  1.69it/s]Extractor Estimating: 146it [01:27,  1.67it/s]Extractor Estimating: 147it [01:28,  1.66it/s]Extractor Estimating: 148it [01:28,  1.68it/s]Extractor Estimating: 149it [01:29,  1.73it/s]Extractor Estimating: 150it [01:29,  1.71it/s]Extractor Estimating: 151it [01:30,  1.79it/s]Extractor Estimating: 152it [01:30,  1.78it/s]Extractor Estimating: 153it [01:31,  1.75it/s]Extractor Estimating: 154it [01:32,  1.75it/s]Extractor Estimating: 155it [01:32,  1.79it/s]Extractor Estimating: 156it [01:33,  1.81it/s]Extractor Estimating: 157it [01:33,  1.81it/s]Extractor Estimating: 158it [01:34,  1.82it/s]Extractor Estimating: 159it [01:34,  1.77it/s]Extractor Estimating: 160it [01:35,  1.74it/s]Extractor Estimating: 161it [01:35,  1.78it/s]Extractor Estimating: 162it [01:36,  1.83it/s]Extractor Estimating: 163it [01:36,  1.85it/s]Extractor Estimating: 164it [01:37,  1.82it/s]Extractor Estimating: 165it [01:38,  1.86it/s]Extractor Estimating: 166it [01:38,  1.82it/s]Extractor Estimating: 167it [01:39,  1.77it/s]Extractor Estimating: 168it [01:39,  1.84it/s]Extractor Estimating: 169it [01:40,  1.84it/s]Extractor Estimating: 170it [01:40,  1.80it/s]Extractor Estimating: 171it [01:41,  1.80it/s]Extractor Estimating: 172it [01:42,  1.72it/s]Extractor Estimating: 173it [01:42,  1.76it/s]Extractor Estimating: 174it [01:43,  1.81it/s]Extractor Estimating: 175it [01:43,  1.83it/s]Extractor Estimating: 176it [01:44,  1.77it/s]Extractor Estimating: 177it [01:44,  1.74it/s]Extractor Estimating: 178it [01:45,  1.69it/s]Extractor Estimating: 179it [01:45,  1.74it/s]Extractor Estimating: 180it [01:46,  1.71it/s]Extractor Estimating: 181it [01:47,  1.70it/s]Extractor Estimating: 182it [01:47,  1.73it/s]Extractor Estimating: 183it [01:48,  1.74it/s]Extractor Estimating: 184it [01:48,  1.72it/s]Extractor Estimating: 185it [01:49,  1.67it/s]Extractor Estimating: 186it [01:50,  1.73it/s]Extractor Estimating: 187it [01:50,  1.70it/s]Extractor Estimating: 188it [01:51,  1.57it/s]Extractor Estimating: 189it [01:52,  1.61it/s]Extractor Estimating: 190it [01:52,  1.61it/s]Extractor Estimating: 191it [01:53,  1.62it/s]Extractor Estimating: 192it [01:53,  1.66it/s]Extractor Estimating: 193it [01:54,  1.64it/s]Extractor Estimating: 194it [01:54,  1.70it/s]Extractor Estimating: 195it [01:55,  1.72it/s]Extractor Estimating: 196it [01:56,  1.74it/s]Extractor Estimating: 197it [01:56,  1.76it/s]Extractor Estimating: 198it [01:57,  1.70it/s]Extractor Estimating: 199it [01:57,  1.70it/s]Extractor Estimating: 200it [01:58,  1.75it/s]Extractor Estimating: 201it [01:58,  1.75it/s]Extractor Estimating: 202it [01:59,  1.67it/s]Extractor Estimating: 203it [02:00,  1.73it/s]Extractor Estimating: 204it [02:00,  1.72it/s]Extractor Estimating: 205it [02:01,  1.71it/s]Extractor Estimating: 206it [02:01,  1.75it/s]Extractor Estimating: 207it [02:02,  1.66it/s]Extractor Estimating: 208it [02:03,  1.69it/s]Extractor Estimating: 209it [02:03,  1.69it/s]Extractor Estimating: 210it [02:04,  1.66it/s]Extractor Estimating: 211it [02:04,  1.68it/s]Extractor Estimating: 212it [02:05,  1.65it/s]Extractor Estimating: 213it [02:06,  1.68it/s]Extractor Estimating: 214it [02:06,  1.63it/s]Extractor Estimating: 215it [02:07,  1.68it/s]Extractor Estimating: 216it [02:07,  1.65it/s]Extractor Estimating: 217it [02:08,  1.64it/s]Extractor Estimating: 218it [02:09,  1.66it/s]Extractor Estimating: 219it [02:09,  1.64it/s]Extractor Estimating: 220it [02:10,  1.58it/s]Extractor Estimating: 221it [02:11,  1.58it/s]Extractor Estimating: 222it [02:11,  1.58it/s]Extractor Estimating: 223it [02:12,  1.61it/s]Extractor Estimating: 224it [02:12,  1.63it/s]Extractor Estimating: 225it [02:13,  1.64it/s]Extractor Estimating: 226it [02:14,  1.62it/s]Extractor Estimating: 227it [02:14,  1.66it/s]Extractor Estimating: 228it [02:15,  1.64it/s]Extractor Estimating: 229it [02:15,  1.67it/s]Extractor Estimating: 230it [02:16,  1.63it/s]Extractor Estimating: 231it [02:17,  1.62it/s]Extractor Estimating: 232it [02:17,  1.62it/s]Extractor Estimating: 233it [02:18,  1.62it/s]Extractor Estimating: 234it [02:19,  1.62it/s]Extractor Estimating: 235it [02:19,  1.55it/s]Extractor Estimating: 236it [02:20,  1.52it/s]Extractor Estimating: 237it [02:21,  1.56it/s]Extractor Estimating: 238it [02:21,  1.60it/s]Extractor Estimating: 239it [02:22,  1.59it/s]Extractor Estimating: 240it [02:22,  1.67it/s]Extractor Estimating: 241it [02:23,  1.58it/s]Extractor Estimating: 242it [02:24,  1.56it/s]Extractor Estimating: 243it [02:24,  1.56it/s]Extractor Estimating: 244it [02:25,  1.63it/s]Extractor Estimating: 245it [02:26,  1.64it/s]Extractor Estimating: 246it [02:26,  1.56it/s]Extractor Estimating: 247it [02:27,  1.57it/s]Extractor Estimating: 248it [02:27,  1.62it/s]Extractor Estimating: 249it [02:28,  1.59it/s]Extractor Estimating: 250it [02:29,  1.62it/s]Extractor Estimating: 251it [02:29,  1.59it/s]Extractor Estimating: 252it [02:30,  1.55it/s]Extractor Estimating: 253it [02:31,  1.53it/s]Extractor Estimating: 254it [02:31,  1.54it/s]Extractor Estimating: 255it [02:32,  1.56it/s]Extractor Estimating: 256it [02:33,  1.51it/s]Extractor Estimating: 257it [02:33,  1.55it/s]Extractor Estimating: 258it [02:34,  1.54it/s]Extractor Estimating: 259it [02:35,  1.58it/s]Extractor Estimating: 260it [02:35,  1.57it/s]Extractor Estimating: 261it [02:36,  1.56it/s]Extractor Estimating: 262it [02:36,  1.60it/s]Extractor Estimating: 263it [02:37,  1.58it/s]Extractor Estimating: 264it [02:38,  1.51it/s]Extractor Estimating: 265it [02:38,  1.52it/s]Extractor Estimating: 266it [02:39,  1.55it/s]Extractor Estimating: 267it [02:40,  1.59it/s]Extractor Estimating: 268it [02:41,  1.40it/s]Extractor Estimating: 269it [02:41,  1.38it/s]Extractor Estimating: 270it [02:42,  1.41it/s]Extractor Estimating: 271it [02:43,  1.44it/s]Extractor Estimating: 272it [02:43,  1.48it/s]Extractor Estimating: 273it [02:44,  1.54it/s]Extractor Estimating: 274it [02:44,  1.55it/s]Extractor Estimating: 275it [02:45,  1.52it/s]Extractor Estimating: 276it [02:46,  1.59it/s]Extractor Estimating: 277it [02:46,  1.56it/s]Extractor Estimating: 278it [02:47,  1.56it/s]Extractor Estimating: 279it [02:48,  1.56it/s]Extractor Estimating: 280it [02:48,  1.61it/s]Extractor Estimating: 281it [02:49,  1.70it/s]Extractor Estimating: 282it [02:49,  1.73it/s]Extractor Estimating: 283it [02:50,  1.71it/s]Extractor Estimating: 284it [02:50,  1.74it/s]Extractor Estimating: 285it [02:51,  1.73it/s]Extractor Estimating: 286it [02:52,  1.73it/s]Extractor Estimating: 287it [02:52,  1.71it/s]Extractor Estimating: 288it [02:53,  1.79it/s]Extractor Estimating: 289it [02:53,  1.76it/s]Extractor Estimating: 290it [02:54,  1.79it/s]Extractor Estimating: 291it [02:54,  1.76it/s]Extractor Estimating: 292it [02:55,  1.80it/s]Extractor Estimating: 293it [02:56,  1.73it/s]Extractor Estimating: 294it [02:56,  1.69it/s]Extractor Estimating: 295it [02:57,  1.68it/s]Extractor Estimating: 296it [02:57,  1.64it/s]Extractor Estimating: 297it [02:58,  1.62it/s]Extractor Estimating: 298it [02:59,  1.66it/s]Extractor Estimating: 299it [02:59,  1.69it/s]Extractor Estimating: 300it [03:00,  1.67it/s]Extractor Estimating: 301it [03:00,  1.71it/s]Extractor Estimating: 302it [03:01,  1.73it/s]Extractor Estimating: 303it [03:02,  1.75it/s]Extractor Estimating: 304it [03:02,  1.80it/s]Extractor Estimating: 305it [03:03,  1.84it/s]Extractor Estimating: 306it [03:03,  1.84it/s]Extractor Estimating: 307it [03:04,  1.86it/s]Extractor Estimating: 308it [03:04,  1.89it/s]Extractor Estimating: 309it [03:05,  1.93it/s]Extractor Estimating: 310it [03:05,  1.88it/s]Extractor Estimating: 311it [03:06,  1.86it/s]Extractor Estimating: 312it [03:06,  1.85it/s]Extractor Estimating: 313it [03:07,  1.87it/s]Extractor Estimating: 314it [03:07,  1.90it/s]Extractor Estimating: 315it [03:08,  1.88it/s]Extractor Estimating: 316it [03:08,  1.88it/s]Extractor Estimating: 317it [03:09,  1.91it/s]Extractor Estimating: 318it [03:09,  1.94it/s]Extractor Estimating: 319it [03:10,  1.90it/s]Extractor Estimating: 320it [03:11,  1.84it/s]Extractor Estimating: 321it [03:11,  1.83it/s]Extractor Estimating: 322it [03:12,  1.92it/s]Extractor Estimating: 323it [03:12,  1.90it/s]Extractor Estimating: 324it [03:13,  1.93it/s]Extractor Estimating: 325it [03:13,  1.84it/s]Extractor Estimating: 326it [03:14,  1.64it/s]Extractor Estimating: 327it [03:15,  1.66it/s]Extractor Estimating: 328it [03:15,  1.66it/s]Extractor Estimating: 329it [03:16,  1.63it/s]Extractor Estimating: 330it [03:16,  1.65it/s]Extractor Estimating: 331it [03:17,  1.63it/s]Extractor Estimating: 332it [03:18,  1.64it/s]Extractor Estimating: 333it [03:18,  1.60it/s]Extractor Estimating: 334it [03:19,  1.55it/s]Extractor Estimating: 335it [03:20,  1.59it/s]Extractor Estimating: 336it [03:20,  1.61it/s]Extractor Estimating: 337it [03:21,  1.59it/s]Extractor Estimating: 338it [03:21,  1.58it/s]Extractor Estimating: 339it [03:22,  1.62it/s]Extractor Estimating: 340it [03:23,  1.61it/s]Extractor Estimating: 341it [03:23,  1.64it/s]Extractor Estimating: 342it [03:24,  1.62it/s]Extractor Estimating: 343it [03:24,  1.67it/s]Extractor Estimating: 344it [03:25,  1.59it/s]Extractor Estimating: 345it [03:26,  1.59it/s]Extractor Estimating: 346it [03:26,  1.56it/s]Extractor Estimating: 347it [03:27,  1.56it/s]Extractor Estimating: 348it [03:28,  1.56it/s]Extractor Estimating: 349it [03:28,  1.57it/s]Extractor Estimating: 350it [03:29,  1.42it/s]Extractor Estimating: 351it [03:30,  1.49it/s]Extractor Estimating: 352it [03:30,  1.54it/s]Extractor Estimating: 353it [03:31,  1.58it/s]Extractor Estimating: 354it [03:32,  1.63it/s]Extractor Estimating: 355it [03:32,  1.74it/s]Extractor Estimating: 356it [03:33,  1.75it/s]Extractor Estimating: 357it [03:33,  1.74it/s]Extractor Estimating: 358it [03:34,  1.68it/s]Extractor Estimating: 359it [03:34,  1.69it/s]Extractor Estimating: 360it [03:35,  1.64it/s]Extractor Estimating: 361it [03:36,  1.66it/s]Extractor Estimating: 362it [03:36,  1.69it/s]Extractor Estimating: 363it [03:37,  1.74it/s]Extractor Estimating: 364it [03:37,  1.71it/s]Extractor Estimating: 365it [03:38,  1.72it/s]Extractor Estimating: 366it [03:39,  1.69it/s]Extractor Estimating: 367it [03:39,  1.72it/s]Extractor Estimating: 368it [03:40,  1.77it/s]Extractor Estimating: 369it [03:40,  1.77it/s]Extractor Estimating: 370it [03:41,  1.79it/s]Extractor Estimating: 371it [03:41,  1.83it/s]Extractor Estimating: 372it [03:42,  1.86it/s]Extractor Estimating: 373it [03:43,  1.66it/s]Extractor Estimating: 374it [03:43,  1.70it/s]Extractor Estimating: 375it [03:44,  1.72it/s]Extractor Estimating: 376it [03:44,  1.69it/s]Extractor Estimating: 377it [03:45,  1.66it/s]Extractor Estimating: 378it [03:46,  1.60it/s]Extractor Estimating: 379it [03:46,  1.55it/s]Extractor Estimating: 380it [03:47,  1.49it/s]Extractor Estimating: 381it [03:48,  1.46it/s]Extractor Estimating: 382it [03:48,  1.42it/s]Extractor Estimating: 383it [03:49,  1.47it/s]Extractor Estimating: 384it [03:50,  1.48it/s]Extractor Estimating: 385it [03:50,  1.50it/s]Extractor Estimating: 386it [03:51,  1.47it/s]Extractor Estimating: 387it [03:52,  1.45it/s]Extractor Estimating: 388it [03:52,  1.51it/s]Extractor Estimating: 389it [03:53,  1.52it/s]Extractor Estimating: 390it [03:54,  1.49it/s]Extractor Estimating: 391it [03:55,  1.41it/s]Extractor Estimating: 392it [03:55,  1.44it/s]Extractor Estimating: 393it [03:56,  1.45it/s]Extractor Estimating: 394it [03:57,  1.48it/s]Extractor Estimating: 395it [03:57,  1.53it/s]Extractor Estimating: 396it [03:58,  1.50it/s]Extractor Estimating: 397it [03:59,  1.35it/s]Extractor Estimating: 398it [03:59,  1.38it/s]Extractor Estimating: 399it [04:00,  1.39it/s]Extractor Estimating: 400it [04:01,  1.40it/s]Extractor Estimating: 401it [04:01,  1.46it/s]Extractor Estimating: 402it [04:02,  1.52it/s]Extractor Estimating: 403it [04:03,  1.57it/s]Extractor Estimating: 404it [04:03,  1.62it/s]Extractor Estimating: 405it [04:04,  1.65it/s]Extractor Estimating: 406it [04:04,  1.66it/s]Extractor Estimating: 407it [04:05,  1.64it/s]Extractor Estimating: 408it [04:06,  1.69it/s]Extractor Estimating: 409it [04:06,  1.63it/s]Extractor Estimating: 410it [04:07,  1.64it/s]Extractor Estimating: 411it [04:07,  1.62it/s]Extractor Estimating: 412it [04:08,  1.66it/s]Extractor Estimating: 413it [04:09,  1.66it/s]Extractor Estimating: 414it [04:09,  1.65it/s]Extractor Estimating: 415it [04:10,  1.60it/s]Extractor Estimating: 416it [04:11,  1.64it/s]Extractor Estimating: 417it [04:11,  1.59it/s]Extractor Estimating: 418it [04:12,  1.62it/s]Extractor Estimating: 419it [04:12,  1.56it/s]Extractor Estimating: 420it [04:13,  1.59it/s]Extractor Estimating: 421it [04:14,  1.56it/s]Extractor Estimating: 422it [04:14,  1.57it/s]Extractor Estimating: 423it [04:15,  1.57it/s]Extractor Estimating: 424it [04:16,  1.57it/s]Extractor Estimating: 425it [04:16,  1.54it/s]Extractor Estimating: 426it [04:17,  1.58it/s]Extractor Estimating: 427it [04:18,  1.59it/s]Extractor Estimating: 428it [04:18,  1.62it/s]Extractor Estimating: 429it [04:19,  1.66it/s]Extractor Estimating: 430it [04:19,  1.68it/s]Extractor Estimating: 431it [04:20,  1.62it/s]Extractor Estimating: 432it [04:21,  1.58it/s]Extractor Estimating: 433it [04:21,  1.61it/s]Extractor Estimating: 434it [04:22,  1.61it/s]Extractor Estimating: 435it [04:22,  1.64it/s]Extractor Estimating: 436it [04:23,  1.43it/s]Extractor Estimating: 437it [04:24,  1.51it/s]Extractor Estimating: 438it [04:24,  1.61it/s]Extractor Estimating: 439it [04:25,  1.65it/s]Extractor Estimating: 440it [04:26,  1.69it/s]Extractor Estimating: 441it [04:26,  1.65it/s]Extractor Estimating: 442it [04:27,  1.60it/s]Extractor Estimating: 443it [04:28,  1.57it/s]Extractor Estimating: 444it [04:28,  1.54it/s]Extractor Estimating: 445it [04:29,  1.57it/s]Extractor Estimating: 446it [04:29,  1.57it/s]Extractor Estimating: 447it [04:30,  1.58it/s]Extractor Estimating: 448it [04:31,  1.60it/s]Extractor Estimating: 449it [04:31,  1.63it/s]Extractor Estimating: 450it [04:32,  1.58it/s]Extractor Estimating: 451it [04:32,  1.66it/s]Extractor Estimating: 452it [04:33,  1.70it/s]Extractor Estimating: 453it [04:34,  1.73it/s]Extractor Estimating: 454it [04:34,  1.74it/s]Extractor Estimating: 455it [04:35,  1.66it/s]Extractor Estimating: 456it [04:35,  1.70it/s]Extractor Estimating: 457it [04:36,  1.70it/s]Extractor Estimating: 458it [04:37,  1.65it/s]Extractor Estimating: 459it [04:37,  1.69it/s]Extractor Estimating: 460it [04:38,  1.72it/s]Extractor Estimating: 461it [04:38,  1.66it/s]Extractor Estimating: 462it [04:39,  1.74it/s]Extractor Estimating: 463it [04:39,  1.76it/s]Extractor Estimating: 464it [04:40,  1.77it/s]Extractor Estimating: 465it [04:41,  1.77it/s]Extractor Estimating: 466it [04:41,  1.73it/s]Extractor Estimating: 467it [04:42,  1.68it/s]Extractor Estimating: 468it [04:42,  1.72it/s]Extractor Estimating: 469it [04:43,  1.71it/s]Extractor Estimating: 470it [04:43,  1.75it/s]Extractor Estimating: 471it [04:44,  1.73it/s]Extractor Estimating: 472it [04:45,  1.75it/s]Extractor Estimating: 473it [04:45,  1.71it/s]Extractor Estimating: 474it [04:46,  1.72it/s]Extractor Estimating: 475it [04:46,  1.72it/s]Extractor Estimating: 476it [04:47,  1.60it/s]Extractor Estimating: 477it [04:48,  1.56it/s]Extractor Estimating: 478it [04:48,  1.61it/s]Extractor Estimating: 479it [04:49,  1.64it/s]Extractor Estimating: 480it [04:50,  1.65it/s]Extractor Estimating: 481it [04:50,  1.68it/s]Extractor Estimating: 482it [04:51,  1.65it/s]Extractor Estimating: 483it [04:51,  1.71it/s]Extractor Estimating: 484it [04:52,  1.70it/s]Extractor Estimating: 485it [04:53,  1.60it/s]Extractor Estimating: 486it [04:53,  1.64it/s]Extractor Estimating: 487it [04:54,  1.70it/s]Extractor Estimating: 488it [04:54,  1.64it/s]Extractor Estimating: 489it [04:55,  1.64it/s]Extractor Estimating: 490it [04:56,  1.62it/s]Extractor Estimating: 491it [04:56,  1.61it/s]Extractor Estimating: 492it [04:57,  1.68it/s]Extractor Estimating: 493it [04:57,  1.64it/s]Extractor Estimating: 494it [04:58,  1.63it/s]Extractor Estimating: 495it [04:59,  1.55it/s]Extractor Estimating: 496it [04:59,  1.60it/s]Extractor Estimating: 497it [05:00,  1.65it/s]Extractor Estimating: 498it [05:01,  1.62it/s]Extractor Estimating: 499it [05:01,  1.70it/s]Extractor Estimating: 500it [05:02,  1.70it/s]Extractor Estimating: 500it [05:02,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:44,515 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:44,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:44,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:44,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:44,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:11:45,129 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:11:45,129 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:11:45,703 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:11:46,788 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:11:46,788 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:48,500 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:48,504 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:48,504 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:48,504 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:48,504 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:11:48,830 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:11:48,831 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:11:49,097 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:11:49,251 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:11:49,251 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 16:58:53,231 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 16:58:53,241 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 9987 mean pseudo reward: 0.9517558956192006
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 27089
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27189, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27189, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.983, loss:696.2824
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.975, loss:691.4169
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.967, loss:652.9596
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.962, loss:637.3915
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 0.956, loss:615.3979
>> valid entity prec:0.5982, rec:0.5764, f1:0.5871
>> valid relation prec:0.2136, rec:0.0914, f1:0.1280
>> valid relation with NER prec:0.2136, rec:0.0914, f1:0.1280
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.201, loss:661.1334
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 0.969, loss:646.5215
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 0.976, loss:666.0284
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 0.961, loss:636.2698
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 0.963, loss:644.0858
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5985, rec:0.5622, f1:0.5797
>> valid relation prec:0.2373, rec:0.0772, f1:0.1165
>> valid relation with NER prec:0.2373, rec:0.0772, f1:0.1165
g_step 1100, step 266, avg_time 2.189, loss:680.9861
g_step 1200, step 366, avg_time 0.975, loss:702.1392
g_step 1300, step 49, avg_time 0.971, loss:647.4451
g_step 1400, step 149, avg_time 0.969, loss:613.7583
g_step 1500, step 249, avg_time 0.993, loss:648.4959
>> valid entity prec:0.5864, rec:0.5704, f1:0.5783
>> valid relation prec:0.2208, rec:0.1060, f1:0.1432
>> valid relation with NER prec:0.2208, rec:0.1060, f1:0.1432
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.213, loss:678.2163
g_step 1700, step 32, avg_time 0.956, loss:603.8963
g_step 1800, step 132, avg_time 0.963, loss:613.2264
g_step 1900, step 232, avg_time 0.961, loss:605.5818
g_step 2000, step 332, avg_time 0.969, loss:613.9252
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5970, rec:0.4827, f1:0.5338
>> valid relation prec:0.2053, rec:0.0792, f1:0.1143
>> valid relation with NER prec:0.2053, rec:0.0792, f1:0.1143
g_step 2100, step 15, avg_time 2.202, loss:618.0641
g_step 2200, step 115, avg_time 0.967, loss:557.0061
g_step 2300, step 215, avg_time 0.966, loss:578.9018
g_step 2400, step 315, avg_time 0.970, loss:580.1080
g_step 2500, step 415, avg_time 0.971, loss:614.9647
>> valid entity prec:0.6270, rec:0.5427, f1:0.5818
>> valid relation prec:0.2502, rec:0.1022, f1:0.1451
>> valid relation with NER prec:0.2502, rec:0.1022, f1:0.1451
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 98, avg_time 2.195, loss:534.5336
g_step 2700, step 198, avg_time 0.969, loss:556.9060
g_step 2800, step 298, avg_time 0.985, loss:559.4806
g_step 2900, step 398, avg_time 0.964, loss:583.7856
g_step 3000, step 81, avg_time 0.974, loss:517.0558
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5982, rec:0.5176, f1:0.5550
>> valid relation prec:0.2236, rec:0.0824, f1:0.1205
>> valid relation with NER prec:0.2236, rec:0.0824, f1:0.1205
g_step 3100, step 181, avg_time 2.195, loss:533.8789
g_step 3200, step 281, avg_time 0.968, loss:531.3378
g_step 3300, step 381, avg_time 0.962, loss:554.3195
g_step 3400, step 64, avg_time 0.969, loss:525.9058
g_step 3500, step 164, avg_time 0.965, loss:509.7015
>> valid entity prec:0.6201, rec:0.5066, f1:0.5576
>> valid relation prec:0.2160, rec:0.0862, f1:0.1232
>> valid relation with NER prec:0.2160, rec:0.0862, f1:0.1232
g_step 3600, step 264, avg_time 2.203, loss:504.0844
g_step 3700, step 364, avg_time 0.971, loss:533.7652
g_step 3800, step 47, avg_time 0.971, loss:513.8277
g_step 3900, step 147, avg_time 0.971, loss:498.9723
g_step 4000, step 247, avg_time 0.964, loss:498.3802
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5877, rec:0.5454, f1:0.5658
>> valid relation prec:0.1952, rec:0.0708, f1:0.1039
>> valid relation with NER prec:0.1952, rec:0.0708, f1:0.1039
g_step 4100, step 347, avg_time 2.172, loss:495.0455
g_step 4200, step 30, avg_time 0.977, loss:485.4256
g_step 4300, step 130, avg_time 0.971, loss:478.7907
g_step 4400, step 230, avg_time 0.961, loss:492.8312
g_step 4500, step 330, avg_time 0.963, loss:513.9191
>> valid entity prec:0.5803, rec:0.5273, f1:0.5525
>> valid relation prec:0.1850, rec:0.0761, f1:0.1078
>> valid relation with NER prec:0.1850, rec:0.0761, f1:0.1078
g_step 4600, step 13, avg_time 2.170, loss:477.0288
g_step 4700, step 113, avg_time 0.959, loss:433.8773
g_step 4800, step 213, avg_time 0.976, loss:484.7372
g_step 4900, step 313, avg_time 0.962, loss:473.0168
g_step 5000, step 413, avg_time 0.968, loss:487.6133
learning rate was adjusted to 0.0008
>> valid entity prec:0.5998, rec:0.5635, f1:0.5811
>> valid relation prec:0.2261, rec:0.0975, f1:0.1363
>> valid relation with NER prec:0.2261, rec:0.0975, f1:0.1363
g_step 5100, step 96, avg_time 2.183, loss:429.1844
g_step 5200, step 196, avg_time 0.967, loss:456.4587
g_step 5300, step 296, avg_time 0.964, loss:449.2647
g_step 5400, step 396, avg_time 0.970, loss:441.8042
g_step 5500, step 79, avg_time 0.952, loss:438.7826
>> valid entity prec:0.5950, rec:0.4899, f1:0.5373
>> valid relation prec:0.1955, rec:0.0877, f1:0.1210
>> valid relation with NER prec:0.1955, rec:0.0877, f1:0.1210
g_step 5600, step 179, avg_time 2.182, loss:437.6121
g_step 5700, step 279, avg_time 0.961, loss:430.2999
g_step 5800, step 379, avg_time 0.969, loss:443.3080
g_step 5900, step 62, avg_time 0.966, loss:423.7309
g_step 6000, step 162, avg_time 0.969, loss:412.9163
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5960, rec:0.5658, f1:0.5805
>> valid relation prec:0.1746, rec:0.0900, f1:0.1188
>> valid relation with NER prec:0.1746, rec:0.0900, f1:0.1188
g_step 6100, step 262, avg_time 2.188, loss:417.8375
g_step 6200, step 362, avg_time 0.957, loss:438.5862
g_step 6300, step 45, avg_time 0.966, loss:421.9899
g_step 6400, step 145, avg_time 0.964, loss:393.8253
g_step 6500, step 245, avg_time 0.955, loss:386.5478
>> valid entity prec:0.6135, rec:0.4754, f1:0.5357
>> valid relation prec:0.2071, rec:0.0865, f1:0.1220
>> valid relation with NER prec:0.2071, rec:0.0865, f1:0.1220
g_step 6600, step 345, avg_time 2.193, loss:425.4356
g_step 6700, step 28, avg_time 0.958, loss:417.9691
g_step 6800, step 128, avg_time 0.965, loss:391.7005
g_step 6900, step 228, avg_time 0.975, loss:394.4823
g_step 7000, step 328, avg_time 0.959, loss:405.0267
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5815, rec:0.5018, f1:0.5387
>> valid relation prec:0.2115, rec:0.0801, f1:0.1162
>> valid relation with NER prec:0.2115, rec:0.0801, f1:0.1162
g_step 7100, step 11, avg_time 2.185, loss:398.6159
g_step 7200, step 111, avg_time 0.960, loss:356.5884
g_step 7300, step 211, avg_time 0.966, loss:381.3215
g_step 7400, step 311, avg_time 0.964, loss:375.8942
g_step 7500, step 411, avg_time 0.967, loss:395.9403
>> valid entity prec:0.5755, rec:0.5422, f1:0.5584
>> valid relation prec:0.1807, rec:0.0781, f1:0.1090
>> valid relation with NER prec:0.1807, rec:0.0781, f1:0.1090
g_step 7600, step 94, avg_time 2.188, loss:345.3949
g_step 7700, step 194, avg_time 0.960, loss:364.8156
g_step 7800, step 294, avg_time 0.961, loss:360.8716
g_step 7900, step 394, avg_time 0.967, loss:373.1224
g_step 8000, step 77, avg_time 0.960, loss:342.8322
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6148, rec:0.5192, f1:0.5630
>> valid relation prec:0.2084, rec:0.0940, f1:0.1296
>> valid relation with NER prec:0.2084, rec:0.0940, f1:0.1296
g_step 8100, step 177, avg_time 2.177, loss:343.9470
g_step 8200, step 277, avg_time 0.978, loss:355.6912
g_step 8300, step 377, avg_time 0.965, loss:384.6054
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 16:58:53 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 16:58:53 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_16-58-53_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 16:58:54 - WARNING - datasets.builder -   Using custom data configuration default-7d863c5c94894c25
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7d863c5c94894c25/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 16:58:54,612 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:58:54,613 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:58:54,614 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:58:54,615 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:58:54,626 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:58:54,631 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:58:54,632 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:58:54,632 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:58:54,632 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:58:54,632 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:58:54,632 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 16:58:54,792 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:58:57,877 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 16:58:57,881 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7d863c5c94894c25/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:04,  2.26ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.28ba/s] 27%|██▋       | 3/11 [00:00<00:02,  3.81ba/s] 36%|███▋      | 4/11 [00:01<00:01,  4.09ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.25ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.37ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.46ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.53ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.57ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.57ba/s]100%|██████████| 11/11 [00:02<00:00,  4.61ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.98ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.30ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.21ba/s]100%|██████████| 4/4 [00:00<00:00,  5.45ba/s]100%|██████████| 4/4 [00:00<00:00,  4.91ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  8.28ba/s] 27%|██▋       | 3/11 [00:00<00:00, 10.10ba/s] 45%|████▌     | 5/11 [00:00<00:00, 10.60ba/s] 64%|██████▎   | 7/11 [00:00<00:00, 10.80ba/s] 82%|████████▏ | 9/11 [00:00<00:00, 10.78ba/s]100%|██████████| 11/11 [00:00<00:00, 11.62ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.00ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.36ba/s]100%|██████████| 4/4 [00:00<00:00, 11.92ba/s]
[INFO|trainer.py:414] 2023-08-28 16:59:03,154 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 16:59:03,188 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 16:59:03,188 >>   Num examples = 10018
[INFO|trainer.py:1149] 2023-08-28 16:59:03,188 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 16:59:03,188 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 16:59:03,188 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 16:59:03,188 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 16:59:03,188 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:55,  3.32it/s]  0%|          | 2/785 [00:00<03:51,  3.38it/s]  0%|          | 3/785 [00:00<03:49,  3.41it/s]  1%|          | 4/785 [00:01<03:51,  3.37it/s]  1%|          | 5/785 [00:01<03:49,  3.39it/s]  1%|          | 6/785 [00:01<03:48,  3.40it/s]  1%|          | 7/785 [00:02<03:47,  3.41it/s]  1%|          | 8/785 [00:02<03:47,  3.41it/s]  1%|          | 9/785 [00:02<03:48,  3.40it/s]  1%|▏         | 10/785 [00:02<03:48,  3.39it/s]  1%|▏         | 11/785 [00:03<03:48,  3.39it/s]  2%|▏         | 12/785 [00:03<03:48,  3.38it/s]  2%|▏         | 13/785 [00:03<03:48,  3.38it/s]  2%|▏         | 14/785 [00:04<03:48,  3.38it/s]  2%|▏         | 15/785 [00:04<03:49,  3.36it/s]  2%|▏         | 16/785 [00:04<03:48,  3.37it/s]  2%|▏         | 17/785 [00:05<03:47,  3.37it/s]  2%|▏         | 18/785 [00:05<03:47,  3.37it/s]  2%|▏         | 19/785 [00:05<03:47,  3.37it/s]  3%|▎         | 20/785 [00:05<03:46,  3.37it/s]  3%|▎         | 21/785 [00:06<03:46,  3.37it/s]  3%|▎         | 22/785 [00:06<03:46,  3.37it/s]  3%|▎         | 23/785 [00:06<03:45,  3.37it/s]  3%|▎         | 24/785 [00:07<03:45,  3.37it/s]  3%|▎         | 25/785 [00:07<03:45,  3.37it/s]  3%|▎         | 26/785 [00:07<04:00,  3.16it/s]  3%|▎         | 27/785 [00:08<03:55,  3.22it/s]  4%|▎         | 28/785 [00:08<03:51,  3.26it/s]  4%|▎         | 29/785 [00:08<03:49,  3.29it/s]  4%|▍         | 30/785 [00:08<03:47,  3.32it/s]  4%|▍         | 31/785 [00:09<03:45,  3.34it/s]  4%|▍         | 32/785 [00:09<03:44,  3.35it/s]  4%|▍         | 33/785 [00:09<03:44,  3.36it/s]  4%|▍         | 34/785 [00:10<03:43,  3.36it/s]  4%|▍         | 35/785 [00:10<03:42,  3.36it/s]  5%|▍         | 36/785 [00:10<03:43,  3.35it/s]  5%|▍         | 37/785 [00:11<03:42,  3.36it/s]  5%|▍         | 38/785 [00:11<03:41,  3.36it/s]  5%|▍         | 39/785 [00:11<03:41,  3.37it/s]  5%|▌         | 40/785 [00:11<03:41,  3.37it/s]  5%|▌         | 41/785 [00:12<03:40,  3.37it/s]  5%|▌         | 42/785 [00:12<03:40,  3.37it/s]  5%|▌         | 43/785 [00:12<03:40,  3.37it/s]  6%|▌         | 44/785 [00:13<03:39,  3.37it/s]  6%|▌         | 45/785 [00:13<03:39,  3.37it/s]  6%|▌         | 46/785 [00:13<03:39,  3.37it/s]  6%|▌         | 47/785 [00:14<03:43,  3.30it/s]  6%|▌         | 48/785 [00:14<03:41,  3.32it/s]  6%|▌         | 49/785 [00:14<03:40,  3.34it/s]  6%|▋         | 50/785 [00:14<03:39,  3.35it/s]  6%|▋         | 51/785 [00:15<03:38,  3.36it/s]  7%|▋         | 52/785 [00:15<03:38,  3.36it/s]  7%|▋         | 53/785 [00:15<03:37,  3.36it/s]  7%|▋         | 54/785 [00:16<03:37,  3.37it/s]  7%|▋         | 55/785 [00:16<03:36,  3.37it/s]  7%|▋         | 56/785 [00:16<03:36,  3.37it/s]  7%|▋         | 57/785 [00:16<03:35,  3.37it/s]  7%|▋         | 58/785 [00:17<03:41,  3.29it/s]  8%|▊         | 59/785 [00:17<03:45,  3.22it/s]  8%|▊         | 60/785 [00:17<03:42,  3.26it/s]  8%|▊         | 61/785 [00:18<03:39,  3.30it/s]  8%|▊         | 62/785 [00:18<03:37,  3.32it/s]  8%|▊         | 63/785 [00:18<03:36,  3.33it/s]  8%|▊         | 64/785 [00:19<03:35,  3.35it/s]  8%|▊         | 65/785 [00:19<03:34,  3.35it/s]  8%|▊         | 66/785 [00:19<03:33,  3.36it/s]  9%|▊         | 67/785 [00:19<03:33,  3.36it/s]  9%|▊         | 68/785 [00:20<03:40,  3.25it/s]  9%|▉         | 69/785 [00:20<03:36,  3.30it/s]  9%|▉         | 70/785 [00:20<03:34,  3.34it/s]  9%|▉         | 71/785 [00:21<03:32,  3.36it/s]  9%|▉         | 72/785 [00:21<03:31,  3.38it/s]  9%|▉         | 73/785 [00:21<03:30,  3.39it/s]  9%|▉         | 74/785 [00:22<03:29,  3.40it/s] 10%|▉         | 75/785 [00:22<03:28,  3.41it/s] 10%|▉         | 76/785 [00:22<03:27,  3.41it/s] 10%|▉         | 77/785 [00:22<03:27,  3.41it/s] 10%|▉         | 78/785 [00:23<03:27,  3.41it/s] 10%|█         | 79/785 [00:23<03:26,  3.41it/s] 10%|█         | 80/785 [00:23<03:26,  3.42it/s] 10%|█         | 81/785 [00:24<03:26,  3.41it/s] 10%|█         | 82/785 [00:24<03:25,  3.42it/s] 11%|█         | 83/785 [00:24<03:25,  3.42it/s] 11%|█         | 84/785 [00:25<03:25,  3.42it/s] 11%|█         | 85/785 [00:25<03:24,  3.42it/s] 11%|█         | 86/785 [00:25<03:25,  3.39it/s] 11%|█         | 87/785 [00:25<03:25,  3.40it/s] 11%|█         | 88/785 [00:26<03:24,  3.41it/s] 11%|█▏        | 89/785 [00:26<03:24,  3.41it/s] 11%|█▏        | 90/785 [00:26<03:23,  3.41it/s] 12%|█▏        | 91/785 [00:27<03:23,  3.42it/s] 12%|█▏        | 92/785 [00:27<03:22,  3.42it/s] 12%|█▏        | 93/785 [00:27<03:22,  3.42it/s] 12%|█▏        | 94/785 [00:27<03:22,  3.42it/s] 12%|█▏        | 95/785 [00:28<03:21,  3.42it/s] 12%|█▏        | 96/785 [00:28<03:21,  3.41it/s] 12%|█▏        | 97/785 [00:28<03:27,  3.31it/s] 12%|█▏        | 98/785 [00:29<03:25,  3.34it/s] 13%|█▎        | 99/785 [00:29<03:23,  3.37it/s] 13%|█▎        | 100/785 [00:29<03:22,  3.38it/s] 13%|█▎        | 101/785 [00:30<03:21,  3.39it/s] 13%|█▎        | 102/785 [00:30<03:20,  3.40it/s] 13%|█▎        | 103/785 [00:30<03:20,  3.40it/s] 13%|█▎        | 104/785 [00:30<03:19,  3.41it/s] 13%|█▎        | 105/785 [00:31<03:19,  3.41it/s] 14%|█▎        | 106/785 [00:31<03:19,  3.41it/s] 14%|█▎        | 107/785 [00:31<03:18,  3.41it/s] 14%|█▍        | 108/785 [00:32<04:11,  2.70it/s] 14%|█▍        | 109/785 [00:32<03:54,  2.88it/s] 14%|█▍        | 110/785 [00:32<03:43,  3.02it/s] 14%|█▍        | 111/785 [00:33<03:35,  3.13it/s] 14%|█▍        | 112/785 [00:33<03:29,  3.21it/s] 14%|█▍        | 113/785 [00:33<03:25,  3.27it/s] 15%|█▍        | 114/785 [00:34<03:22,  3.31it/s] 15%|█▍        | 115/785 [00:34<03:20,  3.34it/s] 15%|█▍        | 116/785 [00:34<03:18,  3.36it/s] 15%|█▍        | 117/785 [00:34<03:17,  3.38it/s] 15%|█▌        | 118/785 [00:35<03:24,  3.26it/s] 15%|█▌        | 119/785 [00:35<03:21,  3.30it/s] 15%|█▌        | 120/785 [00:35<03:19,  3.33it/s] 15%|█▌        | 121/785 [00:36<03:17,  3.36it/s] 16%|█▌        | 122/785 [00:36<03:16,  3.37it/s] 16%|█▌        | 123/785 [00:36<03:15,  3.39it/s] 16%|█▌        | 124/785 [00:37<03:15,  3.39it/s] 16%|█▌        | 125/785 [00:37<03:14,  3.40it/s] 16%|█▌        | 126/785 [00:37<03:13,  3.40it/s] 16%|█▌        | 127/785 [00:37<03:13,  3.41it/s] 16%|█▋        | 128/785 [00:38<03:12,  3.41it/s] 16%|█▋        | 129/785 [00:38<03:15,  3.36it/s] 17%|█▋        | 130/785 [00:38<03:14,  3.38it/s] 17%|█▋        | 131/785 [00:39<03:13,  3.39it/s] 17%|█▋        | 132/785 [00:39<03:12,  3.39it/s] 17%|█▋        | 133/785 [00:39<03:11,  3.40it/s] 17%|█▋        | 134/785 [00:40<03:11,  3.40it/s] 17%|█▋        | 135/785 [00:40<03:10,  3.41it/s] 17%|█▋        | 136/785 [00:40<03:10,  3.41it/s] 17%|█▋        | 137/785 [00:40<03:10,  3.41it/s] 18%|█▊        | 138/785 [00:41<03:09,  3.41it/s] 18%|█▊        | 139/785 [00:41<03:09,  3.41it/s] 18%|█▊        | 140/785 [00:41<03:10,  3.39it/s] 18%|█▊        | 141/785 [00:42<03:09,  3.39it/s] 18%|█▊        | 142/785 [00:42<03:08,  3.40it/s] 18%|█▊        | 143/785 [00:42<03:08,  3.41it/s] 18%|█▊        | 144/785 [00:42<03:07,  3.41it/s] 18%|█▊        | 145/785 [00:43<03:07,  3.42it/s] 19%|█▊        | 146/785 [00:43<03:07,  3.41it/s] 19%|█▊        | 147/785 [00:43<03:06,  3.41it/s] 19%|█▉        | 148/785 [00:44<03:06,  3.42it/s] 19%|█▉        | 149/785 [00:44<03:06,  3.41it/s] 19%|█▉        | 150/785 [00:44<03:06,  3.41it/s] 19%|█▉        | 151/785 [00:44<03:05,  3.41it/s] 19%|█▉        | 152/785 [00:45<03:05,  3.41it/s] 19%|█▉        | 153/785 [00:45<03:05,  3.41it/s] 20%|█▉        | 154/785 [00:45<03:05,  3.41it/s] 20%|█▉        | 155/785 [00:46<03:04,  3.41it/s] 20%|█▉        | 156/785 [00:46<03:04,  3.41it/s] 20%|██        | 157/785 [00:46<02:46,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 16:59:49,848 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:59:49,848 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 16:59:49,848 >>   Batch size = 8

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 55.46it/s][A
  3%|▎         | 12/431 [00:00<00:08, 47.81it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.90it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.97it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.32it/s][A
  7%|▋         | 32/431 [00:00<00:09, 44.08it/s][A
  9%|▊         | 37/431 [00:00<00:08, 43.81it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.39it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.22it/s][A
 12%|█▏        | 52/431 [00:01<00:09, 40.45it/s][A
 13%|█▎        | 57/431 [00:01<00:09, 41.53it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 42.23it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 42.56it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 42.93it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 42.91it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 42.85it/s][A
 20%|██        | 87/431 [00:02<00:08, 42.90it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 42.81it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.03it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.15it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.42it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.53it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.64it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.48it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.25it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.01it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 42.97it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 42.91it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.11it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.16it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.29it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.34it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.36it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.23it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.07it/s][A
 42%|████▏     | 182/431 [00:04<00:06, 39.76it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 40.95it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 41.76it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 42.33it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 42.90it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.09it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.04it/s][A
 50%|█████     | 217/431 [00:05<00:04, 43.01it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 42.74it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 42.84it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 42.96it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.18it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.47it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.62it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.60it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.47it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.23it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.05it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.03it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.23it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.41it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.56it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.67it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.63it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.47it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.30it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.13it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.12it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.22it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.41it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.53it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.63it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.59it/s][A
 81%|████████  | 347/431 [00:08<00:01, 43.40it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.23it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.19it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.25it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.33it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.42it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.57it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.65it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.45it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.37it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.23it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.17it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.28it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.33it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.50it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.59it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.52it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.52it/s][A 20%|██        | 157/785 [00:56<02:46,  3.77it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:59:59,930 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-28 16:59:59,973 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:00:04,595 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:00:04,894 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:00:05,032 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:12<1:24:19,  8.07s/it] 20%|██        | 159/785 [01:13<59:55,  5.74s/it]   20%|██        | 160/785 [01:13<42:48,  4.11s/it] 21%|██        | 161/785 [01:13<30:50,  2.97s/it] 21%|██        | 162/785 [01:14<22:28,  2.16s/it] 21%|██        | 163/785 [01:14<16:37,  1.60s/it] 21%|██        | 164/785 [01:14<12:32,  1.21s/it] 21%|██        | 165/785 [01:15<09:40,  1.07it/s] 21%|██        | 166/785 [01:15<07:41,  1.34it/s] 21%|██▏       | 167/785 [01:15<06:17,  1.64it/s] 21%|██▏       | 168/785 [01:15<05:18,  1.94it/s] 22%|██▏       | 169/785 [01:16<04:37,  2.22it/s] 22%|██▏       | 170/785 [01:16<04:10,  2.46it/s] 22%|██▏       | 171/785 [01:16<03:49,  2.68it/s] 22%|██▏       | 172/785 [01:17<03:34,  2.85it/s] 22%|██▏       | 173/785 [01:17<03:24,  2.99it/s] 22%|██▏       | 174/785 [01:17<03:21,  3.04it/s] 22%|██▏       | 175/785 [01:18<03:38,  2.80it/s] 22%|██▏       | 176/785 [01:18<03:26,  2.95it/s] 23%|██▎       | 177/785 [01:18<03:18,  3.07it/s] 23%|██▎       | 178/785 [01:19<03:12,  3.16it/s] 23%|██▎       | 179/785 [01:19<03:08,  3.22it/s] 23%|██▎       | 180/785 [01:19<03:10,  3.18it/s] 23%|██▎       | 181/785 [01:19<03:06,  3.24it/s] 23%|██▎       | 182/785 [01:20<03:04,  3.27it/s] 23%|██▎       | 183/785 [01:20<03:02,  3.30it/s] 23%|██▎       | 184/785 [01:20<03:00,  3.32it/s] 24%|██▎       | 185/785 [01:21<02:59,  3.34it/s] 24%|██▎       | 186/785 [01:21<02:58,  3.35it/s] 24%|██▍       | 187/785 [01:21<02:58,  3.36it/s] 24%|██▍       | 188/785 [01:22<02:57,  3.36it/s] 24%|██▍       | 189/785 [01:22<02:57,  3.36it/s] 24%|██▍       | 190/785 [01:22<02:56,  3.37it/s] 24%|██▍       | 191/785 [01:22<03:00,  3.29it/s] 24%|██▍       | 192/785 [01:23<02:58,  3.31it/s] 25%|██▍       | 193/785 [01:23<02:57,  3.33it/s] 25%|██▍       | 194/785 [01:23<02:56,  3.34it/s] 25%|██▍       | 195/785 [01:24<02:56,  3.35it/s] 25%|██▍       | 196/785 [01:24<02:55,  3.35it/s] 25%|██▌       | 197/785 [01:24<02:55,  3.36it/s] 25%|██▌       | 198/785 [01:25<02:54,  3.36it/s] 25%|██▌       | 199/785 [01:25<02:53,  3.37it/s] 25%|██▌       | 200/785 [01:25<02:53,  3.37it/s] 26%|██▌       | 201/785 [01:25<02:53,  3.37it/s] 26%|██▌       | 202/785 [01:26<02:53,  3.37it/s] 26%|██▌       | 203/785 [01:26<02:52,  3.37it/s] 26%|██▌       | 204/785 [01:26<02:54,  3.33it/s] 26%|██▌       | 205/785 [01:27<02:53,  3.34it/s] 26%|██▌       | 206/785 [01:27<02:52,  3.35it/s] 26%|██▋       | 207/785 [01:27<02:52,  3.36it/s] 26%|██▋       | 208/785 [01:27<02:51,  3.36it/s] 27%|██▋       | 209/785 [01:28<02:51,  3.36it/s] 27%|██▋       | 210/785 [01:28<02:50,  3.36it/s] 27%|██▋       | 211/785 [01:28<02:50,  3.36it/s] 27%|██▋       | 212/785 [01:29<02:50,  3.37it/s] 27%|██▋       | 213/785 [01:29<02:49,  3.37it/s] 27%|██▋       | 214/785 [01:29<02:49,  3.37it/s] 27%|██▋       | 215/785 [01:30<02:50,  3.34it/s] 28%|██▊       | 216/785 [01:30<02:49,  3.35it/s] 28%|██▊       | 217/785 [01:30<02:49,  3.36it/s] 28%|██▊       | 218/785 [01:30<02:48,  3.36it/s] 28%|██▊       | 219/785 [01:31<02:48,  3.36it/s] 28%|██▊       | 220/785 [01:31<02:47,  3.37it/s] 28%|██▊       | 221/785 [01:31<02:47,  3.37it/s] 28%|██▊       | 222/785 [01:32<02:47,  3.37it/s] 28%|██▊       | 223/785 [01:32<02:46,  3.37it/s] 29%|██▊       | 224/785 [01:32<02:46,  3.37it/s] 29%|██▊       | 225/785 [01:33<02:46,  3.37it/s] 29%|██▉       | 226/785 [01:33<02:47,  3.34it/s] 29%|██▉       | 227/785 [01:33<02:46,  3.35it/s] 29%|██▉       | 228/785 [01:33<02:46,  3.35it/s] 29%|██▉       | 229/785 [01:34<02:45,  3.36it/s] 29%|██▉       | 230/785 [01:34<02:45,  3.36it/s] 29%|██▉       | 231/785 [01:34<02:44,  3.37it/s] 30%|██▉       | 232/785 [01:35<02:44,  3.37it/s] 30%|██▉       | 233/785 [01:35<02:43,  3.37it/s] 30%|██▉       | 234/785 [01:35<02:43,  3.37it/s] 30%|██▉       | 235/785 [01:36<02:43,  3.37it/s] 30%|███       | 236/785 [01:36<02:42,  3.37it/s] 30%|███       | 237/785 [01:36<02:43,  3.36it/s] 30%|███       | 238/785 [01:36<02:42,  3.36it/s] 30%|███       | 239/785 [01:37<02:42,  3.37it/s] 31%|███       | 240/785 [01:37<02:41,  3.37it/s] 31%|███       | 241/785 [01:37<02:41,  3.37it/s] 31%|███       | 242/785 [01:38<02:41,  3.36it/s] 31%|███       | 243/785 [01:38<02:41,  3.36it/s] 31%|███       | 244/785 [01:38<02:40,  3.36it/s] 31%|███       | 245/785 [01:38<02:40,  3.37it/s] 31%|███▏      | 246/785 [01:39<02:39,  3.37it/s] 31%|███▏      | 247/785 [01:39<02:39,  3.37it/s] 32%|███▏      | 248/785 [01:40<03:35,  2.49it/s] 32%|███▏      | 249/785 [01:40<03:18,  2.70it/s] 32%|███▏      | 250/785 [01:40<03:06,  2.87it/s] 32%|███▏      | 251/785 [01:41<02:57,  3.01it/s] 32%|███▏      | 252/785 [01:41<02:51,  3.11it/s] 32%|███▏      | 253/785 [01:41<02:46,  3.19it/s] 32%|███▏      | 254/785 [01:42<02:43,  3.25it/s] 32%|███▏      | 255/785 [01:42<02:40,  3.30it/s] 33%|███▎      | 256/785 [01:42<02:38,  3.33it/s] 33%|███▎      | 257/785 [01:42<02:42,  3.26it/s] 33%|███▎      | 258/785 [01:43<02:39,  3.30it/s] 33%|███▎      | 259/785 [01:43<02:37,  3.33it/s] 33%|███▎      | 260/785 [01:43<02:36,  3.35it/s] 33%|███▎      | 261/785 [01:44<02:35,  3.37it/s] 33%|███▎      | 262/785 [01:44<02:34,  3.38it/s] 34%|███▎      | 263/785 [01:44<02:33,  3.39it/s] 34%|███▎      | 264/785 [01:44<02:33,  3.40it/s] 34%|███▍      | 265/785 [01:45<02:32,  3.41it/s] 34%|███▍      | 266/785 [01:45<02:32,  3.41it/s] 34%|███▍      | 267/785 [01:45<02:31,  3.41it/s] 34%|███▍      | 268/785 [01:46<02:40,  3.22it/s] 34%|███▍      | 269/785 [01:46<02:37,  3.28it/s] 34%|███▍      | 270/785 [01:46<02:35,  3.32it/s] 35%|███▍      | 271/785 [01:47<02:33,  3.34it/s] 35%|███▍      | 272/785 [01:47<02:32,  3.37it/s] 35%|███▍      | 273/785 [01:47<02:31,  3.38it/s] 35%|███▍      | 274/785 [01:47<02:30,  3.39it/s] 35%|███▌      | 275/785 [01:48<02:29,  3.40it/s] 35%|███▌      | 276/785 [01:48<02:29,  3.41it/s] 35%|███▌      | 277/785 [01:48<02:29,  3.41it/s] 35%|███▌      | 278/785 [01:49<02:28,  3.41it/s] 36%|███▌      | 279/785 [01:49<02:40,  3.14it/s] 36%|███▌      | 280/785 [01:49<02:36,  3.22it/s] 36%|███▌      | 281/785 [01:50<02:33,  3.28it/s] 36%|███▌      | 282/785 [01:50<02:31,  3.31it/s] 36%|███▌      | 283/785 [01:50<02:30,  3.34it/s] 36%|███▌      | 284/785 [01:50<02:28,  3.37it/s] 36%|███▋      | 285/785 [01:51<02:28,  3.38it/s] 36%|███▋      | 286/785 [01:51<02:27,  3.39it/s] 37%|███▋      | 287/785 [01:51<02:26,  3.40it/s] 37%|███▋      | 288/785 [01:52<02:26,  3.40it/s] 37%|███▋      | 289/785 [01:52<02:57,  2.79it/s] 37%|███▋      | 290/785 [01:52<02:47,  2.95it/s] 37%|███▋      | 291/785 [01:53<02:40,  3.08it/s] 37%|███▋      | 292/785 [01:53<02:35,  3.17it/s] 37%|███▋      | 293/785 [01:53<02:31,  3.24it/s] 37%|███▋      | 294/785 [01:54<02:29,  3.29it/s] 38%|███▊      | 295/785 [01:54<02:27,  3.33it/s] 38%|███▊      | 296/785 [01:54<02:27,  3.32it/s] 38%|███▊      | 297/785 [01:55<02:25,  3.35it/s] 38%|███▊      | 298/785 [01:55<02:24,  3.37it/s] 38%|███▊      | 299/785 [01:55<02:23,  3.38it/s] 38%|███▊      | 300/785 [01:55<02:23,  3.39it/s] 38%|███▊      | 301/785 [01:56<02:22,  3.40it/s] 38%|███▊      | 302/785 [01:56<02:21,  3.40it/s] 39%|███▊      | 303/785 [01:56<02:21,  3.40it/s] 39%|███▊      | 304/785 [01:57<02:21,  3.41it/s] 39%|███▉      | 305/785 [01:57<02:20,  3.41it/s] 39%|███▉      | 306/785 [01:57<02:22,  3.37it/s] 39%|███▉      | 307/785 [01:57<02:21,  3.38it/s] 39%|███▉      | 308/785 [01:58<02:20,  3.39it/s] 39%|███▉      | 309/785 [01:58<02:20,  3.40it/s] 39%|███▉      | 310/785 [01:58<02:19,  3.40it/s] 40%|███▉      | 311/785 [01:59<02:19,  3.41it/s] 40%|███▉      | 312/785 [01:59<02:18,  3.41it/s] 40%|███▉      | 313/785 [01:59<02:18,  3.41it/s] 40%|████      | 314/785 [01:59<02:03,  3.82it/s][INFO|trainer.py:2140] 2023-08-28 17:01:03,079 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:01:03,079 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 17:01:03,079 >>   Batch size = 8
{'eval_loss': 0.9501715302467346, 'eval_runtime': 9.9929, 'eval_samples_per_second': 344.844, 'eval_steps_per_second': 43.131, 'epoch': 1.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 55.37it/s][A
  3%|▎         | 12/431 [00:00<00:08, 47.38it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.64it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.83it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.26it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.97it/s][A
  9%|▊         | 37/431 [00:00<00:08, 43.82it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.49it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.57it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.56it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.55it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.56it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.52it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.36it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.39it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.36it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.39it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.46it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.45it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.57it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.48it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.41it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.33it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.33it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.40it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.28it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.39it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.42it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.59it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.46it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.37it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 41.81it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 42.23it/s][A
 40%|███▉      | 172/431 [00:03<00:06, 42.71it/s][A
 41%|████      | 177/431 [00:04<00:05, 42.83it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 42.97it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.10it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.31it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.35it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.21it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.31it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.35it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.42it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.48it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.46it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.42it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.38it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.37it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.42it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.43it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.40it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.40it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.43it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.44it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.42it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.27it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.29it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.41it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.51it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.49it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.45it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.42it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.51it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.37it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.41it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.32it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.43it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.53it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.50it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.44it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.48it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.43it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.44it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.49it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.45it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.41it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.47it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.45it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.46it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.49it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.44it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.37it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.37it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.46it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 42.99it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 42.99it/s][A 40%|████      | 314/785 [02:09<02:03,  3.82it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:01:13,171 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-28 17:01:13,265 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:01:17,549 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:01:17,578 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:01:17,587 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:24<59:06,  7.55s/it] 40%|████      | 316/785 [02:24<41:59,  5.37s/it] 40%|████      | 317/785 [02:25<30:01,  3.85s/it] 41%|████      | 318/785 [02:25<21:39,  2.78s/it] 41%|████      | 319/785 [02:25<15:49,  2.04s/it] 41%|████      | 320/785 [02:25<11:44,  1.51s/it] 41%|████      | 321/785 [02:26<08:53,  1.15s/it] 41%|████      | 322/785 [02:26<06:53,  1.12it/s] 41%|████      | 323/785 [02:26<05:29,  1.40it/s] 41%|████▏     | 324/785 [02:27<04:31,  1.70it/s] 41%|████▏     | 325/785 [02:27<03:50,  2.00it/s] 42%|████▏     | 326/785 [02:27<03:21,  2.28it/s] 42%|████▏     | 327/785 [02:27<03:01,  2.53it/s] 42%|████▏     | 328/785 [02:28<02:47,  2.73it/s] 42%|████▏     | 329/785 [02:28<02:36,  2.91it/s] 42%|████▏     | 330/785 [02:28<02:29,  3.04it/s] 42%|████▏     | 331/785 [02:29<02:24,  3.15it/s] 42%|████▏     | 332/785 [02:29<02:20,  3.23it/s] 42%|████▏     | 333/785 [02:29<02:17,  3.28it/s] 43%|████▎     | 334/785 [02:30<02:15,  3.33it/s] 43%|████▎     | 335/785 [02:30<02:14,  3.35it/s] 43%|████▎     | 336/785 [02:30<02:13,  3.37it/s] 43%|████▎     | 337/785 [02:30<02:12,  3.39it/s] 43%|████▎     | 338/785 [02:31<02:11,  3.40it/s] 43%|████▎     | 339/785 [02:31<02:12,  3.37it/s] 43%|████▎     | 340/785 [02:31<02:11,  3.38it/s] 43%|████▎     | 341/785 [02:32<02:10,  3.40it/s] 44%|████▎     | 342/785 [02:32<02:10,  3.40it/s] 44%|████▎     | 343/785 [02:32<02:09,  3.41it/s] 44%|████▍     | 344/785 [02:32<02:09,  3.41it/s] 44%|████▍     | 345/785 [02:33<02:08,  3.41it/s] 44%|████▍     | 346/785 [02:33<02:08,  3.42it/s] 44%|████▍     | 347/785 [02:33<02:08,  3.42it/s] 44%|████▍     | 348/785 [02:34<02:07,  3.42it/s] 44%|████▍     | 349/785 [02:34<02:07,  3.42it/s] 45%|████▍     | 350/785 [02:34<02:09,  3.35it/s] 45%|████▍     | 351/785 [02:35<02:08,  3.38it/s] 45%|████▍     | 352/785 [02:35<02:07,  3.39it/s] 45%|████▍     | 353/785 [02:35<02:06,  3.40it/s] 45%|████▌     | 354/785 [02:35<02:06,  3.41it/s] 45%|████▌     | 355/785 [02:36<02:06,  3.41it/s] 45%|████▌     | 356/785 [02:36<02:05,  3.41it/s] 45%|████▌     | 357/785 [02:36<02:05,  3.42it/s] 46%|████▌     | 358/785 [02:37<02:04,  3.42it/s] 46%|████▌     | 359/785 [02:37<02:04,  3.42it/s] 46%|████▌     | 360/785 [02:37<02:04,  3.42it/s] 46%|████▌     | 361/785 [02:37<02:06,  3.35it/s] 46%|████▌     | 362/785 [02:38<02:05,  3.37it/s] 46%|████▌     | 363/785 [02:38<02:04,  3.38it/s] 46%|████▋     | 364/785 [02:38<02:03,  3.40it/s] 46%|████▋     | 365/785 [02:39<02:03,  3.40it/s] 47%|████▋     | 366/785 [02:39<02:02,  3.41it/s] 47%|████▋     | 367/785 [02:39<02:02,  3.41it/s] 47%|████▋     | 368/785 [02:40<02:02,  3.42it/s] 47%|████▋     | 369/785 [02:40<02:01,  3.42it/s] 47%|████▋     | 370/785 [02:40<02:01,  3.41it/s] 47%|████▋     | 371/785 [02:40<02:01,  3.41it/s] 47%|████▋     | 372/785 [02:41<02:02,  3.37it/s] 48%|████▊     | 373/785 [02:41<02:01,  3.39it/s] 48%|████▊     | 374/785 [02:41<02:01,  3.40it/s] 48%|████▊     | 375/785 [02:42<02:00,  3.40it/s] 48%|████▊     | 376/785 [02:42<02:00,  3.41it/s] 48%|████▊     | 377/785 [02:42<01:59,  3.41it/s] 48%|████▊     | 378/785 [02:42<01:59,  3.41it/s] 48%|████▊     | 379/785 [02:43<01:58,  3.42it/s] 48%|████▊     | 380/785 [02:43<01:58,  3.42it/s] 49%|████▊     | 381/785 [02:43<01:58,  3.42it/s] 49%|████▊     | 382/785 [02:44<01:58,  3.41it/s] 49%|████▉     | 383/785 [02:44<02:07,  3.16it/s] 49%|████▉     | 384/785 [02:44<02:03,  3.24it/s] 49%|████▉     | 385/785 [02:45<02:01,  3.29it/s] 49%|████▉     | 386/785 [02:45<02:00,  3.32it/s] 49%|████▉     | 387/785 [02:45<01:58,  3.35it/s] 49%|████▉     | 388/785 [02:45<01:57,  3.37it/s] 50%|████▉     | 389/785 [02:46<01:57,  3.38it/s] 50%|████▉     | 390/785 [02:46<01:56,  3.39it/s] 50%|████▉     | 391/785 [02:46<01:55,  3.40it/s] 50%|████▉     | 392/785 [02:47<01:55,  3.41it/s] 50%|█████     | 393/785 [02:47<02:18,  2.84it/s] 50%|█████     | 394/785 [02:47<02:10,  2.99it/s] 50%|█████     | 395/785 [02:48<02:05,  3.11it/s] 50%|█████     | 396/785 [02:48<02:01,  3.19it/s] 51%|█████     | 397/785 [02:48<01:59,  3.26it/s] 51%|█████     | 398/785 [02:49<01:57,  3.30it/s] 51%|█████     | 399/785 [02:49<01:55,  3.34it/s] 51%|█████     | 400/785 [02:49<01:54,  3.36it/s] 51%|█████     | 401/785 [02:49<01:53,  3.38it/s] 51%|█████     | 402/785 [02:50<01:52,  3.39it/s] 51%|█████▏    | 403/785 [02:50<02:00,  3.18it/s] 51%|█████▏    | 404/785 [02:50<01:57,  3.23it/s] 52%|█████▏    | 405/785 [02:51<01:55,  3.28it/s] 52%|█████▏    | 406/785 [02:51<01:54,  3.30it/s] 52%|█████▏    | 407/785 [02:51<01:53,  3.33it/s] 52%|█████▏    | 408/785 [02:52<01:52,  3.34it/s] 52%|█████▏    | 409/785 [02:52<01:52,  3.35it/s] 52%|█████▏    | 410/785 [02:52<01:51,  3.35it/s] 52%|█████▏    | 411/785 [02:52<01:51,  3.36it/s] 52%|█████▏    | 412/785 [02:53<01:50,  3.36it/s] 53%|█████▎    | 413/785 [02:53<01:51,  3.33it/s] 53%|█████▎    | 414/785 [02:53<01:50,  3.34it/s] 53%|█████▎    | 415/785 [02:54<01:50,  3.35it/s] 53%|█████▎    | 416/785 [02:54<01:49,  3.36it/s] 53%|█████▎    | 417/785 [02:54<01:49,  3.37it/s] 53%|█████▎    | 418/785 [02:55<01:48,  3.39it/s] 53%|█████▎    | 419/785 [02:55<01:47,  3.40it/s] 54%|█████▎    | 420/785 [02:55<01:47,  3.40it/s] 54%|█████▎    | 421/785 [02:55<01:49,  3.31it/s] 54%|█████▍    | 422/785 [02:56<01:48,  3.35it/s] 54%|█████▍    | 423/785 [02:56<01:47,  3.37it/s] 54%|█████▍    | 424/785 [02:56<01:46,  3.38it/s] 54%|█████▍    | 425/785 [02:57<01:46,  3.39it/s] 54%|█████▍    | 426/785 [02:57<01:45,  3.40it/s] 54%|█████▍    | 427/785 [02:57<01:45,  3.40it/s] 55%|█████▍    | 428/785 [02:58<01:44,  3.41it/s] 55%|█████▍    | 429/785 [02:58<01:44,  3.41it/s] 55%|█████▍    | 430/785 [02:58<01:43,  3.42it/s] 55%|█████▍    | 431/785 [02:58<01:43,  3.42it/s] 55%|█████▌    | 432/785 [02:59<01:48,  3.26it/s] 55%|█████▌    | 433/785 [02:59<01:46,  3.31it/s] 55%|█████▌    | 434/785 [02:59<01:45,  3.34it/s] 55%|█████▌    | 435/785 [03:00<01:44,  3.36it/s] 56%|█████▌    | 436/785 [03:00<01:43,  3.38it/s] 56%|█████▌    | 437/785 [03:00<01:42,  3.39it/s] 56%|█████▌    | 438/785 [03:00<01:42,  3.40it/s] 56%|█████▌    | 439/785 [03:01<01:41,  3.40it/s] 56%|█████▌    | 440/785 [03:01<01:41,  3.41it/s] 56%|█████▌    | 441/785 [03:01<01:40,  3.41it/s] 56%|█████▋    | 442/785 [03:02<01:40,  3.41it/s] 56%|█████▋    | 443/785 [03:02<01:40,  3.39it/s] 57%|█████▋    | 444/785 [03:02<01:40,  3.40it/s] 57%|█████▋    | 445/785 [03:03<01:39,  3.41it/s] 57%|█████▋    | 446/785 [03:03<01:39,  3.41it/s] 57%|█████▋    | 447/785 [03:03<01:39,  3.41it/s] 57%|█████▋    | 448/785 [03:03<01:38,  3.41it/s] 57%|█████▋    | 449/785 [03:04<01:38,  3.42it/s] 57%|█████▋    | 450/785 [03:04<01:38,  3.42it/s] 57%|█████▋    | 451/785 [03:04<01:37,  3.42it/s] 58%|█████▊    | 452/785 [03:05<01:37,  3.42it/s] 58%|█████▊    | 453/785 [03:05<01:37,  3.42it/s] 58%|█████▊    | 454/785 [03:05<01:37,  3.38it/s] 58%|█████▊    | 455/785 [03:05<01:37,  3.39it/s] 58%|█████▊    | 456/785 [03:06<01:36,  3.41it/s] 58%|█████▊    | 457/785 [03:06<01:36,  3.41it/s] 58%|█████▊    | 458/785 [03:06<01:35,  3.41it/s] 58%|█████▊    | 459/785 [03:07<01:35,  3.41it/s] 59%|█████▊    | 460/785 [03:07<01:35,  3.42it/s] 59%|█████▊    | 461/785 [03:07<01:34,  3.42it/s] 59%|█████▉    | 462/785 [03:08<01:34,  3.42it/s] 59%|█████▉    | 463/785 [03:08<01:34,  3.42it/s] 59%|█████▉    | 464/785 [03:08<01:33,  3.42it/s] 59%|█████▉    | 465/785 [03:08<01:35,  3.36it/s] 59%|█████▉    | 466/785 [03:09<01:34,  3.38it/s] 59%|█████▉    | 467/785 [03:09<01:33,  3.39it/s] 60%|█████▉    | 468/785 [03:09<01:33,  3.40it/s] 60%|█████▉    | 469/785 [03:10<01:32,  3.40it/s] 60%|█████▉    | 470/785 [03:10<01:32,  3.41it/s] 60%|██████    | 471/785 [03:10<01:22,  3.81it/s][INFO|trainer.py:2140] 2023-08-28 17:02:13,774 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:02:13,774 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 17:02:13,774 >>   Batch size = 8
{'eval_loss': 0.9564358592033386, 'eval_runtime': 9.9377, 'eval_samples_per_second': 346.759, 'eval_steps_per_second': 43.37, 'epoch': 2.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 55.25it/s][A
  3%|▎         | 12/431 [00:00<00:08, 47.79it/s][A
  4%|▍         | 17/431 [00:00<00:08, 46.06it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.91it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.31it/s][A
  7%|▋         | 32/431 [00:00<00:09, 44.01it/s][A
  9%|▊         | 37/431 [00:00<00:08, 43.83it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.38it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.56it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.70it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.65it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.54it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.52it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.42it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.46it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.21it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.29it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.32it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.51it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.61it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.56it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.36it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.39it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.40it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.34it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.36it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.46it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.58it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.56it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.46it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.47it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.44it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.29it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.26it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.39it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.52it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.51it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.54it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.36it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.38it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.36it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.27it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.39it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.35it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.51it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.56it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.49it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.42it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.48it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.33it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.31it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.35it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.18it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.61it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.66it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.51it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.42it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.42it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.36it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.37it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.24it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.49it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.55it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.47it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 42.64it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 37.45it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 39.29it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 40.63it/s][A
 81%|████████  | 347/431 [00:08<00:02, 41.65it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 42.17it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 42.77it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.10it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.13it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 42.68it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 42.65it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 42.96it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.12it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.43it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.58it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.71it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.76it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.41it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.02it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 42.83it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.16it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.16it/s][A 60%|██████    | 471/785 [03:20<01:22,  3.81it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:02:23,787 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-28 17:02:23,822 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:02:30,015 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:02:30,032 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:02:30,041 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:33<36:48,  7.06s/it] 60%|██████    | 473/785 [03:33<26:09,  5.03s/it] 60%|██████    | 474/785 [03:34<18:42,  3.61s/it] 61%|██████    | 475/785 [03:34<13:31,  2.62s/it] 61%|██████    | 476/785 [03:34<09:53,  1.92s/it] 61%|██████    | 477/785 [03:34<07:21,  1.43s/it] 61%|██████    | 478/785 [03:35<05:35,  1.09s/it] 61%|██████    | 479/785 [03:35<04:21,  1.17it/s] 61%|██████    | 480/785 [03:35<03:29,  1.46it/s] 61%|██████▏   | 481/785 [03:36<02:53,  1.76it/s] 61%|██████▏   | 482/785 [03:36<02:27,  2.05it/s] 62%|██████▏   | 483/785 [03:36<02:09,  2.33it/s] 62%|██████▏   | 484/785 [03:37<01:57,  2.56it/s] 62%|██████▏   | 485/785 [03:37<01:48,  2.76it/s] 62%|██████▏   | 486/785 [03:37<01:42,  2.92it/s] 62%|██████▏   | 487/785 [03:37<01:38,  3.04it/s] 62%|██████▏   | 488/785 [03:38<01:34,  3.13it/s] 62%|██████▏   | 489/785 [03:38<01:32,  3.20it/s] 62%|██████▏   | 490/785 [03:38<01:30,  3.25it/s] 63%|██████▎   | 491/785 [03:39<01:29,  3.29it/s] 63%|██████▎   | 492/785 [03:39<01:28,  3.31it/s] 63%|██████▎   | 493/785 [03:39<01:27,  3.33it/s] 63%|██████▎   | 494/785 [03:40<01:26,  3.35it/s] 63%|██████▎   | 495/785 [03:40<01:38,  2.94it/s] 63%|██████▎   | 496/785 [03:40<01:34,  3.06it/s] 63%|██████▎   | 497/785 [03:41<01:31,  3.14it/s] 63%|██████▎   | 498/785 [03:41<01:29,  3.21it/s] 64%|██████▎   | 499/785 [03:41<01:27,  3.26it/s] 64%|██████▎   | 500/785 [03:41<01:26,  3.29it/s]                                                  64%|██████▎   | 500/785 [03:41<01:26,  3.29it/s] 64%|██████▍   | 501/785 [03:42<01:25,  3.31it/s] 64%|██████▍   | 502/785 [03:42<01:24,  3.33it/s] 64%|██████▍   | 503/785 [03:42<01:24,  3.34it/s] 64%|██████▍   | 504/785 [03:43<01:23,  3.35it/s] 64%|██████▍   | 505/785 [03:43<01:24,  3.33it/s] 64%|██████▍   | 506/785 [03:43<01:23,  3.34it/s] 65%|██████▍   | 507/785 [03:44<01:23,  3.34it/s] 65%|██████▍   | 508/785 [03:44<01:22,  3.35it/s] 65%|██████▍   | 509/785 [03:44<01:22,  3.36it/s] 65%|██████▍   | 510/785 [03:44<01:21,  3.36it/s] 65%|██████▌   | 511/785 [03:45<01:21,  3.37it/s] 65%|██████▌   | 512/785 [03:45<01:21,  3.37it/s] 65%|██████▌   | 513/785 [03:45<01:20,  3.37it/s] 65%|██████▌   | 514/785 [03:46<01:20,  3.37it/s] 66%|██████▌   | 515/785 [03:46<01:20,  3.37it/s] 66%|██████▌   | 516/785 [03:46<01:23,  3.23it/s] 66%|██████▌   | 517/785 [03:47<01:21,  3.27it/s] 66%|██████▌   | 518/785 [03:47<01:20,  3.30it/s] 66%|██████▌   | 519/785 [03:47<01:20,  3.32it/s] 66%|██████▌   | 520/785 [03:47<01:19,  3.34it/s] 66%|██████▋   | 521/785 [03:48<01:18,  3.35it/s] 66%|██████▋   | 522/785 [03:48<01:18,  3.35it/s] 67%|██████▋   | 523/785 [03:48<01:17,  3.36it/s] 67%|██████▋   | 524/785 [03:49<01:17,  3.36it/s] 67%|██████▋   | 525/785 [03:49<01:17,  3.37it/s] 67%|██████▋   | 526/785 [03:49<01:27,  2.95it/s] 67%|██████▋   | 527/785 [03:50<01:24,  3.06it/s] 67%|██████▋   | 528/785 [03:50<01:21,  3.15it/s] 67%|██████▋   | 529/785 [03:50<01:19,  3.21it/s] 68%|██████▊   | 530/785 [03:51<01:18,  3.26it/s] 68%|██████▊   | 531/785 [03:51<01:17,  3.29it/s] 68%|██████▊   | 532/785 [03:51<01:16,  3.31it/s] 68%|██████▊   | 533/785 [03:51<01:15,  3.33it/s] 68%|██████▊   | 534/785 [03:52<01:15,  3.34it/s] 68%|██████▊   | 535/785 [03:52<01:14,  3.35it/s] 68%|██████▊   | 536/785 [03:53<01:39,  2.49it/s] 68%|██████▊   | 537/785 [03:53<01:31,  2.70it/s] 69%|██████▊   | 538/785 [03:53<01:25,  2.88it/s] 69%|██████▊   | 539/785 [03:54<01:21,  3.01it/s] 69%|██████▉   | 540/785 [03:54<01:18,  3.11it/s] 69%|██████▉   | 541/785 [03:54<01:16,  3.18it/s] 69%|██████▉   | 542/785 [03:54<01:15,  3.23it/s] 69%|██████▉   | 543/785 [03:55<01:13,  3.27it/s] 69%|██████▉   | 544/785 [03:55<01:12,  3.30it/s] 69%|██████▉   | 545/785 [03:56<01:31,  2.62it/s] 70%|██████▉   | 546/785 [03:56<01:25,  2.81it/s] 70%|██████▉   | 547/785 [03:56<01:20,  2.96it/s] 70%|██████▉   | 548/785 [03:56<01:17,  3.07it/s] 70%|██████▉   | 549/785 [03:57<01:14,  3.15it/s] 70%|███████   | 550/785 [03:57<01:13,  3.21it/s] 70%|███████   | 551/785 [03:57<01:11,  3.26it/s] 70%|███████   | 552/785 [03:58<01:10,  3.29it/s] 70%|███████   | 553/785 [03:58<01:09,  3.32it/s] 71%|███████   | 554/785 [03:58<01:09,  3.33it/s] 71%|███████   | 555/785 [03:59<01:08,  3.34it/s] 71%|███████   | 556/785 [03:59<01:08,  3.35it/s] 71%|███████   | 557/785 [03:59<01:07,  3.35it/s] 71%|███████   | 558/785 [03:59<01:07,  3.36it/s] 71%|███████   | 559/785 [04:00<01:07,  3.36it/s] 71%|███████▏  | 560/785 [04:00<01:07,  3.35it/s] 71%|███████▏  | 561/785 [04:00<01:06,  3.36it/s] 72%|███████▏  | 562/785 [04:01<01:06,  3.36it/s] 72%|███████▏  | 563/785 [04:01<01:05,  3.36it/s] 72%|███████▏  | 564/785 [04:01<01:05,  3.37it/s] 72%|███████▏  | 565/785 [04:02<01:05,  3.37it/s] 72%|███████▏  | 566/785 [04:02<01:04,  3.37it/s] 72%|███████▏  | 567/785 [04:02<01:04,  3.37it/s] 72%|███████▏  | 568/785 [04:02<01:04,  3.37it/s] 72%|███████▏  | 569/785 [04:03<01:04,  3.37it/s] 73%|███████▎  | 570/785 [04:03<01:03,  3.38it/s] 73%|███████▎  | 571/785 [04:03<01:03,  3.37it/s] 73%|███████▎  | 572/785 [04:04<01:02,  3.39it/s] 73%|███████▎  | 573/785 [04:04<01:02,  3.39it/s] 73%|███████▎  | 574/785 [04:04<01:01,  3.40it/s] 73%|███████▎  | 575/785 [04:04<01:01,  3.41it/s] 73%|███████▎  | 576/785 [04:05<01:01,  3.41it/s] 74%|███████▎  | 577/785 [04:05<01:00,  3.42it/s] 74%|███████▎  | 578/785 [04:05<01:00,  3.41it/s] 74%|███████▍  | 579/785 [04:06<01:00,  3.41it/s] 74%|███████▍  | 580/785 [04:06<01:00,  3.42it/s] 74%|███████▍  | 581/785 [04:06<00:59,  3.41it/s] 74%|███████▍  | 582/785 [04:07<01:00,  3.36it/s] 74%|███████▍  | 583/785 [04:07<00:59,  3.38it/s] 74%|███████▍  | 584/785 [04:07<00:59,  3.39it/s] 75%|███████▍  | 585/785 [04:07<00:58,  3.40it/s] 75%|███████▍  | 586/785 [04:08<00:58,  3.40it/s] 75%|███████▍  | 587/785 [04:08<00:58,  3.41it/s] 75%|███████▍  | 588/785 [04:08<00:57,  3.41it/s] 75%|███████▌  | 589/785 [04:09<00:57,  3.41it/s] 75%|███████▌  | 590/785 [04:09<00:57,  3.41it/s] 75%|███████▌  | 591/785 [04:09<00:56,  3.41it/s] 75%|███████▌  | 592/785 [04:09<00:56,  3.42it/s] 76%|███████▌  | 593/785 [04:10<00:56,  3.40it/s] 76%|███████▌  | 594/785 [04:10<00:56,  3.41it/s] 76%|███████▌  | 595/785 [04:10<00:55,  3.41it/s] 76%|███████▌  | 596/785 [04:11<00:55,  3.42it/s] 76%|███████▌  | 597/785 [04:11<00:55,  3.41it/s] 76%|███████▌  | 598/785 [04:11<00:54,  3.42it/s] 76%|███████▋  | 599/785 [04:12<00:54,  3.42it/s] 76%|███████▋  | 600/785 [04:12<00:54,  3.42it/s] 77%|███████▋  | 601/785 [04:12<00:53,  3.42it/s] 77%|███████▋  | 602/785 [04:12<00:53,  3.42it/s] 77%|███████▋  | 603/785 [04:13<00:53,  3.42it/s] 77%|███████▋  | 604/785 [04:13<00:53,  3.37it/s] 77%|███████▋  | 605/785 [04:13<00:53,  3.38it/s] 77%|███████▋  | 606/785 [04:14<00:52,  3.39it/s] 77%|███████▋  | 607/785 [04:14<00:52,  3.40it/s] 77%|███████▋  | 608/785 [04:14<00:51,  3.41it/s] 78%|███████▊  | 609/785 [04:14<00:51,  3.41it/s] 78%|███████▊  | 610/785 [04:15<00:51,  3.41it/s] 78%|███████▊  | 611/785 [04:15<00:50,  3.41it/s] 78%|███████▊  | 612/785 [04:15<00:50,  3.42it/s] 78%|███████▊  | 613/785 [04:16<00:50,  3.42it/s] 78%|███████▊  | 614/785 [04:16<00:50,  3.42it/s] 78%|███████▊  | 615/785 [04:16<00:50,  3.39it/s] 78%|███████▊  | 616/785 [04:17<00:49,  3.40it/s] 79%|███████▊  | 617/785 [04:17<00:49,  3.40it/s] 79%|███████▊  | 618/785 [04:17<00:49,  3.41it/s] 79%|███████▉  | 619/785 [04:17<00:48,  3.41it/s] 79%|███████▉  | 620/785 [04:18<00:48,  3.41it/s] 79%|███████▉  | 621/785 [04:18<00:48,  3.41it/s] 79%|███████▉  | 622/785 [04:18<00:47,  3.41it/s] 79%|███████▉  | 623/785 [04:19<00:47,  3.41it/s] 79%|███████▉  | 624/785 [04:19<00:47,  3.41it/s] 80%|███████▉  | 625/785 [04:19<00:46,  3.41it/s] 80%|███████▉  | 626/785 [04:19<00:47,  3.38it/s] 80%|███████▉  | 627/785 [04:20<00:46,  3.39it/s] 80%|████████  | 628/785 [04:20<00:41,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 17:03:23,632 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:03:23,632 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 17:03:23,632 >>   Batch size = 8
{'eval_loss': 0.9661027193069458, 'eval_runtime': 9.9781, 'eval_samples_per_second': 345.357, 'eval_steps_per_second': 43.195, 'epoch': 3.0}
{'loss': 0.7051, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 55.01it/s][A
  3%|▎         | 12/431 [00:00<00:08, 47.25it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.78it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.86it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.26it/s][A
  7%|▋         | 32/431 [00:00<00:09, 44.00it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.69it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.44it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.44it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.61it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.74it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.65it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.54it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.48it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.38it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.32it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.28it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.37it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.61it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.61it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.57it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.51it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.41it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.34it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.26it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.25it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.44it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.58it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.57it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.48it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.49it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.47it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.33it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.29it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.31it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.50it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.57it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.54it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.46it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.42it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.35it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.31it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.32it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.48it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.48it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.45it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.23it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.34it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.40it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.29it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.27it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.36it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.45it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.52it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.48it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.39it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.31it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.48it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.28it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.39it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.48it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.48it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.50it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.37it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.50it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.46it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.39it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.39it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.40it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.49it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.50it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.44it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.39it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.36it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.49it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.43it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.40it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 43.52it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.49it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.45it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.47it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.46it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.42it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.43it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.31it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.31it/s][A 80%|████████  | 628/785 [04:30<00:41,  3.80it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:03:33,664 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-28 17:03:33,789 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:03:38,176 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:03:38,194 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:03:38,212 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:45<20:00,  7.70s/it] 80%|████████  | 630/785 [04:45<14:10,  5.48s/it] 80%|████████  | 631/785 [04:46<10:04,  3.93s/it] 81%|████████  | 632/785 [04:46<07:13,  2.84s/it] 81%|████████  | 633/785 [04:46<05:15,  2.07s/it] 81%|████████  | 634/785 [04:46<03:52,  1.54s/it] 81%|████████  | 635/785 [04:47<02:54,  1.16s/it] 81%|████████  | 636/785 [04:47<02:14,  1.11it/s] 81%|████████  | 637/785 [04:47<01:46,  1.39it/s] 81%|████████▏ | 638/785 [04:48<01:27,  1.69it/s] 81%|████████▏ | 639/785 [04:48<01:13,  1.99it/s] 82%|████████▏ | 640/785 [04:48<01:03,  2.28it/s] 82%|████████▏ | 641/785 [04:49<01:11,  2.02it/s] 82%|████████▏ | 642/785 [04:49<01:02,  2.30it/s] 82%|████████▏ | 643/785 [04:49<00:55,  2.55it/s] 82%|████████▏ | 644/785 [04:50<00:51,  2.76it/s] 82%|████████▏ | 645/785 [04:50<00:47,  2.93it/s] 82%|████████▏ | 646/785 [04:50<00:45,  3.06it/s] 82%|████████▏ | 647/785 [04:51<00:43,  3.16it/s] 83%|████████▎ | 648/785 [04:51<00:42,  3.23it/s] 83%|████████▎ | 649/785 [04:51<00:41,  3.29it/s] 83%|████████▎ | 650/785 [04:51<00:40,  3.33it/s] 83%|████████▎ | 651/785 [04:52<00:47,  2.80it/s] 83%|████████▎ | 652/785 [04:52<00:44,  2.96it/s] 83%|████████▎ | 653/785 [04:53<00:42,  3.09it/s] 83%|████████▎ | 654/785 [04:53<00:41,  3.18it/s] 83%|████████▎ | 655/785 [04:53<00:40,  3.25it/s] 84%|████████▎ | 656/785 [04:53<00:39,  3.29it/s] 84%|████████▎ | 657/785 [04:54<00:38,  3.33it/s] 84%|████████▍ | 658/785 [04:54<00:37,  3.36it/s] 84%|████████▍ | 659/785 [04:54<00:37,  3.38it/s] 84%|████████▍ | 660/785 [04:55<00:36,  3.39it/s] 84%|████████▍ | 661/785 [04:55<00:38,  3.23it/s] 84%|████████▍ | 662/785 [04:55<00:37,  3.29it/s] 84%|████████▍ | 663/785 [04:56<00:36,  3.33it/s] 85%|████████▍ | 664/785 [04:56<00:36,  3.35it/s] 85%|████████▍ | 665/785 [04:56<00:35,  3.37it/s] 85%|████████▍ | 666/785 [04:56<00:35,  3.39it/s] 85%|████████▍ | 667/785 [04:57<00:34,  3.40it/s] 85%|████████▌ | 668/785 [04:57<00:34,  3.40it/s] 85%|████████▌ | 669/785 [04:57<00:34,  3.41it/s] 85%|████████▌ | 670/785 [04:58<00:33,  3.41it/s] 85%|████████▌ | 671/785 [04:58<00:33,  3.42it/s] 86%|████████▌ | 672/785 [04:58<00:33,  3.37it/s] 86%|████████▌ | 673/785 [04:58<00:33,  3.39it/s] 86%|████████▌ | 674/785 [04:59<00:32,  3.40it/s] 86%|████████▌ | 675/785 [04:59<00:32,  3.40it/s] 86%|████████▌ | 676/785 [04:59<00:31,  3.41it/s] 86%|████████▌ | 677/785 [05:00<00:31,  3.41it/s] 86%|████████▋ | 678/785 [05:00<00:31,  3.42it/s] 86%|████████▋ | 679/785 [05:00<00:30,  3.42it/s] 87%|████████▋ | 680/785 [05:01<00:30,  3.42it/s] 87%|████████▋ | 681/785 [05:01<00:30,  3.42it/s] 87%|████████▋ | 682/785 [05:01<00:30,  3.42it/s] 87%|████████▋ | 683/785 [05:01<00:30,  3.37it/s] 87%|████████▋ | 684/785 [05:02<00:29,  3.38it/s] 87%|████████▋ | 685/785 [05:02<00:29,  3.39it/s] 87%|████████▋ | 686/785 [05:02<00:29,  3.40it/s] 88%|████████▊ | 687/785 [05:03<00:28,  3.41it/s] 88%|████████▊ | 688/785 [05:03<00:28,  3.41it/s] 88%|████████▊ | 689/785 [05:03<00:28,  3.41it/s] 88%|████████▊ | 690/785 [05:03<00:27,  3.41it/s] 88%|████████▊ | 691/785 [05:04<00:27,  3.42it/s] 88%|████████▊ | 692/785 [05:04<00:27,  3.41it/s] 88%|████████▊ | 693/785 [05:04<00:26,  3.42it/s] 88%|████████▊ | 694/785 [05:05<00:26,  3.39it/s] 89%|████████▊ | 695/785 [05:05<00:26,  3.40it/s] 89%|████████▊ | 696/785 [05:05<00:26,  3.40it/s] 89%|████████▉ | 697/785 [05:06<00:25,  3.41it/s] 89%|████████▉ | 698/785 [05:06<00:25,  3.41it/s] 89%|████████▉ | 699/785 [05:06<00:25,  3.41it/s] 89%|████████▉ | 700/785 [05:06<00:24,  3.42it/s] 89%|████████▉ | 701/785 [05:07<00:24,  3.42it/s] 89%|████████▉ | 702/785 [05:07<00:24,  3.42it/s] 90%|████████▉ | 703/785 [05:07<00:23,  3.42it/s] 90%|████████▉ | 704/785 [05:08<00:23,  3.42it/s] 90%|████████▉ | 705/785 [05:08<00:23,  3.40it/s] 90%|████████▉ | 706/785 [05:08<00:23,  3.40it/s] 90%|█████████ | 707/785 [05:08<00:22,  3.41it/s] 90%|█████████ | 708/785 [05:09<00:22,  3.41it/s] 90%|█████████ | 709/785 [05:09<00:22,  3.41it/s] 90%|█████████ | 710/785 [05:09<00:21,  3.41it/s] 91%|█████████ | 711/785 [05:10<00:21,  3.41it/s] 91%|█████████ | 712/785 [05:10<00:21,  3.42it/s] 91%|█████████ | 713/785 [05:10<00:21,  3.42it/s] 91%|█████████ | 714/785 [05:10<00:20,  3.42it/s] 91%|█████████ | 715/785 [05:11<00:20,  3.42it/s] 91%|█████████ | 716/785 [05:11<00:21,  3.14it/s] 91%|█████████▏| 717/785 [05:11<00:21,  3.22it/s] 91%|█████████▏| 718/785 [05:12<00:20,  3.27it/s] 92%|█████████▏| 719/785 [05:12<00:19,  3.31it/s] 92%|█████████▏| 720/785 [05:12<00:19,  3.34it/s] 92%|█████████▏| 721/785 [05:13<00:19,  3.37it/s] 92%|█████████▏| 722/785 [05:13<00:18,  3.38it/s] 92%|█████████▏| 723/785 [05:13<00:18,  3.39it/s] 92%|█████████▏| 724/785 [05:14<00:17,  3.40it/s] 92%|█████████▏| 725/785 [05:14<00:17,  3.41it/s] 92%|█████████▏| 726/785 [05:14<00:17,  3.38it/s] 93%|█████████▎| 727/785 [05:14<00:17,  3.39it/s] 93%|█████████▎| 728/785 [05:15<00:16,  3.40it/s] 93%|█████████▎| 729/785 [05:15<00:16,  3.41it/s] 93%|█████████▎| 730/785 [05:15<00:16,  3.41it/s] 93%|█████████▎| 731/785 [05:16<00:15,  3.41it/s] 93%|█████████▎| 732/785 [05:16<00:15,  3.41it/s] 93%|█████████▎| 733/785 [05:16<00:15,  3.42it/s] 94%|█████████▎| 734/785 [05:16<00:14,  3.41it/s] 94%|█████████▎| 735/785 [05:17<00:14,  3.42it/s] 94%|█████████▍| 736/785 [05:17<00:14,  3.42it/s] 94%|█████████▍| 737/785 [05:17<00:14,  3.24it/s] 94%|█████████▍| 738/785 [05:18<00:16,  2.78it/s] 94%|█████████▍| 739/785 [05:18<00:15,  2.94it/s] 94%|█████████▍| 740/785 [05:18<00:14,  3.07it/s] 94%|█████████▍| 741/785 [05:19<00:13,  3.16it/s] 95%|█████████▍| 742/785 [05:19<00:13,  3.24it/s] 95%|█████████▍| 743/785 [05:19<00:12,  3.29it/s] 95%|█████████▍| 744/785 [05:20<00:12,  3.33it/s] 95%|█████████▍| 745/785 [05:20<00:11,  3.35it/s] 95%|█████████▌| 746/785 [05:20<00:11,  3.37it/s] 95%|█████████▌| 747/785 [05:20<00:11,  3.34it/s] 95%|█████████▌| 748/785 [05:21<00:10,  3.36it/s] 95%|█████████▌| 749/785 [05:21<00:10,  3.38it/s] 96%|█████████▌| 750/785 [05:21<00:10,  3.39it/s] 96%|█████████▌| 751/785 [05:22<00:10,  3.40it/s] 96%|█████████▌| 752/785 [05:22<00:09,  3.40it/s] 96%|█████████▌| 753/785 [05:22<00:09,  3.41it/s] 96%|█████████▌| 754/785 [05:23<00:09,  3.41it/s] 96%|█████████▌| 755/785 [05:23<00:08,  3.41it/s] 96%|█████████▋| 756/785 [05:23<00:08,  3.41it/s] 96%|█████████▋| 757/785 [05:23<00:08,  3.41it/s] 97%|█████████▋| 758/785 [05:24<00:08,  3.09it/s] 97%|█████████▋| 759/785 [05:24<00:08,  3.18it/s] 97%|█████████▋| 760/785 [05:24<00:07,  3.25it/s] 97%|█████████▋| 761/785 [05:25<00:07,  3.30it/s] 97%|█████████▋| 762/785 [05:25<00:06,  3.33it/s] 97%|█████████▋| 763/785 [05:25<00:06,  3.35it/s] 97%|█████████▋| 764/785 [05:26<00:06,  3.37it/s] 97%|█████████▋| 765/785 [05:26<00:05,  3.38it/s] 98%|█████████▊| 766/785 [05:26<00:05,  3.39it/s] 98%|█████████▊| 767/785 [05:26<00:05,  3.40it/s] 98%|█████████▊| 768/785 [05:27<00:05,  3.37it/s] 98%|█████████▊| 769/785 [05:27<00:04,  3.39it/s] 98%|█████████▊| 770/785 [05:27<00:04,  3.39it/s] 98%|█████████▊| 771/785 [05:28<00:04,  3.40it/s] 98%|█████████▊| 772/785 [05:28<00:03,  3.40it/s] 98%|█████████▊| 773/785 [05:28<00:03,  3.41it/s] 99%|█████████▊| 774/785 [05:29<00:03,  3.41it/s] 99%|█████████▊| 775/785 [05:29<00:02,  3.41it/s] 99%|█████████▉| 776/785 [05:29<00:02,  3.42it/s] 99%|█████████▉| 777/785 [05:29<00:02,  3.42it/s] 99%|█████████▉| 778/785 [05:30<00:02,  3.42it/s] 99%|█████████▉| 779/785 [05:30<00:01,  3.42it/s] 99%|█████████▉| 780/785 [05:30<00:01,  3.42it/s] 99%|█████████▉| 781/785 [05:31<00:01,  3.42it/s]100%|█████████▉| 782/785 [05:31<00:00,  3.42it/s]100%|█████████▉| 783/785 [05:31<00:00,  3.42it/s]100%|█████████▉| 784/785 [05:31<00:00,  3.42it/s]100%|██████████| 785/785 [05:32<00:00,  3.82it/s][INFO|trainer.py:2140] 2023-08-28 17:04:35,324 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:04:35,324 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 17:04:35,324 >>   Batch size = 8
{'eval_loss': 0.9750474691390991, 'eval_runtime': 9.9191, 'eval_samples_per_second': 347.41, 'eval_steps_per_second': 43.452, 'epoch': 4.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 55.75it/s][A
  3%|▎         | 12/431 [00:00<00:08, 47.37it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.65it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.71it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.19it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.97it/s][A
  9%|▊         | 37/431 [00:00<00:08, 43.85it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.44it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.46it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.53it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.41it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.54it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.41it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.53it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.45it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.35it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.35it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.41it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.51it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.57it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.53it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.57it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.56it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.46it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.42it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.36it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.46it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 42.28it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 42.81it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.10it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.18it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.22it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.34it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.30it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.31it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.15it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.23it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.45it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.48it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.56it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.53it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.54it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.46it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.39it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.35it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.43it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.38it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.39it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.55it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.51it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.45it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.44it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.29it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.40it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.36it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.32it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.45it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.42it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.41it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.40it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.27it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.17it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.33it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.38it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.28it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.30it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.43it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.41it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.32it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.21it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.34it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.20it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.19it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.14it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.35it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.26it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.46it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.28it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.31it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 42.00it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 42.59it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 42.85it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.02it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.14it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.27it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.27it/s][A100%|██████████| 785/785 [05:42<00:00,  3.82it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:04:45,766 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-28 17:04:45,794 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:04:50,433 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:04:50,795 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:04:50,891 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:05:07,430 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:05:07,443 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-157 (score: 0.9501715302467346).
                                                 100%|██████████| 785/785 [06:15<00:00,  3.82it/s]100%|██████████| 785/785 [06:15<00:00,  2.09it/s]
[INFO|trainer.py:1894] 2023-08-28 17:05:19,215 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 17:05:19,463 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:05:23,585 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:05:23,816 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:05:23,966 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:05:24,312 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:24,312 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:24,312 >>   train_loss               =     0.6941
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:24,312 >>   train_runtime            = 0:06:15.92
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:24,312 >>   train_samples            =      10018
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:24,312 >>   train_samples_per_second =    133.245
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:24,312 >>   train_steps_per_second   =      2.088
{'eval_loss': 0.9805427193641663, 'eval_runtime': 9.9449, 'eval_samples_per_second': 346.509, 'eval_steps_per_second': 43.339, 'epoch': 5.0}
{'train_runtime': 375.9231, 'train_samples_per_second': 133.245, 'train_steps_per_second': 2.088, 'train_loss': 0.6940777626766521, 'epoch': 5.0}
08/28/2023 17:05:24 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:05:24,463 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:05:24,463 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 17:05:24,463 >>   Batch size = 8
  0%|          | 0/431 [00:00<?, ?it/s]  1%|▏         | 6/431 [00:00<00:07, 55.68it/s]  3%|▎         | 12/431 [00:00<00:08, 48.34it/s]  4%|▍         | 17/431 [00:00<00:08, 46.66it/s]  5%|▌         | 22/431 [00:00<00:08, 45.81it/s]  6%|▋         | 27/431 [00:00<00:08, 45.24it/s]  7%|▋         | 32/431 [00:00<00:08, 44.98it/s]  9%|▊         | 37/431 [00:00<00:08, 44.71it/s] 10%|▉         | 42/431 [00:00<00:08, 44.14it/s] 11%|█         | 47/431 [00:01<00:08, 43.54it/s] 12%|█▏        | 52/431 [00:01<00:08, 43.31it/s] 13%|█▎        | 57/431 [00:01<00:08, 43.46it/s] 14%|█▍        | 62/431 [00:01<00:08, 43.68it/s] 16%|█▌        | 67/431 [00:01<00:08, 43.77it/s] 17%|█▋        | 72/431 [00:01<00:08, 43.92it/s] 18%|█▊        | 77/431 [00:01<00:08, 44.11it/s] 19%|█▉        | 82/431 [00:01<00:07, 43.96it/s] 20%|██        | 87/431 [00:01<00:07, 43.77it/s] 21%|██▏       | 92/431 [00:02<00:07, 43.40it/s] 23%|██▎       | 97/431 [00:02<00:07, 43.34it/s] 24%|██▎       | 102/431 [00:02<00:08, 40.94it/s] 25%|██▍       | 107/431 [00:02<00:07, 41.88it/s] 26%|██▌       | 112/431 [00:02<00:07, 42.54it/s] 27%|██▋       | 117/431 [00:02<00:07, 43.06it/s] 28%|██▊       | 122/431 [00:02<00:07, 43.29it/s] 29%|██▉       | 127/431 [00:02<00:06, 43.48it/s] 31%|███       | 132/431 [00:03<00:06, 43.31it/s] 32%|███▏      | 137/431 [00:03<00:06, 43.27it/s] 33%|███▎      | 142/431 [00:03<00:06, 43.15it/s] 34%|███▍      | 147/431 [00:03<00:06, 43.23it/s] 35%|███▌      | 152/431 [00:03<00:06, 43.37it/s] 36%|███▋      | 157/431 [00:03<00:06, 43.65it/s] 38%|███▊      | 162/431 [00:03<00:06, 43.82it/s] 39%|███▊      | 167/431 [00:03<00:06, 43.87it/s] 40%|███▉      | 172/431 [00:03<00:05, 43.91it/s] 41%|████      | 177/431 [00:04<00:05, 43.62it/s] 42%|████▏     | 182/431 [00:04<00:05, 43.30it/s] 43%|████▎     | 187/431 [00:04<00:05, 43.35it/s] 45%|████▍     | 192/431 [00:04<00:05, 43.34it/s] 46%|████▌     | 197/431 [00:04<00:05, 43.42it/s] 47%|████▋     | 202/431 [00:04<00:05, 43.60it/s] 48%|████▊     | 207/431 [00:04<00:05, 43.67it/s] 49%|████▉     | 212/431 [00:04<00:04, 43.85it/s] 50%|█████     | 217/431 [00:04<00:04, 43.92it/s] 52%|█████▏    | 222/431 [00:05<00:04, 43.71it/s] 53%|█████▎    | 227/431 [00:05<00:04, 43.55it/s] 54%|█████▍    | 232/431 [00:05<00:04, 42.89it/s] 55%|█████▍    | 237/431 [00:05<00:04, 43.11it/s] 56%|█████▌    | 242/431 [00:05<00:04, 43.34it/s] 57%|█████▋    | 247/431 [00:05<00:04, 43.47it/s] 58%|█████▊    | 252/431 [00:05<00:04, 43.61it/s] 60%|█████▉    | 257/431 [00:05<00:03, 43.69it/s] 61%|██████    | 262/431 [00:05<00:03, 43.66it/s] 62%|██████▏   | 267/431 [00:06<00:03, 43.57it/s] 63%|██████▎   | 272/431 [00:06<00:03, 43.44it/s] 64%|██████▍   | 277/431 [00:06<00:03, 43.44it/s] 65%|██████▌   | 282/431 [00:06<00:03, 43.45it/s] 67%|██████▋   | 287/431 [00:06<00:03, 43.50it/s] 68%|██████▊   | 292/431 [00:06<00:03, 43.72it/s] 69%|██████▉   | 297/431 [00:06<00:03, 43.77it/s] 70%|███████   | 302/431 [00:06<00:02, 43.81it/s] 71%|███████   | 307/431 [00:07<00:02, 43.65it/s] 72%|███████▏  | 312/431 [00:07<00:02, 43.53it/s] 74%|███████▎  | 317/431 [00:07<00:02, 43.49it/s] 75%|███████▍  | 322/431 [00:07<00:02, 43.41it/s] 76%|███████▌  | 327/431 [00:07<00:02, 43.47it/s] 77%|███████▋  | 332/431 [00:07<00:02, 43.54it/s] 78%|███████▊  | 337/431 [00:07<00:02, 43.72it/s] 79%|███████▉  | 342/431 [00:07<00:02, 43.63it/s] 81%|████████  | 347/431 [00:07<00:01, 43.63it/s] 82%|████████▏ | 352/431 [00:08<00:01, 43.69it/s] 83%|████████▎ | 357/431 [00:08<00:01, 43.56it/s] 84%|████████▍ | 362/431 [00:08<00:01, 43.40it/s] 85%|████████▌ | 367/431 [00:08<00:01, 43.42it/s] 86%|████████▋ | 372/431 [00:08<00:01, 43.44it/s] 87%|████████▋ | 377/431 [00:08<00:01, 43.61it/s] 89%|████████▊ | 382/431 [00:08<00:01, 43.70it/s] 90%|████████▉ | 387/431 [00:08<00:01, 43.73it/s] 91%|█████████ | 392/431 [00:08<00:00, 43.68it/s] 92%|█████████▏| 397/431 [00:09<00:00, 43.52it/s] 93%|█████████▎| 402/431 [00:09<00:00, 43.51it/s] 94%|█████████▍| 407/431 [00:09<00:00, 43.51it/s] 96%|█████████▌| 412/431 [00:09<00:00, 43.50it/s] 97%|█████████▋| 417/431 [00:09<00:00, 43.51it/s] 98%|█████████▊| 422/431 [00:09<00:00, 43.55it/s] 99%|█████████▉| 427/431 [00:09<00:00, 43.68it/s]100%|██████████| 431/431 [00:09<00:00, 43.61it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:05:34,363 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:34,364 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:34,364 >>   eval_loss               =     0.9502
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:34,364 >>   eval_runtime            = 0:00:09.90
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:34,364 >>   eval_samples            =       3446
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:34,364 >>   eval_samples_per_second =    348.076
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:34,364 >>   eval_steps_per_second   =     43.535
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:34,364 >>   perplexity              =     2.5862
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:40,166 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:40,181 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:40,181 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:40,181 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:40,181 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:05:40,579 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:05:40,580 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:05:40,832 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:05:41,884 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:05:41,885 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:43,209 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:43,219 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:43,219 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:43,219 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:43,219 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:05:43,527 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:05:43,528 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:05:43,795 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:05:43,938 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:05:43,938 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-471
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-157
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-314
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-785
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-628
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.50it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.50it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:13,  1.54it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.50it/s]Extractor Predicting: 27it [00:17,  1.48it/s]Extractor Predicting: 28it [00:18,  1.50it/s]Extractor Predicting: 29it [00:18,  1.50it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:20,  1.51it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.58it/s]Extractor Predicting: 35it [00:22,  1.60it/s]Extractor Predicting: 36it [00:23,  1.60it/s]Extractor Predicting: 37it [00:24,  1.62it/s]Extractor Predicting: 38it [00:24,  1.58it/s]Extractor Predicting: 39it [00:25,  1.61it/s]Extractor Predicting: 40it [00:25,  1.61it/s]Extractor Predicting: 41it [00:26,  1.56it/s]Extractor Predicting: 42it [00:27,  1.49it/s]Extractor Predicting: 43it [00:28,  1.50it/s]Extractor Predicting: 44it [00:28,  1.53it/s]Extractor Predicting: 45it [00:29,  1.56it/s]Extractor Predicting: 46it [00:29,  1.57it/s]Extractor Predicting: 47it [00:30,  1.63it/s]Extractor Predicting: 48it [00:31,  1.63it/s]Extractor Predicting: 49it [00:31,  1.65it/s]Extractor Predicting: 50it [00:32,  1.61it/s]Extractor Predicting: 51it [00:32,  1.60it/s]Extractor Predicting: 52it [00:33,  1.58it/s]Extractor Predicting: 53it [00:34,  1.56it/s]Extractor Predicting: 54it [00:34,  1.59it/s]Extractor Predicting: 55it [00:35,  1.59it/s]Extractor Predicting: 56it [00:36,  1.62it/s]Extractor Predicting: 57it [00:36,  1.65it/s]Extractor Predicting: 58it [00:37,  1.62it/s]Extractor Predicting: 59it [00:37,  1.66it/s]Extractor Predicting: 60it [00:38,  1.62it/s]Extractor Predicting: 61it [00:39,  1.63it/s]Extractor Predicting: 62it [00:39,  1.69it/s]Extractor Predicting: 63it [00:40,  1.69it/s]Extractor Predicting: 64it [00:40,  1.73it/s]Extractor Predicting: 65it [00:41,  1.73it/s]Extractor Predicting: 66it [00:41,  1.75it/s]Extractor Predicting: 67it [00:42,  1.72it/s]Extractor Predicting: 68it [00:43,  1.72it/s]Extractor Predicting: 69it [00:43,  1.75it/s]Extractor Predicting: 70it [00:44,  1.72it/s]Extractor Predicting: 71it [00:44,  1.69it/s]Extractor Predicting: 72it [00:45,  1.72it/s]Extractor Predicting: 73it [00:46,  1.65it/s]Extractor Predicting: 74it [00:46,  1.65it/s]Extractor Predicting: 75it [00:47,  1.69it/s]Extractor Predicting: 76it [00:47,  1.70it/s]Extractor Predicting: 77it [00:48,  1.71it/s]Extractor Predicting: 78it [00:48,  1.73it/s]Extractor Predicting: 79it [00:49,  1.73it/s]Extractor Predicting: 80it [00:50,  1.71it/s]Extractor Predicting: 81it [00:50,  1.67it/s]Extractor Predicting: 82it [00:51,  1.64it/s]Extractor Predicting: 83it [00:51,  1.67it/s]Extractor Predicting: 84it [00:52,  1.66it/s]Extractor Predicting: 85it [00:53,  1.61it/s]Extractor Predicting: 86it [00:53,  1.59it/s]Extractor Predicting: 87it [00:54,  1.61it/s]Extractor Predicting: 88it [00:55,  1.58it/s]Extractor Predicting: 89it [00:55,  1.57it/s]Extractor Predicting: 90it [00:56,  1.57it/s]Extractor Predicting: 91it [00:57,  1.58it/s]Extractor Predicting: 92it [00:57,  1.56it/s]Extractor Predicting: 93it [00:58,  1.55it/s]Extractor Predicting: 94it [00:59,  1.58it/s]Extractor Predicting: 95it [00:59,  1.56it/s]Extractor Predicting: 96it [01:00,  1.42it/s]Extractor Predicting: 97it [01:01,  1.45it/s]Extractor Predicting: 98it [01:01,  1.49it/s]Extractor Predicting: 99it [01:02,  1.52it/s]Extractor Predicting: 100it [01:03,  1.55it/s]Extractor Predicting: 101it [01:03,  1.55it/s]Extractor Predicting: 102it [01:04,  1.57it/s]Extractor Predicting: 103it [01:04,  1.58it/s]Extractor Predicting: 104it [01:05,  1.61it/s]Extractor Predicting: 105it [01:06,  1.63it/s]Extractor Predicting: 106it [01:06,  1.60it/s]Extractor Predicting: 107it [01:07,  1.59it/s]Extractor Predicting: 108it [01:08,  1.59it/s]Extractor Predicting: 109it [01:08,  1.58it/s]Extractor Predicting: 110it [01:09,  1.57it/s]Extractor Predicting: 111it [01:10,  1.47it/s]Extractor Predicting: 112it [01:10,  1.52it/s]Extractor Predicting: 113it [01:11,  1.55it/s]Extractor Predicting: 114it [01:11,  1.56it/s]Extractor Predicting: 115it [01:12,  1.56it/s]Extractor Predicting: 116it [01:13,  1.59it/s]Extractor Predicting: 117it [01:13,  1.60it/s]Extractor Predicting: 118it [01:14,  1.59it/s]Extractor Predicting: 119it [01:15,  1.59it/s]Extractor Predicting: 120it [01:15,  1.59it/s]Extractor Predicting: 121it [01:16,  1.57it/s]Extractor Predicting: 122it [01:17,  1.56it/s]Extractor Predicting: 123it [01:17,  1.56it/s]Extractor Predicting: 124it [01:18,  1.54it/s]Extractor Predicting: 125it [01:18,  1.56it/s]Extractor Predicting: 126it [01:19,  1.55it/s]Extractor Predicting: 127it [01:20,  1.42it/s]Extractor Predicting: 128it [01:21,  1.44it/s]Extractor Predicting: 129it [01:21,  1.48it/s]Extractor Predicting: 130it [01:22,  1.53it/s]Extractor Predicting: 131it [01:23,  1.51it/s]Extractor Predicting: 132it [01:23,  1.55it/s]Extractor Predicting: 133it [01:24,  1.56it/s]Extractor Predicting: 134it [01:24,  1.51it/s]Extractor Predicting: 135it [01:25,  1.50it/s]Extractor Predicting: 136it [01:26,  1.52it/s]Extractor Predicting: 137it [01:26,  1.55it/s]Extractor Predicting: 138it [01:27,  1.58it/s]Extractor Predicting: 139it [01:28,  1.57it/s]Extractor Predicting: 140it [01:28,  1.58it/s]Extractor Predicting: 141it [01:29,  1.64it/s]Extractor Predicting: 141it [01:29,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:22,781 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:22,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:22,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:22,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:22,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:07:23,479 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:07:23,480 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:07:24,057 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:07:25,088 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:07:25,088 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:28,083 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:28,089 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:28,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:28,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:28,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:07:28,738 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:07:28,739 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:07:29,335 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:07:29,499 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:07:29,499 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3040983606557377,
  "recall": 0.10766105629715612,
  "score": 0.15902271753107586,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.70it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.48it/s]Extractor Predicting: 17it [00:10,  1.51it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:11,  1.57it/s]Extractor Predicting: 20it [00:12,  1.44it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.59it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:18,  1.54it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:20,  1.54it/s]Extractor Predicting: 34it [00:21,  1.56it/s]Extractor Predicting: 35it [00:22,  1.51it/s]Extractor Predicting: 36it [00:22,  1.54it/s]Extractor Predicting: 37it [00:23,  1.60it/s]Extractor Predicting: 38it [00:23,  1.60it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:25,  1.59it/s]Extractor Predicting: 41it [00:25,  1.59it/s]Extractor Predicting: 42it [00:26,  1.59it/s]Extractor Predicting: 43it [00:27,  1.59it/s]Extractor Predicting: 44it [00:27,  1.59it/s]Extractor Predicting: 45it [00:28,  1.56it/s]Extractor Predicting: 46it [00:29,  1.61it/s]Extractor Predicting: 47it [00:29,  1.60it/s]Extractor Predicting: 48it [00:30,  1.60it/s]Extractor Predicting: 49it [00:30,  1.59it/s]Extractor Predicting: 50it [00:31,  1.56it/s]Extractor Predicting: 51it [00:32,  1.57it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:33,  1.58it/s]Extractor Predicting: 54it [00:34,  1.60it/s]Extractor Predicting: 55it [00:34,  1.56it/s]Extractor Predicting: 56it [00:35,  1.56it/s]Extractor Predicting: 57it [00:36,  1.59it/s]Extractor Predicting: 58it [00:36,  1.62it/s]Extractor Predicting: 59it [00:37,  1.63it/s]Extractor Predicting: 60it [00:37,  1.63it/s]Extractor Predicting: 61it [00:38,  1.62it/s]Extractor Predicting: 62it [00:39,  1.62it/s]Extractor Predicting: 63it [00:39,  1.59it/s]Extractor Predicting: 64it [00:40,  1.61it/s]Extractor Predicting: 65it [00:40,  1.59it/s]Extractor Predicting: 66it [00:41,  1.56it/s]Extractor Predicting: 67it [00:42,  1.60it/s]Extractor Predicting: 68it [00:42,  1.61it/s]Extractor Predicting: 69it [00:43,  1.64it/s]Extractor Predicting: 70it [00:44,  1.62it/s]Extractor Predicting: 71it [00:44,  1.61it/s]Extractor Predicting: 72it [00:45,  1.63it/s]Extractor Predicting: 73it [00:46,  1.54it/s]Extractor Predicting: 74it [00:46,  1.56it/s]Extractor Predicting: 75it [00:47,  1.57it/s]Extractor Predicting: 76it [00:47,  1.59it/s]Extractor Predicting: 77it [00:48,  1.56it/s]Extractor Predicting: 78it [00:49,  1.56it/s]Extractor Predicting: 79it [00:49,  1.58it/s]Extractor Predicting: 80it [00:50,  1.57it/s]Extractor Predicting: 81it [00:51,  1.58it/s]Extractor Predicting: 82it [00:51,  1.57it/s]Extractor Predicting: 83it [00:52,  1.55it/s]Extractor Predicting: 84it [00:52,  1.57it/s]Extractor Predicting: 85it [00:53,  1.58it/s]Extractor Predicting: 86it [00:54,  1.57it/s]Extractor Predicting: 87it [00:54,  1.57it/s]Extractor Predicting: 88it [00:55,  1.57it/s]Extractor Predicting: 89it [00:56,  1.55it/s]Extractor Predicting: 90it [00:56,  1.55it/s]Extractor Predicting: 91it [00:57,  1.56it/s]Extractor Predicting: 92it [00:58,  1.56it/s]Extractor Predicting: 93it [00:58,  1.53it/s]Extractor Predicting: 94it [00:59,  1.50it/s]Extractor Predicting: 95it [01:00,  1.50it/s]Extractor Predicting: 96it [01:00,  1.52it/s]Extractor Predicting: 97it [01:01,  1.53it/s]Extractor Predicting: 98it [01:02,  1.54it/s]Extractor Predicting: 99it [01:02,  1.53it/s]Extractor Predicting: 100it [01:03,  1.52it/s]Extractor Predicting: 101it [01:04,  1.55it/s]Extractor Predicting: 102it [01:04,  1.54it/s]Extractor Predicting: 103it [01:05,  1.51it/s]Extractor Predicting: 104it [01:06,  1.51it/s]Extractor Predicting: 105it [01:06,  1.52it/s]Extractor Predicting: 106it [01:07,  1.53it/s]Extractor Predicting: 107it [01:08,  1.49it/s]Extractor Predicting: 108it [01:08,  1.51it/s]Extractor Predicting: 109it [01:09,  1.52it/s]Extractor Predicting: 110it [01:10,  1.50it/s]Extractor Predicting: 111it [01:10,  1.41it/s]Extractor Predicting: 112it [01:11,  1.45it/s]Extractor Predicting: 113it [01:12,  1.51it/s]Extractor Predicting: 114it [01:12,  1.54it/s]Extractor Predicting: 115it [01:13,  1.53it/s]Extractor Predicting: 116it [01:14,  1.47it/s]Extractor Predicting: 117it [01:14,  1.53it/s]Extractor Predicting: 118it [01:15,  1.54it/s]Extractor Predicting: 119it [01:15,  1.57it/s]Extractor Predicting: 120it [01:16,  1.58it/s]Extractor Predicting: 121it [01:17,  1.59it/s]Extractor Predicting: 122it [01:17,  1.64it/s]Extractor Predicting: 123it [01:18,  1.63it/s]Extractor Predicting: 124it [01:18,  1.63it/s]Extractor Predicting: 125it [01:19,  1.61it/s]Extractor Predicting: 126it [01:20,  1.58it/s]Extractor Predicting: 127it [01:20,  1.60it/s]Extractor Predicting: 128it [01:21,  1.63it/s]Extractor Predicting: 129it [01:22,  1.42it/s]Extractor Predicting: 130it [01:23,  1.46it/s]Extractor Predicting: 131it [01:23,  1.49it/s]Extractor Predicting: 132it [01:24,  1.53it/s]Extractor Predicting: 133it [01:24,  1.54it/s]Extractor Predicting: 134it [01:25,  1.55it/s]Extractor Predicting: 135it [01:26,  1.57it/s]Extractor Predicting: 136it [01:26,  1.58it/s]Extractor Predicting: 137it [01:27,  1.59it/s]Extractor Predicting: 138it [01:28,  1.59it/s]Extractor Predicting: 139it [01:28,  1.62it/s]Extractor Predicting: 140it [01:29,  1.61it/s]Extractor Predicting: 141it [01:29,  1.59it/s]Extractor Predicting: 142it [01:30,  1.61it/s]Extractor Predicting: 143it [01:31,  1.60it/s]Extractor Predicting: 144it [01:31,  1.58it/s]Extractor Predicting: 145it [01:32,  1.60it/s]Extractor Predicting: 146it [01:33,  1.59it/s]Extractor Predicting: 147it [01:33,  1.57it/s]Extractor Predicting: 148it [01:34,  1.56it/s]Extractor Predicting: 149it [01:34,  1.57it/s]Extractor Predicting: 150it [01:35,  1.58it/s]Extractor Predicting: 151it [01:36,  1.59it/s]Extractor Predicting: 152it [01:36,  1.61it/s]Extractor Predicting: 153it [01:37,  1.62it/s]Extractor Predicting: 154it [01:38,  1.59it/s]Extractor Predicting: 155it [01:38,  1.59it/s]Extractor Predicting: 156it [01:39,  1.57it/s]Extractor Predicting: 157it [01:40,  1.56it/s]Extractor Predicting: 158it [01:40,  1.55it/s]Extractor Predicting: 159it [01:41,  1.53it/s]Extractor Predicting: 160it [01:41,  1.54it/s]Extractor Predicting: 161it [01:42,  1.57it/s]Extractor Predicting: 162it [01:43,  1.59it/s]Extractor Predicting: 163it [01:43,  1.59it/s]Extractor Predicting: 164it [01:44,  1.58it/s]Extractor Predicting: 165it [01:45,  1.57it/s]Extractor Predicting: 166it [01:45,  1.58it/s]Extractor Predicting: 167it [01:46,  1.61it/s]Extractor Predicting: 168it [01:46,  1.60it/s]Extractor Predicting: 169it [01:47,  1.58it/s]Extractor Predicting: 170it [01:48,  1.57it/s]Extractor Predicting: 171it [01:48,  1.54it/s]Extractor Predicting: 172it [01:49,  1.53it/s]Extractor Predicting: 173it [01:50,  1.52it/s]Extractor Predicting: 174it [01:51,  1.48it/s]Extractor Predicting: 175it [01:51,  1.45it/s]Extractor Predicting: 176it [01:52,  1.48it/s]Extractor Predicting: 177it [01:53,  1.51it/s]Extractor Predicting: 178it [01:53,  1.52it/s]Extractor Predicting: 179it [01:54,  1.56it/s]Extractor Predicting: 180it [01:54,  1.54it/s]Extractor Predicting: 181it [01:55,  1.57it/s]Extractor Predicting: 182it [01:56,  1.58it/s]Extractor Predicting: 183it [01:56,  1.58it/s]Extractor Predicting: 184it [01:57,  1.59it/s]Extractor Predicting: 185it [01:58,  1.61it/s]Extractor Predicting: 186it [01:58,  1.63it/s]Extractor Predicting: 187it [01:59,  1.62it/s]Extractor Predicting: 188it [01:59,  1.63it/s]Extractor Predicting: 189it [02:00,  1.62it/s]Extractor Predicting: 190it [02:01,  1.62it/s]Extractor Predicting: 191it [02:01,  1.58it/s]Extractor Predicting: 192it [02:02,  1.56it/s]Extractor Predicting: 193it [02:03,  1.57it/s]Extractor Predicting: 194it [02:03,  1.61it/s]Extractor Predicting: 195it [02:04,  1.62it/s]Extractor Predicting: 196it [02:04,  1.62it/s]Extractor Predicting: 197it [02:05,  1.63it/s]Extractor Predicting: 198it [02:06,  1.62it/s]Extractor Predicting: 199it [02:06,  1.60it/s]Extractor Predicting: 200it [02:07,  1.59it/s]Extractor Predicting: 201it [02:07,  1.59it/s]Extractor Predicting: 202it [02:08,  1.62it/s]Extractor Predicting: 203it [02:09,  1.64it/s]Extractor Predicting: 204it [02:09,  1.65it/s]Extractor Predicting: 205it [02:10,  1.63it/s]Extractor Predicting: 206it [02:11,  1.62it/s]Extractor Predicting: 207it [02:11,  1.58it/s]Extractor Predicting: 208it [02:12,  1.49it/s]Extractor Predicting: 209it [02:13,  1.52it/s]Extractor Predicting: 210it [02:13,  1.54it/s]Extractor Predicting: 211it [02:14,  1.58it/s]Extractor Predicting: 212it [02:14,  1.57it/s]Extractor Predicting: 213it [02:15,  1.59it/s]Extractor Predicting: 214it [02:16,  1.59it/s]Extractor Predicting: 215it [02:16,  1.63it/s]Extractor Predicting: 216it [02:17,  1.63it/s]Extractor Predicting: 217it [02:18,  1.55it/s]Extractor Predicting: 218it [02:18,  1.54it/s]Extractor Predicting: 219it [02:19,  1.59it/s]Extractor Predicting: 220it [02:19,  1.62it/s]Extractor Predicting: 221it [02:20,  1.60it/s]Extractor Predicting: 222it [02:21,  1.61it/s]Extractor Predicting: 223it [02:21,  1.60it/s]Extractor Predicting: 224it [02:22,  1.62it/s]Extractor Predicting: 225it [02:23,  1.60it/s]Extractor Predicting: 226it [02:23,  1.56it/s]Extractor Predicting: 227it [02:24,  1.56it/s]Extractor Predicting: 228it [02:25,  1.52it/s]Extractor Predicting: 229it [02:25,  1.53it/s]Extractor Predicting: 230it [02:26,  1.57it/s]Extractor Predicting: 231it [02:26,  1.56it/s]Extractor Predicting: 232it [02:27,  1.56it/s]Extractor Predicting: 233it [02:28,  1.55it/s]Extractor Predicting: 234it [02:28,  1.56it/s]Extractor Predicting: 235it [02:29,  1.53it/s]Extractor Predicting: 236it [02:30,  1.57it/s]Extractor Predicting: 237it [02:30,  1.62it/s]Extractor Predicting: 238it [02:31,  1.58it/s]Extractor Predicting: 239it [02:32,  1.56it/s]Extractor Predicting: 240it [02:32,  1.55it/s]Extractor Predicting: 241it [02:33,  1.55it/s]Extractor Predicting: 242it [02:33,  1.58it/s]Extractor Predicting: 243it [02:34,  1.40it/s]Extractor Predicting: 244it [02:35,  1.42it/s]Extractor Predicting: 245it [02:36,  1.47it/s]Extractor Predicting: 246it [02:36,  1.51it/s]Extractor Predicting: 247it [02:37,  1.54it/s]Extractor Predicting: 248it [02:38,  1.47it/s]Extractor Predicting: 249it [02:38,  1.50it/s]Extractor Predicting: 250it [02:39,  1.52it/s]Extractor Predicting: 251it [02:40,  1.51it/s]Extractor Predicting: 252it [02:40,  1.50it/s]Extractor Predicting: 253it [02:41,  1.51it/s]Extractor Predicting: 254it [02:42,  1.55it/s]Extractor Predicting: 255it [02:42,  1.55it/s]Extractor Predicting: 256it [02:43,  1.60it/s]Extractor Predicting: 257it [02:43,  1.63it/s]Extractor Predicting: 258it [02:44,  1.61it/s]Extractor Predicting: 259it [02:45,  1.60it/s]Extractor Predicting: 260it [02:45,  1.57it/s]Extractor Predicting: 261it [02:46,  1.56it/s]Extractor Predicting: 262it [02:47,  1.57it/s]Extractor Predicting: 263it [02:47,  1.51it/s]Extractor Predicting: 264it [02:48,  1.53it/s]Extractor Predicting: 265it [02:49,  1.51it/s]Extractor Predicting: 266it [02:49,  1.54it/s]Extractor Predicting: 267it [02:50,  1.55it/s]Extractor Predicting: 268it [02:50,  1.58it/s]Extractor Predicting: 269it [02:51,  1.61it/s]Extractor Predicting: 270it [02:52,  1.61it/s]Extractor Predicting: 271it [02:52,  1.58it/s]Extractor Predicting: 272it [02:53,  1.61it/s]Extractor Predicting: 273it [02:54,  1.60it/s]Extractor Predicting: 274it [02:54,  1.60it/s]Extractor Predicting: 275it [02:55,  1.59it/s]Extractor Predicting: 276it [02:55,  1.59it/s]Extractor Predicting: 277it [02:56,  1.57it/s]Extractor Predicting: 278it [02:57,  1.61it/s]Extractor Predicting: 279it [02:57,  1.57it/s]Extractor Predicting: 280it [02:58,  1.57it/s]Extractor Predicting: 281it [02:59,  1.58it/s]Extractor Predicting: 282it [02:59,  1.59it/s]Extractor Predicting: 283it [03:00,  1.60it/s]Extractor Predicting: 284it [03:01,  1.57it/s]Extractor Predicting: 285it [03:01,  1.57it/s]Extractor Predicting: 286it [03:02,  1.59it/s]Extractor Predicting: 287it [03:02,  1.61it/s]Extractor Predicting: 288it [03:03,  1.60it/s]Extractor Predicting: 289it [03:04,  1.56it/s]Extractor Predicting: 290it [03:04,  1.61it/s]Extractor Predicting: 291it [03:05,  1.59it/s]Extractor Predicting: 292it [03:06,  1.58it/s]Extractor Predicting: 293it [03:06,  1.55it/s]Extractor Predicting: 294it [03:07,  1.52it/s]Extractor Predicting: 295it [03:08,  1.56it/s]Extractor Predicting: 296it [03:08,  1.54it/s]Extractor Predicting: 297it [03:09,  1.57it/s]Extractor Predicting: 298it [03:10,  1.51it/s]Extractor Predicting: 299it [03:10,  1.52it/s]Extractor Predicting: 300it [03:11,  1.53it/s]Extractor Predicting: 301it [03:11,  1.53it/s]Extractor Predicting: 302it [03:12,  1.53it/s]Extractor Predicting: 303it [03:13,  1.51it/s]Extractor Predicting: 304it [03:14,  1.47it/s]Extractor Predicting: 305it [03:14,  1.49it/s]Extractor Predicting: 306it [03:15,  1.48it/s]Extractor Predicting: 307it [03:16,  1.48it/s]Extractor Predicting: 308it [03:16,  1.41it/s]Extractor Predicting: 309it [03:17,  1.44it/s]Extractor Predicting: 310it [03:18,  1.43it/s]Extractor Predicting: 311it [03:18,  1.44it/s]Extractor Predicting: 312it [03:19,  1.47it/s]Extractor Predicting: 313it [03:20,  1.45it/s]Extractor Predicting: 314it [03:20,  1.45it/s]Extractor Predicting: 315it [03:21,  1.51it/s]Extractor Predicting: 316it [03:22,  1.50it/s]Extractor Predicting: 317it [03:22,  1.48it/s]Extractor Predicting: 318it [03:23,  1.50it/s]Extractor Predicting: 319it [03:24,  1.50it/s]Extractor Predicting: 320it [03:24,  1.59it/s]Extractor Predicting: 321it [03:25,  1.63it/s]Extractor Predicting: 322it [03:25,  1.71it/s]Extractor Predicting: 323it [03:26,  1.74it/s]Extractor Predicting: 324it [03:26,  1.75it/s]Extractor Predicting: 325it [03:27,  1.77it/s]Extractor Predicting: 326it [03:28,  1.77it/s]Extractor Predicting: 327it [03:28,  1.76it/s]Extractor Predicting: 328it [03:29,  1.78it/s]Extractor Predicting: 329it [03:29,  1.81it/s]Extractor Predicting: 330it [03:30,  1.80it/s]Extractor Predicting: 331it [03:30,  1.85it/s]Extractor Predicting: 332it [03:31,  1.81it/s]Extractor Predicting: 333it [03:31,  1.85it/s]Extractor Predicting: 334it [03:32,  1.84it/s]Extractor Predicting: 335it [03:32,  1.83it/s]Extractor Predicting: 336it [03:33,  1.80it/s]Extractor Predicting: 337it [03:34,  1.84it/s]Extractor Predicting: 338it [03:34,  1.55it/s]Extractor Predicting: 339it [03:35,  1.60it/s]Extractor Predicting: 340it [03:36,  1.70it/s]Extractor Predicting: 341it [03:36,  1.69it/s]Extractor Predicting: 342it [03:37,  1.74it/s]Extractor Predicting: 343it [03:37,  1.75it/s]Extractor Predicting: 344it [03:38,  1.74it/s]Extractor Predicting: 345it [03:38,  1.78it/s]Extractor Predicting: 346it [03:39,  1.77it/s]Extractor Predicting: 347it [03:40,  1.75it/s]Extractor Predicting: 348it [03:40,  1.71it/s]Extractor Predicting: 349it [03:41,  1.67it/s]Extractor Predicting: 350it [03:41,  1.63it/s]Extractor Predicting: 351it [03:42,  1.61it/s]Extractor Predicting: 352it [03:43,  1.61it/s]Extractor Predicting: 353it [03:43,  1.63it/s]Extractor Predicting: 354it [03:44,  1.61it/s]Extractor Predicting: 355it [03:45,  1.60it/s]Extractor Predicting: 356it [03:45,  1.57it/s]Extractor Predicting: 357it [03:46,  1.55it/s]Extractor Predicting: 358it [03:47,  1.57it/s]Extractor Predicting: 359it [03:47,  1.57it/s]Extractor Predicting: 360it [03:48,  1.60it/s]Extractor Predicting: 361it [03:48,  1.59it/s]Extractor Predicting: 362it [03:49,  1.62it/s]Extractor Predicting: 363it [03:50,  1.59it/s]Extractor Predicting: 364it [03:50,  1.57it/s]Extractor Predicting: 365it [03:51,  1.58it/s]Extractor Predicting: 366it [03:52,  1.58it/s]Extractor Predicting: 367it [03:52,  1.57it/s]Extractor Predicting: 368it [03:53,  1.59it/s]Extractor Predicting: 369it [03:53,  1.58it/s]Extractor Predicting: 370it [03:54,  1.57it/s]Extractor Predicting: 371it [03:55,  1.57it/s]Extractor Predicting: 372it [03:55,  1.57it/s]Extractor Predicting: 373it [03:56,  1.56it/s]Extractor Predicting: 374it [03:57,  1.57it/s]Extractor Predicting: 375it [03:57,  1.59it/s]Extractor Predicting: 376it [03:58,  1.56it/s]Extractor Predicting: 377it [03:59,  1.58it/s]Extractor Predicting: 378it [03:59,  1.55it/s]Extractor Predicting: 379it [04:00,  1.55it/s]Extractor Predicting: 380it [04:01,  1.54it/s]Extractor Predicting: 381it [04:01,  1.52it/s]Extractor Predicting: 382it [04:02,  1.51it/s]Extractor Predicting: 383it [04:03,  1.52it/s]Extractor Predicting: 384it [04:03,  1.50it/s]Extractor Predicting: 385it [04:04,  1.51it/s]Extractor Predicting: 386it [04:04,  1.53it/s]Extractor Predicting: 387it [04:05,  1.51it/s]Extractor Predicting: 388it [04:06,  1.49it/s]Extractor Predicting: 389it [04:06,  1.52it/s]Extractor Predicting: 390it [04:07,  1.53it/s]Extractor Predicting: 391it [04:08,  1.53it/s]Extractor Predicting: 392it [04:08,  1.50it/s]Extractor Predicting: 393it [04:09,  1.54it/s]Extractor Predicting: 394it [04:10,  1.51it/s]Extractor Predicting: 395it [04:10,  1.50it/s]Extractor Predicting: 396it [04:11,  1.50it/s]Extractor Predicting: 397it [04:12,  1.48it/s]Extractor Predicting: 398it [04:12,  1.49it/s]Extractor Predicting: 399it [04:13,  1.45it/s]Extractor Predicting: 400it [04:14,  1.46it/s]Extractor Predicting: 401it [04:15,  1.41it/s]Extractor Predicting: 402it [04:15,  1.45it/s]Extractor Predicting: 403it [04:16,  1.46it/s]Extractor Predicting: 404it [04:17,  1.51it/s]Extractor Predicting: 405it [04:17,  1.51it/s]Extractor Predicting: 406it [04:18,  1.45it/s]Extractor Predicting: 407it [04:19,  1.48it/s]Extractor Predicting: 408it [04:19,  1.48it/s]Extractor Predicting: 409it [04:20,  1.53it/s]Extractor Predicting: 410it [04:21,  1.54it/s]Extractor Predicting: 411it [04:21,  1.54it/s]Extractor Predicting: 412it [04:22,  1.52it/s]Extractor Predicting: 413it [04:22,  1.57it/s]Extractor Predicting: 414it [04:23,  1.57it/s]Extractor Predicting: 415it [04:24,  1.57it/s]Extractor Predicting: 416it [04:25,  1.48it/s]Extractor Predicting: 417it [04:25,  1.53it/s]Extractor Predicting: 418it [04:26,  1.54it/s]Extractor Predicting: 419it [04:26,  1.53it/s]Extractor Predicting: 420it [04:27,  1.55it/s]Extractor Predicting: 421it [04:28,  1.56it/s]Extractor Predicting: 422it [04:28,  1.53it/s]Extractor Predicting: 423it [04:29,  1.54it/s]Extractor Predicting: 424it [04:30,  1.51it/s]Extractor Predicting: 425it [04:30,  1.53it/s]Extractor Predicting: 426it [04:31,  1.52it/s]Extractor Predicting: 427it [04:32,  1.54it/s]Extractor Predicting: 428it [04:32,  1.53it/s]Extractor Predicting: 429it [04:33,  1.55it/s]Extractor Predicting: 430it [04:33,  1.93it/s]Extractor Predicting: 430it [04:33,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:12:13,108 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:12:13,113 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:12:13,113 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:12:13,113 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:12:13,113 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:12:13,726 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:12:13,727 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:12:14,329 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:12:15,427 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:12:15,427 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:12:18,487 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:12:18,523 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:12:18,523 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:12:18,523 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:12:18,523 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:12:19,217 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:12:19,218 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:12:19,798 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:12:19,975 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:12:19,975 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.33055885850178357,
  "recall": 0.08098659933967761,
  "score": 0.13009905623586304,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.40it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:02,  1.94it/s]Extractor Predicting: 5it [00:02,  1.67it/s]
[INFO|configuration_utils.py:515] 2023-08-28 17:12:23,417 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:12:23,418 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:12:23,425 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:12:23,426 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 17:12:23,433 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:12:30,085 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 17:12:30,094 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 17:12:30,107 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:12:30,107 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:12:30,114 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:12:30,119 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:12:30,119 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:12:30,119 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:12:30,119 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:12:30,119 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:12:30,119 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.39285714285714285,
  "recall": 0.054455445544554455,
  "score": 0.09565217391304348,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 17:12:30,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:30,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:31,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:32,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:32,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:33,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:34,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:34,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:35,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:35,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:36,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:37,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:37,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:38,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:38,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:39,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:40,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:40,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:41,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:41,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:42,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:43,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:13<04:18, 13.61s/it][WARNING|generation_utils.py:914] 2023-08-28 17:12:43,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:44,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:45,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:45,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:46,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:47,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:48,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:48,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:49,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:50,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:50,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:51,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:52,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:52,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:53,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:53,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:54,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:55,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:55,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:56,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:56,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:27<04:05, 13.62s/it][WARNING|generation_utils.py:914] 2023-08-28 17:12:57,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:58,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:58,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:59,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:59,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:00,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:00,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:01,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:01,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:02,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:02,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:03,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:04,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:05,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:06,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:06,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:07,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:08,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:08,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:09,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:10,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:40<03:46, 13.30s/it][WARNING|generation_utils.py:914] 2023-08-28 17:13:10,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:11,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:11,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:12,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:12,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:13,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:13,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:14,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:15,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:16,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:16,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:17,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:17,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:18,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:19,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:19,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:20,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:20,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:21,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:22,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:22,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:52<03:29, 13.07s/it][WARNING|generation_utils.py:914] 2023-08-28 17:13:23,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:23,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:24,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:25,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:25,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:26,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:26,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:27,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:28,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:28,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:29,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:29,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:30,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:31,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:31,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:32,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:32,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:33,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:34,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:34,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:04<03:10, 12.68s/it][WARNING|generation_utils.py:914] 2023-08-28 17:13:35,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:35,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:36,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:37,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:37,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:38,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:38,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:39,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:40,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:40,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:41,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:42,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:42,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:43,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:43,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:45,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:45,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:46,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:46,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:48,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:48,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:49,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:19<03:06, 13.35s/it][WARNING|generation_utils.py:914] 2023-08-28 17:13:49,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:50,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:50,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:51,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:51,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:52,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:53,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:53,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:54,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:54,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:55,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:55,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:56,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:56,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:57,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:57,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:58,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:58,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:59,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:59,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:00,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:30<02:44, 12.68s/it][WARNING|generation_utils.py:914] 2023-08-28 17:14:01,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:01,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:02,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:03,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:03,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:04,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:04,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:05,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:05,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:06,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:07,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:07,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:08,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:09,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:09,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:10,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:10,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:11,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:12,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:12,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:13,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:14,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:15,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:45<02:38, 13.25s/it][WARNING|generation_utils.py:914] 2023-08-28 17:14:15,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:16,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:16,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:17,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:18,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:18,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:19,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:19,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:20,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:21,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:21,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:22,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:23,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:23,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:24,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:25,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:25,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:26,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:27,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:27,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:28,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:28,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:29,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:30,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:31,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:31,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:02<02:37, 14.36s/it][WARNING|generation_utils.py:914] 2023-08-28 17:14:32,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:33,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:33,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:34,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:34,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:35,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:35,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:36,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:37,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:37,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:38,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:38,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:39,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:39,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:40,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:41,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:41,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:42,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:42,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:43,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:43,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:44,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:14<02:18, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-28 17:14:45,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:45,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:46,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:47,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:47,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:48,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:48,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:49,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:50,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:50,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:51,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:52,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:52,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:53,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:54,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:54,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:55,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:56,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:56,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:57,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:58,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:28<02:03, 13.75s/it][WARNING|generation_utils.py:914] 2023-08-28 17:14:58,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:59,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:59,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:00,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:00,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:01,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:02,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:02,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:03,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:03,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:04,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:04,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:05,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:05,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:06,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:07,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:07,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:08,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:08,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:09,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:10,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:10,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:11,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:41<01:48, 13.59s/it][WARNING|generation_utils.py:914] 2023-08-28 17:15:11,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:12,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:13,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:13,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:14,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:14,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:15,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:16,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:16,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:17,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:17,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:18,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:18,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:19,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:20,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:20,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:21,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:21,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:22,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:22,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:53<01:31, 13.02s/it][WARNING|generation_utils.py:914] 2023-08-28 17:15:23,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:24,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:24,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:25,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:26,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:26,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:27,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:28,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:28,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:29,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:30,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:30,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:31,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:32,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:32,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:33,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:33,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:34,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:35,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:35,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:36,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:37,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:07<01:20, 13.35s/it][WARNING|generation_utils.py:914] 2023-08-28 17:15:37,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:38,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:39,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:39,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:40,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:40,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:41,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:41,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:42,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:43,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:44,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:44,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:45,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:45,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:46,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:47,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:47,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:48,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:48,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:49,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:50,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:50,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:21<01:07, 13.45s/it][WARNING|generation_utils.py:914] 2023-08-28 17:15:51,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:52,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:52,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:53,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:54,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:54,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:55,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:56,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:56,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:57,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:57,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:58,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:59,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:15:59,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:00,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:01,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:01,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:02,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:03,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:03,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:04,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:05,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:35<00:55, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-28 17:16:06,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:06,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:07,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:07,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:08,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:09,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:09,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:10,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:11,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:11,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:12,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:12,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:13,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:14,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:14,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:15,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:16,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:16,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:17,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:17,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:18,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:19,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:49<00:41, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-28 17:16:20,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:20,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:21,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:21,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:22,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:22,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:23,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:24,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:24,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:25,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:25,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:26,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:26,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:27,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:28,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:28,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:29,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:29,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:30,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:30,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:31,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:01<00:26, 13.30s/it][WARNING|generation_utils.py:914] 2023-08-28 17:16:32,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:32,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:33,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:34,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:34,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:35,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:35,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:36,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:37,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:37,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:38,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:38,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:39,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:40,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:40,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:41,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:41,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:42,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:42,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:43,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:43,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:44,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:45,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:45,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:46,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:46,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:47,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:48,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:18<00:14, 14.34s/it][WARNING|generation_utils.py:914] 2023-08-28 17:16:48,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:49,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:50,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:50,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:51,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:51,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:53,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:53,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:54,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:54,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:55,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:56,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:56,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:57,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:58,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:16:58,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:17:00,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:17:00,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:17:01,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:17:01,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:17:02,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:17:03,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:33<00:00, 14.59s/it]Generating: 100%|██████████| 20/20 [04:33<00:00, 13.68s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:12,137 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:12,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:12,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:12,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:12,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:17:12,979 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:17:12,980 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:17:14,801 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:17:15,854 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:17:15,866 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:19,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:19,797 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:19,797 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:19,797 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:19,797 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:17:20,694 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:17:20,695 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:17:21,271 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:17:21,441 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:17:21,441 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : followed by .', 'success_rate': 0.8579545454545454, 'errors': {'', '(\'"\', \'followed by\', \'\', \'He starred opposite Michael Douglas and David Cronenberg in " " , and starred in a number of subsequent episodes .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : military rank .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('John William MacLean', 'military rank', '', 'John William MacLean ’s mother , Mary MacLean is the eldest surviving son of James MacLean and William MacLean , and grandson of Sir William MacLean .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '("When My Heart \'s in the Air", \'record label\', \'\', \'The album singles on the album include " When My Heart\\\'s in the Air " and " You Don\\\'t Want to Know " .\')', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : tributary .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : architect .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : constellation .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8396739130434783, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : country of origin . Context : The city is located in the southern part of the state of Victoria in Western Australia at the end of the Victoria Sub -continent . Head Entity : Victoria , Tail Entity : western Australia .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 325, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 373, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 413, 'raw': 576}
{'target': 600, 'success': 441, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 537, 'raw': 736}
{'target': 600, 'success': 557, 'raw': 768}
{'target': 600, 'success': 584, 'raw': 800}
{'target': 600, 'success': 608, 'raw': 832}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7307692307692307, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8522727272727273, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : league .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8821022727272727, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.859375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : notable work .', 'success_rate': 0.8877840909090909, 'errors': {'', "('John Grisham', 'notable work', '', 'The book was described as the first work by John Grisham , a British historian of the Second World War , that had studied history of the Second World War .')", 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.8835227272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original broadcaster . Context : Later in 2003 , the station became a part of a new broadcast network , the AM station in Los Angeles , with a different format than the current AM format , AM 96.3 FM . Head Entity : AM 96.3 FM , Tail Entity : AM station in Los Angeles , California .\n']
['Relation : original broadcaster . Context : Later in 2003 , the station became a part of a new broadcast network , the AM station in Los Angeles , with a different format than the current AM format , AM 96.3 FM . Head Entity : AM 96.3 FM , Tail Entity : AM station in Los Angeles , California .\n', "Relation : original broadcaster . Context : After the death of station 's founder John Hoey , SBS transferred radio stations to his flagship station SBS - FM in North Carolina . Head Entity : SBS - FM , Tail Entity : SBS - WAM .\n"]
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : On 31 March 2014 , the Brazilian national football team won their first match as the hosts of the Copa Libertadores . Head Entity : Brazilian national football team , Tail Entity : footballer .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 135, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 252, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 346, 'raw': 480}
{'target': 600, 'success': 364, 'raw': 512}
{'target': 600, 'success': 386, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 451, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 495, 'raw': 704}
{'target': 600, 'success': 521, 'raw': 736}
{'target': 600, 'success': 537, 'raw': 768}
{'target': 600, 'success': 560, 'raw': 800}
{'target': 600, 'success': 576, 'raw': 832}
{'target': 600, 'success': 598, 'raw': 864}
{'target': 600, 'success': 619, 'raw': 896}
{'prompt': 'Relation : position held .', 'success_rate': 0.6908482142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 13336
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13436, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.45it/s]Extractor Estimating: 2it [00:01,  1.42it/s]Extractor Estimating: 3it [00:02,  1.47it/s]Extractor Estimating: 4it [00:02,  1.46it/s]Extractor Estimating: 5it [00:03,  1.45it/s]Extractor Estimating: 6it [00:04,  1.50it/s]Extractor Estimating: 7it [00:04,  1.54it/s]Extractor Estimating: 8it [00:05,  1.54it/s]Extractor Estimating: 9it [00:06,  1.49it/s]Extractor Estimating: 10it [00:06,  1.48it/s]Extractor Estimating: 11it [00:07,  1.50it/s]Extractor Estimating: 12it [00:08,  1.50it/s]Extractor Estimating: 13it [00:08,  1.55it/s]Extractor Estimating: 14it [00:09,  1.54it/s]Extractor Estimating: 15it [00:09,  1.57it/s]Extractor Estimating: 16it [00:10,  1.59it/s]Extractor Estimating: 17it [00:11,  1.53it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:12,  1.59it/s]Extractor Estimating: 20it [00:13,  1.57it/s]Extractor Estimating: 21it [00:13,  1.52it/s]Extractor Estimating: 22it [00:14,  1.44it/s]Extractor Estimating: 23it [00:15,  1.48it/s]Extractor Estimating: 24it [00:15,  1.45it/s]Extractor Estimating: 25it [00:16,  1.48it/s]Extractor Estimating: 26it [00:17,  1.54it/s]Extractor Estimating: 27it [00:17,  1.56it/s]Extractor Estimating: 28it [00:18,  1.62it/s]Extractor Estimating: 29it [00:18,  1.66it/s]Extractor Estimating: 30it [00:19,  1.65it/s]Extractor Estimating: 31it [00:20,  1.64it/s]Extractor Estimating: 32it [00:20,  1.60it/s]Extractor Estimating: 33it [00:21,  1.62it/s]Extractor Estimating: 34it [00:22,  1.61it/s]Extractor Estimating: 35it [00:22,  1.64it/s]Extractor Estimating: 36it [00:23,  1.64it/s]Extractor Estimating: 37it [00:23,  1.62it/s]Extractor Estimating: 38it [00:24,  1.57it/s]Extractor Estimating: 39it [00:25,  1.53it/s]Extractor Estimating: 40it [00:25,  1.56it/s]Extractor Estimating: 41it [00:26,  1.61it/s]Extractor Estimating: 42it [00:26,  1.67it/s]Extractor Estimating: 43it [00:27,  1.64it/s]Extractor Estimating: 44it [00:28,  1.67it/s]Extractor Estimating: 45it [00:28,  1.64it/s]Extractor Estimating: 46it [00:29,  1.65it/s]Extractor Estimating: 47it [00:30,  1.63it/s]Extractor Estimating: 48it [00:30,  1.61it/s]Extractor Estimating: 49it [00:31,  1.63it/s]Extractor Estimating: 50it [00:31,  1.64it/s]Extractor Estimating: 51it [00:32,  1.74it/s]Extractor Estimating: 52it [00:32,  1.75it/s]Extractor Estimating: 53it [00:33,  1.87it/s]Extractor Estimating: 54it [00:33,  1.87it/s]Extractor Estimating: 55it [00:34,  2.00it/s]Extractor Estimating: 56it [00:34,  1.97it/s]Extractor Estimating: 57it [00:35,  2.01it/s]Extractor Estimating: 58it [00:35,  2.04it/s]Extractor Estimating: 59it [00:36,  2.08it/s]Extractor Estimating: 60it [00:36,  2.07it/s]Extractor Estimating: 61it [00:37,  1.89it/s]Extractor Estimating: 62it [00:37,  1.91it/s]Extractor Estimating: 63it [00:38,  1.59it/s]Extractor Estimating: 64it [00:39,  1.76it/s]Extractor Estimating: 65it [00:39,  1.80it/s]Extractor Estimating: 66it [00:40,  1.83it/s]Extractor Estimating: 67it [00:40,  1.87it/s]Extractor Estimating: 68it [00:41,  1.73it/s]Extractor Estimating: 69it [00:41,  1.83it/s]Extractor Estimating: 70it [00:42,  1.82it/s]Extractor Estimating: 71it [00:42,  1.85it/s]Extractor Estimating: 72it [00:43,  1.85it/s]Extractor Estimating: 73it [00:44,  1.85it/s]Extractor Estimating: 74it [00:44,  1.87it/s]Extractor Estimating: 75it [00:45,  1.86it/s]Extractor Estimating: 76it [00:45,  1.80it/s]Extractor Estimating: 77it [00:46,  1.74it/s]Extractor Estimating: 78it [00:46,  1.71it/s]Extractor Estimating: 79it [00:47,  1.69it/s]Extractor Estimating: 80it [00:48,  1.72it/s]Extractor Estimating: 81it [00:48,  1.74it/s]Extractor Estimating: 82it [00:49,  1.71it/s]Extractor Estimating: 83it [00:49,  1.71it/s]Extractor Estimating: 84it [00:50,  1.74it/s]Extractor Estimating: 85it [00:51,  1.63it/s]Extractor Estimating: 86it [00:51,  1.66it/s]Extractor Estimating: 87it [00:52,  1.69it/s]Extractor Estimating: 88it [00:52,  1.76it/s]Extractor Estimating: 89it [00:53,  1.68it/s]Extractor Estimating: 90it [00:54,  1.67it/s]Extractor Estimating: 91it [00:54,  1.62it/s]Extractor Estimating: 92it [00:55,  1.60it/s]Extractor Estimating: 93it [00:55,  1.61it/s]Extractor Estimating: 94it [00:56,  1.60it/s]Extractor Estimating: 95it [00:57,  1.65it/s]Extractor Estimating: 96it [00:57,  1.62it/s]Extractor Estimating: 97it [00:58,  1.61it/s]Extractor Estimating: 98it [00:59,  1.59it/s]Extractor Estimating: 99it [00:59,  1.60it/s]Extractor Estimating: 100it [01:00,  1.58it/s]Extractor Estimating: 101it [01:00,  1.70it/s]Extractor Estimating: 102it [01:01,  1.76it/s]Extractor Estimating: 103it [01:01,  1.84it/s]Extractor Estimating: 104it [01:02,  1.87it/s]Extractor Estimating: 105it [01:02,  1.92it/s]Extractor Estimating: 106it [01:03,  1.98it/s]Extractor Estimating: 107it [01:03,  1.96it/s]Extractor Estimating: 108it [01:04,  1.96it/s]Extractor Estimating: 109it [01:04,  1.93it/s]Extractor Estimating: 110it [01:05,  1.89it/s]Extractor Estimating: 111it [01:05,  1.94it/s]Extractor Estimating: 112it [01:06,  1.92it/s]Extractor Estimating: 113it [01:06,  1.95it/s]Extractor Estimating: 114it [01:07,  1.94it/s]Extractor Estimating: 115it [01:07,  1.93it/s]Extractor Estimating: 116it [01:08,  1.93it/s]Extractor Estimating: 117it [01:08,  1.98it/s]Extractor Estimating: 118it [01:09,  1.99it/s]Extractor Estimating: 119it [01:10,  1.97it/s]Extractor Estimating: 120it [01:10,  1.94it/s]Extractor Estimating: 121it [01:11,  1.92it/s]Extractor Estimating: 122it [01:11,  1.94it/s]Extractor Estimating: 123it [01:12,  1.96it/s]Extractor Estimating: 124it [01:12,  2.01it/s]Extractor Estimating: 125it [01:13,  2.02it/s]Extractor Estimating: 126it [01:13,  1.82it/s]Extractor Estimating: 127it [01:14,  1.82it/s]Extractor Estimating: 128it [01:14,  1.71it/s]Extractor Estimating: 129it [01:15,  1.71it/s]Extractor Estimating: 130it [01:16,  1.73it/s]Extractor Estimating: 131it [01:16,  1.74it/s]Extractor Estimating: 132it [01:17,  1.82it/s]Extractor Estimating: 133it [01:17,  1.80it/s]Extractor Estimating: 134it [01:18,  1.76it/s]Extractor Estimating: 135it [01:18,  1.81it/s]Extractor Estimating: 136it [01:19,  1.82it/s]Extractor Estimating: 137it [01:19,  1.78it/s]Extractor Estimating: 138it [01:20,  1.79it/s]Extractor Estimating: 139it [01:21,  1.77it/s]Extractor Estimating: 140it [01:21,  1.84it/s]Extractor Estimating: 141it [01:22,  1.86it/s]Extractor Estimating: 142it [01:22,  1.76it/s]Extractor Estimating: 143it [01:23,  1.76it/s]Extractor Estimating: 144it [01:23,  1.75it/s]Extractor Estimating: 145it [01:24,  1.76it/s]Extractor Estimating: 146it [01:25,  1.69it/s]Extractor Estimating: 147it [01:25,  1.72it/s]Extractor Estimating: 148it [01:26,  1.74it/s]Extractor Estimating: 149it [01:27,  1.56it/s]Extractor Estimating: 150it [01:27,  1.61it/s]Extractor Estimating: 151it [01:28,  1.69it/s]Extractor Estimating: 152it [01:28,  1.74it/s]Extractor Estimating: 153it [01:29,  1.82it/s]Extractor Estimating: 154it [01:29,  1.83it/s]Extractor Estimating: 155it [01:30,  1.90it/s]Extractor Estimating: 156it [01:30,  1.89it/s]Extractor Estimating: 157it [01:31,  1.89it/s]Extractor Estimating: 158it [01:31,  1.92it/s]Extractor Estimating: 159it [01:32,  1.92it/s]Extractor Estimating: 160it [01:32,  1.99it/s]Extractor Estimating: 161it [01:33,  1.90it/s]Extractor Estimating: 162it [01:33,  1.93it/s]Extractor Estimating: 163it [01:34,  1.87it/s]Extractor Estimating: 164it [01:34,  1.88it/s]Extractor Estimating: 165it [01:35,  1.92it/s]Extractor Estimating: 166it [01:35,  1.91it/s]Extractor Estimating: 167it [01:36,  1.92it/s]Extractor Estimating: 168it [01:36,  1.94it/s]Extractor Estimating: 169it [01:37,  1.94it/s]Extractor Estimating: 170it [01:37,  2.02it/s]Extractor Estimating: 171it [01:38,  1.97it/s]Extractor Estimating: 172it [01:38,  2.00it/s]Extractor Estimating: 173it [01:39,  1.99it/s]Extractor Estimating: 174it [01:39,  1.90it/s]Extractor Estimating: 175it [01:40,  1.95it/s]Extractor Estimating: 176it [01:41,  1.87it/s]Extractor Estimating: 177it [01:41,  1.81it/s]Extractor Estimating: 178it [01:42,  1.82it/s]Extractor Estimating: 179it [01:42,  1.81it/s]Extractor Estimating: 180it [01:43,  1.85it/s]Extractor Estimating: 181it [01:43,  1.81it/s]Extractor Estimating: 182it [01:44,  1.80it/s]Extractor Estimating: 183it [01:44,  1.81it/s]Extractor Estimating: 184it [01:45,  1.83it/s]Extractor Estimating: 185it [01:46,  1.75it/s]Extractor Estimating: 186it [01:46,  1.80it/s]Extractor Estimating: 187it [01:47,  1.82it/s]Extractor Estimating: 188it [01:47,  1.79it/s]Extractor Estimating: 189it [01:48,  1.78it/s]Extractor Estimating: 190it [01:48,  1.82it/s]Extractor Estimating: 191it [01:49,  1.78it/s]Extractor Estimating: 192it [01:49,  1.81it/s]Extractor Estimating: 193it [01:50,  1.77it/s]Extractor Estimating: 194it [01:51,  1.72it/s]Extractor Estimating: 195it [01:51,  1.73it/s]Extractor Estimating: 196it [01:52,  1.69it/s]Extractor Estimating: 197it [01:52,  1.69it/s]Extractor Estimating: 198it [01:53,  1.66it/s]Extractor Estimating: 199it [01:54,  1.68it/s]Extractor Estimating: 200it [01:54,  1.72it/s]Extractor Estimating: 201it [01:55,  1.66it/s]Extractor Estimating: 202it [01:55,  1.64it/s]Extractor Estimating: 203it [01:56,  1.64it/s]Extractor Estimating: 204it [01:57,  1.66it/s]Extractor Estimating: 205it [01:57,  1.70it/s]Extractor Estimating: 206it [01:58,  1.75it/s]Extractor Estimating: 207it [01:58,  1.78it/s]Extractor Estimating: 208it [01:59,  1.72it/s]Extractor Estimating: 209it [01:59,  1.75it/s]Extractor Estimating: 210it [02:00,  1.75it/s]Extractor Estimating: 211it [02:01,  1.71it/s]Extractor Estimating: 212it [02:01,  1.69it/s]Extractor Estimating: 213it [02:02,  1.69it/s]Extractor Estimating: 214it [02:02,  1.69it/s]Extractor Estimating: 215it [02:03,  1.65it/s]Extractor Estimating: 216it [02:04,  1.58it/s]Extractor Estimating: 217it [02:04,  1.63it/s]Extractor Estimating: 218it [02:05,  1.59it/s]Extractor Estimating: 219it [02:06,  1.66it/s]Extractor Estimating: 220it [02:06,  1.65it/s]Extractor Estimating: 221it [02:07,  1.59it/s]Extractor Estimating: 222it [02:08,  1.58it/s]Extractor Estimating: 223it [02:08,  1.67it/s]Extractor Estimating: 224it [02:09,  1.66it/s]Extractor Estimating: 225it [02:09,  1.65it/s]Extractor Estimating: 226it [02:10,  1.63it/s]Extractor Estimating: 227it [02:11,  1.58it/s]Extractor Estimating: 228it [02:11,  1.62it/s]Extractor Estimating: 229it [02:12,  1.63it/s]Extractor Estimating: 230it [02:12,  1.62it/s]Extractor Estimating: 231it [02:13,  1.54it/s]Extractor Estimating: 232it [02:14,  1.61it/s]Extractor Estimating: 233it [02:15,  1.45it/s]Extractor Estimating: 234it [02:15,  1.49it/s]Extractor Estimating: 235it [02:16,  1.54it/s]Extractor Estimating: 236it [02:16,  1.50it/s]Extractor Estimating: 237it [02:17,  1.55it/s]Extractor Estimating: 238it [02:18,  1.60it/s]Extractor Estimating: 239it [02:18,  1.58it/s]Extractor Estimating: 240it [02:19,  1.66it/s]Extractor Estimating: 241it [02:20,  1.57it/s]Extractor Estimating: 242it [02:20,  1.62it/s]Extractor Estimating: 243it [02:21,  1.61it/s]Extractor Estimating: 244it [02:21,  1.58it/s]Extractor Estimating: 245it [02:22,  1.58it/s]Extractor Estimating: 246it [02:23,  1.56it/s]Extractor Estimating: 247it [02:23,  1.60it/s]Extractor Estimating: 248it [02:24,  1.51it/s]Extractor Estimating: 249it [02:25,  1.58it/s]Extractor Estimating: 250it [02:25,  1.60it/s]Extractor Estimating: 251it [02:26,  1.55it/s]Extractor Estimating: 252it [02:27,  1.49it/s]Extractor Estimating: 253it [02:27,  1.52it/s]Extractor Estimating: 254it [02:28,  1.44it/s]Extractor Estimating: 255it [02:29,  1.46it/s]Extractor Estimating: 256it [02:29,  1.47it/s]Extractor Estimating: 257it [02:30,  1.49it/s]Extractor Estimating: 258it [02:31,  1.46it/s]Extractor Estimating: 259it [02:31,  1.49it/s]Extractor Estimating: 260it [02:32,  1.45it/s]Extractor Estimating: 261it [02:33,  1.46it/s]Extractor Estimating: 262it [02:33,  1.49it/s]Extractor Estimating: 263it [02:34,  1.47it/s]Extractor Estimating: 264it [02:35,  1.47it/s]Extractor Estimating: 265it [02:35,  1.49it/s]Extractor Estimating: 266it [02:36,  1.49it/s]Extractor Estimating: 267it [02:37,  1.49it/s]Extractor Estimating: 268it [02:37,  1.49it/s]Extractor Estimating: 269it [02:38,  1.45it/s]Extractor Estimating: 270it [02:39,  1.46it/s]Extractor Estimating: 271it [02:40,  1.45it/s]Extractor Estimating: 272it [02:40,  1.45it/s]Extractor Estimating: 273it [02:41,  1.44it/s]Extractor Estimating: 274it [02:42,  1.39it/s]Extractor Estimating: 275it [02:42,  1.44it/s]Extractor Estimating: 276it [02:43,  1.53it/s]Extractor Estimating: 277it [02:43,  1.62it/s]Extractor Estimating: 278it [02:44,  1.66it/s]Extractor Estimating: 279it [02:45,  1.68it/s]Extractor Estimating: 280it [02:45,  1.68it/s]Extractor Estimating: 281it [02:46,  1.67it/s]Extractor Estimating: 282it [02:46,  1.75it/s]Extractor Estimating: 283it [02:47,  1.79it/s]Extractor Estimating: 284it [02:47,  1.80it/s]Extractor Estimating: 285it [02:48,  1.74it/s]Extractor Estimating: 286it [02:49,  1.74it/s]Extractor Estimating: 287it [02:49,  1.77it/s]Extractor Estimating: 288it [02:50,  1.61it/s]Extractor Estimating: 289it [02:51,  1.59it/s]Extractor Estimating: 290it [02:51,  1.62it/s]Extractor Estimating: 291it [02:52,  1.63it/s]Extractor Estimating: 292it [02:52,  1.65it/s]Extractor Estimating: 293it [02:53,  1.69it/s]Extractor Estimating: 294it [02:53,  1.64it/s]Extractor Estimating: 295it [02:54,  1.65it/s]Extractor Estimating: 296it [02:55,  1.62it/s]Extractor Estimating: 297it [02:55,  1.61it/s]Extractor Estimating: 298it [02:56,  1.64it/s]Extractor Estimating: 299it [02:57,  1.64it/s]Extractor Estimating: 300it [02:57,  1.73it/s]Extractor Estimating: 301it [02:58,  1.78it/s]Extractor Estimating: 302it [02:58,  1.81it/s]Extractor Estimating: 303it [02:59,  1.89it/s]Extractor Estimating: 304it [02:59,  1.99it/s]Extractor Estimating: 305it [03:00,  2.01it/s]Extractor Estimating: 306it [03:00,  2.01it/s]Extractor Estimating: 307it [03:01,  2.00it/s]Extractor Estimating: 308it [03:01,  1.91it/s]Extractor Estimating: 309it [03:02,  1.71it/s]Extractor Estimating: 310it [03:02,  1.82it/s]Extractor Estimating: 311it [03:03,  1.86it/s]Extractor Estimating: 312it [03:03,  1.87it/s]Extractor Estimating: 313it [03:04,  1.92it/s]Extractor Estimating: 314it [03:04,  2.01it/s]Extractor Estimating: 315it [03:05,  1.99it/s]Extractor Estimating: 316it [03:05,  2.05it/s]Extractor Estimating: 317it [03:06,  2.03it/s]Extractor Estimating: 318it [03:06,  1.99it/s]Extractor Estimating: 319it [03:07,  2.01it/s]Extractor Estimating: 320it [03:07,  1.96it/s]Extractor Estimating: 321it [03:08,  1.85it/s]Extractor Estimating: 322it [03:08,  1.83it/s]Extractor Estimating: 323it [03:09,  1.90it/s]Extractor Estimating: 324it [03:09,  1.95it/s]Extractor Estimating: 325it [03:10,  1.84it/s]Extractor Estimating: 326it [03:11,  1.77it/s]Extractor Estimating: 327it [03:11,  1.67it/s]Extractor Estimating: 328it [03:12,  1.68it/s]Extractor Estimating: 329it [03:13,  1.55it/s]Extractor Estimating: 330it [03:13,  1.57it/s]Extractor Estimating: 331it [03:14,  1.56it/s]Extractor Estimating: 332it [03:15,  1.52it/s]Extractor Estimating: 333it [03:15,  1.56it/s]Extractor Estimating: 334it [03:16,  1.55it/s]Extractor Estimating: 335it [03:16,  1.63it/s]Extractor Estimating: 336it [03:17,  1.69it/s]Extractor Estimating: 337it [03:18,  1.66it/s]Extractor Estimating: 338it [03:18,  1.58it/s]Extractor Estimating: 339it [03:19,  1.53it/s]Extractor Estimating: 340it [03:20,  1.55it/s]Extractor Estimating: 341it [03:20,  1.56it/s]Extractor Estimating: 342it [03:21,  1.59it/s]Extractor Estimating: 343it [03:21,  1.61it/s]Extractor Estimating: 344it [03:22,  1.66it/s]Extractor Estimating: 345it [03:23,  1.64it/s]Extractor Estimating: 346it [03:23,  1.62it/s]Extractor Estimating: 347it [03:24,  1.66it/s]Extractor Estimating: 348it [03:24,  1.65it/s]Extractor Estimating: 349it [03:25,  1.64it/s]Extractor Estimating: 350it [03:26,  1.69it/s]Extractor Estimating: 351it [03:26,  1.69it/s]Extractor Estimating: 352it [03:27,  1.66it/s]Extractor Estimating: 353it [03:27,  1.68it/s]Extractor Estimating: 354it [03:28,  1.73it/s]Extractor Estimating: 355it [03:29,  1.71it/s]Extractor Estimating: 356it [03:29,  1.75it/s]Extractor Estimating: 357it [03:30,  1.74it/s]Extractor Estimating: 358it [03:30,  1.71it/s]Extractor Estimating: 359it [03:31,  1.60it/s]Extractor Estimating: 360it [03:32,  1.66it/s]Extractor Estimating: 361it [03:32,  1.69it/s]Extractor Estimating: 362it [03:33,  1.70it/s]Extractor Estimating: 363it [03:33,  1.68it/s]Extractor Estimating: 364it [03:34,  1.67it/s]Extractor Estimating: 365it [03:34,  1.73it/s]Extractor Estimating: 366it [03:35,  1.71it/s]Extractor Estimating: 367it [03:36,  1.69it/s]Extractor Estimating: 368it [03:36,  1.71it/s]Extractor Estimating: 369it [03:37,  1.73it/s]Extractor Estimating: 370it [03:37,  1.74it/s]Extractor Estimating: 371it [03:38,  1.76it/s]Extractor Estimating: 372it [03:38,  1.76it/s]Extractor Estimating: 373it [03:39,  1.74it/s]Extractor Estimating: 374it [03:40,  1.67it/s]Extractor Estimating: 375it [03:40,  1.63it/s]Extractor Estimating: 376it [03:41,  1.61it/s]Extractor Estimating: 377it [03:42,  1.62it/s]Extractor Estimating: 378it [03:42,  1.53it/s]Extractor Estimating: 379it [03:43,  1.49it/s]Extractor Estimating: 380it [03:44,  1.51it/s]Extractor Estimating: 381it [03:44,  1.52it/s]Extractor Estimating: 382it [03:45,  1.53it/s]Extractor Estimating: 383it [03:46,  1.53it/s]Extractor Estimating: 384it [03:46,  1.54it/s]Extractor Estimating: 385it [03:47,  1.58it/s]Extractor Estimating: 386it [03:48,  1.59it/s]Extractor Estimating: 387it [03:48,  1.52it/s]Extractor Estimating: 388it [03:49,  1.50it/s]Extractor Estimating: 389it [03:50,  1.49it/s]Extractor Estimating: 390it [03:51,  1.34it/s]Extractor Estimating: 391it [03:51,  1.35it/s]Extractor Estimating: 392it [03:52,  1.37it/s]Extractor Estimating: 393it [03:53,  1.36it/s]Extractor Estimating: 394it [03:53,  1.39it/s]Extractor Estimating: 395it [03:54,  1.42it/s]Extractor Estimating: 396it [03:55,  1.46it/s]Extractor Estimating: 397it [03:56,  1.39it/s]Extractor Estimating: 398it [03:56,  1.40it/s]Extractor Estimating: 399it [03:57,  1.42it/s]Extractor Estimating: 400it [03:58,  1.43it/s]Extractor Estimating: 401it [03:58,  1.53it/s]Extractor Estimating: 402it [03:59,  1.54it/s]Extractor Estimating: 403it [03:59,  1.58it/s]Extractor Estimating: 404it [04:00,  1.58it/s]Extractor Estimating: 405it [04:01,  1.61it/s]Extractor Estimating: 406it [04:01,  1.63it/s]Extractor Estimating: 407it [04:02,  1.71it/s]Extractor Estimating: 408it [04:02,  1.73it/s]Extractor Estimating: 409it [04:03,  1.70it/s]Extractor Estimating: 410it [04:03,  1.75it/s]Extractor Estimating: 411it [04:04,  1.76it/s]Extractor Estimating: 412it [04:05,  1.73it/s]Extractor Estimating: 413it [04:05,  1.63it/s]Extractor Estimating: 414it [04:06,  1.65it/s]Extractor Estimating: 415it [04:06,  1.67it/s]Extractor Estimating: 416it [04:07,  1.63it/s]Extractor Estimating: 417it [04:08,  1.69it/s]Extractor Estimating: 418it [04:08,  1.70it/s]Extractor Estimating: 419it [04:09,  1.71it/s]Extractor Estimating: 420it [04:09,  1.69it/s]Extractor Estimating: 421it [04:10,  1.71it/s]Extractor Estimating: 422it [04:11,  1.71it/s]Extractor Estimating: 423it [04:11,  1.73it/s]Extractor Estimating: 424it [04:12,  1.72it/s]Extractor Estimating: 425it [04:12,  1.67it/s]Extractor Estimating: 426it [04:13,  1.67it/s]Extractor Estimating: 427it [04:14,  1.66it/s]Extractor Estimating: 428it [04:14,  1.66it/s]Extractor Estimating: 429it [04:15,  1.67it/s]Extractor Estimating: 430it [04:15,  1.66it/s]Extractor Estimating: 431it [04:16,  1.68it/s]Extractor Estimating: 432it [04:16,  1.70it/s]Extractor Estimating: 433it [04:17,  1.69it/s]Extractor Estimating: 434it [04:18,  1.70it/s]Extractor Estimating: 435it [04:18,  1.69it/s]Extractor Estimating: 436it [04:19,  1.72it/s]Extractor Estimating: 437it [04:19,  1.74it/s]Extractor Estimating: 438it [04:20,  1.77it/s]Extractor Estimating: 439it [04:21,  1.64it/s]Extractor Estimating: 440it [04:21,  1.66it/s]Extractor Estimating: 441it [04:22,  1.72it/s]Extractor Estimating: 442it [04:22,  1.75it/s]Extractor Estimating: 443it [04:23,  1.73it/s]Extractor Estimating: 444it [04:24,  1.71it/s]Extractor Estimating: 445it [04:25,  1.30it/s]Extractor Estimating: 446it [04:25,  1.41it/s]Extractor Estimating: 447it [04:26,  1.53it/s]Extractor Estimating: 448it [04:26,  1.61it/s]Extractor Estimating: 449it [04:27,  1.68it/s]Extractor Estimating: 450it [04:27,  1.68it/s]Extractor Estimating: 451it [04:28,  1.71it/s]Extractor Estimating: 452it [04:29,  1.70it/s]Extractor Estimating: 453it [04:29,  1.69it/s]Extractor Estimating: 454it [04:30,  1.75it/s]Extractor Estimating: 455it [04:30,  1.72it/s]Extractor Estimating: 456it [04:31,  1.76it/s]Extractor Estimating: 457it [04:32,  1.71it/s]Extractor Estimating: 458it [04:32,  1.71it/s]Extractor Estimating: 459it [04:33,  1.72it/s]Extractor Estimating: 460it [04:33,  1.76it/s]Extractor Estimating: 461it [04:34,  1.70it/s]Extractor Estimating: 462it [04:34,  1.69it/s]Extractor Estimating: 463it [04:35,  1.74it/s]Extractor Estimating: 464it [04:36,  1.76it/s]Extractor Estimating: 465it [04:36,  1.77it/s]Extractor Estimating: 466it [04:37,  1.81it/s]Extractor Estimating: 467it [04:37,  1.61it/s]Extractor Estimating: 468it [04:38,  1.61it/s]Extractor Estimating: 469it [04:39,  1.68it/s]Extractor Estimating: 470it [04:39,  1.72it/s]Extractor Estimating: 471it [04:40,  1.77it/s]Extractor Estimating: 472it [04:40,  1.55it/s]Extractor Estimating: 473it [04:41,  1.56it/s]Extractor Estimating: 474it [04:42,  1.66it/s]Extractor Estimating: 475it [04:42,  1.65it/s]Extractor Estimating: 476it [04:43,  1.62it/s]Extractor Estimating: 477it [04:44,  1.55it/s]Extractor Estimating: 478it [04:44,  1.44it/s]Extractor Estimating: 479it [04:45,  1.52it/s]Extractor Estimating: 480it [04:46,  1.58it/s]Extractor Estimating: 481it [04:46,  1.59it/s]Extractor Estimating: 482it [04:47,  1.57it/s]Extractor Estimating: 483it [04:47,  1.60it/s]Extractor Estimating: 484it [04:48,  1.62it/s]Extractor Estimating: 485it [04:49,  1.64it/s]Extractor Estimating: 486it [04:49,  1.71it/s]Extractor Estimating: 487it [04:50,  1.72it/s]Extractor Estimating: 488it [04:50,  1.63it/s]Extractor Estimating: 489it [04:51,  1.59it/s]Extractor Estimating: 490it [04:52,  1.62it/s]Extractor Estimating: 491it [04:52,  1.67it/s]Extractor Estimating: 492it [04:53,  1.65it/s]Extractor Estimating: 493it [04:53,  1.63it/s]Extractor Estimating: 494it [04:54,  1.61it/s]Extractor Estimating: 495it [04:55,  1.69it/s]Extractor Estimating: 496it [04:55,  1.65it/s]Extractor Estimating: 497it [04:56,  1.61it/s]Extractor Estimating: 498it [04:57,  1.62it/s]Extractor Estimating: 499it [04:57,  1.64it/s]Extractor Estimating: 500it [04:58,  1.83it/s]Extractor Estimating: 500it [04:58,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:34,307 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:34,316 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:34,316 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:34,316 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:34,316 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:22:35,020 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:22:35,021 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:22:36,777 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:22:37,952 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:22:38,025 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:42,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:42,419 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:42,419 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:42,419 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:22:42,419 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:22:43,344 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:22:43,345 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:22:44,595 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:22:44,780 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:22:44,780 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 20:11:02,635 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 20:11:02,640 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9987 mean pseudo reward: 0.9662366119659499
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 23892
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23992, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23992, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.993, loss:510.5174
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.971, loss:518.8223
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.954, loss:530.1659
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.976, loss:515.8835
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 0.971, loss:527.6436
>> valid entity prec:0.5940, rec:0.6180, f1:0.6057
>> valid relation prec:0.2113, rec:0.0967, f1:0.1326
>> valid relation with NER prec:0.2113, rec:0.0967, f1:0.1326
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.201, loss:473.4353
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 0.983, loss:519.0107
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 0.979, loss:525.4679
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 0.966, loss:499.1329
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 0.963, loss:529.2078
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6014, rec:0.5557, f1:0.5776
>> valid relation prec:0.2827, rec:0.1036, f1:0.1517
>> valid relation with NER prec:0.2827, rec:0.1036, f1:0.1517
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.192, loss:515.8814
g_step 1200, step 366, avg_time 0.987, loss:545.0968
g_step 1300, step 49, avg_time 0.972, loss:518.6486
g_step 1400, step 149, avg_time 0.970, loss:483.6677
g_step 1500, step 249, avg_time 0.993, loss:507.6851
>> valid entity prec:0.6037, rec:0.6028, f1:0.6032
>> valid relation prec:0.2347, rec:0.0999, f1:0.1401
>> valid relation with NER prec:0.2347, rec:0.0999, f1:0.1401
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.211, loss:516.9806
g_step 1700, step 32, avg_time 0.977, loss:514.7856
g_step 1800, step 132, avg_time 0.967, loss:472.5368
g_step 1900, step 232, avg_time 0.987, loss:489.4906
g_step 2000, step 332, avg_time 0.966, loss:485.2717
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6422, rec:0.4814, f1:0.5503
>> valid relation prec:0.2614, rec:0.1016, f1:0.1463
>> valid relation with NER prec:0.2614, rec:0.1016, f1:0.1463
g_step 2100, step 15, avg_time 2.203, loss:492.6190
g_step 2200, step 115, avg_time 0.964, loss:456.0655
g_step 2300, step 215, avg_time 0.984, loss:460.0056
g_step 2400, step 315, avg_time 0.970, loss:452.2842
g_step 2500, step 415, avg_time 0.978, loss:482.9248
>> valid entity prec:0.5784, rec:0.5529, f1:0.5653
>> valid relation prec:0.2180, rec:0.0920, f1:0.1294
>> valid relation with NER prec:0.2180, rec:0.0920, f1:0.1294
g_step 2600, step 98, avg_time 2.207, loss:402.2973
g_step 2700, step 198, avg_time 0.966, loss:428.8186
g_step 2800, step 298, avg_time 0.954, loss:455.1513
g_step 2900, step 398, avg_time 0.984, loss:438.1708
g_step 3000, step 81, avg_time 0.955, loss:401.4075
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6004, rec:0.5855, f1:0.5928
>> valid relation prec:0.2112, rec:0.0874, f1:0.1236
>> valid relation with NER prec:0.2112, rec:0.0874, f1:0.1236
g_step 3100, step 181, avg_time 2.207, loss:413.8332
g_step 3200, step 281, avg_time 0.970, loss:421.2581
g_step 3300, step 381, avg_time 0.984, loss:421.6524
g_step 3400, step 64, avg_time 0.976, loss:417.3234
g_step 3500, step 164, avg_time 0.978, loss:379.0151
>> valid entity prec:0.5877, rec:0.5512, f1:0.5688
>> valid relation prec:0.2239, rec:0.1054, f1:0.1433
>> valid relation with NER prec:0.2239, rec:0.1054, f1:0.1433
g_step 3600, step 264, avg_time 2.210, loss:411.4526
g_step 3700, step 364, avg_time 0.968, loss:413.6801
g_step 3800, step 47, avg_time 0.966, loss:400.1967
g_step 3900, step 147, avg_time 0.978, loss:375.1963
g_step 4000, step 247, avg_time 0.963, loss:397.2457
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5914, rec:0.5806, f1:0.5859
>> valid relation prec:0.1822, rec:0.0906, f1:0.1210
>> valid relation with NER prec:0.1822, rec:0.0906, f1:0.1210
g_step 4100, step 347, avg_time 2.209, loss:410.2411
g_step 4200, step 30, avg_time 0.974, loss:382.4782
g_step 4300, step 130, avg_time 0.986, loss:360.9783
g_step 4400, step 230, avg_time 0.978, loss:365.9602
g_step 4500, step 330, avg_time 0.962, loss:405.1670
>> valid entity prec:0.5912, rec:0.5440, f1:0.5666
>> valid relation prec:0.2050, rec:0.0935, f1:0.1284
>> valid relation with NER prec:0.2050, rec:0.0935, f1:0.1284
g_step 4600, step 13, avg_time 2.192, loss:389.5044
g_step 4700, step 113, avg_time 0.981, loss:335.3839
g_step 4800, step 213, avg_time 0.975, loss:349.8965
g_step 4900, step 313, avg_time 0.967, loss:378.3325
g_step 5000, step 413, avg_time 0.977, loss:368.6816
learning rate was adjusted to 0.0008
>> valid entity prec:0.5732, rec:0.5344, f1:0.5531
>> valid relation prec:0.2196, rec:0.1022, f1:0.1395
>> valid relation with NER prec:0.2196, rec:0.1022, f1:0.1395
g_step 5100, step 96, avg_time 2.203, loss:321.7149
g_step 5200, step 196, avg_time 0.982, loss:342.5705
g_step 5300, step 296, avg_time 0.979, loss:355.6083
g_step 5400, step 396, avg_time 0.978, loss:355.5012
g_step 5500, step 79, avg_time 0.962, loss:320.3245
>> valid entity prec:0.5799, rec:0.5673, f1:0.5735
>> valid relation prec:0.2146, rec:0.0981, f1:0.1347
>> valid relation with NER prec:0.2146, rec:0.0981, f1:0.1347
g_step 5600, step 179, avg_time 2.214, loss:341.2853
g_step 5700, step 279, avg_time 0.971, loss:320.9999
g_step 5800, step 379, avg_time 0.974, loss:351.7586
g_step 5900, step 62, avg_time 0.973, loss:336.4518
g_step 6000, step 162, avg_time 0.979, loss:315.0026
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6159, rec:0.5626, f1:0.5880
>> valid relation prec:0.2108, rec:0.1115, f1:0.1458
>> valid relation with NER prec:0.2108, rec:0.1115, f1:0.1458
g_step 6100, step 262, avg_time 2.206, loss:333.9854
g_step 6200, step 362, avg_time 0.968, loss:322.8431
g_step 6300, step 45, avg_time 0.970, loss:318.3560
g_step 6400, step 145, avg_time 0.986, loss:311.6099
g_step 6500, step 245, avg_time 0.976, loss:316.5079
>> valid entity prec:0.5951, rec:0.5155, f1:0.5525
>> valid relation prec:0.2176, rec:0.0981, f1:0.1353
>> valid relation with NER prec:0.2176, rec:0.0981, f1:0.1353
g_step 6600, step 345, avg_time 2.214, loss:321.3911
g_step 6700, step 28, avg_time 0.952, loss:319.5292
g_step 6800, step 128, avg_time 0.974, loss:295.1332
g_step 6900, step 228, avg_time 0.978, loss:298.5638
g_step 7000, step 328, avg_time 0.976, loss:313.1879
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5835, rec:0.5534, f1:0.5680
>> valid relation prec:0.2081, rec:0.1080, f1:0.1422
>> valid relation with NER prec:0.2081, rec:0.1080, f1:0.1422
g_step 7100, step 11, avg_time 2.197, loss:312.9351
g_step 7200, step 111, avg_time 0.974, loss:267.6790
g_step 7300, step 211, avg_time 0.980, loss:288.3983
g_step 7400, step 311, avg_time 0.971, loss:299.3326
g_step 7500, step 411, avg_time 0.985, loss:315.1533
>> valid entity prec:0.5861, rec:0.5644, f1:0.5750
>> valid relation prec:0.2010, rec:0.1042, f1:0.1373
>> valid relation with NER prec:0.2010, rec:0.1042, f1:0.1373
g_step 7600, step 94, avg_time 2.190, loss:277.3222
g_step 7700, step 194, avg_time 0.971, loss:270.2933
g_step 7800, step 294, avg_time 0.966, loss:288.2070
g_step 7900, step 394, avg_time 0.991, loss:297.6939
g_step 8000, step 77, avg_time 0.973, loss:255.3711
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6003, rec:0.5352, f1:0.5659
>> valid relation prec:0.2041, rec:0.0946, f1:0.1293
>> valid relation with NER prec:0.2041, rec:0.0946, f1:0.1293
g_step 8100, step 177, avg_time 2.258, loss:265.6774
g_step 8200, step 277, avg_time 0.956, loss:276.5780
g_step 8300, step 377, avg_time 0.979, loss:280.1520
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 20:11:02 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 20:11:02 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_20-11-02_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 20:11:03 - WARNING - datasets.builder -   Using custom data configuration default-80bb765237ce7870
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-80bb765237ce7870/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  6.86 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 20:11:04,521 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:11:04,523 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:11:04,523 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:11:04,524 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:11:04,550 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:11:04,556 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:11:04,557 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:11:04,557 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:11:04,557 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:11:04,557 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:11:04,557 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 20:11:04,810 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:11:07,986 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 20:11:08,001 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-80bb765237ce7870/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:03,  2.91ba/s] 20%|██        | 2/10 [00:00<00:02,  3.82ba/s] 30%|███       | 3/10 [00:00<00:01,  4.23ba/s] 40%|████      | 4/10 [00:00<00:01,  4.41ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.52ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.60ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.66ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.70ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.70ba/s]100%|██████████| 10/10 [00:02<00:00,  4.72ba/s]100%|██████████| 10/10 [00:02<00:00,  4.49ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.60ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.12ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.31ba/s]100%|██████████| 4/4 [00:00<00:00,  5.49ba/s]100%|██████████| 4/4 [00:00<00:00,  4.87ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  7.17ba/s] 30%|███       | 3/10 [00:00<00:00,  9.70ba/s] 50%|█████     | 5/10 [00:00<00:00, 10.30ba/s] 70%|███████   | 7/10 [00:00<00:00, 10.54ba/s] 90%|█████████ | 9/10 [00:00<00:00, 10.80ba/s]100%|██████████| 10/10 [00:00<00:00, 10.48ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.12ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.43ba/s]100%|██████████| 4/4 [00:00<00:00, 11.95ba/s]
[INFO|trainer.py:414] 2023-08-28 20:11:13,228 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 20:11:13,298 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 20:11:13,298 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-28 20:11:13,298 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 20:11:13,298 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 20:11:13,298 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 20:11:13,298 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 20:11:13,298 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:57,  3.28it/s]  0%|          | 2/780 [00:00<03:51,  3.36it/s]  0%|          | 3/780 [00:00<03:49,  3.38it/s]  1%|          | 4/780 [00:01<03:50,  3.37it/s]  1%|          | 5/780 [00:01<03:49,  3.38it/s]  1%|          | 6/780 [00:01<03:48,  3.39it/s]  1%|          | 7/780 [00:02<03:47,  3.40it/s]  1%|          | 8/780 [00:02<03:47,  3.39it/s]  1%|          | 9/780 [00:02<03:47,  3.38it/s]  1%|▏         | 10/780 [00:02<03:47,  3.38it/s]  1%|▏         | 11/780 [00:03<03:47,  3.38it/s]  2%|▏         | 12/780 [00:03<03:47,  3.37it/s]  2%|▏         | 13/780 [00:03<03:47,  3.37it/s]  2%|▏         | 14/780 [00:04<03:47,  3.37it/s]  2%|▏         | 15/780 [00:04<03:47,  3.36it/s]  2%|▏         | 16/780 [00:04<03:47,  3.36it/s]  2%|▏         | 17/780 [00:05<03:47,  3.36it/s]  2%|▏         | 18/780 [00:05<03:46,  3.36it/s]  2%|▏         | 19/780 [00:05<03:46,  3.36it/s]  3%|▎         | 20/780 [00:05<03:45,  3.36it/s]  3%|▎         | 21/780 [00:06<03:45,  3.36it/s]  3%|▎         | 22/780 [00:06<03:45,  3.37it/s]  3%|▎         | 23/780 [00:06<03:44,  3.37it/s]  3%|▎         | 24/780 [00:07<03:44,  3.37it/s]  3%|▎         | 25/780 [00:07<03:44,  3.37it/s]  3%|▎         | 26/780 [00:07<03:55,  3.20it/s]  3%|▎         | 27/780 [00:08<03:52,  3.24it/s]  4%|▎         | 28/780 [00:08<03:49,  3.28it/s]  4%|▎         | 29/780 [00:08<03:47,  3.31it/s]  4%|▍         | 30/780 [00:08<03:45,  3.32it/s]  4%|▍         | 31/780 [00:09<03:44,  3.33it/s]  4%|▍         | 32/780 [00:09<03:45,  3.31it/s]  4%|▍         | 33/780 [00:09<03:44,  3.33it/s]  4%|▍         | 34/780 [00:10<03:43,  3.34it/s]  4%|▍         | 35/780 [00:10<03:42,  3.35it/s]  5%|▍         | 36/780 [00:10<03:44,  3.31it/s]  5%|▍         | 37/780 [00:11<03:43,  3.33it/s]  5%|▍         | 38/780 [00:11<03:42,  3.34it/s]  5%|▌         | 39/780 [00:11<03:41,  3.35it/s]  5%|▌         | 40/780 [00:11<03:40,  3.35it/s]  5%|▌         | 41/780 [00:12<03:40,  3.36it/s]  5%|▌         | 42/780 [00:12<03:39,  3.36it/s]  6%|▌         | 43/780 [00:12<03:39,  3.36it/s]  6%|▌         | 44/780 [00:13<03:38,  3.36it/s]  6%|▌         | 45/780 [00:13<03:38,  3.36it/s]  6%|▌         | 46/780 [00:13<03:38,  3.36it/s]  6%|▌         | 47/780 [00:14<03:45,  3.24it/s]  6%|▌         | 48/780 [00:14<03:43,  3.28it/s]  6%|▋         | 49/780 [00:14<03:41,  3.31it/s]  6%|▋         | 50/780 [00:14<03:39,  3.32it/s]  7%|▋         | 51/780 [00:15<03:43,  3.26it/s]  7%|▋         | 52/780 [00:15<03:40,  3.30it/s]  7%|▋         | 53/780 [00:15<03:39,  3.32it/s]  7%|▋         | 54/780 [00:16<03:38,  3.33it/s]  7%|▋         | 55/780 [00:16<03:37,  3.34it/s]  7%|▋         | 56/780 [00:16<03:36,  3.34it/s]  7%|▋         | 57/780 [00:17<03:36,  3.34it/s]  7%|▋         | 58/780 [00:17<03:35,  3.35it/s]  8%|▊         | 59/780 [00:17<03:34,  3.35it/s]  8%|▊         | 60/780 [00:17<03:34,  3.36it/s]  8%|▊         | 61/780 [00:18<03:34,  3.36it/s]  8%|▊         | 62/780 [00:18<03:33,  3.36it/s]  8%|▊         | 63/780 [00:18<03:33,  3.36it/s]  8%|▊         | 64/780 [00:19<03:33,  3.35it/s]  8%|▊         | 65/780 [00:19<03:33,  3.36it/s]  8%|▊         | 66/780 [00:19<03:32,  3.36it/s]  9%|▊         | 67/780 [00:20<03:32,  3.36it/s]  9%|▊         | 68/780 [00:20<03:32,  3.34it/s]  9%|▉         | 69/780 [00:20<03:32,  3.35it/s]  9%|▉         | 70/780 [00:20<03:31,  3.35it/s]  9%|▉         | 71/780 [00:21<03:31,  3.35it/s]  9%|▉         | 72/780 [00:21<03:30,  3.36it/s]  9%|▉         | 73/780 [00:21<03:30,  3.36it/s]  9%|▉         | 74/780 [00:22<03:30,  3.36it/s] 10%|▉         | 75/780 [00:22<03:30,  3.36it/s] 10%|▉         | 76/780 [00:22<03:29,  3.36it/s] 10%|▉         | 77/780 [00:23<03:29,  3.36it/s] 10%|█         | 78/780 [00:23<03:29,  3.35it/s] 10%|█         | 79/780 [00:23<03:31,  3.31it/s] 10%|█         | 80/780 [00:23<03:30,  3.33it/s] 10%|█         | 81/780 [00:24<03:29,  3.34it/s] 11%|█         | 82/780 [00:24<03:28,  3.34it/s] 11%|█         | 83/780 [00:24<03:27,  3.35it/s] 11%|█         | 84/780 [00:25<03:27,  3.36it/s] 11%|█         | 85/780 [00:25<03:27,  3.35it/s] 11%|█         | 86/780 [00:25<03:26,  3.35it/s] 11%|█         | 87/780 [00:26<03:26,  3.36it/s] 11%|█▏        | 88/780 [00:26<03:26,  3.36it/s] 11%|█▏        | 89/780 [00:26<03:25,  3.36it/s] 12%|█▏        | 90/780 [00:26<03:25,  3.36it/s] 12%|█▏        | 91/780 [00:27<03:25,  3.36it/s] 12%|█▏        | 92/780 [00:27<03:24,  3.36it/s] 12%|█▏        | 93/780 [00:27<03:24,  3.36it/s] 12%|█▏        | 94/780 [00:28<03:23,  3.36it/s] 12%|█▏        | 95/780 [00:28<03:23,  3.36it/s] 12%|█▏        | 96/780 [00:28<03:27,  3.29it/s] 12%|█▏        | 97/780 [00:29<03:26,  3.31it/s] 13%|█▎        | 98/780 [00:29<03:24,  3.34it/s] 13%|█▎        | 99/780 [00:29<03:22,  3.36it/s] 13%|█▎        | 100/780 [00:29<03:21,  3.37it/s] 13%|█▎        | 101/780 [00:30<03:21,  3.38it/s] 13%|█▎        | 102/780 [00:30<03:20,  3.39it/s] 13%|█▎        | 103/780 [00:30<03:19,  3.39it/s] 13%|█▎        | 104/780 [00:31<03:19,  3.40it/s] 13%|█▎        | 105/780 [00:31<03:18,  3.40it/s] 14%|█▎        | 106/780 [00:31<03:18,  3.40it/s] 14%|█▎        | 107/780 [00:31<03:19,  3.38it/s] 14%|█▍        | 108/780 [00:32<03:18,  3.39it/s] 14%|█▍        | 109/780 [00:32<03:17,  3.39it/s] 14%|█▍        | 110/780 [00:32<03:17,  3.40it/s] 14%|█▍        | 111/780 [00:33<03:16,  3.40it/s] 14%|█▍        | 112/780 [00:33<03:16,  3.40it/s] 14%|█▍        | 113/780 [00:33<03:15,  3.40it/s] 15%|█▍        | 114/780 [00:34<03:15,  3.40it/s] 15%|█▍        | 115/780 [00:34<03:15,  3.40it/s] 15%|█▍        | 116/780 [00:34<03:15,  3.40it/s] 15%|█▌        | 117/780 [00:34<03:14,  3.40it/s] 15%|█▌        | 118/780 [00:35<03:17,  3.35it/s] 15%|█▌        | 119/780 [00:35<03:16,  3.37it/s] 15%|█▌        | 120/780 [00:35<03:15,  3.38it/s] 16%|█▌        | 121/780 [00:36<03:14,  3.39it/s] 16%|█▌        | 122/780 [00:36<03:13,  3.39it/s] 16%|█▌        | 123/780 [00:36<03:13,  3.40it/s] 16%|█▌        | 124/780 [00:36<03:13,  3.40it/s] 16%|█▌        | 125/780 [00:37<03:12,  3.40it/s] 16%|█▌        | 126/780 [00:37<03:12,  3.40it/s] 16%|█▋        | 127/780 [00:37<03:11,  3.40it/s] 16%|█▋        | 128/780 [00:38<03:11,  3.40it/s] 17%|█▋        | 129/780 [00:38<03:11,  3.40it/s] 17%|█▋        | 130/780 [00:38<03:11,  3.40it/s] 17%|█▋        | 131/780 [00:39<03:10,  3.40it/s] 17%|█▋        | 132/780 [00:39<03:10,  3.40it/s] 17%|█▋        | 133/780 [00:39<03:10,  3.40it/s] 17%|█▋        | 134/780 [00:39<03:09,  3.40it/s] 17%|█▋        | 135/780 [00:40<03:09,  3.40it/s] 17%|█▋        | 136/780 [00:40<03:09,  3.41it/s] 18%|█▊        | 137/780 [00:40<03:08,  3.41it/s] 18%|█▊        | 138/780 [00:41<03:08,  3.41it/s] 18%|█▊        | 139/780 [00:41<03:08,  3.41it/s] 18%|█▊        | 140/780 [00:41<03:10,  3.36it/s] 18%|█▊        | 141/780 [00:41<03:09,  3.38it/s] 18%|█▊        | 142/780 [00:42<03:08,  3.39it/s] 18%|█▊        | 143/780 [00:42<03:07,  3.39it/s] 18%|█▊        | 144/780 [00:42<03:07,  3.40it/s] 19%|█▊        | 145/780 [00:43<03:06,  3.40it/s] 19%|█▊        | 146/780 [00:43<03:06,  3.40it/s] 19%|█▉        | 147/780 [00:43<03:05,  3.40it/s] 19%|█▉        | 148/780 [00:44<03:05,  3.40it/s] 19%|█▉        | 149/780 [00:44<03:05,  3.40it/s] 19%|█▉        | 150/780 [00:44<03:05,  3.40it/s] 19%|█▉        | 151/780 [00:44<03:07,  3.36it/s] 19%|█▉        | 152/780 [00:45<03:06,  3.37it/s] 20%|█▉        | 153/780 [00:45<03:05,  3.38it/s] 20%|█▉        | 154/780 [00:45<03:05,  3.38it/s] 20%|█▉        | 155/780 [00:46<03:04,  3.39it/s] 20%|██        | 156/780 [00:46<03:03,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 20:11:59,761 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:11:59,761 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 20:11:59,761 >>   Batch size = 8

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 53.52it/s][A
  3%|▎         | 12/431 [00:00<00:09, 46.17it/s][A
  4%|▍         | 17/431 [00:00<00:09, 44.67it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.09it/s][A
  6%|▋         | 27/431 [00:00<00:09, 43.73it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.66it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.57it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.38it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.54it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.46it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.31it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.17it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.19it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.26it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.25it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.18it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.25it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.31it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.42it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.26it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.09it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.14it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.18it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.30it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 40.99it/s][A
 31%|███       | 132/431 [00:03<00:07, 42.11it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 42.58it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 42.82it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 42.87it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 42.92it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 42.91it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.06it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 42.92it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.17it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.28it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.37it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.25it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.20it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.21it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.24it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.18it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.25it/s][A
 50%|█████     | 217/431 [00:05<00:04, 43.25it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.30it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.42it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.28it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.19it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.14it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.17it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.22it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.27it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.26it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.29it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.20it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.24it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.10it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.03it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.00it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.23it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.29it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.23it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.36it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.41it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.26it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.24it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.17it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.22it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.26it/s][A
 81%|████████  | 347/431 [00:08<00:01, 43.23it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.37it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.28it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.28it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.15it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.18it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.23it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.27it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.21it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.39it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.27it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 41.98it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 42.41it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 42.66it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 42.77it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 42.91it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.01it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.01it/s][A 20%|██        | 156/780 [00:56<03:03,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:12:09,801 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 20:12:09,888 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:12:14,549 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:12:14,570 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:12:14,581 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:10<1:15:58,  7.32s/it] 20%|██        | 158/780 [01:10<54:03,  5.21s/it]   20%|██        | 159/780 [01:10<38:42,  3.74s/it] 21%|██        | 160/780 [01:10<27:58,  2.71s/it] 21%|██        | 161/780 [01:11<20:28,  1.98s/it] 21%|██        | 162/780 [01:11<15:13,  1.48s/it] 21%|██        | 163/780 [01:11<11:33,  1.12s/it] 21%|██        | 164/780 [01:12<08:59,  1.14it/s] 21%|██        | 165/780 [01:12<07:11,  1.42it/s] 21%|██▏       | 166/780 [01:12<05:56,  1.72it/s] 21%|██▏       | 167/780 [01:13<05:03,  2.02it/s] 22%|██▏       | 168/780 [01:13<04:26,  2.29it/s] 22%|██▏       | 169/780 [01:13<04:05,  2.49it/s] 22%|██▏       | 170/780 [01:13<03:45,  2.70it/s] 22%|██▏       | 171/780 [01:14<03:31,  2.87it/s] 22%|██▏       | 172/780 [01:14<03:22,  3.01it/s] 22%|██▏       | 173/780 [01:14<03:15,  3.11it/s] 22%|██▏       | 174/780 [01:15<03:10,  3.18it/s] 22%|██▏       | 175/780 [01:15<03:25,  2.94it/s] 23%|██▎       | 176/780 [01:15<03:17,  3.05it/s] 23%|██▎       | 177/780 [01:16<03:11,  3.14it/s] 23%|██▎       | 178/780 [01:16<03:07,  3.20it/s] 23%|██▎       | 179/780 [01:16<03:05,  3.24it/s] 23%|██▎       | 180/780 [01:17<03:03,  3.27it/s] 23%|██▎       | 181/780 [01:17<03:01,  3.30it/s] 23%|██▎       | 182/780 [01:17<03:00,  3.32it/s] 23%|██▎       | 183/780 [01:17<02:59,  3.33it/s] 24%|██▎       | 184/780 [01:18<02:58,  3.34it/s] 24%|██▎       | 185/780 [01:18<02:57,  3.35it/s] 24%|██▍       | 186/780 [01:18<02:57,  3.35it/s] 24%|██▍       | 187/780 [01:19<02:56,  3.36it/s] 24%|██▍       | 188/780 [01:19<02:56,  3.36it/s] 24%|██▍       | 189/780 [01:19<02:55,  3.36it/s] 24%|██▍       | 190/780 [01:20<02:57,  3.33it/s] 24%|██▍       | 191/780 [01:20<02:56,  3.34it/s] 25%|██▍       | 192/780 [01:20<02:56,  3.34it/s] 25%|██▍       | 193/780 [01:20<02:55,  3.34it/s] 25%|██▍       | 194/780 [01:21<02:55,  3.35it/s] 25%|██▌       | 195/780 [01:21<02:54,  3.35it/s] 25%|██▌       | 196/780 [01:21<02:53,  3.36it/s] 25%|██▌       | 197/780 [01:22<02:53,  3.36it/s] 25%|██▌       | 198/780 [01:22<02:53,  3.36it/s] 26%|██▌       | 199/780 [01:22<02:52,  3.36it/s] 26%|██▌       | 200/780 [01:23<02:52,  3.36it/s] 26%|██▌       | 201/780 [01:23<02:52,  3.35it/s] 26%|██▌       | 202/780 [01:23<02:54,  3.30it/s] 26%|██▌       | 203/780 [01:23<02:53,  3.32it/s] 26%|██▌       | 204/780 [01:24<02:52,  3.34it/s] 26%|██▋       | 205/780 [01:24<02:52,  3.34it/s] 26%|██▋       | 206/780 [01:24<02:51,  3.35it/s] 27%|██▋       | 207/780 [01:25<02:50,  3.35it/s] 27%|██▋       | 208/780 [01:25<02:50,  3.36it/s] 27%|██▋       | 209/780 [01:25<02:50,  3.36it/s] 27%|██▋       | 210/780 [01:26<02:49,  3.36it/s] 27%|██▋       | 211/780 [01:26<02:49,  3.36it/s] 27%|██▋       | 212/780 [01:26<02:49,  3.36it/s] 27%|██▋       | 213/780 [01:26<02:49,  3.35it/s] 27%|██▋       | 214/780 [01:27<02:48,  3.35it/s] 28%|██▊       | 215/780 [01:27<02:48,  3.36it/s] 28%|██▊       | 216/780 [01:27<02:48,  3.36it/s] 28%|██▊       | 217/780 [01:28<02:47,  3.36it/s] 28%|██▊       | 218/780 [01:28<02:47,  3.36it/s] 28%|██▊       | 219/780 [01:28<02:46,  3.36it/s] 28%|██▊       | 220/780 [01:28<02:46,  3.36it/s] 28%|██▊       | 221/780 [01:29<02:46,  3.36it/s] 28%|██▊       | 222/780 [01:29<02:46,  3.36it/s] 29%|██▊       | 223/780 [01:29<02:45,  3.36it/s] 29%|██▊       | 224/780 [01:30<02:46,  3.34it/s] 29%|██▉       | 225/780 [01:30<02:45,  3.35it/s] 29%|██▉       | 226/780 [01:30<02:45,  3.35it/s] 29%|██▉       | 227/780 [01:31<02:44,  3.35it/s] 29%|██▉       | 228/780 [01:31<02:44,  3.36it/s] 29%|██▉       | 229/780 [01:31<02:43,  3.36it/s] 29%|██▉       | 230/780 [01:31<02:43,  3.36it/s] 30%|██▉       | 231/780 [01:32<02:43,  3.36it/s] 30%|██▉       | 232/780 [01:32<02:42,  3.36it/s] 30%|██▉       | 233/780 [01:32<02:42,  3.36it/s] 30%|███       | 234/780 [01:33<02:42,  3.36it/s] 30%|███       | 235/780 [01:33<02:43,  3.33it/s] 30%|███       | 236/780 [01:33<02:42,  3.34it/s] 30%|███       | 237/780 [01:34<02:42,  3.35it/s] 31%|███       | 238/780 [01:34<02:41,  3.35it/s] 31%|███       | 239/780 [01:34<02:41,  3.36it/s] 31%|███       | 240/780 [01:34<02:40,  3.36it/s] 31%|███       | 241/780 [01:35<02:40,  3.36it/s] 31%|███       | 242/780 [01:35<02:40,  3.36it/s] 31%|███       | 243/780 [01:35<02:39,  3.36it/s] 31%|███▏      | 244/780 [01:36<02:39,  3.36it/s] 31%|███▏      | 245/780 [01:36<02:39,  3.36it/s] 32%|███▏      | 246/780 [01:36<02:41,  3.31it/s] 32%|███▏      | 247/780 [01:37<02:39,  3.34it/s] 32%|███▏      | 248/780 [01:37<02:38,  3.36it/s] 32%|███▏      | 249/780 [01:37<02:37,  3.37it/s] 32%|███▏      | 250/780 [01:37<02:36,  3.38it/s] 32%|███▏      | 251/780 [01:38<02:36,  3.39it/s] 32%|███▏      | 252/780 [01:38<02:35,  3.39it/s] 32%|███▏      | 253/780 [01:38<02:35,  3.40it/s] 33%|███▎      | 254/780 [01:39<02:34,  3.40it/s] 33%|███▎      | 255/780 [01:39<02:34,  3.40it/s] 33%|███▎      | 256/780 [01:39<02:34,  3.40it/s] 33%|███▎      | 257/780 [01:39<02:34,  3.39it/s] 33%|███▎      | 258/780 [01:40<02:33,  3.39it/s] 33%|███▎      | 259/780 [01:40<02:33,  3.40it/s] 33%|███▎      | 260/780 [01:40<02:32,  3.40it/s] 33%|███▎      | 261/780 [01:41<02:32,  3.40it/s] 34%|███▎      | 262/780 [01:41<02:32,  3.40it/s] 34%|███▎      | 263/780 [01:41<02:31,  3.40it/s] 34%|███▍      | 264/780 [01:42<02:31,  3.41it/s] 34%|███▍      | 265/780 [01:42<02:31,  3.40it/s] 34%|███▍      | 266/780 [01:42<02:31,  3.40it/s] 34%|███▍      | 267/780 [01:42<02:30,  3.40it/s] 34%|███▍      | 268/780 [01:43<02:33,  3.34it/s] 34%|███▍      | 269/780 [01:43<02:32,  3.36it/s] 35%|███▍      | 270/780 [01:43<02:31,  3.37it/s] 35%|███▍      | 271/780 [01:44<02:30,  3.38it/s] 35%|███▍      | 272/780 [01:44<02:30,  3.39it/s] 35%|███▌      | 273/780 [01:44<02:29,  3.39it/s] 35%|███▌      | 274/780 [01:45<02:29,  3.39it/s] 35%|███▌      | 275/780 [01:45<02:28,  3.40it/s] 35%|███▌      | 276/780 [01:45<02:28,  3.40it/s] 36%|███▌      | 277/780 [01:45<02:27,  3.41it/s] 36%|███▌      | 278/780 [01:46<02:27,  3.40it/s] 36%|███▌      | 279/780 [01:46<02:28,  3.38it/s] 36%|███▌      | 280/780 [01:46<02:27,  3.39it/s] 36%|███▌      | 281/780 [01:47<02:26,  3.39it/s] 36%|███▌      | 282/780 [01:47<02:26,  3.39it/s] 36%|███▋      | 283/780 [01:47<02:26,  3.40it/s] 36%|███▋      | 284/780 [01:47<02:25,  3.40it/s] 37%|███▋      | 285/780 [01:48<02:25,  3.40it/s] 37%|███▋      | 286/780 [01:48<02:25,  3.40it/s] 37%|███▋      | 287/780 [01:48<02:24,  3.41it/s] 37%|███▋      | 288/780 [01:49<02:24,  3.41it/s] 37%|███▋      | 289/780 [01:49<02:24,  3.40it/s] 37%|███▋      | 290/780 [01:49<02:24,  3.39it/s] 37%|███▋      | 291/780 [01:50<02:24,  3.39it/s] 37%|███▋      | 292/780 [01:50<02:23,  3.40it/s] 38%|███▊      | 293/780 [01:50<02:23,  3.40it/s] 38%|███▊      | 294/780 [01:50<02:23,  3.40it/s] 38%|███▊      | 295/780 [01:51<02:22,  3.40it/s] 38%|███▊      | 296/780 [01:51<02:22,  3.40it/s] 38%|███▊      | 297/780 [01:51<02:21,  3.40it/s] 38%|███▊      | 298/780 [01:52<02:21,  3.41it/s] 38%|███▊      | 299/780 [01:52<02:21,  3.41it/s] 38%|███▊      | 300/780 [01:52<02:20,  3.40it/s] 39%|███▊      | 301/780 [01:52<02:22,  3.37it/s] 39%|███▊      | 302/780 [01:53<02:21,  3.38it/s] 39%|███▉      | 303/780 [01:53<02:20,  3.39it/s] 39%|███▉      | 304/780 [01:53<02:20,  3.39it/s] 39%|███▉      | 305/780 [01:54<02:20,  3.39it/s] 39%|███▉      | 306/780 [01:54<02:29,  3.18it/s] 39%|███▉      | 307/780 [01:54<02:25,  3.25it/s] 39%|███▉      | 308/780 [01:55<02:23,  3.29it/s] 40%|███▉      | 309/780 [01:55<02:21,  3.32it/s] 40%|███▉      | 310/780 [01:55<02:20,  3.35it/s] 40%|███▉      | 311/780 [01:55<02:19,  3.36it/s] 40%|████      | 312/780 [01:56<02:18,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 20:13:09,598 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:13:09,598 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 20:13:09,598 >>   Batch size = 8
{'eval_loss': 0.9673958420753479, 'eval_runtime': 10.0002, 'eval_samples_per_second': 344.592, 'eval_steps_per_second': 43.099, 'epoch': 1.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 53.89it/s][A
  3%|▎         | 12/431 [00:00<00:08, 46.58it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.05it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.39it/s][A
  6%|▋         | 27/431 [00:00<00:09, 43.88it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.75it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.61it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.41it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.58it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.51it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.32it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.13it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.20it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.25it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.32it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.23it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.35it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.41it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.34it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.27it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.10it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.14it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.25it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.32it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.29it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.30it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.37it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.31it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.15it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.13it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.21it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.24it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.25it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.23it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.34it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.33it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.23it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.09it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.17it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.23it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.28it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.27it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.34it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.32it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.24it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.26it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.09it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.26it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.23it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.33it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.27it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.37it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.35it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.25it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.20it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.21it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.29it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.36it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.31it/s][A
 70%|███████   | 302/431 [00:06<00:03, 41.22it/s][A
 71%|███████   | 307/431 [00:07<00:02, 42.05it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 42.30it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 42.67it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 42.68it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 42.93it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.09it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.02it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 42.96it/s][A
 81%|████████  | 347/431 [00:08<00:01, 43.12it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.35it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.24it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.29it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.19it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.28it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.25it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.21it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.19it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.27it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.35it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.31it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.26it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.21it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.33it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.19it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.22it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.22it/s][A 40%|████      | 312/780 [02:06<02:18,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:13:19,650 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 20:13:19,677 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:13:23,524 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:13:23,647 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:13:23,662 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:20<57:15,  7.36s/it] 40%|████      | 314/780 [02:20<40:41,  5.24s/it] 40%|████      | 315/780 [02:20<29:06,  3.76s/it] 41%|████      | 316/780 [02:20<21:01,  2.72s/it] 41%|████      | 317/780 [02:21<15:22,  1.99s/it] 41%|████      | 318/780 [02:21<11:25,  1.48s/it] 41%|████      | 319/780 [02:21<08:40,  1.13s/it] 41%|████      | 320/780 [02:22<06:44,  1.14it/s] 41%|████      | 321/780 [02:22<05:23,  1.42it/s] 41%|████▏     | 322/780 [02:22<04:26,  1.72it/s] 41%|████▏     | 323/780 [02:23<03:46,  2.01it/s] 42%|████▏     | 324/780 [02:23<03:18,  2.29it/s] 42%|████▏     | 325/780 [02:23<03:00,  2.52it/s] 42%|████▏     | 326/780 [02:23<02:46,  2.72it/s] 42%|████▏     | 327/780 [02:24<02:36,  2.89it/s] 42%|████▏     | 328/780 [02:24<02:29,  3.02it/s] 42%|████▏     | 329/780 [02:24<02:24,  3.11it/s] 42%|████▏     | 330/780 [02:25<02:21,  3.19it/s] 42%|████▏     | 331/780 [02:25<02:18,  3.24it/s] 43%|████▎     | 332/780 [02:25<02:16,  3.27it/s] 43%|████▎     | 333/780 [02:26<02:15,  3.30it/s] 43%|████▎     | 334/780 [02:26<02:14,  3.32it/s] 43%|████▎     | 335/780 [02:26<02:13,  3.34it/s] 43%|████▎     | 336/780 [02:26<02:12,  3.36it/s] 43%|████▎     | 337/780 [02:27<02:11,  3.38it/s] 43%|████▎     | 338/780 [02:27<02:10,  3.38it/s] 43%|████▎     | 339/780 [02:27<02:10,  3.39it/s] 44%|████▎     | 340/780 [02:28<02:09,  3.39it/s] 44%|████▎     | 341/780 [02:28<02:09,  3.40it/s] 44%|████▍     | 342/780 [02:28<02:08,  3.40it/s] 44%|████▍     | 343/780 [02:28<02:08,  3.40it/s] 44%|████▍     | 344/780 [02:29<02:07,  3.41it/s] 44%|████▍     | 345/780 [02:29<02:07,  3.41it/s] 44%|████▍     | 346/780 [02:29<02:07,  3.41it/s] 44%|████▍     | 347/780 [02:30<02:06,  3.41it/s] 45%|████▍     | 348/780 [02:30<02:06,  3.41it/s] 45%|████▍     | 349/780 [02:30<02:06,  3.41it/s] 45%|████▍     | 350/780 [02:31<02:06,  3.41it/s] 45%|████▌     | 351/780 [02:31<02:06,  3.40it/s] 45%|████▌     | 352/780 [02:31<02:05,  3.40it/s] 45%|████▌     | 353/780 [02:31<02:05,  3.41it/s] 45%|████▌     | 354/780 [02:32<02:05,  3.41it/s] 46%|████▌     | 355/780 [02:32<02:04,  3.41it/s] 46%|████▌     | 356/780 [02:32<02:04,  3.41it/s] 46%|████▌     | 357/780 [02:33<02:04,  3.41it/s] 46%|████▌     | 358/780 [02:33<02:03,  3.41it/s] 46%|████▌     | 359/780 [02:33<02:03,  3.41it/s] 46%|████▌     | 360/780 [02:33<02:03,  3.41it/s] 46%|████▋     | 361/780 [02:34<02:02,  3.41it/s] 46%|████▋     | 362/780 [02:34<02:03,  3.39it/s] 47%|████▋     | 363/780 [02:34<02:02,  3.40it/s] 47%|████▋     | 364/780 [02:35<02:02,  3.40it/s] 47%|████▋     | 365/780 [02:35<02:01,  3.40it/s] 47%|████▋     | 366/780 [02:35<02:01,  3.41it/s] 47%|████▋     | 367/780 [02:36<02:01,  3.41it/s] 47%|████▋     | 368/780 [02:36<02:00,  3.41it/s] 47%|████▋     | 369/780 [02:36<02:00,  3.41it/s] 47%|████▋     | 370/780 [02:36<02:00,  3.41it/s] 48%|████▊     | 371/780 [02:37<01:59,  3.41it/s] 48%|████▊     | 372/780 [02:37<01:59,  3.41it/s] 48%|████▊     | 373/780 [02:37<01:59,  3.40it/s] 48%|████▊     | 374/780 [02:38<01:59,  3.40it/s] 48%|████▊     | 375/780 [02:38<01:59,  3.40it/s] 48%|████▊     | 376/780 [02:38<01:58,  3.40it/s] 48%|████▊     | 377/780 [02:38<01:58,  3.40it/s] 48%|████▊     | 378/780 [02:39<01:58,  3.40it/s] 49%|████▊     | 379/780 [02:39<01:57,  3.41it/s] 49%|████▊     | 380/780 [02:39<01:57,  3.41it/s] 49%|████▉     | 381/780 [02:40<01:57,  3.41it/s] 49%|████▉     | 382/780 [02:40<01:56,  3.41it/s] 49%|████▉     | 383/780 [02:40<01:56,  3.41it/s] 49%|████▉     | 384/780 [02:41<01:56,  3.40it/s] 49%|████▉     | 385/780 [02:41<01:56,  3.40it/s] 49%|████▉     | 386/780 [02:41<01:56,  3.39it/s] 50%|████▉     | 387/780 [02:41<01:55,  3.40it/s] 50%|████▉     | 388/780 [02:42<01:55,  3.40it/s] 50%|████▉     | 389/780 [02:42<01:55,  3.39it/s] 50%|█████     | 390/780 [02:42<01:55,  3.38it/s] 50%|█████     | 391/780 [02:43<01:55,  3.38it/s] 50%|█████     | 392/780 [02:43<01:55,  3.37it/s] 50%|█████     | 393/780 [02:43<01:54,  3.37it/s] 51%|█████     | 394/780 [02:43<01:54,  3.37it/s] 51%|█████     | 395/780 [02:44<01:56,  3.30it/s] 51%|█████     | 396/780 [02:44<01:55,  3.32it/s] 51%|█████     | 397/780 [02:44<01:54,  3.33it/s] 51%|█████     | 398/780 [02:45<01:54,  3.34it/s] 51%|█████     | 399/780 [02:45<01:53,  3.35it/s] 51%|█████▏    | 400/780 [02:45<01:53,  3.35it/s] 51%|█████▏    | 401/780 [02:46<01:53,  3.35it/s] 52%|█████▏    | 402/780 [02:46<01:52,  3.35it/s] 52%|█████▏    | 403/780 [02:46<01:52,  3.35it/s] 52%|█████▏    | 404/780 [02:46<01:52,  3.36it/s] 52%|█████▏    | 405/780 [02:47<01:51,  3.36it/s] 52%|█████▏    | 406/780 [02:47<01:51,  3.34it/s] 52%|█████▏    | 407/780 [02:47<01:51,  3.35it/s] 52%|█████▏    | 408/780 [02:48<01:50,  3.36it/s] 52%|█████▏    | 409/780 [02:48<01:50,  3.36it/s] 53%|█████▎    | 410/780 [02:48<01:50,  3.36it/s] 53%|█████▎    | 411/780 [02:49<01:49,  3.36it/s] 53%|█████▎    | 412/780 [02:49<01:49,  3.36it/s] 53%|█████▎    | 413/780 [02:49<01:52,  3.27it/s] 53%|█████▎    | 414/780 [02:49<01:50,  3.30it/s] 53%|█████▎    | 415/780 [02:50<01:49,  3.32it/s] 53%|█████▎    | 416/780 [02:50<01:49,  3.32it/s] 53%|█████▎    | 417/780 [02:50<01:49,  3.33it/s] 54%|█████▎    | 418/780 [02:51<01:48,  3.34it/s] 54%|█████▎    | 419/780 [02:51<01:47,  3.34it/s] 54%|█████▍    | 420/780 [02:51<01:47,  3.35it/s] 54%|█████▍    | 421/780 [02:52<01:47,  3.35it/s] 54%|█████▍    | 422/780 [02:52<01:46,  3.35it/s] 54%|█████▍    | 423/780 [02:52<01:46,  3.36it/s] 54%|█████▍    | 424/780 [02:52<01:46,  3.36it/s] 54%|█████▍    | 425/780 [02:53<01:45,  3.36it/s] 55%|█████▍    | 426/780 [02:53<01:45,  3.35it/s] 55%|█████▍    | 427/780 [02:53<01:46,  3.32it/s] 55%|█████▍    | 428/780 [02:54<01:45,  3.33it/s] 55%|█████▌    | 429/780 [02:54<01:45,  3.34it/s] 55%|█████▌    | 430/780 [02:54<01:44,  3.34it/s] 55%|█████▌    | 431/780 [02:55<01:44,  3.34it/s] 55%|█████▌    | 432/780 [02:55<01:43,  3.35it/s] 56%|█████▌    | 433/780 [02:55<01:43,  3.35it/s] 56%|█████▌    | 434/780 [02:55<01:43,  3.35it/s] 56%|█████▌    | 435/780 [02:56<01:42,  3.35it/s] 56%|█████▌    | 436/780 [02:56<01:42,  3.36it/s] 56%|█████▌    | 437/780 [02:56<01:42,  3.36it/s] 56%|█████▌    | 438/780 [02:57<01:41,  3.36it/s] 56%|█████▋    | 439/780 [02:57<01:41,  3.35it/s] 56%|█████▋    | 440/780 [02:57<01:41,  3.35it/s] 57%|█████▋    | 441/780 [02:58<01:41,  3.36it/s] 57%|█████▋    | 442/780 [02:58<01:40,  3.35it/s] 57%|█████▋    | 443/780 [02:58<01:40,  3.35it/s] 57%|█████▋    | 444/780 [02:58<01:42,  3.28it/s] 57%|█████▋    | 445/780 [02:59<01:41,  3.30it/s] 57%|█████▋    | 446/780 [02:59<01:40,  3.32it/s] 57%|█████▋    | 447/780 [02:59<01:39,  3.33it/s] 57%|█████▋    | 448/780 [03:00<01:39,  3.34it/s] 58%|█████▊    | 449/780 [03:00<01:38,  3.35it/s] 58%|█████▊    | 450/780 [03:00<01:38,  3.35it/s] 58%|█████▊    | 451/780 [03:01<01:38,  3.35it/s] 58%|█████▊    | 452/780 [03:01<01:37,  3.36it/s] 58%|█████▊    | 453/780 [03:01<01:37,  3.36it/s] 58%|█████▊    | 454/780 [03:01<01:38,  3.30it/s] 58%|█████▊    | 455/780 [03:02<01:37,  3.32it/s] 58%|█████▊    | 456/780 [03:02<01:37,  3.33it/s] 59%|█████▊    | 457/780 [03:02<01:36,  3.34it/s] 59%|█████▊    | 458/780 [03:03<01:36,  3.34it/s] 59%|█████▉    | 459/780 [03:03<01:35,  3.35it/s] 59%|█████▉    | 460/780 [03:03<01:35,  3.35it/s] 59%|█████▉    | 461/780 [03:04<01:35,  3.35it/s] 59%|█████▉    | 462/780 [03:04<01:34,  3.35it/s] 59%|█████▉    | 463/780 [03:04<01:34,  3.36it/s] 59%|█████▉    | 464/780 [03:04<01:34,  3.36it/s] 60%|█████▉    | 465/780 [03:05<01:35,  3.31it/s] 60%|█████▉    | 466/780 [03:05<01:34,  3.32it/s] 60%|█████▉    | 467/780 [03:05<01:33,  3.33it/s] 60%|██████    | 468/780 [03:06<01:33,  3.34it/s][INFO|trainer.py:2140] 2023-08-28 20:14:19,478 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:14:19,478 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 20:14:19,478 >>   Batch size = 8
{'eval_loss': 0.9747625589370728, 'eval_runtime': 10.012, 'eval_samples_per_second': 344.188, 'eval_steps_per_second': 43.048, 'epoch': 2.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 54.59it/s][A
  3%|▎         | 12/431 [00:00<00:08, 46.80it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.19it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.30it/s][A
  6%|▋         | 27/431 [00:00<00:09, 43.83it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.61it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.46it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.26it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.37it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.53it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.46it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.29it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.21it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.16it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.08it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.15it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.16it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.35it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.40it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.33it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.21it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.20it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.23it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.06it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.10it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.25it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.43it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.42it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.34it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.27it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.30it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.06it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.03it/s][A
 40%|███▉      | 172/431 [00:03<00:06, 43.14it/s][A
 41%|████      | 177/431 [00:04<00:06, 41.62it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 42.45it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 42.74it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 42.93it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.09it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.03it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.04it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 42.85it/s][A
 50%|█████     | 217/431 [00:05<00:04, 42.95it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.08it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.31it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.44it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.40it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.35it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.16it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.05it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.13it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.01it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.19it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.35it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.46it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.47it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.25it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.18it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.16it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.09it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.02it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.13it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.38it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.47it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.46it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.15it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.21it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.15it/s][A
 81%|████████  | 347/431 [00:08<00:01, 43.12it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.05it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.26it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.28it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.47it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.34it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.30it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.23it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.18it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.01it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.02it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.26it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.43it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.40it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.26it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.22it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.24it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.24it/s][A 60%|██████    | 468/780 [03:16<01:33,  3.34it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:14:29,502 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 20:14:29,521 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:14:33,648 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:14:33,742 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:14:33,876 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:31<40:10,  7.75s/it] 60%|██████    | 470/780 [03:31<28:30,  5.52s/it] 60%|██████    | 471/780 [03:31<20:20,  3.95s/it] 61%|██████    | 472/780 [03:32<14:39,  2.85s/it] 61%|██████    | 473/780 [03:32<10:40,  2.09s/it] 61%|██████    | 474/780 [03:32<07:54,  1.55s/it] 61%|██████    | 475/780 [03:33<05:58,  1.17s/it] 61%|██████    | 476/780 [03:33<04:36,  1.10it/s] 61%|██████    | 477/780 [03:33<03:40,  1.38it/s] 61%|██████▏   | 478/780 [03:33<03:00,  1.67it/s] 61%|██████▏   | 479/780 [03:34<02:32,  1.97it/s] 62%|██████▏   | 480/780 [03:34<02:13,  2.25it/s] 62%|██████▏   | 481/780 [03:34<02:00,  2.49it/s] 62%|██████▏   | 482/780 [03:35<01:50,  2.70it/s] 62%|██████▏   | 483/780 [03:35<01:43,  2.87it/s] 62%|██████▏   | 484/780 [03:35<01:38,  3.00it/s] 62%|██████▏   | 485/780 [03:36<01:35,  3.10it/s] 62%|██████▏   | 486/780 [03:36<01:32,  3.18it/s] 62%|██████▏   | 487/780 [03:36<01:30,  3.24it/s] 63%|██████▎   | 488/780 [03:36<01:28,  3.29it/s] 63%|██████▎   | 489/780 [03:37<01:27,  3.33it/s] 63%|██████▎   | 490/780 [03:37<01:26,  3.35it/s] 63%|██████▎   | 491/780 [03:37<01:25,  3.37it/s] 63%|██████▎   | 492/780 [03:38<01:26,  3.35it/s] 63%|██████▎   | 493/780 [03:38<01:25,  3.36it/s] 63%|██████▎   | 494/780 [03:38<01:24,  3.38it/s] 63%|██████▎   | 495/780 [03:38<01:24,  3.39it/s] 64%|██████▎   | 496/780 [03:39<01:23,  3.39it/s] 64%|██████▎   | 497/780 [03:39<01:23,  3.40it/s] 64%|██████▍   | 498/780 [03:39<01:22,  3.40it/s] 64%|██████▍   | 499/780 [03:40<01:22,  3.41it/s] 64%|██████▍   | 500/780 [03:40<01:22,  3.41it/s]                                                  64%|██████▍   | 500/780 [03:40<01:22,  3.41it/s] 64%|██████▍   | 501/780 [03:40<01:21,  3.41it/s] 64%|██████▍   | 502/780 [03:41<01:21,  3.41it/s] 64%|██████▍   | 503/780 [03:41<01:24,  3.30it/s] 65%|██████▍   | 504/780 [03:41<01:22,  3.33it/s] 65%|██████▍   | 505/780 [03:41<01:21,  3.36it/s] 65%|██████▍   | 506/780 [03:42<01:21,  3.37it/s] 65%|██████▌   | 507/780 [03:42<01:20,  3.38it/s] 65%|██████▌   | 508/780 [03:42<01:20,  3.39it/s] 65%|██████▌   | 509/780 [03:43<01:19,  3.40it/s] 65%|██████▌   | 510/780 [03:43<01:19,  3.40it/s] 66%|██████▌   | 511/780 [03:43<01:19,  3.40it/s] 66%|██████▌   | 512/780 [03:44<01:18,  3.41it/s] 66%|██████▌   | 513/780 [03:44<01:18,  3.41it/s] 66%|██████▌   | 514/780 [03:44<01:18,  3.39it/s] 66%|██████▌   | 515/780 [03:44<01:18,  3.40it/s] 66%|██████▌   | 516/780 [03:45<01:17,  3.40it/s] 66%|██████▋   | 517/780 [03:45<01:17,  3.41it/s] 66%|██████▋   | 518/780 [03:45<01:16,  3.41it/s] 67%|██████▋   | 519/780 [03:46<01:16,  3.41it/s] 67%|██████▋   | 520/780 [03:46<01:16,  3.41it/s] 67%|██████▋   | 521/780 [03:46<01:15,  3.41it/s] 67%|██████▋   | 522/780 [03:46<01:15,  3.41it/s] 67%|██████▋   | 523/780 [03:47<01:15,  3.41it/s] 67%|██████▋   | 524/780 [03:47<01:15,  3.41it/s] 67%|██████▋   | 525/780 [03:47<01:16,  3.35it/s] 67%|██████▋   | 526/780 [03:48<01:15,  3.37it/s] 68%|██████▊   | 527/780 [03:48<01:14,  3.38it/s] 68%|██████▊   | 528/780 [03:48<01:14,  3.39it/s] 68%|██████▊   | 529/780 [03:49<01:13,  3.40it/s] 68%|██████▊   | 530/780 [03:49<01:13,  3.40it/s] 68%|██████▊   | 531/780 [03:49<01:13,  3.40it/s] 68%|██████▊   | 532/780 [03:49<01:13,  3.37it/s] 68%|██████▊   | 533/780 [03:50<01:12,  3.38it/s] 68%|██████▊   | 534/780 [03:50<01:12,  3.39it/s] 69%|██████▊   | 535/780 [03:50<01:12,  3.40it/s] 69%|██████▊   | 536/780 [03:51<01:12,  3.39it/s] 69%|██████▉   | 537/780 [03:51<01:11,  3.39it/s] 69%|██████▉   | 538/780 [03:51<01:11,  3.39it/s] 69%|██████▉   | 539/780 [03:51<01:10,  3.40it/s] 69%|██████▉   | 540/780 [03:52<01:10,  3.40it/s] 69%|██████▉   | 541/780 [03:52<01:10,  3.40it/s] 69%|██████▉   | 542/780 [03:52<01:09,  3.41it/s] 70%|██████▉   | 543/780 [03:53<01:09,  3.41it/s] 70%|██████▉   | 544/780 [03:53<01:09,  3.41it/s] 70%|██████▉   | 545/780 [03:53<01:09,  3.40it/s] 70%|███████   | 546/780 [03:54<01:08,  3.41it/s] 70%|███████   | 547/780 [03:54<01:08,  3.38it/s] 70%|███████   | 548/780 [03:54<01:08,  3.39it/s] 70%|███████   | 549/780 [03:54<01:08,  3.39it/s] 71%|███████   | 550/780 [03:55<01:07,  3.40it/s] 71%|███████   | 551/780 [03:55<01:07,  3.40it/s] 71%|███████   | 552/780 [03:55<01:07,  3.40it/s] 71%|███████   | 553/780 [03:56<01:06,  3.40it/s] 71%|███████   | 554/780 [03:56<01:06,  3.40it/s] 71%|███████   | 555/780 [03:56<01:06,  3.40it/s] 71%|███████▏  | 556/780 [03:56<01:05,  3.41it/s] 71%|███████▏  | 557/780 [03:57<01:05,  3.41it/s] 72%|███████▏  | 558/780 [03:57<01:05,  3.41it/s] 72%|███████▏  | 559/780 [03:57<01:04,  3.41it/s] 72%|███████▏  | 560/780 [03:58<01:04,  3.41it/s] 72%|███████▏  | 561/780 [03:58<01:04,  3.40it/s] 72%|███████▏  | 562/780 [03:58<01:04,  3.40it/s] 72%|███████▏  | 563/780 [03:59<01:03,  3.40it/s] 72%|███████▏  | 564/780 [03:59<01:03,  3.40it/s] 72%|███████▏  | 565/780 [03:59<01:03,  3.40it/s] 73%|███████▎  | 566/780 [03:59<01:02,  3.41it/s] 73%|███████▎  | 567/780 [04:00<01:02,  3.40it/s] 73%|███████▎  | 568/780 [04:00<01:02,  3.39it/s] 73%|███████▎  | 569/780 [04:00<01:02,  3.39it/s] 73%|███████▎  | 570/780 [04:01<01:01,  3.40it/s] 73%|███████▎  | 571/780 [04:01<01:01,  3.40it/s] 73%|███████▎  | 572/780 [04:01<01:01,  3.40it/s] 73%|███████▎  | 573/780 [04:01<01:00,  3.39it/s] 74%|███████▎  | 574/780 [04:02<01:00,  3.40it/s] 74%|███████▎  | 575/780 [04:02<01:00,  3.40it/s] 74%|███████▍  | 576/780 [04:02<00:59,  3.40it/s] 74%|███████▍  | 577/780 [04:03<00:59,  3.40it/s] 74%|███████▍  | 578/780 [04:03<00:59,  3.40it/s] 74%|███████▍  | 579/780 [04:03<01:00,  3.30it/s] 74%|███████▍  | 580/780 [04:04<00:59,  3.33it/s] 74%|███████▍  | 581/780 [04:04<00:59,  3.36it/s] 75%|███████▍  | 582/780 [04:04<00:58,  3.37it/s] 75%|███████▍  | 583/780 [04:04<00:58,  3.38it/s] 75%|███████▍  | 584/780 [04:05<00:57,  3.39it/s] 75%|███████▌  | 585/780 [04:05<00:57,  3.39it/s] 75%|███████▌  | 586/780 [04:05<00:57,  3.40it/s] 75%|███████▌  | 587/780 [04:06<00:56,  3.40it/s] 75%|███████▌  | 588/780 [04:06<00:56,  3.40it/s] 76%|███████▌  | 589/780 [04:06<00:56,  3.40it/s] 76%|███████▌  | 590/780 [04:06<00:56,  3.39it/s] 76%|███████▌  | 591/780 [04:07<00:55,  3.39it/s] 76%|███████▌  | 592/780 [04:07<00:55,  3.40it/s] 76%|███████▌  | 593/780 [04:07<00:55,  3.40it/s] 76%|███████▌  | 594/780 [04:08<00:54,  3.40it/s] 76%|███████▋  | 595/780 [04:08<00:54,  3.40it/s] 76%|███████▋  | 596/780 [04:08<00:54,  3.40it/s] 77%|███████▋  | 597/780 [04:09<00:53,  3.40it/s] 77%|███████▋  | 598/780 [04:09<00:53,  3.41it/s] 77%|███████▋  | 599/780 [04:09<00:53,  3.41it/s] 77%|███████▋  | 600/780 [04:09<00:52,  3.41it/s] 77%|███████▋  | 601/780 [04:10<00:52,  3.38it/s] 77%|███████▋  | 602/780 [04:10<00:57,  3.10it/s] 77%|███████▋  | 603/780 [04:10<00:55,  3.19it/s] 77%|███████▋  | 604/780 [04:11<00:54,  3.25it/s] 78%|███████▊  | 605/780 [04:11<00:53,  3.30it/s] 78%|███████▊  | 606/780 [04:11<00:52,  3.32it/s] 78%|███████▊  | 607/780 [04:12<00:51,  3.35it/s] 78%|███████▊  | 608/780 [04:12<00:51,  3.35it/s] 78%|███████▊  | 609/780 [04:12<00:50,  3.35it/s] 78%|███████▊  | 610/780 [04:12<00:50,  3.35it/s] 78%|███████▊  | 611/780 [04:13<00:52,  3.23it/s] 78%|███████▊  | 612/780 [04:13<00:51,  3.27it/s] 79%|███████▊  | 613/780 [04:13<00:50,  3.29it/s] 79%|███████▊  | 614/780 [04:14<00:50,  3.32it/s] 79%|███████▉  | 615/780 [04:14<00:49,  3.33it/s] 79%|███████▉  | 616/780 [04:14<00:49,  3.34it/s] 79%|███████▉  | 617/780 [04:15<00:48,  3.35it/s] 79%|███████▉  | 618/780 [04:15<00:48,  3.35it/s] 79%|███████▉  | 619/780 [04:15<00:47,  3.37it/s] 79%|███████▉  | 620/780 [04:15<00:47,  3.38it/s] 80%|███████▉  | 621/780 [04:16<00:47,  3.36it/s] 80%|███████▉  | 622/780 [04:16<00:46,  3.37it/s] 80%|███████▉  | 623/780 [04:16<00:46,  3.38it/s] 80%|████████  | 624/780 [04:17<00:46,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 20:15:30,491 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:15:30,491 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 20:15:30,492 >>   Batch size = 8
{'eval_loss': 0.9971184730529785, 'eval_runtime': 10.0113, 'eval_samples_per_second': 344.21, 'eval_steps_per_second': 43.051, 'epoch': 3.0}
{'loss': 0.6176, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 54.17it/s][A
  3%|▎         | 12/431 [00:00<00:08, 46.95it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.06it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.21it/s][A
  6%|▋         | 27/431 [00:00<00:09, 43.78it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.43it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.46it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.25it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.36it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.53it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.51it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.39it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.21it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.17it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.19it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 41.61it/s][A
 20%|██        | 87/431 [00:02<00:08, 42.19it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 42.74it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 42.94it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.01it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.08it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 42.98it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.02it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 42.83it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.06it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.23it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.41it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.25it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.27it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.15it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.14it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.04it/s][A
 39%|███▊      | 167/431 [00:04<00:10, 24.57it/s][A
 40%|███▉      | 172/431 [00:04<00:09, 28.68it/s][A
 41%|████      | 177/431 [00:04<00:07, 32.01it/s][A
 42%|████▏     | 182/431 [00:04<00:07, 34.84it/s][A
 43%|████▎     | 187/431 [00:04<00:06, 37.10it/s][A
 45%|████▍     | 192/431 [00:04<00:06, 38.77it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 40.26it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 41.11it/s][A
 48%|████▊     | 207/431 [00:05<00:05, 41.47it/s][A
 49%|████▉     | 212/431 [00:05<00:05, 41.51it/s][A
 50%|█████     | 217/431 [00:05<00:05, 41.95it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 42.41it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 42.78it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.11it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.22it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.36it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.24it/s][A
 58%|█████▊    | 252/431 [00:06<00:04, 42.85it/s][A
 60%|█████▉    | 257/431 [00:06<00:04, 42.88it/s][A
 61%|██████    | 262/431 [00:06<00:03, 42.85it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.10it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.27it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.49it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.54it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.37it/s][A
 68%|██████▊   | 292/431 [00:07<00:03, 43.29it/s][A
 69%|██████▉   | 297/431 [00:07<00:03, 43.20it/s][A
 70%|███████   | 302/431 [00:07<00:03, 42.97it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.03it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.17it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.34it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.39it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.52it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.34it/s][A
 78%|███████▊  | 337/431 [00:08<00:02, 43.19it/s][A
 79%|███████▉  | 342/431 [00:08<00:02, 43.08it/s][A
 81%|████████  | 347/431 [00:08<00:01, 42.95it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.09it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.19it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.28it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.51it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.38it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.41it/s][A
 89%|████████▊ | 382/431 [00:09<00:01, 43.16it/s][A
 90%|████████▉ | 387/431 [00:09<00:01, 43.08it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.14it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.13it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.17it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.35it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.52it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.47it/s][A
 98%|█████████▊| 422/431 [00:10<00:00, 43.30it/s][A
 99%|█████████▉| 427/431 [00:10<00:00, 43.13it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:10<00:00, 43.13it/s][A 80%|████████  | 624/780 [04:27<00:46,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:15:41,024 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-28 20:15:41,085 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:15:45,647 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:15:45,681 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:15:45,694 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:42<20:00,  7.74s/it] 80%|████████  | 626/780 [04:42<14:08,  5.51s/it] 80%|████████  | 627/780 [04:42<10:04,  3.95s/it] 81%|████████  | 628/780 [04:43<07:13,  2.85s/it] 81%|████████  | 629/780 [04:43<05:14,  2.09s/it] 81%|████████  | 630/780 [04:43<03:52,  1.55s/it] 81%|████████  | 631/780 [04:44<02:54,  1.17s/it] 81%|████████  | 632/780 [04:44<02:14,  1.10it/s] 81%|████████  | 633/780 [04:44<01:46,  1.38it/s] 81%|████████▏ | 634/780 [04:44<01:27,  1.67it/s] 81%|████████▏ | 635/780 [04:45<01:13,  1.97it/s] 82%|████████▏ | 636/780 [04:45<01:03,  2.25it/s] 82%|████████▏ | 637/780 [04:45<00:57,  2.47it/s] 82%|████████▏ | 638/780 [04:46<00:52,  2.68it/s] 82%|████████▏ | 639/780 [04:46<00:49,  2.86it/s] 82%|████████▏ | 640/780 [04:46<00:46,  3.00it/s] 82%|████████▏ | 641/780 [04:47<00:44,  3.10it/s] 82%|████████▏ | 642/780 [04:47<00:43,  3.17it/s] 82%|████████▏ | 643/780 [04:47<00:42,  3.23it/s] 83%|████████▎ | 644/780 [04:47<00:41,  3.27it/s] 83%|████████▎ | 645/780 [04:48<00:40,  3.30it/s] 83%|████████▎ | 646/780 [04:48<00:40,  3.32it/s] 83%|████████▎ | 647/780 [04:48<00:39,  3.34it/s] 83%|████████▎ | 648/780 [04:49<00:39,  3.31it/s] 83%|████████▎ | 649/780 [04:49<00:39,  3.33it/s] 83%|████████▎ | 650/780 [04:49<00:38,  3.34it/s] 83%|████████▎ | 651/780 [04:50<00:38,  3.34it/s] 84%|████████▎ | 652/780 [04:50<00:38,  3.35it/s] 84%|████████▎ | 653/780 [04:50<00:37,  3.35it/s] 84%|████████▍ | 654/780 [04:50<00:37,  3.36it/s] 84%|████████▍ | 655/780 [04:51<00:37,  3.36it/s] 84%|████████▍ | 656/780 [04:51<00:36,  3.36it/s] 84%|████████▍ | 657/780 [04:51<00:36,  3.37it/s] 84%|████████▍ | 658/780 [04:52<00:36,  3.37it/s] 84%|████████▍ | 659/780 [04:52<00:36,  3.35it/s] 85%|████████▍ | 660/780 [04:52<00:35,  3.35it/s] 85%|████████▍ | 661/780 [04:53<00:35,  3.36it/s] 85%|████████▍ | 662/780 [04:53<00:35,  3.36it/s] 85%|████████▌ | 663/780 [04:53<00:34,  3.37it/s] 85%|████████▌ | 664/780 [04:53<00:34,  3.38it/s] 85%|████████▌ | 665/780 [04:54<00:33,  3.39it/s] 85%|████████▌ | 666/780 [04:54<00:33,  3.40it/s] 86%|████████▌ | 667/780 [04:54<00:33,  3.40it/s] 86%|████████▌ | 668/780 [04:55<00:32,  3.40it/s] 86%|████████▌ | 669/780 [04:55<00:32,  3.41it/s] 86%|████████▌ | 670/780 [04:55<00:32,  3.37it/s] 86%|████████▌ | 671/780 [04:55<00:32,  3.38it/s] 86%|████████▌ | 672/780 [04:56<00:31,  3.39it/s] 86%|████████▋ | 673/780 [04:56<00:31,  3.40it/s] 86%|████████▋ | 674/780 [04:56<00:31,  3.40it/s] 87%|████████▋ | 675/780 [04:57<00:30,  3.41it/s] 87%|████████▋ | 676/780 [04:57<00:30,  3.41it/s] 87%|████████▋ | 677/780 [04:57<00:30,  3.41it/s] 87%|████████▋ | 678/780 [04:58<00:29,  3.41it/s] 87%|████████▋ | 679/780 [04:58<00:29,  3.41it/s] 87%|████████▋ | 680/780 [04:58<00:29,  3.41it/s] 87%|████████▋ | 681/780 [04:58<00:29,  3.41it/s] 87%|████████▋ | 682/780 [04:59<00:28,  3.41it/s] 88%|████████▊ | 683/780 [04:59<00:28,  3.41it/s] 88%|████████▊ | 684/780 [04:59<00:28,  3.41it/s] 88%|████████▊ | 685/780 [05:00<00:27,  3.41it/s] 88%|████████▊ | 686/780 [05:00<00:27,  3.41it/s] 88%|████████▊ | 687/780 [05:00<00:27,  3.41it/s] 88%|████████▊ | 688/780 [05:00<00:26,  3.41it/s] 88%|████████▊ | 689/780 [05:01<00:26,  3.41it/s] 88%|████████▊ | 690/780 [05:01<00:26,  3.41it/s] 89%|████████▊ | 691/780 [05:01<00:26,  3.39it/s] 89%|████████▊ | 692/780 [05:02<00:25,  3.40it/s] 89%|████████▉ | 693/780 [05:02<00:25,  3.40it/s] 89%|████████▉ | 694/780 [05:02<00:25,  3.40it/s] 89%|████████▉ | 695/780 [05:02<00:24,  3.40it/s] 89%|████████▉ | 696/780 [05:03<00:24,  3.41it/s] 89%|████████▉ | 697/780 [05:03<00:24,  3.41it/s] 89%|████████▉ | 698/780 [05:03<00:24,  3.41it/s] 90%|████████▉ | 699/780 [05:04<00:23,  3.41it/s] 90%|████████▉ | 700/780 [05:04<00:23,  3.41it/s] 90%|████████▉ | 701/780 [05:04<00:23,  3.41it/s] 90%|█████████ | 702/780 [05:05<00:23,  3.35it/s] 90%|█████████ | 703/780 [05:05<00:22,  3.36it/s] 90%|█████████ | 704/780 [05:05<00:22,  3.38it/s] 90%|█████████ | 705/780 [05:05<00:22,  3.39it/s] 91%|█████████ | 706/780 [05:06<00:21,  3.39it/s] 91%|█████████ | 707/780 [05:06<00:21,  3.40it/s] 91%|█████████ | 708/780 [05:06<00:21,  3.40it/s] 91%|█████████ | 709/780 [05:07<00:20,  3.40it/s] 91%|█████████ | 710/780 [05:07<00:20,  3.40it/s] 91%|█████████ | 711/780 [05:07<00:20,  3.41it/s] 91%|█████████▏| 712/780 [05:07<00:19,  3.41it/s] 91%|█████████▏| 713/780 [05:08<00:20,  3.32it/s] 92%|█████████▏| 714/780 [05:08<00:19,  3.35it/s] 92%|█████████▏| 715/780 [05:08<00:19,  3.37it/s] 92%|█████████▏| 716/780 [05:09<00:18,  3.38it/s] 92%|█████████▏| 717/780 [05:09<00:18,  3.39it/s] 92%|█████████▏| 718/780 [05:09<00:18,  3.40it/s] 92%|█████████▏| 719/780 [05:10<00:17,  3.40it/s] 92%|█████████▏| 720/780 [05:10<00:17,  3.40it/s] 92%|█████████▏| 721/780 [05:10<00:17,  3.29it/s] 93%|█████████▎| 722/780 [05:10<00:17,  3.33it/s] 93%|█████████▎| 723/780 [05:11<00:17,  3.35it/s] 93%|█████████▎| 724/780 [05:11<00:16,  3.33it/s] 93%|█████████▎| 725/780 [05:11<00:16,  3.36it/s] 93%|█████████▎| 726/780 [05:12<00:16,  3.37it/s] 93%|█████████▎| 727/780 [05:12<00:15,  3.38it/s] 93%|█████████▎| 728/780 [05:12<00:15,  3.39it/s] 93%|█████████▎| 729/780 [05:13<00:15,  3.39it/s] 94%|█████████▎| 730/780 [05:13<00:14,  3.40it/s] 94%|█████████▎| 731/780 [05:13<00:14,  3.40it/s] 94%|█████████▍| 732/780 [05:13<00:14,  3.40it/s] 94%|█████████▍| 733/780 [05:14<00:13,  3.41it/s] 94%|█████████▍| 734/780 [05:14<00:13,  3.41it/s] 94%|█████████▍| 735/780 [05:14<00:13,  3.39it/s] 94%|█████████▍| 736/780 [05:15<00:12,  3.39it/s] 94%|█████████▍| 737/780 [05:15<00:12,  3.40it/s] 95%|█████████▍| 738/780 [05:15<00:12,  3.40it/s] 95%|█████████▍| 739/780 [05:15<00:12,  3.40it/s] 95%|█████████▍| 740/780 [05:16<00:11,  3.40it/s] 95%|█████████▌| 741/780 [05:16<00:11,  3.40it/s] 95%|█████████▌| 742/780 [05:16<00:11,  3.40it/s] 95%|█████████▌| 743/780 [05:17<00:10,  3.41it/s] 95%|█████████▌| 744/780 [05:17<00:10,  3.41it/s] 96%|█████████▌| 745/780 [05:17<00:10,  3.40it/s] 96%|█████████▌| 746/780 [05:18<00:10,  3.38it/s] 96%|█████████▌| 747/780 [05:18<00:09,  3.39it/s] 96%|█████████▌| 748/780 [05:18<00:09,  3.39it/s] 96%|█████████▌| 749/780 [05:18<00:09,  3.39it/s] 96%|█████████▌| 750/780 [05:19<00:08,  3.40it/s] 96%|█████████▋| 751/780 [05:19<00:08,  3.40it/s] 96%|█████████▋| 752/780 [05:19<00:08,  3.40it/s] 97%|█████████▋| 753/780 [05:20<00:07,  3.41it/s] 97%|█████████▋| 754/780 [05:20<00:07,  3.41it/s] 97%|█████████▋| 755/780 [05:20<00:07,  3.41it/s] 97%|█████████▋| 756/780 [05:20<00:07,  3.41it/s] 97%|█████████▋| 757/780 [05:21<00:07,  3.17it/s] 97%|█████████▋| 758/780 [05:21<00:06,  3.23it/s] 97%|█████████▋| 759/780 [05:21<00:06,  3.28it/s] 97%|█████████▋| 760/780 [05:22<00:06,  3.32it/s] 98%|█████████▊| 761/780 [05:22<00:05,  3.34it/s] 98%|█████████▊| 762/780 [05:22<00:05,  3.36it/s] 98%|█████████▊| 763/780 [05:23<00:05,  3.38it/s] 98%|█████████▊| 764/780 [05:23<00:04,  3.38it/s] 98%|█████████▊| 765/780 [05:23<00:04,  3.39it/s] 98%|█████████▊| 766/780 [05:24<00:04,  3.40it/s] 98%|█████████▊| 767/780 [05:24<00:03,  3.38it/s] 98%|█████████▊| 768/780 [05:24<00:03,  3.39it/s] 99%|█████████▊| 769/780 [05:24<00:03,  3.39it/s] 99%|█████████▊| 770/780 [05:25<00:02,  3.40it/s] 99%|█████████▉| 771/780 [05:25<00:02,  3.40it/s] 99%|█████████▉| 772/780 [05:25<00:02,  3.40it/s] 99%|█████████▉| 773/780 [05:26<00:02,  3.40it/s] 99%|█████████▉| 774/780 [05:26<00:01,  3.41it/s] 99%|█████████▉| 775/780 [05:26<00:01,  3.41it/s] 99%|█████████▉| 776/780 [05:26<00:01,  3.41it/s]100%|█████████▉| 777/780 [05:27<00:00,  3.41it/s]100%|█████████▉| 778/780 [05:27<00:00,  3.39it/s]100%|█████████▉| 779/780 [05:27<00:00,  3.39it/s]100%|██████████| 780/780 [05:28<00:00,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 20:16:41,426 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:16:41,426 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 20:16:41,426 >>   Batch size = 8
{'eval_loss': 1.0080606937408447, 'eval_runtime': 10.2938, 'eval_samples_per_second': 334.764, 'eval_steps_per_second': 41.87, 'epoch': 4.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 54.38it/s][A
  3%|▎         | 12/431 [00:00<00:08, 46.88it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.21it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.39it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.02it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.69it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.62it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.44it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.54it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.49it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.42it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.39it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.30it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.28it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.29it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.33it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.27it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.34it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.30it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.30it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.14it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.25it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.25it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.24it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.28it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.30it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.31it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.27it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.21it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.21it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.25it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.18it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.24it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.27it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.32it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.25it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.31it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.30it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.30it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.26it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.23it/s][A
 49%|████▉     | 212/431 [00:04<00:06, 32.71it/s][A
 50%|█████     | 217/431 [00:05<00:06, 35.49it/s][A
 52%|█████▏    | 222/431 [00:05<00:05, 37.60it/s][A
 53%|█████▎    | 227/431 [00:05<00:05, 39.24it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 40.58it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 41.46it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 42.10it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 42.34it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 42.09it/s][A
 60%|█████▉    | 257/431 [00:06<00:04, 42.16it/s][A
 61%|██████    | 262/431 [00:06<00:03, 42.44it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 42.77it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.08it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.21it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.43it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.37it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.23it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 42.96it/s][A
 70%|███████   | 302/431 [00:07<00:03, 42.84it/s][A
 71%|███████   | 307/431 [00:07<00:02, 42.98it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.16it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.39it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.41it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.54it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.44it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.17it/s][A
 79%|███████▉  | 342/431 [00:08<00:02, 42.94it/s][A
 81%|████████  | 347/431 [00:08<00:01, 42.81it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 42.95it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.16it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.26it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.44it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.50it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.39it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.15it/s][A
 90%|████████▉ | 387/431 [00:09<00:01, 42.96it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 42.99it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 42.98it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.31it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.46it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.55it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.41it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.27it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.07it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:10<00:00, 43.07it/s][A100%|██████████| 780/780 [05:38<00:00,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:16:51,540 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-28 20:16:51,574 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:16:56,733 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:16:56,763 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:16:56,772 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 20:17:05,160 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 20:17:05,162 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156 (score: 0.9673958420753479).
                                                 100%|██████████| 780/780 [05:57<00:00,  3.40it/s]100%|██████████| 780/780 [05:57<00:00,  2.18it/s]
[INFO|trainer.py:1894] 2023-08-28 20:17:10,928 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 20:17:10,957 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:17:14,544 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:17:14,566 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:17:14,578 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:17:14,775 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:14,775 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:14,775 >>   train_loss               =     0.6068
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:14,775 >>   train_runtime            = 0:05:57.61
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:14,775 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:14,775 >>   train_samples_per_second =    139.817
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:14,775 >>   train_steps_per_second   =      2.181
{'eval_loss': 1.010120153427124, 'eval_runtime': 10.0845, 'eval_samples_per_second': 341.714, 'eval_steps_per_second': 42.739, 'epoch': 5.0}
{'train_runtime': 357.6114, 'train_samples_per_second': 139.817, 'train_steps_per_second': 2.181, 'train_loss': 0.6067933693910257, 'epoch': 5.0}
08/28/2023 20:17:14 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 20:17:14,815 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:17:14,815 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 20:17:14,815 >>   Batch size = 8
  0%|          | 0/431 [00:00<?, ?it/s]  1%|▏         | 6/431 [00:00<00:07, 54.71it/s]  3%|▎         | 12/431 [00:00<00:08, 47.81it/s]  4%|▍         | 17/431 [00:00<00:08, 46.23it/s]  5%|▌         | 22/431 [00:00<00:08, 45.47it/s]  6%|▋         | 27/431 [00:00<00:08, 44.91it/s]  7%|▋         | 32/431 [00:00<00:08, 44.50it/s]  9%|▊         | 37/431 [00:00<00:08, 44.19it/s] 10%|▉         | 42/431 [00:00<00:08, 43.69it/s] 11%|█         | 47/431 [00:01<00:08, 43.13it/s] 12%|█▏        | 52/431 [00:01<00:08, 43.02it/s] 13%|█▎        | 57/431 [00:01<00:08, 43.17it/s] 14%|█▍        | 62/431 [00:01<00:08, 43.46it/s] 16%|█▌        | 67/431 [00:01<00:08, 41.19it/s] 17%|█▋        | 72/431 [00:01<00:08, 42.01it/s] 18%|█▊        | 77/431 [00:01<00:08, 42.51it/s] 19%|█▉        | 82/431 [00:01<00:08, 42.87it/s] 20%|██        | 87/431 [00:01<00:08, 42.82it/s] 21%|██▏       | 92/431 [00:02<00:07, 42.82it/s] 23%|██▎       | 97/431 [00:02<00:07, 42.92it/s] 24%|██▎       | 102/431 [00:02<00:07, 43.00it/s] 25%|██▍       | 107/431 [00:02<00:07, 43.04it/s] 26%|██▌       | 112/431 [00:02<00:07, 43.19it/s] 27%|██▋       | 117/431 [00:02<00:07, 43.35it/s] 28%|██▊       | 122/431 [00:02<00:07, 43.54it/s] 29%|██▉       | 127/431 [00:02<00:06, 43.53it/s] 31%|███       | 132/431 [00:03<00:06, 43.29it/s] 32%|███▏      | 137/431 [00:03<00:06, 43.31it/s] 33%|███▎      | 142/431 [00:03<00:06, 43.24it/s] 34%|███▍      | 147/431 [00:03<00:06, 43.19it/s] 35%|███▌      | 152/431 [00:03<00:06, 43.15it/s] 36%|███▋      | 157/431 [00:03<00:06, 43.20it/s] 38%|███▊      | 162/431 [00:03<00:06, 43.48it/s] 39%|███▊      | 167/431 [00:03<00:06, 43.62it/s] 40%|███▉      | 172/431 [00:03<00:05, 43.47it/s] 41%|████      | 177/431 [00:04<00:05, 43.33it/s] 42%|████▏     | 182/431 [00:04<00:05, 43.32it/s] 43%|████▎     | 187/431 [00:04<00:05, 43.19it/s] 45%|████▍     | 192/431 [00:04<00:05, 43.16it/s] 46%|████▌     | 197/431 [00:04<00:05, 43.08it/s] 47%|████▋     | 202/431 [00:04<00:05, 43.30it/s] 48%|████▊     | 207/431 [00:04<00:05, 43.44it/s] 49%|████▉     | 212/431 [00:04<00:05, 43.45it/s] 50%|█████     | 217/431 [00:04<00:04, 43.45it/s] 52%|█████▏    | 222/431 [00:05<00:04, 43.39it/s] 53%|█████▎    | 227/431 [00:05<00:04, 43.19it/s] 54%|█████▍    | 232/431 [00:05<00:04, 43.21it/s] 55%|█████▍    | 237/431 [00:05<00:04, 43.19it/s] 56%|█████▌    | 242/431 [00:05<00:04, 43.16it/s] 57%|█████▋    | 247/431 [00:05<00:04, 43.38it/s] 58%|█████▊    | 252/431 [00:05<00:04, 43.43it/s] 60%|█████▉    | 257/431 [00:05<00:04, 43.45it/s] 61%|██████    | 262/431 [00:06<00:03, 43.47it/s] 62%|██████▏   | 267/431 [00:06<00:03, 43.37it/s] 63%|██████▎   | 272/431 [00:06<00:03, 43.18it/s] 64%|██████▍   | 277/431 [00:06<00:03, 43.27it/s] 65%|██████▌   | 282/431 [00:06<00:03, 43.27it/s] 67%|██████▋   | 287/431 [00:06<00:03, 43.29it/s] 68%|██████▊   | 292/431 [00:06<00:03, 43.21it/s] 69%|██████▉   | 297/431 [00:06<00:03, 43.33it/s] 70%|███████   | 302/431 [00:06<00:02, 43.39it/s] 71%|███████   | 307/431 [00:07<00:02, 43.34it/s] 72%|███████▏  | 312/431 [00:07<00:02, 43.28it/s] 74%|███████▎  | 317/431 [00:07<00:02, 43.18it/s] 75%|███████▍  | 322/431 [00:07<00:02, 43.25it/s] 76%|███████▌  | 327/431 [00:07<00:02, 43.26it/s] 77%|███████▋  | 332/431 [00:07<00:02, 43.35it/s] 78%|███████▊  | 337/431 [00:07<00:02, 43.33it/s] 79%|███████▉  | 342/431 [00:07<00:02, 43.40it/s] 81%|████████  | 347/431 [00:07<00:01, 43.32it/s] 82%|████████▏ | 352/431 [00:08<00:01, 43.38it/s] 83%|████████▎ | 357/431 [00:08<00:01, 43.38it/s] 84%|████████▍ | 362/431 [00:08<00:01, 43.28it/s] 85%|████████▌ | 367/431 [00:08<00:01, 43.28it/s] 86%|████████▋ | 372/431 [00:08<00:01, 43.38it/s] 87%|████████▋ | 377/431 [00:08<00:01, 43.47it/s] 89%|████████▊ | 382/431 [00:08<00:01, 43.58it/s] 90%|████████▉ | 387/431 [00:08<00:01, 43.50it/s] 91%|█████████ | 392/431 [00:09<00:00, 43.45it/s] 92%|█████████▏| 397/431 [00:09<00:00, 43.40it/s] 93%|█████████▎| 402/431 [00:09<00:00, 43.55it/s] 94%|█████████▍| 407/431 [00:09<00:00, 43.35it/s] 96%|█████████▌| 412/431 [00:09<00:00, 43.31it/s] 97%|█████████▋| 417/431 [00:09<00:00, 43.47it/s] 98%|█████████▊| 422/431 [00:09<00:00, 43.59it/s] 99%|█████████▉| 427/431 [00:09<00:00, 43.58it/s]100%|██████████| 431/431 [00:09<00:00, 43.37it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:17:24,772 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:24,772 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:24,772 >>   eval_loss               =     0.9674
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:24,772 >>   eval_runtime            = 0:00:09.95
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:24,772 >>   eval_samples            =       3446
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:24,772 >>   eval_samples_per_second =    346.105
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:24,772 >>   eval_steps_per_second   =     43.288
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:17:24,772 >>   perplexity              =     2.6311
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:17:30,981 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:17:30,992 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:17:30,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:17:30,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:17:30,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:17:31,379 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:17:31,380 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:17:31,874 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:17:32,933 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:17:32,933 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:17:35,995 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:17:36,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:17:36,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:17:36,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:17:36,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:17:36,404 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:17:36,405 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:17:36,704 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:17:36,873 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:17:36,873 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.48it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.49it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.50it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:17,  1.46it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:19,  1.47it/s]Extractor Predicting: 31it [00:20,  1.47it/s]Extractor Predicting: 32it [00:21,  1.49it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:22,  1.56it/s]Extractor Predicting: 35it [00:23,  1.59it/s]Extractor Predicting: 36it [00:23,  1.61it/s]Extractor Predicting: 37it [00:24,  1.61it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:25,  1.60it/s]Extractor Predicting: 40it [00:26,  1.59it/s]Extractor Predicting: 41it [00:26,  1.56it/s]Extractor Predicting: 42it [00:27,  1.60it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:29,  1.57it/s]Extractor Predicting: 46it [00:30,  1.59it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:31,  1.56it/s]Extractor Predicting: 49it [00:31,  1.58it/s]Extractor Predicting: 50it [00:32,  1.54it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:33,  1.53it/s]Extractor Predicting: 53it [00:34,  1.52it/s]Extractor Predicting: 54it [00:35,  1.56it/s]Extractor Predicting: 55it [00:35,  1.56it/s]Extractor Predicting: 56it [00:36,  1.59it/s]Extractor Predicting: 57it [00:37,  1.62it/s]Extractor Predicting: 58it [00:37,  1.60it/s]Extractor Predicting: 59it [00:38,  1.64it/s]Extractor Predicting: 60it [00:38,  1.60it/s]Extractor Predicting: 61it [00:39,  1.61it/s]Extractor Predicting: 62it [00:40,  1.67it/s]Extractor Predicting: 63it [00:40,  1.67it/s]Extractor Predicting: 64it [00:41,  1.71it/s]Extractor Predicting: 65it [00:41,  1.72it/s]Extractor Predicting: 66it [00:42,  1.73it/s]Extractor Predicting: 67it [00:42,  1.72it/s]Extractor Predicting: 68it [00:43,  1.71it/s]Extractor Predicting: 69it [00:44,  1.74it/s]Extractor Predicting: 70it [00:44,  1.72it/s]Extractor Predicting: 71it [00:45,  1.69it/s]Extractor Predicting: 72it [00:45,  1.72it/s]Extractor Predicting: 73it [00:46,  1.66it/s]Extractor Predicting: 74it [00:47,  1.66it/s]Extractor Predicting: 75it [00:47,  1.70it/s]Extractor Predicting: 76it [00:48,  1.70it/s]Extractor Predicting: 77it [00:48,  1.72it/s]Extractor Predicting: 78it [00:49,  1.73it/s]Extractor Predicting: 79it [00:49,  1.74it/s]Extractor Predicting: 80it [00:50,  1.71it/s]Extractor Predicting: 81it [00:51,  1.67it/s]Extractor Predicting: 82it [00:51,  1.64it/s]Extractor Predicting: 83it [00:52,  1.67it/s]Extractor Predicting: 84it [00:53,  1.66it/s]Extractor Predicting: 85it [00:53,  1.61it/s]Extractor Predicting: 86it [00:54,  1.60it/s]Extractor Predicting: 87it [00:54,  1.61it/s]Extractor Predicting: 88it [00:55,  1.58it/s]Extractor Predicting: 89it [00:56,  1.57it/s]Extractor Predicting: 90it [00:56,  1.56it/s]Extractor Predicting: 91it [00:57,  1.58it/s]Extractor Predicting: 92it [00:58,  1.56it/s]Extractor Predicting: 93it [00:58,  1.55it/s]Extractor Predicting: 94it [00:59,  1.57it/s]Extractor Predicting: 95it [01:00,  1.55it/s]Extractor Predicting: 96it [01:00,  1.57it/s]Extractor Predicting: 97it [01:01,  1.56it/s]Extractor Predicting: 98it [01:02,  1.56it/s]Extractor Predicting: 99it [01:02,  1.57it/s]Extractor Predicting: 100it [01:03,  1.58it/s]Extractor Predicting: 101it [01:03,  1.58it/s]Extractor Predicting: 102it [01:04,  1.59it/s]Extractor Predicting: 103it [01:05,  1.59it/s]Extractor Predicting: 104it [01:05,  1.62it/s]Extractor Predicting: 105it [01:06,  1.63it/s]Extractor Predicting: 106it [01:06,  1.61it/s]Extractor Predicting: 107it [01:07,  1.59it/s]Extractor Predicting: 108it [01:08,  1.59it/s]Extractor Predicting: 109it [01:08,  1.58it/s]Extractor Predicting: 110it [01:09,  1.57it/s]Extractor Predicting: 111it [01:10,  1.55it/s]Extractor Predicting: 112it [01:10,  1.57it/s]Extractor Predicting: 113it [01:11,  1.59it/s]Extractor Predicting: 114it [01:12,  1.59it/s]Extractor Predicting: 115it [01:12,  1.58it/s]Extractor Predicting: 116it [01:13,  1.61it/s]Extractor Predicting: 117it [01:13,  1.60it/s]Extractor Predicting: 118it [01:14,  1.59it/s]Extractor Predicting: 119it [01:15,  1.60it/s]Extractor Predicting: 120it [01:15,  1.59it/s]Extractor Predicting: 121it [01:16,  1.58it/s]Extractor Predicting: 122it [01:17,  1.56it/s]Extractor Predicting: 123it [01:17,  1.55it/s]Extractor Predicting: 124it [01:18,  1.52it/s]Extractor Predicting: 125it [01:19,  1.55it/s]Extractor Predicting: 126it [01:19,  1.55it/s]Extractor Predicting: 127it [01:20,  1.54it/s]Extractor Predicting: 128it [01:21,  1.52it/s]Extractor Predicting: 129it [01:21,  1.41it/s]Extractor Predicting: 130it [01:22,  1.47it/s]Extractor Predicting: 131it [01:23,  1.47it/s]Extractor Predicting: 132it [01:23,  1.50it/s]Extractor Predicting: 133it [01:24,  1.53it/s]Extractor Predicting: 134it [01:25,  1.53it/s]Extractor Predicting: 135it [01:25,  1.52it/s]Extractor Predicting: 136it [01:26,  1.54it/s]Extractor Predicting: 137it [01:27,  1.51it/s]Extractor Predicting: 138it [01:27,  1.55it/s]Extractor Predicting: 139it [01:28,  1.56it/s]Extractor Predicting: 140it [01:28,  1.58it/s]Extractor Predicting: 141it [01:29,  1.64it/s]Extractor Predicting: 141it [01:29,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:16,466 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:16,468 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:16,468 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:16,468 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:16,468 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:19:17,080 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:19:17,081 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:19:17,663 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:19:18,692 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:19:18,692 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:22,034 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:22,054 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:22,054 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:22,054 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:22,054 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:19:22,716 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:19:22,717 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:19:23,499 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:19:23,654 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:19:23,654 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2954014029618083,
  "recall": 0.10998258850841555,
  "score": 0.1602875872277437,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.68it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.62it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.58it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:12,  1.60it/s]Extractor Predicting: 22it [00:13,  1.63it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:14,  1.59it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:16,  1.59it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:17,  1.59it/s]Extractor Predicting: 30it [00:18,  1.54it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:19,  1.51it/s]Extractor Predicting: 33it [00:20,  1.53it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:21,  1.56it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:23,  1.60it/s]Extractor Predicting: 39it [00:24,  1.59it/s]Extractor Predicting: 40it [00:24,  1.61it/s]Extractor Predicting: 41it [00:25,  1.60it/s]Extractor Predicting: 42it [00:26,  1.59it/s]Extractor Predicting: 43it [00:26,  1.57it/s]Extractor Predicting: 44it [00:27,  1.57it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:28,  1.46it/s]Extractor Predicting: 47it [00:29,  1.49it/s]Extractor Predicting: 48it [00:30,  1.51it/s]Extractor Predicting: 49it [00:30,  1.52it/s]Extractor Predicting: 50it [00:31,  1.52it/s]Extractor Predicting: 51it [00:32,  1.53it/s]Extractor Predicting: 52it [00:32,  1.54it/s]Extractor Predicting: 53it [00:33,  1.53it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:34,  1.52it/s]Extractor Predicting: 56it [00:35,  1.54it/s]Extractor Predicting: 57it [00:36,  1.58it/s]Extractor Predicting: 58it [00:36,  1.59it/s]Extractor Predicting: 59it [00:37,  1.60it/s]Extractor Predicting: 60it [00:37,  1.61it/s]Extractor Predicting: 61it [00:38,  1.61it/s]Extractor Predicting: 62it [00:39,  1.60it/s]Extractor Predicting: 63it [00:39,  1.57it/s]Extractor Predicting: 64it [00:40,  1.59it/s]Extractor Predicting: 65it [00:41,  1.57it/s]Extractor Predicting: 66it [00:41,  1.55it/s]Extractor Predicting: 67it [00:42,  1.59it/s]Extractor Predicting: 68it [00:42,  1.61it/s]Extractor Predicting: 69it [00:43,  1.63it/s]Extractor Predicting: 70it [00:44,  1.62it/s]Extractor Predicting: 71it [00:44,  1.60it/s]Extractor Predicting: 72it [00:45,  1.61it/s]Extractor Predicting: 73it [00:46,  1.55it/s]Extractor Predicting: 74it [00:46,  1.56it/s]Extractor Predicting: 75it [00:47,  1.57it/s]Extractor Predicting: 76it [00:47,  1.59it/s]Extractor Predicting: 77it [00:48,  1.55it/s]Extractor Predicting: 78it [00:49,  1.55it/s]Extractor Predicting: 79it [00:49,  1.56it/s]Extractor Predicting: 80it [00:50,  1.56it/s]Extractor Predicting: 81it [00:51,  1.56it/s]Extractor Predicting: 82it [00:51,  1.56it/s]Extractor Predicting: 83it [00:52,  1.54it/s]Extractor Predicting: 84it [00:53,  1.56it/s]Extractor Predicting: 85it [00:53,  1.57it/s]Extractor Predicting: 86it [00:54,  1.55it/s]Extractor Predicting: 87it [00:55,  1.54it/s]Extractor Predicting: 88it [00:55,  1.55it/s]Extractor Predicting: 89it [00:56,  1.53it/s]Extractor Predicting: 90it [00:57,  1.53it/s]Extractor Predicting: 91it [00:57,  1.54it/s]Extractor Predicting: 92it [00:58,  1.55it/s]Extractor Predicting: 93it [00:59,  1.54it/s]Extractor Predicting: 94it [00:59,  1.51it/s]Extractor Predicting: 95it [01:00,  1.51it/s]Extractor Predicting: 96it [01:01,  1.52it/s]Extractor Predicting: 97it [01:01,  1.52it/s]Extractor Predicting: 98it [01:02,  1.54it/s]Extractor Predicting: 99it [01:02,  1.53it/s]Extractor Predicting: 100it [01:03,  1.53it/s]Extractor Predicting: 101it [01:04,  1.55it/s]Extractor Predicting: 102it [01:04,  1.55it/s]Extractor Predicting: 103it [01:05,  1.54it/s]Extractor Predicting: 104it [01:06,  1.53it/s]Extractor Predicting: 105it [01:06,  1.54it/s]Extractor Predicting: 106it [01:07,  1.53it/s]Extractor Predicting: 107it [01:08,  1.49it/s]Extractor Predicting: 108it [01:08,  1.51it/s]Extractor Predicting: 109it [01:09,  1.52it/s]Extractor Predicting: 110it [01:10,  1.50it/s]Extractor Predicting: 111it [01:10,  1.50it/s]Extractor Predicting: 112it [01:11,  1.52it/s]Extractor Predicting: 113it [01:12,  1.55it/s]Extractor Predicting: 114it [01:12,  1.58it/s]Extractor Predicting: 115it [01:13,  1.55it/s]Extractor Predicting: 116it [01:14,  1.56it/s]Extractor Predicting: 117it [01:14,  1.60it/s]Extractor Predicting: 118it [01:15,  1.58it/s]Extractor Predicting: 119it [01:15,  1.60it/s]Extractor Predicting: 120it [01:16,  1.60it/s]Extractor Predicting: 121it [01:17,  1.54it/s]Extractor Predicting: 122it [01:17,  1.60it/s]Extractor Predicting: 123it [01:18,  1.60it/s]Extractor Predicting: 124it [01:19,  1.60it/s]Extractor Predicting: 125it [01:19,  1.60it/s]Extractor Predicting: 126it [01:20,  1.58it/s]Extractor Predicting: 127it [01:20,  1.59it/s]Extractor Predicting: 128it [01:21,  1.63it/s]Extractor Predicting: 129it [01:22,  1.58it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:23,  1.58it/s]Extractor Predicting: 132it [01:24,  1.59it/s]Extractor Predicting: 133it [01:24,  1.59it/s]Extractor Predicting: 134it [01:25,  1.56it/s]Extractor Predicting: 135it [01:25,  1.59it/s]Extractor Predicting: 136it [01:26,  1.60it/s]Extractor Predicting: 137it [01:27,  1.60it/s]Extractor Predicting: 138it [01:27,  1.59it/s]Extractor Predicting: 139it [01:28,  1.60it/s]Extractor Predicting: 140it [01:29,  1.60it/s]Extractor Predicting: 141it [01:29,  1.58it/s]Extractor Predicting: 142it [01:30,  1.61it/s]Extractor Predicting: 143it [01:30,  1.61it/s]Extractor Predicting: 144it [01:31,  1.57it/s]Extractor Predicting: 145it [01:32,  1.60it/s]Extractor Predicting: 146it [01:32,  1.59it/s]Extractor Predicting: 147it [01:33,  1.57it/s]Extractor Predicting: 148it [01:34,  1.56it/s]Extractor Predicting: 149it [01:34,  1.56it/s]Extractor Predicting: 150it [01:35,  1.58it/s]Extractor Predicting: 151it [01:36,  1.61it/s]Extractor Predicting: 152it [01:36,  1.62it/s]Extractor Predicting: 153it [01:37,  1.62it/s]Extractor Predicting: 154it [01:37,  1.58it/s]Extractor Predicting: 155it [01:38,  1.58it/s]Extractor Predicting: 156it [01:39,  1.57it/s]Extractor Predicting: 157it [01:39,  1.56it/s]Extractor Predicting: 158it [01:40,  1.54it/s]Extractor Predicting: 159it [01:41,  1.54it/s]Extractor Predicting: 160it [01:42,  1.37it/s]Extractor Predicting: 161it [01:42,  1.44it/s]Extractor Predicting: 162it [01:43,  1.48it/s]Extractor Predicting: 163it [01:43,  1.51it/s]Extractor Predicting: 164it [01:44,  1.51it/s]Extractor Predicting: 165it [01:45,  1.51it/s]Extractor Predicting: 166it [01:45,  1.53it/s]Extractor Predicting: 167it [01:46,  1.57it/s]Extractor Predicting: 168it [01:47,  1.56it/s]Extractor Predicting: 169it [01:47,  1.56it/s]Extractor Predicting: 170it [01:48,  1.55it/s]Extractor Predicting: 171it [01:49,  1.52it/s]Extractor Predicting: 172it [01:49,  1.51it/s]Extractor Predicting: 173it [01:50,  1.51it/s]Extractor Predicting: 174it [01:51,  1.48it/s]Extractor Predicting: 175it [01:51,  1.45it/s]Extractor Predicting: 176it [01:52,  1.47it/s]Extractor Predicting: 177it [01:53,  1.51it/s]Extractor Predicting: 178it [01:53,  1.52it/s]Extractor Predicting: 179it [01:54,  1.55it/s]Extractor Predicting: 180it [01:55,  1.54it/s]Extractor Predicting: 181it [01:55,  1.56it/s]Extractor Predicting: 182it [01:56,  1.57it/s]Extractor Predicting: 183it [01:56,  1.57it/s]Extractor Predicting: 184it [01:57,  1.59it/s]Extractor Predicting: 185it [01:58,  1.61it/s]Extractor Predicting: 186it [01:58,  1.63it/s]Extractor Predicting: 187it [01:59,  1.62it/s]Extractor Predicting: 188it [02:00,  1.62it/s]Extractor Predicting: 189it [02:00,  1.63it/s]Extractor Predicting: 190it [02:01,  1.63it/s]Extractor Predicting: 191it [02:01,  1.57it/s]Extractor Predicting: 192it [02:02,  1.55it/s]Extractor Predicting: 193it [02:03,  1.57it/s]Extractor Predicting: 194it [02:03,  1.61it/s]Extractor Predicting: 195it [02:04,  1.62it/s]Extractor Predicting: 196it [02:05,  1.61it/s]Extractor Predicting: 197it [02:05,  1.63it/s]Extractor Predicting: 198it [02:06,  1.62it/s]Extractor Predicting: 199it [02:06,  1.61it/s]Extractor Predicting: 200it [02:07,  1.60it/s]Extractor Predicting: 201it [02:08,  1.57it/s]Extractor Predicting: 202it [02:08,  1.60it/s]Extractor Predicting: 203it [02:09,  1.63it/s]Extractor Predicting: 204it [02:09,  1.64it/s]Extractor Predicting: 205it [02:10,  1.63it/s]Extractor Predicting: 206it [02:11,  1.60it/s]Extractor Predicting: 207it [02:11,  1.57it/s]Extractor Predicting: 208it [02:12,  1.57it/s]Extractor Predicting: 209it [02:13,  1.59it/s]Extractor Predicting: 210it [02:13,  1.58it/s]Extractor Predicting: 211it [02:14,  1.60it/s]Extractor Predicting: 212it [02:15,  1.58it/s]Extractor Predicting: 213it [02:15,  1.62it/s]Extractor Predicting: 214it [02:16,  1.60it/s]Extractor Predicting: 215it [02:16,  1.63it/s]Extractor Predicting: 216it [02:17,  1.63it/s]Extractor Predicting: 217it [02:18,  1.56it/s]Extractor Predicting: 218it [02:18,  1.58it/s]Extractor Predicting: 219it [02:19,  1.61it/s]Extractor Predicting: 220it [02:20,  1.63it/s]Extractor Predicting: 221it [02:20,  1.53it/s]Extractor Predicting: 222it [02:21,  1.56it/s]Extractor Predicting: 223it [02:21,  1.59it/s]Extractor Predicting: 224it [02:22,  1.61it/s]Extractor Predicting: 225it [02:23,  1.59it/s]Extractor Predicting: 226it [02:23,  1.56it/s]Extractor Predicting: 227it [02:24,  1.56it/s]Extractor Predicting: 228it [02:25,  1.53it/s]Extractor Predicting: 229it [02:25,  1.54it/s]Extractor Predicting: 230it [02:26,  1.56it/s]Extractor Predicting: 231it [02:27,  1.55it/s]Extractor Predicting: 232it [02:27,  1.55it/s]Extractor Predicting: 233it [02:28,  1.57it/s]Extractor Predicting: 234it [02:29,  1.57it/s]Extractor Predicting: 235it [02:29,  1.53it/s]Extractor Predicting: 236it [02:30,  1.56it/s]Extractor Predicting: 237it [02:30,  1.61it/s]Extractor Predicting: 238it [02:31,  1.57it/s]Extractor Predicting: 239it [02:32,  1.55it/s]Extractor Predicting: 240it [02:32,  1.52it/s]Extractor Predicting: 241it [02:33,  1.53it/s]Extractor Predicting: 242it [02:34,  1.57it/s]Extractor Predicting: 243it [02:34,  1.57it/s]Extractor Predicting: 244it [02:35,  1.54it/s]Extractor Predicting: 245it [02:36,  1.55it/s]Extractor Predicting: 246it [02:36,  1.57it/s]Extractor Predicting: 247it [02:37,  1.59it/s]Extractor Predicting: 248it [02:38,  1.53it/s]Extractor Predicting: 249it [02:38,  1.55it/s]Extractor Predicting: 250it [02:39,  1.54it/s]Extractor Predicting: 251it [02:40,  1.52it/s]Extractor Predicting: 252it [02:40,  1.52it/s]Extractor Predicting: 253it [02:41,  1.52it/s]Extractor Predicting: 254it [02:41,  1.56it/s]Extractor Predicting: 255it [02:42,  1.56it/s]Extractor Predicting: 256it [02:43,  1.60it/s]Extractor Predicting: 257it [02:43,  1.63it/s]Extractor Predicting: 258it [02:44,  1.60it/s]Extractor Predicting: 259it [02:45,  1.59it/s]Extractor Predicting: 260it [02:46,  1.37it/s]Extractor Predicting: 261it [02:46,  1.42it/s]Extractor Predicting: 262it [02:47,  1.46it/s]Extractor Predicting: 263it [02:48,  1.43it/s]Extractor Predicting: 264it [02:48,  1.46it/s]Extractor Predicting: 265it [02:49,  1.48it/s]Extractor Predicting: 266it [02:49,  1.51it/s]Extractor Predicting: 267it [02:50,  1.52it/s]Extractor Predicting: 268it [02:51,  1.55it/s]Extractor Predicting: 269it [02:51,  1.59it/s]Extractor Predicting: 270it [02:52,  1.53it/s]Extractor Predicting: 271it [02:53,  1.52it/s]Extractor Predicting: 272it [02:53,  1.55it/s]Extractor Predicting: 273it [02:54,  1.56it/s]Extractor Predicting: 274it [02:55,  1.56it/s]Extractor Predicting: 275it [02:55,  1.58it/s]Extractor Predicting: 276it [02:56,  1.58it/s]Extractor Predicting: 277it [02:56,  1.56it/s]Extractor Predicting: 278it [02:57,  1.59it/s]Extractor Predicting: 279it [02:58,  1.56it/s]Extractor Predicting: 280it [02:58,  1.56it/s]Extractor Predicting: 281it [02:59,  1.56it/s]Extractor Predicting: 282it [03:00,  1.58it/s]Extractor Predicting: 283it [03:00,  1.58it/s]Extractor Predicting: 284it [03:01,  1.55it/s]Extractor Predicting: 285it [03:02,  1.56it/s]Extractor Predicting: 286it [03:02,  1.57it/s]Extractor Predicting: 287it [03:03,  1.60it/s]Extractor Predicting: 288it [03:03,  1.58it/s]Extractor Predicting: 289it [03:04,  1.55it/s]Extractor Predicting: 290it [03:05,  1.60it/s]Extractor Predicting: 291it [03:05,  1.59it/s]Extractor Predicting: 292it [03:06,  1.58it/s]Extractor Predicting: 293it [03:07,  1.54it/s]Extractor Predicting: 294it [03:07,  1.50it/s]Extractor Predicting: 295it [03:08,  1.55it/s]Extractor Predicting: 296it [03:09,  1.53it/s]Extractor Predicting: 297it [03:09,  1.56it/s]Extractor Predicting: 298it [03:10,  1.50it/s]Extractor Predicting: 299it [03:11,  1.52it/s]Extractor Predicting: 300it [03:11,  1.53it/s]Extractor Predicting: 301it [03:12,  1.54it/s]Extractor Predicting: 302it [03:13,  1.53it/s]Extractor Predicting: 303it [03:13,  1.51it/s]Extractor Predicting: 304it [03:14,  1.48it/s]Extractor Predicting: 305it [03:15,  1.50it/s]Extractor Predicting: 306it [03:15,  1.49it/s]Extractor Predicting: 307it [03:16,  1.48it/s]Extractor Predicting: 308it [03:17,  1.47it/s]Extractor Predicting: 309it [03:17,  1.48it/s]Extractor Predicting: 310it [03:18,  1.46it/s]Extractor Predicting: 311it [03:19,  1.46it/s]Extractor Predicting: 312it [03:19,  1.48it/s]Extractor Predicting: 313it [03:20,  1.49it/s]Extractor Predicting: 314it [03:21,  1.46it/s]Extractor Predicting: 315it [03:21,  1.52it/s]Extractor Predicting: 316it [03:22,  1.50it/s]Extractor Predicting: 317it [03:23,  1.49it/s]Extractor Predicting: 318it [03:23,  1.50it/s]Extractor Predicting: 319it [03:24,  1.50it/s]Extractor Predicting: 320it [03:25,  1.59it/s]Extractor Predicting: 321it [03:25,  1.63it/s]Extractor Predicting: 322it [03:26,  1.71it/s]Extractor Predicting: 323it [03:26,  1.74it/s]Extractor Predicting: 324it [03:27,  1.75it/s]Extractor Predicting: 325it [03:27,  1.77it/s]Extractor Predicting: 326it [03:28,  1.77it/s]Extractor Predicting: 327it [03:29,  1.75it/s]Extractor Predicting: 328it [03:29,  1.77it/s]Extractor Predicting: 329it [03:30,  1.81it/s]Extractor Predicting: 330it [03:30,  1.80it/s]Extractor Predicting: 331it [03:31,  1.84it/s]Extractor Predicting: 332it [03:31,  1.80it/s]Extractor Predicting: 333it [03:32,  1.84it/s]Extractor Predicting: 334it [03:32,  1.84it/s]Extractor Predicting: 335it [03:33,  1.83it/s]Extractor Predicting: 336it [03:33,  1.80it/s]Extractor Predicting: 337it [03:34,  1.84it/s]Extractor Predicting: 338it [03:35,  1.79it/s]Extractor Predicting: 339it [03:35,  1.78it/s]Extractor Predicting: 340it [03:36,  1.84it/s]Extractor Predicting: 341it [03:36,  1.81it/s]Extractor Predicting: 342it [03:37,  1.83it/s]Extractor Predicting: 343it [03:37,  1.82it/s]Extractor Predicting: 344it [03:38,  1.79it/s]Extractor Predicting: 345it [03:38,  1.81it/s]Extractor Predicting: 346it [03:39,  1.81it/s]Extractor Predicting: 347it [03:40,  1.77it/s]Extractor Predicting: 348it [03:40,  1.73it/s]Extractor Predicting: 349it [03:41,  1.66it/s]Extractor Predicting: 350it [03:41,  1.62it/s]Extractor Predicting: 351it [03:42,  1.60it/s]Extractor Predicting: 352it [03:43,  1.60it/s]Extractor Predicting: 353it [03:43,  1.62it/s]Extractor Predicting: 354it [03:44,  1.60it/s]Extractor Predicting: 355it [03:45,  1.59it/s]Extractor Predicting: 356it [03:45,  1.56it/s]Extractor Predicting: 357it [03:46,  1.54it/s]Extractor Predicting: 358it [03:47,  1.56it/s]Extractor Predicting: 359it [03:47,  1.49it/s]Extractor Predicting: 360it [03:48,  1.54it/s]Extractor Predicting: 361it [03:49,  1.55it/s]Extractor Predicting: 362it [03:49,  1.58it/s]Extractor Predicting: 363it [03:50,  1.57it/s]Extractor Predicting: 364it [03:50,  1.55it/s]Extractor Predicting: 365it [03:51,  1.37it/s]Extractor Predicting: 366it [03:52,  1.42it/s]Extractor Predicting: 367it [03:53,  1.45it/s]Extractor Predicting: 368it [03:53,  1.49it/s]Extractor Predicting: 369it [03:54,  1.50it/s]Extractor Predicting: 370it [03:55,  1.50it/s]Extractor Predicting: 371it [03:55,  1.51it/s]Extractor Predicting: 372it [03:56,  1.53it/s]Extractor Predicting: 373it [03:57,  1.52it/s]Extractor Predicting: 374it [03:57,  1.54it/s]Extractor Predicting: 375it [03:58,  1.56it/s]Extractor Predicting: 376it [03:59,  1.52it/s]Extractor Predicting: 377it [03:59,  1.54it/s]Extractor Predicting: 378it [04:00,  1.52it/s]Extractor Predicting: 379it [04:01,  1.52it/s]Extractor Predicting: 380it [04:01,  1.52it/s]Extractor Predicting: 381it [04:02,  1.50it/s]Extractor Predicting: 382it [04:03,  1.50it/s]Extractor Predicting: 383it [04:03,  1.51it/s]Extractor Predicting: 384it [04:04,  1.50it/s]Extractor Predicting: 385it [04:05,  1.50it/s]Extractor Predicting: 386it [04:05,  1.52it/s]Extractor Predicting: 387it [04:06,  1.50it/s]Extractor Predicting: 388it [04:07,  1.49it/s]Extractor Predicting: 389it [04:07,  1.53it/s]Extractor Predicting: 390it [04:08,  1.53it/s]Extractor Predicting: 391it [04:08,  1.53it/s]Extractor Predicting: 392it [04:09,  1.49it/s]Extractor Predicting: 393it [04:10,  1.53it/s]Extractor Predicting: 394it [04:10,  1.52it/s]Extractor Predicting: 395it [04:11,  1.50it/s]Extractor Predicting: 396it [04:12,  1.47it/s]Extractor Predicting: 397it [04:13,  1.46it/s]Extractor Predicting: 398it [04:13,  1.47it/s]Extractor Predicting: 399it [04:14,  1.44it/s]Extractor Predicting: 400it [04:15,  1.46it/s]Extractor Predicting: 401it [04:15,  1.46it/s]Extractor Predicting: 402it [04:16,  1.49it/s]Extractor Predicting: 403it [04:17,  1.49it/s]Extractor Predicting: 404it [04:17,  1.54it/s]Extractor Predicting: 405it [04:18,  1.53it/s]Extractor Predicting: 406it [04:18,  1.56it/s]Extractor Predicting: 407it [04:19,  1.55it/s]Extractor Predicting: 408it [04:20,  1.53it/s]Extractor Predicting: 409it [04:20,  1.57it/s]Extractor Predicting: 410it [04:21,  1.57it/s]Extractor Predicting: 411it [04:22,  1.59it/s]Extractor Predicting: 412it [04:22,  1.56it/s]Extractor Predicting: 413it [04:23,  1.59it/s]Extractor Predicting: 414it [04:24,  1.59it/s]Extractor Predicting: 415it [04:24,  1.58it/s]Extractor Predicting: 416it [04:25,  1.55it/s]Extractor Predicting: 417it [04:25,  1.58it/s]Extractor Predicting: 418it [04:26,  1.57it/s]Extractor Predicting: 419it [04:27,  1.55it/s]Extractor Predicting: 420it [04:27,  1.56it/s]Extractor Predicting: 421it [04:28,  1.56it/s]Extractor Predicting: 422it [04:29,  1.54it/s]Extractor Predicting: 423it [04:29,  1.55it/s]Extractor Predicting: 424it [04:30,  1.52it/s]Extractor Predicting: 425it [04:31,  1.53it/s]Extractor Predicting: 426it [04:31,  1.52it/s]Extractor Predicting: 427it [04:32,  1.54it/s]Extractor Predicting: 428it [04:33,  1.53it/s]Extractor Predicting: 429it [04:33,  1.55it/s]Extractor Predicting: 430it [04:33,  1.92it/s]Extractor Predicting: 430it [04:33,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:24:08,432 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:24:08,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:24:08,447 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:24:08,447 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:24:08,447 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:24:09,110 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:24:09,111 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:24:09,687 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:24:10,748 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:24:10,748 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:24:14,490 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:24:14,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:24:14,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:24:14,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:24:14,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:24:15,137 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:24:15,139 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:24:15,742 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:24:15,913 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:24:15,913 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3284365162644281,
  "recall": 0.09118275393280248,
  "score": 0.14273770616401915,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:02,  1.97it/s]Extractor Predicting: 5it [00:02,  1.69it/s]
[INFO|configuration_utils.py:515] 2023-08-28 20:24:19,719 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:24:19,720 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:24:19,725 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:24:19,726 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 20:24:19,731 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:24:26,696 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 20:24:26,711 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 20:24:26,743 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:24:26,744 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:24:26,768 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:24:26,787 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:24:26,787 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:24:26,787 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:24:26,788 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:24:26,788 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:24:26,788 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5789473684210527,
  "recall": 0.054455445544554455,
  "score": 0.09954751131221719,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 20:24:27,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:27,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:28,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:28,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:29,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:30,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:30,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:31,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:32,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:32,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:33,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:33,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:34,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:35,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:35,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:36,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:37,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:37,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:38,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:38,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:39,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:13<04:09, 13.12s/it][WARNING|generation_utils.py:914] 2023-08-28 20:24:40,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:40,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:41,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:41,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:42,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:42,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:43,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:44,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:44,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:45,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:45,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:46,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:46,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:47,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:48,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:48,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:49,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:50,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:51,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:51,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:25<03:44, 12.49s/it][WARNING|generation_utils.py:914] 2023-08-28 20:24:52,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:52,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:53,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:53,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:54,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:54,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:55,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:55,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:56,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:56,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:57,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:57,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:58,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:58,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:59,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:24:59,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:00,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:00,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:01,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:01,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:02,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:35<03:18, 11.66s/it][WARNING|generation_utils.py:914] 2023-08-28 20:25:02,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:03,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:03,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:04,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:05,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:05,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:06,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:06,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:07,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:08,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:08,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:09,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:09,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:10,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:10,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:11,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:12,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:12,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:13,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:13,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:47<03:06, 11.67s/it][WARNING|generation_utils.py:914] 2023-08-28 20:25:14,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:15,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:15,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:16,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:16,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:17,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:17,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:18,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:18,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:19,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:20,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:20,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:21,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:21,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:22,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:22,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:23,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:23,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:24,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:24,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [00:58<02:51, 11.40s/it][WARNING|generation_utils.py:914] 2023-08-28 20:25:25,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:26,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:27,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:27,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:28,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:29,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:30,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:30,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:31,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:31,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:32,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:33,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:34,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:34,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:35,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:35,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:36,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:37,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:38,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:38,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:39,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:40,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:13<02:57, 12.65s/it][WARNING|generation_utils.py:914] 2023-08-28 20:25:40,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:41,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:41,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:42,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:42,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:43,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:43,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:43,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:44,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:45,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:45,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:46,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:46,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:46,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:47,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:48,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:48,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:49,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:49,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:50,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:23<02:33, 11.78s/it][WARNING|generation_utils.py:914] 2023-08-28 20:25:50,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:51,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:51,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:52,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:52,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:53,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:54,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:54,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:55,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:56,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:56,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:57,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:57,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:58,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:59,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:25:59,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:00,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:00,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:01,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:02,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:02,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:03,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:37<02:28, 12.34s/it][WARNING|generation_utils.py:914] 2023-08-28 20:26:04,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:04,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:05,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:05,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:06,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:07,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:07,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:08,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:09,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:09,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:10,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:11,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:11,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:12,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:13,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:14,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:14,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:15,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:16,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:16,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:17,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:18,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:18,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:19,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:52<02:27, 13.37s/it][WARNING|generation_utils.py:914] 2023-08-28 20:26:19,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:20,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:20,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:21,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:21,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:22,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:23,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:23,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:24,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:24,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:25,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:26,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:26,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:27,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:27,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:28,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:29,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:29,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:30,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:30,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:31,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:31,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:05<02:11, 13.15s/it][WARNING|generation_utils.py:914] 2023-08-28 20:26:32,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:33,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:33,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:34,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:35,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:35,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:36,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:37,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:37,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:38,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:38,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:39,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:40,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:40,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:41,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:42,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:42,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:43,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:44,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:44,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:45,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:18<01:59, 13.25s/it][WARNING|generation_utils.py:914] 2023-08-28 20:26:45,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:46,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:47,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:47,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:48,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:48,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:49,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:49,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:50,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:50,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:51,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:52,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:52,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:53,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:53,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:54,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:54,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:55,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:55,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:56,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:57,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:57,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:31<01:44, 13.04s/it][WARNING|generation_utils.py:914] 2023-08-28 20:26:58,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:59,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:26:59,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:00,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:00,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:01,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:01,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:02,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:02,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:03,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:03,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:04,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:05,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:05,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:06,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:06,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:07,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:07,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:08,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:09,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:42<01:27, 12.47s/it][WARNING|generation_utils.py:914] 2023-08-28 20:27:09,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:10,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:10,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:11,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:12,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:12,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:13,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:14,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:14,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:15,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:15,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:16,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:17,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:17,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:18,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:19,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:19,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:20,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:20,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:21,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:22,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [02:56<01:17, 12.88s/it][WARNING|generation_utils.py:914] 2023-08-28 20:27:23,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:23,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:24,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:25,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:25,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:26,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:26,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:27,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:28,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:28,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:29,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:29,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:30,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:30,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:31,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:32,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:32,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:33,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:33,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:34,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:35,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:35,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:09<01:04, 12.93s/it][WARNING|generation_utils.py:914] 2023-08-28 20:27:36,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:37,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:37,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:38,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:39,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:39,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:40,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:41,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:41,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:42,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:43,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:43,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:44,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:45,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:45,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:46,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:47,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:47,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:48,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:49,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:49,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:23<00:53, 13.34s/it][WARNING|generation_utils.py:914] 2023-08-28 20:27:50,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:51,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:51,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:52,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:53,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:53,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:54,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:55,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:55,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:56,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:56,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:57,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:58,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:58,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:27:59,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:00,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:00,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:01,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:02,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:02,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:03,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:03,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:37<00:40, 13.48s/it][WARNING|generation_utils.py:914] 2023-08-28 20:28:04,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:05,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:05,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:06,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:06,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:07,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:07,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:08,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:08,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:09,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:10,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:10,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:11,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:11,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:12,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:12,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:13,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:13,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:14,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:14,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:15,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:49<00:25, 12.94s/it][WARNING|generation_utils.py:914] 2023-08-28 20:28:16,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:16,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:17,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:17,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:18,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:19,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:19,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:20,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:20,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:21,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:21,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:22,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:22,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:23,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:24,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:24,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:25,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:25,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:26,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:26,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:27,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:27,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:28,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:29,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:02<00:13, 13.09s/it][WARNING|generation_utils.py:914] 2023-08-28 20:28:29,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:30,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:30,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:31,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:32,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:33,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:33,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:34,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:34,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:36,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:36,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:37,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:37,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:38,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:39,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:39,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:40,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:41,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:41,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:42,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:43,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:28:43,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:17<00:00, 13.65s/it]Generating: 100%|██████████| 20/20 [04:17<00:00, 12.88s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:28:52,103 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:28:52,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:28:52,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:28:52,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:28:52,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:28:52,782 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:28:52,783 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:28:53,375 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:28:54,473 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:28:54,473 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:28:57,446 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:28:57,468 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:28:57,469 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:28:57,469 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:28:57,469 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:28:58,189 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:28:58,190 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:28:58,786 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:28:58,975 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:28:58,975 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : followed by .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : military rank .', 'success_rate': 0.9640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : tributary .', 'success_rate': 0.9546875, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : architect .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : constellation .', 'success_rate': 0.9734375, 'errors': {''}}
['Relation : contains administrative territorial entity . Context : The city of Marchec is under the control of the Marchec Municipal Municipality . Head Entity : Marchec Municipal Municipality , Tail Entity : Marchec , Tail Entity : Marchec .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.890625, 'errors': {'too many values to unpack (expected 2)', ''}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 480, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 559, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8607954545454546, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9360119047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : league .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('next millennium', 'member of', '', 'However , since he had been placed back in the group , many players from that group had joined in the early days of the next millennium .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8849431818181818, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : notable work .', 'success_rate': 0.9166666666666666, 'errors': {''}}
['Relation : operator . Context : Later in 2003 , the operator of the New River Transport fleet installed an engine that would power the service from New York City to New York City once the new system was operational . Head Entity : New River Transport , Tail Entity : New York City Transit .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.8607954545454546, 'errors': {'', "('K', 'operator', '', 'The line will be replaced by a new K - line , built from 2004 and running from April 2005 .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 454, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 556, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : position held .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : residence . Context : On 31 March 2014 , the couple moved to Puyallup , New York at the suggestion of their son , Mark A. Head Entity : Mark A. , Tail Entity : Puyallup , N.Y .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8835227272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 11584
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11684, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.64it/s]Extractor Estimating: 2it [00:01,  1.40it/s]Extractor Estimating: 3it [00:02,  1.46it/s]Extractor Estimating: 4it [00:02,  1.45it/s]Extractor Estimating: 5it [00:03,  1.52it/s]Extractor Estimating: 6it [00:04,  1.46it/s]Extractor Estimating: 7it [00:04,  1.49it/s]Extractor Estimating: 8it [00:05,  1.50it/s]Extractor Estimating: 9it [00:06,  1.46it/s]Extractor Estimating: 10it [00:06,  1.47it/s]Extractor Estimating: 11it [00:07,  1.53it/s]Extractor Estimating: 12it [00:08,  1.50it/s]Extractor Estimating: 13it [00:08,  1.53it/s]Extractor Estimating: 14it [00:09,  1.55it/s]Extractor Estimating: 15it [00:10,  1.51it/s]Extractor Estimating: 16it [00:10,  1.52it/s]Extractor Estimating: 17it [00:11,  1.50it/s]Extractor Estimating: 18it [00:12,  1.51it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:13,  1.53it/s]Extractor Estimating: 21it [00:13,  1.53it/s]Extractor Estimating: 22it [00:14,  1.61it/s]Extractor Estimating: 23it [00:15,  1.55it/s]Extractor Estimating: 24it [00:15,  1.58it/s]Extractor Estimating: 25it [00:16,  1.60it/s]Extractor Estimating: 26it [00:16,  1.68it/s]Extractor Estimating: 27it [00:17,  1.69it/s]Extractor Estimating: 28it [00:18,  1.72it/s]Extractor Estimating: 29it [00:18,  1.69it/s]Extractor Estimating: 30it [00:19,  1.73it/s]Extractor Estimating: 31it [00:19,  1.65it/s]Extractor Estimating: 32it [00:20,  1.65it/s]Extractor Estimating: 33it [00:21,  1.66it/s]Extractor Estimating: 34it [00:21,  1.65it/s]Extractor Estimating: 35it [00:22,  1.63it/s]Extractor Estimating: 36it [00:22,  1.66it/s]Extractor Estimating: 37it [00:23,  1.68it/s]Extractor Estimating: 38it [00:24,  1.71it/s]Extractor Estimating: 39it [00:24,  1.71it/s]Extractor Estimating: 40it [00:25,  1.73it/s]Extractor Estimating: 41it [00:25,  1.71it/s]Extractor Estimating: 42it [00:26,  1.69it/s]Extractor Estimating: 43it [00:26,  1.70it/s]Extractor Estimating: 44it [00:27,  1.73it/s]Extractor Estimating: 45it [00:28,  1.75it/s]Extractor Estimating: 46it [00:28,  1.69it/s]Extractor Estimating: 47it [00:29,  1.67it/s]Extractor Estimating: 48it [00:29,  1.70it/s]Extractor Estimating: 49it [00:30,  1.72it/s]Extractor Estimating: 50it [00:31,  1.70it/s]Extractor Estimating: 51it [00:31,  1.78it/s]Extractor Estimating: 52it [00:32,  1.82it/s]Extractor Estimating: 53it [00:32,  1.90it/s]Extractor Estimating: 54it [00:33,  2.00it/s]Extractor Estimating: 55it [00:33,  1.98it/s]Extractor Estimating: 56it [00:34,  1.97it/s]Extractor Estimating: 57it [00:34,  1.96it/s]Extractor Estimating: 58it [00:35,  2.04it/s]Extractor Estimating: 59it [00:35,  2.02it/s]Extractor Estimating: 60it [00:35,  2.14it/s]Extractor Estimating: 61it [00:36,  2.10it/s]Extractor Estimating: 62it [00:36,  2.08it/s]Extractor Estimating: 63it [00:37,  2.01it/s]Extractor Estimating: 64it [00:37,  1.99it/s]Extractor Estimating: 65it [00:38,  2.03it/s]Extractor Estimating: 66it [00:38,  1.99it/s]Extractor Estimating: 67it [00:39,  1.97it/s]Extractor Estimating: 68it [00:39,  2.05it/s]Extractor Estimating: 69it [00:40,  2.00it/s]Extractor Estimating: 70it [00:40,  2.08it/s]Extractor Estimating: 71it [00:41,  2.07it/s]Extractor Estimating: 72it [00:41,  2.10it/s]Extractor Estimating: 73it [00:42,  2.08it/s]Extractor Estimating: 74it [00:42,  2.05it/s]Extractor Estimating: 75it [00:43,  2.02it/s]Extractor Estimating: 76it [00:43,  1.94it/s]Extractor Estimating: 77it [00:44,  1.95it/s]Extractor Estimating: 78it [00:45,  1.85it/s]Extractor Estimating: 79it [00:45,  1.77it/s]Extractor Estimating: 80it [00:46,  1.77it/s]Extractor Estimating: 81it [00:46,  1.74it/s]Extractor Estimating: 82it [00:47,  1.75it/s]Extractor Estimating: 83it [00:47,  1.75it/s]Extractor Estimating: 84it [00:48,  1.73it/s]Extractor Estimating: 85it [00:49,  1.73it/s]Extractor Estimating: 86it [00:49,  1.65it/s]Extractor Estimating: 87it [00:50,  1.65it/s]Extractor Estimating: 88it [00:50,  1.69it/s]Extractor Estimating: 89it [00:51,  1.68it/s]Extractor Estimating: 90it [00:52,  1.76it/s]Extractor Estimating: 91it [00:52,  1.75it/s]Extractor Estimating: 92it [00:53,  1.79it/s]Extractor Estimating: 93it [00:53,  1.72it/s]Extractor Estimating: 94it [00:54,  1.70it/s]Extractor Estimating: 95it [00:54,  1.70it/s]Extractor Estimating: 96it [00:55,  1.74it/s]Extractor Estimating: 97it [00:56,  1.68it/s]Extractor Estimating: 98it [00:56,  1.72it/s]Extractor Estimating: 99it [00:57,  1.68it/s]Extractor Estimating: 100it [00:57,  1.65it/s]Extractor Estimating: 101it [00:58,  1.80it/s]Extractor Estimating: 102it [00:58,  1.91it/s]Extractor Estimating: 103it [00:59,  2.03it/s]Extractor Estimating: 104it [00:59,  2.03it/s]Extractor Estimating: 105it [01:00,  1.96it/s]Extractor Estimating: 106it [01:00,  2.03it/s]Extractor Estimating: 107it [01:01,  2.12it/s]Extractor Estimating: 108it [01:01,  2.16it/s]Extractor Estimating: 109it [01:02,  2.13it/s]Extractor Estimating: 110it [01:02,  2.13it/s]Extractor Estimating: 111it [01:03,  2.13it/s]Extractor Estimating: 112it [01:03,  2.15it/s]Extractor Estimating: 113it [01:03,  2.15it/s]Extractor Estimating: 114it [01:04,  2.11it/s]Extractor Estimating: 115it [01:04,  2.19it/s]Extractor Estimating: 116it [01:05,  2.23it/s]Extractor Estimating: 117it [01:05,  2.17it/s]Extractor Estimating: 118it [01:06,  1.91it/s]Extractor Estimating: 119it [01:06,  2.00it/s]Extractor Estimating: 120it [01:07,  2.06it/s]Extractor Estimating: 121it [01:07,  2.11it/s]Extractor Estimating: 122it [01:08,  2.18it/s]Extractor Estimating: 123it [01:08,  2.18it/s]Extractor Estimating: 124it [01:09,  2.13it/s]Extractor Estimating: 125it [01:09,  2.12it/s]Extractor Estimating: 126it [01:10,  2.02it/s]Extractor Estimating: 127it [01:10,  1.86it/s]Extractor Estimating: 128it [01:11,  1.86it/s]Extractor Estimating: 129it [01:11,  1.86it/s]Extractor Estimating: 130it [01:12,  1.85it/s]Extractor Estimating: 131it [01:13,  1.87it/s]Extractor Estimating: 132it [01:13,  1.91it/s]Extractor Estimating: 133it [01:14,  1.85it/s]Extractor Estimating: 134it [01:14,  1.82it/s]Extractor Estimating: 135it [01:15,  1.77it/s]Extractor Estimating: 136it [01:15,  1.78it/s]Extractor Estimating: 137it [01:16,  1.82it/s]Extractor Estimating: 138it [01:16,  1.89it/s]Extractor Estimating: 139it [01:17,  1.87it/s]Extractor Estimating: 140it [01:17,  1.83it/s]Extractor Estimating: 141it [01:18,  1.89it/s]Extractor Estimating: 142it [01:18,  1.89it/s]Extractor Estimating: 143it [01:19,  1.86it/s]Extractor Estimating: 144it [01:20,  1.84it/s]Extractor Estimating: 145it [01:20,  1.85it/s]Extractor Estimating: 146it [01:21,  1.81it/s]Extractor Estimating: 147it [01:21,  1.84it/s]Extractor Estimating: 148it [01:22,  1.83it/s]Extractor Estimating: 149it [01:22,  1.86it/s]Extractor Estimating: 150it [01:23,  1.87it/s]Extractor Estimating: 151it [01:23,  1.96it/s]Extractor Estimating: 152it [01:24,  1.97it/s]Extractor Estimating: 153it [01:24,  1.98it/s]Extractor Estimating: 154it [01:25,  2.03it/s]Extractor Estimating: 155it [01:25,  2.10it/s]Extractor Estimating: 156it [01:26,  2.04it/s]Extractor Estimating: 157it [01:26,  2.17it/s]Extractor Estimating: 158it [01:27,  2.14it/s]Extractor Estimating: 159it [01:27,  2.10it/s]Extractor Estimating: 160it [01:28,  2.11it/s]Extractor Estimating: 161it [01:28,  2.07it/s]Extractor Estimating: 162it [01:29,  2.02it/s]Extractor Estimating: 163it [01:29,  2.00it/s]Extractor Estimating: 164it [01:30,  1.97it/s]Extractor Estimating: 165it [01:30,  1.93it/s]Extractor Estimating: 166it [01:31,  2.01it/s]Extractor Estimating: 167it [01:31,  2.02it/s]Extractor Estimating: 168it [01:32,  2.00it/s]Extractor Estimating: 169it [01:32,  2.05it/s]Extractor Estimating: 170it [01:33,  1.96it/s]Extractor Estimating: 171it [01:33,  2.02it/s]Extractor Estimating: 172it [01:34,  1.98it/s]Extractor Estimating: 173it [01:34,  2.05it/s]Extractor Estimating: 174it [01:34,  2.11it/s]Extractor Estimating: 175it [01:35,  2.10it/s]Extractor Estimating: 176it [01:36,  2.04it/s]Extractor Estimating: 177it [01:36,  2.01it/s]Extractor Estimating: 178it [01:37,  1.87it/s]Extractor Estimating: 179it [01:37,  1.88it/s]Extractor Estimating: 180it [01:38,  1.82it/s]Extractor Estimating: 181it [01:38,  1.81it/s]Extractor Estimating: 182it [01:39,  1.83it/s]Extractor Estimating: 183it [01:39,  1.88it/s]Extractor Estimating: 184it [01:40,  1.93it/s]Extractor Estimating: 185it [01:40,  1.88it/s]Extractor Estimating: 186it [01:41,  1.80it/s]Extractor Estimating: 187it [01:42,  1.80it/s]Extractor Estimating: 188it [01:42,  1.80it/s]Extractor Estimating: 189it [01:43,  1.80it/s]Extractor Estimating: 190it [01:43,  1.82it/s]Extractor Estimating: 191it [01:44,  1.84it/s]Extractor Estimating: 192it [01:44,  1.71it/s]Extractor Estimating: 193it [01:45,  1.72it/s]Extractor Estimating: 194it [01:46,  1.76it/s]Extractor Estimating: 195it [01:46,  1.74it/s]Extractor Estimating: 196it [01:47,  1.77it/s]Extractor Estimating: 197it [01:47,  1.83it/s]Extractor Estimating: 198it [01:48,  1.78it/s]Extractor Estimating: 199it [01:48,  1.72it/s]Extractor Estimating: 200it [01:49,  1.77it/s]Extractor Estimating: 201it [01:49,  1.76it/s]Extractor Estimating: 202it [01:50,  1.78it/s]Extractor Estimating: 203it [01:51,  1.71it/s]Extractor Estimating: 204it [01:51,  1.67it/s]Extractor Estimating: 205it [01:52,  1.71it/s]Extractor Estimating: 206it [01:52,  1.70it/s]Extractor Estimating: 207it [01:53,  1.71it/s]Extractor Estimating: 208it [01:54,  1.74it/s]Extractor Estimating: 209it [01:54,  1.75it/s]Extractor Estimating: 210it [01:55,  1.58it/s]Extractor Estimating: 211it [01:56,  1.57it/s]Extractor Estimating: 212it [01:56,  1.60it/s]Extractor Estimating: 213it [01:57,  1.64it/s]Extractor Estimating: 214it [01:57,  1.63it/s]Extractor Estimating: 215it [01:58,  1.65it/s]Extractor Estimating: 216it [01:59,  1.50it/s]Extractor Estimating: 217it [01:59,  1.59it/s]Extractor Estimating: 218it [02:00,  1.60it/s]Extractor Estimating: 219it [02:01,  1.58it/s]Extractor Estimating: 220it [02:01,  1.61it/s]Extractor Estimating: 221it [02:02,  1.63it/s]Extractor Estimating: 222it [02:02,  1.70it/s]Extractor Estimating: 223it [02:03,  1.76it/s]Extractor Estimating: 224it [02:03,  1.79it/s]Extractor Estimating: 225it [02:04,  1.74it/s]Extractor Estimating: 226it [02:05,  1.74it/s]Extractor Estimating: 227it [02:05,  1.68it/s]Extractor Estimating: 228it [02:06,  1.63it/s]Extractor Estimating: 229it [02:06,  1.63it/s]Extractor Estimating: 230it [02:07,  1.67it/s]Extractor Estimating: 231it [02:08,  1.65it/s]Extractor Estimating: 232it [02:08,  1.58it/s]Extractor Estimating: 233it [02:09,  1.59it/s]Extractor Estimating: 234it [02:10,  1.60it/s]Extractor Estimating: 235it [02:10,  1.58it/s]Extractor Estimating: 236it [02:11,  1.54it/s]Extractor Estimating: 237it [02:12,  1.57it/s]Extractor Estimating: 238it [02:12,  1.53it/s]Extractor Estimating: 239it [02:13,  1.54it/s]Extractor Estimating: 240it [02:13,  1.54it/s]Extractor Estimating: 241it [02:14,  1.58it/s]Extractor Estimating: 242it [02:15,  1.53it/s]Extractor Estimating: 243it [02:15,  1.53it/s]Extractor Estimating: 244it [02:16,  1.62it/s]Extractor Estimating: 245it [02:17,  1.60it/s]Extractor Estimating: 246it [02:17,  1.61it/s]Extractor Estimating: 247it [02:18,  1.64it/s]Extractor Estimating: 248it [02:18,  1.65it/s]Extractor Estimating: 249it [02:19,  1.65it/s]Extractor Estimating: 250it [02:20,  1.66it/s]Extractor Estimating: 251it [02:20,  1.57it/s]Extractor Estimating: 252it [02:21,  1.57it/s]Extractor Estimating: 253it [02:22,  1.53it/s]Extractor Estimating: 254it [02:22,  1.54it/s]Extractor Estimating: 255it [02:23,  1.52it/s]Extractor Estimating: 256it [02:24,  1.57it/s]Extractor Estimating: 257it [02:24,  1.55it/s]Extractor Estimating: 258it [02:25,  1.49it/s]Extractor Estimating: 259it [02:26,  1.46it/s]Extractor Estimating: 260it [02:26,  1.50it/s]Extractor Estimating: 261it [02:27,  1.51it/s]Extractor Estimating: 262it [02:28,  1.49it/s]Extractor Estimating: 263it [02:28,  1.44it/s]Extractor Estimating: 264it [02:29,  1.47it/s]Extractor Estimating: 265it [02:30,  1.50it/s]Extractor Estimating: 266it [02:30,  1.54it/s]Extractor Estimating: 267it [02:31,  1.46it/s]Extractor Estimating: 268it [02:32,  1.41it/s]Extractor Estimating: 269it [02:32,  1.44it/s]Extractor Estimating: 270it [02:33,  1.44it/s]Extractor Estimating: 271it [02:34,  1.47it/s]Extractor Estimating: 272it [02:34,  1.50it/s]Extractor Estimating: 273it [02:35,  1.52it/s]Extractor Estimating: 274it [02:36,  1.53it/s]Extractor Estimating: 275it [02:36,  1.51it/s]Extractor Estimating: 276it [02:37,  1.59it/s]Extractor Estimating: 277it [02:38,  1.65it/s]Extractor Estimating: 278it [02:38,  1.70it/s]Extractor Estimating: 279it [02:39,  1.74it/s]Extractor Estimating: 280it [02:39,  1.72it/s]Extractor Estimating: 281it [02:40,  1.71it/s]Extractor Estimating: 282it [02:40,  1.73it/s]Extractor Estimating: 283it [02:41,  1.76it/s]Extractor Estimating: 284it [02:42,  1.72it/s]Extractor Estimating: 285it [02:42,  1.77it/s]Extractor Estimating: 286it [02:43,  1.79it/s]Extractor Estimating: 287it [02:43,  1.74it/s]Extractor Estimating: 288it [02:44,  1.74it/s]Extractor Estimating: 289it [02:44,  1.67it/s]Extractor Estimating: 290it [02:45,  1.67it/s]Extractor Estimating: 291it [02:46,  1.70it/s]Extractor Estimating: 292it [02:46,  1.71it/s]Extractor Estimating: 293it [02:47,  1.68it/s]Extractor Estimating: 294it [02:47,  1.75it/s]Extractor Estimating: 295it [02:48,  1.78it/s]Extractor Estimating: 296it [02:48,  1.81it/s]Extractor Estimating: 297it [02:49,  1.78it/s]Extractor Estimating: 298it [02:50,  1.75it/s]Extractor Estimating: 299it [02:50,  1.72it/s]Extractor Estimating: 300it [02:51,  1.82it/s]Extractor Estimating: 301it [02:51,  1.90it/s]Extractor Estimating: 302it [02:52,  2.00it/s]Extractor Estimating: 303it [02:52,  2.05it/s]Extractor Estimating: 304it [02:52,  2.07it/s]Extractor Estimating: 305it [02:53,  2.09it/s]Extractor Estimating: 306it [02:53,  2.16it/s]Extractor Estimating: 307it [02:54,  2.15it/s]Extractor Estimating: 308it [02:55,  1.86it/s]Extractor Estimating: 309it [02:55,  1.88it/s]Extractor Estimating: 310it [02:56,  1.88it/s]Extractor Estimating: 311it [02:56,  1.96it/s]Extractor Estimating: 312it [02:57,  2.01it/s]Extractor Estimating: 313it [02:57,  2.09it/s]Extractor Estimating: 314it [02:58,  1.77it/s]Extractor Estimating: 315it [02:58,  1.85it/s]Extractor Estimating: 316it [02:59,  1.88it/s]Extractor Estimating: 317it [02:59,  1.98it/s]Extractor Estimating: 318it [03:00,  2.03it/s]Extractor Estimating: 319it [03:00,  2.09it/s]Extractor Estimating: 320it [03:01,  2.09it/s]Extractor Estimating: 321it [03:01,  2.13it/s]Extractor Estimating: 322it [03:01,  2.19it/s]Extractor Estimating: 323it [03:02,  2.11it/s]Extractor Estimating: 324it [03:02,  2.15it/s]Extractor Estimating: 325it [03:03,  2.10it/s]Extractor Estimating: 326it [03:04,  1.90it/s]Extractor Estimating: 327it [03:04,  1.81it/s]Extractor Estimating: 328it [03:05,  1.69it/s]Extractor Estimating: 329it [03:05,  1.67it/s]Extractor Estimating: 330it [03:06,  1.60it/s]Extractor Estimating: 331it [03:07,  1.61it/s]Extractor Estimating: 332it [03:07,  1.61it/s]Extractor Estimating: 333it [03:08,  1.63it/s]Extractor Estimating: 334it [03:09,  1.61it/s]Extractor Estimating: 335it [03:09,  1.61it/s]Extractor Estimating: 336it [03:10,  1.58it/s]Extractor Estimating: 337it [03:10,  1.62it/s]Extractor Estimating: 338it [03:11,  1.68it/s]Extractor Estimating: 339it [03:12,  1.64it/s]Extractor Estimating: 340it [03:12,  1.64it/s]Extractor Estimating: 341it [03:13,  1.61it/s]Extractor Estimating: 342it [03:14,  1.57it/s]Extractor Estimating: 343it [03:14,  1.57it/s]Extractor Estimating: 344it [03:15,  1.61it/s]Extractor Estimating: 345it [03:15,  1.62it/s]Extractor Estimating: 346it [03:16,  1.56it/s]Extractor Estimating: 347it [03:17,  1.62it/s]Extractor Estimating: 348it [03:17,  1.62it/s]Extractor Estimating: 349it [03:18,  1.56it/s]Extractor Estimating: 350it [03:19,  1.58it/s]Extractor Estimating: 351it [03:19,  1.65it/s]Extractor Estimating: 352it [03:20,  1.66it/s]Extractor Estimating: 353it [03:20,  1.66it/s]Extractor Estimating: 354it [03:21,  1.70it/s]Extractor Estimating: 355it [03:21,  1.72it/s]Extractor Estimating: 356it [03:22,  1.74it/s]Extractor Estimating: 357it [03:23,  1.76it/s]Extractor Estimating: 358it [03:23,  1.73it/s]Extractor Estimating: 359it [03:24,  1.69it/s]Extractor Estimating: 360it [03:24,  1.73it/s]Extractor Estimating: 361it [03:25,  1.76it/s]Extractor Estimating: 362it [03:25,  1.72it/s]Extractor Estimating: 363it [03:26,  1.74it/s]Extractor Estimating: 364it [03:27,  1.74it/s]Extractor Estimating: 365it [03:27,  1.74it/s]Extractor Estimating: 366it [03:28,  1.77it/s]Extractor Estimating: 367it [03:28,  1.74it/s]Extractor Estimating: 368it [03:29,  1.77it/s]Extractor Estimating: 369it [03:30,  1.71it/s]Extractor Estimating: 370it [03:30,  1.68it/s]Extractor Estimating: 371it [03:31,  1.70it/s]Extractor Estimating: 372it [03:31,  1.65it/s]Extractor Estimating: 373it [03:32,  1.66it/s]Extractor Estimating: 374it [03:32,  1.73it/s]Extractor Estimating: 375it [03:33,  1.74it/s]Extractor Estimating: 376it [03:34,  1.68it/s]Extractor Estimating: 377it [03:34,  1.54it/s]Extractor Estimating: 378it [03:35,  1.55it/s]Extractor Estimating: 379it [03:36,  1.57it/s]Extractor Estimating: 380it [03:36,  1.61it/s]Extractor Estimating: 381it [03:37,  1.59it/s]Extractor Estimating: 382it [03:38,  1.56it/s]Extractor Estimating: 383it [03:38,  1.54it/s]Extractor Estimating: 384it [03:39,  1.56it/s]Extractor Estimating: 385it [03:40,  1.56it/s]Extractor Estimating: 386it [03:40,  1.55it/s]Extractor Estimating: 387it [03:41,  1.47it/s]Extractor Estimating: 388it [03:42,  1.50it/s]Extractor Estimating: 389it [03:42,  1.48it/s]Extractor Estimating: 390it [03:43,  1.52it/s]Extractor Estimating: 391it [03:44,  1.47it/s]Extractor Estimating: 392it [03:44,  1.51it/s]Extractor Estimating: 393it [03:45,  1.55it/s]Extractor Estimating: 394it [03:45,  1.57it/s]Extractor Estimating: 395it [03:46,  1.57it/s]Extractor Estimating: 396it [03:47,  1.61it/s]Extractor Estimating: 397it [03:47,  1.62it/s]Extractor Estimating: 398it [03:48,  1.60it/s]Extractor Estimating: 399it [03:49,  1.57it/s]Extractor Estimating: 400it [03:49,  1.60it/s]Extractor Estimating: 401it [03:50,  1.65it/s]Extractor Estimating: 402it [03:50,  1.67it/s]Extractor Estimating: 403it [03:51,  1.64it/s]Extractor Estimating: 404it [03:52,  1.65it/s]Extractor Estimating: 405it [03:52,  1.68it/s]Extractor Estimating: 406it [03:53,  1.67it/s]Extractor Estimating: 407it [03:53,  1.70it/s]Extractor Estimating: 408it [03:54,  1.68it/s]Extractor Estimating: 409it [03:55,  1.72it/s]Extractor Estimating: 410it [03:55,  1.56it/s]Extractor Estimating: 411it [03:56,  1.62it/s]Extractor Estimating: 412it [03:56,  1.63it/s]Extractor Estimating: 413it [03:57,  1.61it/s]Extractor Estimating: 414it [03:58,  1.62it/s]Extractor Estimating: 415it [03:58,  1.71it/s]Extractor Estimating: 416it [03:59,  1.69it/s]Extractor Estimating: 417it [03:59,  1.67it/s]Extractor Estimating: 418it [04:00,  1.74it/s]Extractor Estimating: 419it [04:01,  1.72it/s]Extractor Estimating: 420it [04:01,  1.72it/s]Extractor Estimating: 421it [04:02,  1.67it/s]Extractor Estimating: 422it [04:02,  1.69it/s]Extractor Estimating: 423it [04:03,  1.67it/s]Extractor Estimating: 424it [04:04,  1.71it/s]Extractor Estimating: 425it [04:04,  1.73it/s]Extractor Estimating: 426it [04:05,  1.70it/s]Extractor Estimating: 427it [04:05,  1.67it/s]Extractor Estimating: 428it [04:06,  1.68it/s]Extractor Estimating: 429it [04:06,  1.73it/s]Extractor Estimating: 430it [04:07,  1.72it/s]Extractor Estimating: 431it [04:08,  1.78it/s]Extractor Estimating: 432it [04:08,  1.80it/s]Extractor Estimating: 433it [04:09,  1.79it/s]Extractor Estimating: 434it [04:09,  1.81it/s]Extractor Estimating: 435it [04:10,  1.85it/s]Extractor Estimating: 436it [04:10,  1.85it/s]Extractor Estimating: 437it [04:11,  1.87it/s]Extractor Estimating: 438it [04:11,  1.82it/s]Extractor Estimating: 439it [04:12,  1.81it/s]Extractor Estimating: 440it [04:12,  1.83it/s]Extractor Estimating: 441it [04:13,  1.89it/s]Extractor Estimating: 442it [04:13,  1.84it/s]Extractor Estimating: 443it [04:14,  1.86it/s]Extractor Estimating: 444it [04:15,  1.86it/s]Extractor Estimating: 445it [04:15,  1.85it/s]Extractor Estimating: 446it [04:16,  1.85it/s]Extractor Estimating: 447it [04:16,  1.87it/s]Extractor Estimating: 448it [04:17,  1.85it/s]Extractor Estimating: 449it [04:17,  1.80it/s]Extractor Estimating: 450it [04:18,  1.74it/s]Extractor Estimating: 451it [04:18,  1.77it/s]Extractor Estimating: 452it [04:19,  1.83it/s]Extractor Estimating: 453it [04:19,  1.85it/s]Extractor Estimating: 454it [04:20,  1.81it/s]Extractor Estimating: 455it [04:21,  1.88it/s]Extractor Estimating: 456it [04:21,  1.84it/s]Extractor Estimating: 457it [04:22,  1.81it/s]Extractor Estimating: 458it [04:22,  1.86it/s]Extractor Estimating: 459it [04:23,  1.86it/s]Extractor Estimating: 460it [04:23,  1.83it/s]Extractor Estimating: 461it [04:24,  1.83it/s]Extractor Estimating: 462it [04:24,  1.88it/s]Extractor Estimating: 463it [04:25,  1.88it/s]Extractor Estimating: 464it [04:25,  1.81it/s]Extractor Estimating: 465it [04:26,  1.81it/s]Extractor Estimating: 466it [04:27,  1.77it/s]Extractor Estimating: 467it [04:27,  1.80it/s]Extractor Estimating: 468it [04:28,  1.79it/s]Extractor Estimating: 469it [04:28,  1.80it/s]Extractor Estimating: 470it [04:29,  1.81it/s]Extractor Estimating: 471it [04:29,  1.82it/s]Extractor Estimating: 472it [04:30,  1.85it/s]Extractor Estimating: 473it [04:30,  1.83it/s]Extractor Estimating: 474it [04:31,  1.79it/s]Extractor Estimating: 475it [04:32,  1.84it/s]Extractor Estimating: 476it [04:32,  1.84it/s]Extractor Estimating: 477it [04:33,  1.81it/s]Extractor Estimating: 478it [04:33,  1.67it/s]Extractor Estimating: 479it [04:34,  1.63it/s]Extractor Estimating: 480it [04:35,  1.68it/s]Extractor Estimating: 481it [04:35,  1.70it/s]Extractor Estimating: 482it [04:36,  1.77it/s]Extractor Estimating: 483it [04:36,  1.80it/s]Extractor Estimating: 484it [04:37,  1.73it/s]Extractor Estimating: 485it [04:37,  1.77it/s]Extractor Estimating: 486it [04:38,  1.76it/s]Extractor Estimating: 487it [04:39,  1.69it/s]Extractor Estimating: 488it [04:39,  1.71it/s]Extractor Estimating: 489it [04:40,  1.76it/s]Extractor Estimating: 490it [04:40,  1.74it/s]Extractor Estimating: 491it [04:41,  1.74it/s]Extractor Estimating: 492it [04:41,  1.75it/s]Extractor Estimating: 493it [04:42,  1.69it/s]Extractor Estimating: 494it [04:43,  1.68it/s]Extractor Estimating: 495it [04:43,  1.68it/s]Extractor Estimating: 496it [04:44,  1.63it/s]Extractor Estimating: 497it [04:45,  1.60it/s]Extractor Estimating: 498it [04:45,  1.58it/s]Extractor Estimating: 499it [04:46,  1.56it/s]Extractor Estimating: 500it [04:46,  1.61it/s]Extractor Estimating: 500it [04:46,  1.74it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:33:58,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:33:58,761 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:33:58,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:33:58,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:33:58,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:33:59,469 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:33:59,470 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:34:00,066 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:34:01,172 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:34:01,172 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:34:04,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:34:04,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:34:04,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:34:04,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:34:04,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:34:04,967 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:34:04,968 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:34:05,553 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:34:05,760 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:34:05,760 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 23:20:38,888 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 23:20:38,911 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9989 mean pseudo reward: 0.9563147472256004
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 20338
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20438, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20438, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.964, loss:505.9869
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.960, loss:470.4369
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.965, loss:456.5990
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.971, loss:460.2941
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 0.946, loss:426.0046
>> valid entity prec:0.6110, rec:0.5570, f1:0.5828
>> valid relation prec:0.2162, rec:0.0967, f1:0.1336
>> valid relation with NER prec:0.2162, rec:0.0967, f1:0.1336
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.188, loss:419.3390
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 0.973, loss:471.5654
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 0.975, loss:468.3796
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 0.971, loss:455.5450
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 0.983, loss:468.1189
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6016, rec:0.5783, f1:0.5897
>> valid relation prec:0.2151, rec:0.0920, f1:0.1289
>> valid relation with NER prec:0.2151, rec:0.0920, f1:0.1289
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.189, loss:453.5492
g_step 1200, step 366, avg_time 0.959, loss:465.8366
g_step 1300, step 49, avg_time 0.958, loss:453.6101
g_step 1400, step 149, avg_time 0.951, loss:436.4471
g_step 1500, step 249, avg_time 0.964, loss:426.6854
>> valid entity prec:0.6156, rec:0.4993, f1:0.5514
>> valid relation prec:0.2110, rec:0.0769, f1:0.1127
>> valid relation with NER prec:0.2110, rec:0.0769, f1:0.1127
g_step 1600, step 349, avg_time 2.203, loss:447.5762
g_step 1700, step 32, avg_time 0.964, loss:428.4412
g_step 1800, step 132, avg_time 0.954, loss:395.4619
g_step 1900, step 232, avg_time 0.966, loss:434.4340
g_step 2000, step 332, avg_time 0.970, loss:426.9929
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6104, rec:0.5478, f1:0.5774
>> valid relation prec:0.2183, rec:0.0859, f1:0.1233
>> valid relation with NER prec:0.2183, rec:0.0859, f1:0.1233
g_step 2100, step 15, avg_time 2.179, loss:410.4154
g_step 2200, step 115, avg_time 0.963, loss:370.4608
g_step 2300, step 215, avg_time 0.966, loss:386.7962
g_step 2400, step 315, avg_time 0.960, loss:393.2341
g_step 2500, step 415, avg_time 0.976, loss:398.4099
>> valid entity prec:0.5840, rec:0.5685, f1:0.5761
>> valid relation prec:0.2047, rec:0.0911, f1:0.1261
>> valid relation with NER prec:0.2047, rec:0.0911, f1:0.1261
g_step 2600, step 98, avg_time 2.190, loss:343.1483
g_step 2700, step 198, avg_time 0.955, loss:372.3479
g_step 2800, step 298, avg_time 0.957, loss:390.3439
g_step 2900, step 398, avg_time 0.982, loss:384.6410
g_step 3000, step 81, avg_time 0.972, loss:366.8167
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5774, rec:0.5655, f1:0.5714
>> valid relation prec:0.2130, rec:0.0964, f1:0.1327
>> valid relation with NER prec:0.2130, rec:0.0964, f1:0.1327
g_step 3100, step 181, avg_time 2.195, loss:357.5139
g_step 3200, step 281, avg_time 0.966, loss:353.8965
g_step 3300, step 381, avg_time 0.962, loss:364.0837
g_step 3400, step 64, avg_time 0.959, loss:336.0171
g_step 3500, step 164, avg_time 0.975, loss:338.1953
>> valid entity prec:0.5892, rec:0.5381, f1:0.5625
>> valid relation prec:0.2212, rec:0.0868, f1:0.1247
>> valid relation with NER prec:0.2212, rec:0.0868, f1:0.1247
g_step 3600, step 264, avg_time 2.176, loss:360.5419
g_step 3700, step 364, avg_time 0.972, loss:348.2319
g_step 3800, step 47, avg_time 0.966, loss:317.8867
g_step 3900, step 147, avg_time 0.959, loss:334.7112
g_step 4000, step 247, avg_time 0.970, loss:333.7177
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5927, rec:0.5729, f1:0.5826
>> valid relation prec:0.2100, rec:0.0935, f1:0.1294
>> valid relation with NER prec:0.2100, rec:0.0935, f1:0.1294
g_step 4100, step 347, avg_time 2.184, loss:326.6351
g_step 4200, step 30, avg_time 0.966, loss:321.3564
g_step 4300, step 130, avg_time 0.940, loss:304.8451
g_step 4400, step 230, avg_time 0.962, loss:292.9196
g_step 4500, step 330, avg_time 0.969, loss:329.9402
>> valid entity prec:0.5550, rec:0.6365, f1:0.5930
>> valid relation prec:0.2013, rec:0.1086, f1:0.1411
>> valid relation with NER prec:0.2013, rec:0.1086, f1:0.1411
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4600, step 13, avg_time 2.202, loss:328.8901
g_step 4700, step 113, avg_time 0.976, loss:295.0255
g_step 4800, step 213, avg_time 0.967, loss:310.4530
g_step 4900, step 313, avg_time 0.954, loss:300.8210
g_step 5000, step 413, avg_time 0.952, loss:325.4636
learning rate was adjusted to 0.0008
>> valid entity prec:0.6031, rec:0.5057, f1:0.5502
>> valid relation prec:0.2145, rec:0.0775, f1:0.1139
>> valid relation with NER prec:0.2145, rec:0.0775, f1:0.1139
g_step 5100, step 96, avg_time 2.167, loss:273.0135
g_step 5200, step 196, avg_time 0.964, loss:297.4850
g_step 5300, step 296, avg_time 0.982, loss:279.6190
g_step 5400, step 396, avg_time 0.957, loss:316.9935
g_step 5500, step 79, avg_time 0.936, loss:267.1816
>> valid entity prec:0.5968, rec:0.5644, f1:0.5801
>> valid relation prec:0.2188, rec:0.1109, f1:0.1472
>> valid relation with NER prec:0.2188, rec:0.1109, f1:0.1472
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5600, step 179, avg_time 2.187, loss:274.7450
g_step 5700, step 279, avg_time 0.969, loss:289.7935
g_step 5800, step 379, avg_time 0.968, loss:280.9922
g_step 5900, step 62, avg_time 0.961, loss:272.5194
g_step 6000, step 162, avg_time 0.961, loss:262.6222
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5464, rec:0.6230, f1:0.5822
>> valid relation prec:0.1686, rec:0.0888, f1:0.1163
>> valid relation with NER prec:0.1686, rec:0.0888, f1:0.1163
g_step 6100, step 262, avg_time 2.212, loss:261.6710
g_step 6200, step 362, avg_time 0.945, loss:286.9977
g_step 6300, step 45, avg_time 0.954, loss:256.9696
g_step 6400, step 145, avg_time 0.958, loss:254.9645
g_step 6500, step 245, avg_time 0.951, loss:249.6530
>> valid entity prec:0.5889, rec:0.5921, f1:0.5905
>> valid relation prec:0.2063, rec:0.0987, f1:0.1335
>> valid relation with NER prec:0.2063, rec:0.0987, f1:0.1335
g_step 6600, step 345, avg_time 2.188, loss:269.8720
g_step 6700, step 28, avg_time 0.968, loss:264.8160
g_step 6800, step 128, avg_time 0.951, loss:251.9152
g_step 6900, step 228, avg_time 0.964, loss:243.6008
g_step 7000, step 328, avg_time 0.963, loss:257.5281
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5616, rec:0.5447, f1:0.5530
>> valid relation prec:0.1807, rec:0.0830, f1:0.1138
>> valid relation with NER prec:0.1807, rec:0.0830, f1:0.1138
g_step 7100, step 11, avg_time 2.169, loss:259.5752
g_step 7200, step 111, avg_time 0.963, loss:232.8038
g_step 7300, step 211, avg_time 0.981, loss:239.7203
g_step 7400, step 311, avg_time 0.954, loss:252.0527
g_step 7500, step 411, avg_time 0.956, loss:241.4686
>> valid entity prec:0.5628, rec:0.5701, f1:0.5664
>> valid relation prec:0.1946, rec:0.1016, f1:0.1335
>> valid relation with NER prec:0.1946, rec:0.1016, f1:0.1335
g_step 7600, step 94, avg_time 2.172, loss:225.8290
g_step 7700, step 194, avg_time 0.965, loss:232.8755
g_step 7800, step 294, avg_time 0.947, loss:225.7978
g_step 7900, step 394, avg_time 0.960, loss:245.3439
g_step 8000, step 77, avg_time 0.973, loss:223.5904
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5636, rec:0.5145, f1:0.5379
>> valid relation prec:0.1951, rec:0.0923, f1:0.1253
>> valid relation with NER prec:0.1951, rec:0.0923, f1:0.1253
g_step 8100, step 177, avg_time 2.175, loss:220.7025
g_step 8200, step 277, avg_time 0.950, loss:232.4156
g_step 8300, step 377, avg_time 0.996, loss:240.5942
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 23:20:38 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 23:20:38 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_23-20-38_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 23:20:40 - WARNING - datasets.builder -   Using custom data configuration default-824ab90da89b6e7c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-824ab90da89b6e7c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 23:20:40,952 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:20:40,954 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:20:40,954 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:20:40,955 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:20:40,971 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:20:40,979 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:20:40,979 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:20:40,979 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:20:40,979 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:20:40,979 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:20:40,979 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 23:20:41,174 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:20:44,268 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 23:20:44,304 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-824ab90da89b6e7c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.42ba/s] 20%|██        | 2/10 [00:00<00:01,  4.20ba/s] 30%|███       | 3/10 [00:00<00:01,  4.54ba/s] 40%|████      | 4/10 [00:00<00:01,  4.71ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.81ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.85ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.88ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.92ba/s] 90%|█████████ | 9/10 [00:01<00:00,  4.93ba/s]100%|██████████| 10/10 [00:02<00:00,  4.93ba/s]100%|██████████| 10/10 [00:02<00:00,  4.76ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.91ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.29ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.40ba/s]100%|██████████| 4/4 [00:00<00:00,  5.63ba/s]100%|██████████| 4/4 [00:00<00:00,  4.95ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:00,  9.01ba/s] 20%|██        | 2/10 [00:00<00:01,  5.44ba/s] 40%|████      | 4/10 [00:00<00:00,  8.00ba/s] 60%|██████    | 6/10 [00:00<00:00,  9.08ba/s] 80%|████████  | 8/10 [00:00<00:00,  9.65ba/s]100%|██████████| 10/10 [00:01<00:00, 10.10ba/s]100%|██████████| 10/10 [00:01<00:00,  9.23ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.04ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.73ba/s]100%|██████████| 4/4 [00:00<00:00, 11.15ba/s]
[INFO|trainer.py:414] 2023-08-28 23:20:49,173 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 23:20:49,191 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 23:20:49,192 >>   Num examples = 9999
[INFO|trainer.py:1149] 2023-08-28 23:20:49,192 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 23:20:49,192 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 23:20:49,192 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 23:20:49,192 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 23:20:49,192 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:54,  3.32it/s]  0%|          | 2/780 [00:00<03:50,  3.38it/s]  0%|          | 3/780 [00:00<03:48,  3.40it/s]  1%|          | 4/780 [00:01<03:47,  3.41it/s]  1%|          | 5/780 [00:01<03:48,  3.40it/s]  1%|          | 6/780 [00:01<03:47,  3.41it/s]  1%|          | 7/780 [00:02<03:47,  3.40it/s]  1%|          | 8/780 [00:02<03:47,  3.39it/s]  1%|          | 9/780 [00:02<03:48,  3.38it/s]  1%|▏         | 10/780 [00:02<03:48,  3.38it/s]  1%|▏         | 11/780 [00:03<03:47,  3.37it/s]  2%|▏         | 12/780 [00:03<03:47,  3.37it/s]  2%|▏         | 13/780 [00:03<03:47,  3.37it/s]  2%|▏         | 14/780 [00:04<03:47,  3.37it/s]  2%|▏         | 15/780 [00:04<03:47,  3.37it/s]  2%|▏         | 16/780 [00:04<04:09,  3.06it/s]  2%|▏         | 17/780 [00:05<04:02,  3.15it/s]  2%|▏         | 18/780 [00:05<03:57,  3.21it/s]  2%|▏         | 19/780 [00:05<03:53,  3.25it/s]  3%|▎         | 20/780 [00:06<03:51,  3.28it/s]  3%|▎         | 21/780 [00:06<03:49,  3.31it/s]  3%|▎         | 22/780 [00:06<03:48,  3.32it/s]  3%|▎         | 23/780 [00:06<03:46,  3.34it/s]  3%|▎         | 24/780 [00:07<03:46,  3.34it/s]  3%|▎         | 25/780 [00:07<03:44,  3.36it/s]  3%|▎         | 26/780 [00:07<04:14,  2.97it/s]  3%|▎         | 27/780 [00:08<04:04,  3.07it/s]  4%|▎         | 28/780 [00:08<03:58,  3.16it/s]  4%|▎         | 29/780 [00:08<03:53,  3.22it/s]  4%|▍         | 30/780 [00:09<03:49,  3.26it/s]  4%|▍         | 31/780 [00:09<03:47,  3.29it/s]  4%|▍         | 32/780 [00:09<03:45,  3.31it/s]  4%|▍         | 33/780 [00:10<03:44,  3.33it/s]  4%|▍         | 34/780 [00:10<03:43,  3.34it/s]  4%|▍         | 35/780 [00:10<03:42,  3.34it/s]  5%|▍         | 36/780 [00:11<04:04,  3.04it/s]  5%|▍         | 37/780 [00:11<03:57,  3.13it/s]  5%|▍         | 38/780 [00:11<03:51,  3.20it/s]  5%|▌         | 39/780 [00:11<03:48,  3.25it/s]  5%|▌         | 40/780 [00:12<03:45,  3.28it/s]  5%|▌         | 41/780 [00:12<03:43,  3.30it/s]  5%|▌         | 42/780 [00:12<03:42,  3.32it/s]  6%|▌         | 43/780 [00:13<03:41,  3.33it/s]  6%|▌         | 44/780 [00:13<03:40,  3.34it/s]  6%|▌         | 45/780 [00:13<03:39,  3.35it/s]  6%|▌         | 46/780 [00:13<03:40,  3.33it/s]  6%|▌         | 47/780 [00:14<03:39,  3.34it/s]  6%|▌         | 48/780 [00:14<03:38,  3.34it/s]  6%|▋         | 49/780 [00:14<03:38,  3.35it/s]  6%|▋         | 50/780 [00:15<03:37,  3.35it/s]  7%|▋         | 51/780 [00:15<03:37,  3.36it/s]  7%|▋         | 52/780 [00:15<03:36,  3.36it/s]  7%|▋         | 53/780 [00:16<03:36,  3.36it/s]  7%|▋         | 54/780 [00:16<03:36,  3.36it/s]  7%|▋         | 55/780 [00:16<03:35,  3.36it/s]  7%|▋         | 56/780 [00:16<03:35,  3.36it/s]  7%|▋         | 57/780 [00:17<03:35,  3.35it/s]  7%|▋         | 58/780 [00:17<03:35,  3.35it/s]  8%|▊         | 59/780 [00:17<03:34,  3.36it/s]  8%|▊         | 60/780 [00:18<03:34,  3.36it/s]  8%|▊         | 61/780 [00:18<03:33,  3.36it/s]  8%|▊         | 62/780 [00:18<03:33,  3.36it/s]  8%|▊         | 63/780 [00:19<03:33,  3.36it/s]  8%|▊         | 64/780 [00:19<03:32,  3.36it/s]  8%|▊         | 65/780 [00:19<03:32,  3.36it/s]  8%|▊         | 66/780 [00:19<03:32,  3.36it/s]  9%|▊         | 67/780 [00:20<03:32,  3.36it/s]  9%|▊         | 68/780 [00:20<03:32,  3.35it/s]  9%|▉         | 69/780 [00:20<03:31,  3.36it/s]  9%|▉         | 70/780 [00:21<03:30,  3.37it/s]  9%|▉         | 71/780 [00:21<03:29,  3.39it/s]  9%|▉         | 72/780 [00:21<03:28,  3.39it/s]  9%|▉         | 73/780 [00:22<03:28,  3.40it/s]  9%|▉         | 74/780 [00:22<03:27,  3.40it/s] 10%|▉         | 75/780 [00:22<03:27,  3.41it/s] 10%|▉         | 76/780 [00:22<03:26,  3.40it/s] 10%|▉         | 77/780 [00:23<03:26,  3.41it/s] 10%|█         | 78/780 [00:23<03:25,  3.41it/s] 10%|█         | 79/780 [00:23<03:26,  3.39it/s] 10%|█         | 80/780 [00:24<03:26,  3.40it/s] 10%|█         | 81/780 [00:24<03:26,  3.38it/s] 11%|█         | 82/780 [00:24<03:25,  3.39it/s] 11%|█         | 83/780 [00:24<03:25,  3.40it/s] 11%|█         | 84/780 [00:25<03:24,  3.40it/s] 11%|█         | 85/780 [00:25<03:24,  3.40it/s] 11%|█         | 86/780 [00:25<03:23,  3.41it/s] 11%|█         | 87/780 [00:26<03:23,  3.41it/s] 11%|█▏        | 88/780 [00:26<03:23,  3.41it/s] 11%|█▏        | 89/780 [00:26<03:22,  3.41it/s] 12%|█▏        | 90/780 [00:27<03:22,  3.41it/s] 12%|█▏        | 91/780 [00:27<03:23,  3.38it/s] 12%|█▏        | 92/780 [00:27<03:23,  3.39it/s] 12%|█▏        | 93/780 [00:27<03:22,  3.39it/s] 12%|█▏        | 94/780 [00:28<03:21,  3.40it/s] 12%|█▏        | 95/780 [00:28<03:21,  3.40it/s] 12%|█▏        | 96/780 [00:28<03:21,  3.40it/s] 12%|█▏        | 97/780 [00:29<03:20,  3.40it/s] 13%|█▎        | 98/780 [00:29<03:20,  3.40it/s] 13%|█▎        | 99/780 [00:29<03:19,  3.41it/s] 13%|█▎        | 100/780 [00:29<03:19,  3.41it/s] 13%|█▎        | 101/780 [00:30<03:19,  3.41it/s] 13%|█▎        | 102/780 [00:30<03:19,  3.39it/s] 13%|█▎        | 103/780 [00:30<03:19,  3.40it/s] 13%|█▎        | 104/780 [00:31<03:18,  3.40it/s] 13%|█▎        | 105/780 [00:31<03:18,  3.40it/s] 14%|█▎        | 106/780 [00:31<03:17,  3.41it/s] 14%|█▎        | 107/780 [00:32<03:17,  3.41it/s] 14%|█▍        | 108/780 [00:32<03:19,  3.38it/s] 14%|█▍        | 109/780 [00:32<03:18,  3.39it/s] 14%|█▍        | 110/780 [00:32<03:17,  3.39it/s] 14%|█▍        | 111/780 [00:33<03:16,  3.40it/s] 14%|█▍        | 112/780 [00:33<03:16,  3.40it/s] 14%|█▍        | 113/780 [00:33<03:19,  3.34it/s] 15%|█▍        | 114/780 [00:34<03:18,  3.36it/s] 15%|█▍        | 115/780 [00:34<03:17,  3.37it/s] 15%|█▍        | 116/780 [00:34<03:16,  3.38it/s] 15%|█▌        | 117/780 [00:34<03:15,  3.39it/s] 15%|█▌        | 118/780 [00:35<03:14,  3.40it/s] 15%|█▌        | 119/780 [00:35<03:14,  3.40it/s] 15%|█▌        | 120/780 [00:35<03:14,  3.40it/s] 16%|█▌        | 121/780 [00:36<03:13,  3.40it/s] 16%|█▌        | 122/780 [00:36<03:13,  3.40it/s] 16%|█▌        | 123/780 [00:36<03:12,  3.41it/s] 16%|█▌        | 124/780 [00:37<03:13,  3.39it/s] 16%|█▌        | 125/780 [00:37<03:13,  3.39it/s] 16%|█▌        | 126/780 [00:37<03:12,  3.40it/s] 16%|█▋        | 127/780 [00:37<03:12,  3.40it/s] 16%|█▋        | 128/780 [00:38<03:11,  3.40it/s] 17%|█▋        | 129/780 [00:38<03:11,  3.40it/s] 17%|█▋        | 130/780 [00:38<03:10,  3.40it/s] 17%|█▋        | 131/780 [00:39<03:10,  3.40it/s] 17%|█▋        | 132/780 [00:39<03:10,  3.40it/s] 17%|█▋        | 133/780 [00:39<03:10,  3.40it/s] 17%|█▋        | 134/780 [00:39<03:09,  3.40it/s] 17%|█▋        | 135/780 [00:40<03:10,  3.39it/s] 17%|█▋        | 136/780 [00:40<03:09,  3.39it/s] 18%|█▊        | 137/780 [00:40<03:09,  3.40it/s] 18%|█▊        | 138/780 [00:41<03:08,  3.40it/s] 18%|█▊        | 139/780 [00:41<03:08,  3.40it/s] 18%|█▊        | 140/780 [00:41<03:08,  3.40it/s] 18%|█▊        | 141/780 [00:42<03:07,  3.40it/s] 18%|█▊        | 142/780 [00:42<03:07,  3.40it/s] 18%|█▊        | 143/780 [00:42<03:07,  3.40it/s] 18%|█▊        | 144/780 [00:42<03:06,  3.40it/s] 19%|█▊        | 145/780 [00:43<03:06,  3.40it/s] 19%|█▊        | 146/780 [00:43<03:06,  3.40it/s] 19%|█▉        | 147/780 [00:43<03:06,  3.40it/s] 19%|█▉        | 148/780 [00:44<03:05,  3.40it/s] 19%|█▉        | 149/780 [00:44<03:05,  3.40it/s] 19%|█▉        | 150/780 [00:44<03:05,  3.40it/s] 19%|█▉        | 151/780 [00:44<03:04,  3.40it/s] 19%|█▉        | 152/780 [00:45<03:04,  3.40it/s] 20%|█▉        | 153/780 [00:45<03:04,  3.41it/s] 20%|█▉        | 154/780 [00:45<03:03,  3.41it/s] 20%|█▉        | 155/780 [00:46<03:03,  3.40it/s] 20%|██        | 156/780 [00:46<03:03,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 23:21:35,672 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:21:35,672 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 23:21:35,672 >>   Batch size = 8

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 54.45it/s][A
  3%|▎         | 12/431 [00:00<00:08, 46.79it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.33it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.58it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.14it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.76it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.44it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.44it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.46it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.60it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.54it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.61it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.55it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.42it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.31it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.31it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.29it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.44it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.49it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.46it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.41it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.43it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.35it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.32it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.25it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.38it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.34it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 41.49it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 42.28it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 42.71it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 42.81it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 42.95it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.11it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.17it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.24it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.10it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.10it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.39it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.45it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.35it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.45it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.37it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.39it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.37it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.27it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.38it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.42it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.43it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.45it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.43it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.44it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.39it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.27it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.33it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.48it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.48it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.45it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.41it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.38it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.16it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.29it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.29it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.30it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.33it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.47it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.48it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.49it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.43it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.34it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.37it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.38it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.38it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.40it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.36it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.41it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.44it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.30it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.22it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.30it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.39it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.42it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.44it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.49it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.47it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.45it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.45it/s][A 20%|██        | 156/780 [00:56<03:03,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:21:45,691 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 23:21:45,749 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:21:48,670 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:21:48,685 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:21:48,692 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:05<1:02:07,  5.98s/it] 20%|██        | 158/780 [01:05<44:21,  4.28s/it]   20%|██        | 159/780 [01:06<31:55,  3.08s/it] 21%|██        | 160/780 [01:06<23:13,  2.25s/it] 21%|██        | 161/780 [01:06<17:09,  1.66s/it] 21%|██        | 162/780 [01:07<12:54,  1.25s/it] 21%|██        | 163/780 [01:07<09:56,  1.04it/s] 21%|██        | 164/780 [01:07<07:51,  1.31it/s] 21%|██        | 165/780 [01:08<06:24,  1.60it/s] 21%|██▏       | 166/780 [01:08<05:23,  1.90it/s] 21%|██▏       | 167/780 [01:08<04:40,  2.18it/s] 22%|██▏       | 168/780 [01:08<04:10,  2.44it/s] 22%|██▏       | 169/780 [01:09<03:51,  2.64it/s] 22%|██▏       | 170/780 [01:09<03:36,  2.82it/s] 22%|██▏       | 171/780 [01:09<03:25,  2.97it/s] 22%|██▏       | 172/780 [01:10<03:17,  3.08it/s] 22%|██▏       | 173/780 [01:10<03:12,  3.16it/s] 22%|██▏       | 174/780 [01:10<03:08,  3.22it/s] 22%|██▏       | 175/780 [01:11<03:05,  3.26it/s] 23%|██▎       | 176/780 [01:11<03:03,  3.29it/s] 23%|██▎       | 177/780 [01:11<03:01,  3.31it/s] 23%|██▎       | 178/780 [01:11<03:00,  3.33it/s] 23%|██▎       | 179/780 [01:12<02:59,  3.34it/s] 23%|██▎       | 180/780 [01:12<03:01,  3.31it/s] 23%|██▎       | 181/780 [01:12<03:00,  3.33it/s] 23%|██▎       | 182/780 [01:13<02:59,  3.34it/s] 23%|██▎       | 183/780 [01:13<02:58,  3.35it/s] 24%|██▎       | 184/780 [01:13<02:57,  3.35it/s] 24%|██▎       | 185/780 [01:14<02:57,  3.35it/s] 24%|██▍       | 186/780 [01:14<02:57,  3.36it/s] 24%|██▍       | 187/780 [01:14<02:56,  3.36it/s] 24%|██▍       | 188/780 [01:14<02:56,  3.36it/s] 24%|██▍       | 189/780 [01:15<02:55,  3.36it/s] 24%|██▍       | 190/780 [01:15<02:55,  3.37it/s] 24%|██▍       | 191/780 [01:15<02:56,  3.33it/s] 25%|██▍       | 192/780 [01:16<02:55,  3.34it/s] 25%|██▍       | 193/780 [01:16<02:55,  3.35it/s] 25%|██▍       | 194/780 [01:16<02:54,  3.35it/s] 25%|██▌       | 195/780 [01:17<02:54,  3.36it/s] 25%|██▌       | 196/780 [01:17<02:53,  3.36it/s] 25%|██▌       | 197/780 [01:17<02:52,  3.37it/s] 25%|██▌       | 198/780 [01:17<02:51,  3.38it/s] 26%|██▌       | 199/780 [01:18<02:51,  3.39it/s] 26%|██▌       | 200/780 [01:18<02:50,  3.39it/s] 26%|██▌       | 201/780 [01:18<02:50,  3.40it/s] 26%|██▌       | 202/780 [01:19<02:50,  3.38it/s] 26%|██▌       | 203/780 [01:19<02:50,  3.39it/s] 26%|██▌       | 204/780 [01:19<02:49,  3.40it/s] 26%|██▋       | 205/780 [01:19<02:49,  3.40it/s] 26%|██▋       | 206/780 [01:20<02:48,  3.40it/s] 27%|██▋       | 207/780 [01:20<02:48,  3.40it/s] 27%|██▋       | 208/780 [01:20<02:47,  3.41it/s] 27%|██▋       | 209/780 [01:21<02:47,  3.41it/s] 27%|██▋       | 210/780 [01:21<02:47,  3.41it/s] 27%|██▋       | 211/780 [01:21<02:46,  3.41it/s] 27%|██▋       | 212/780 [01:22<02:46,  3.41it/s] 27%|██▋       | 213/780 [01:22<02:47,  3.39it/s] 27%|██▋       | 214/780 [01:22<02:46,  3.40it/s] 28%|██▊       | 215/780 [01:22<02:46,  3.40it/s] 28%|██▊       | 216/780 [01:23<02:45,  3.40it/s] 28%|██▊       | 217/780 [01:23<02:45,  3.40it/s] 28%|██▊       | 218/780 [01:23<02:44,  3.41it/s] 28%|██▊       | 219/780 [01:24<02:44,  3.41it/s] 28%|██▊       | 220/780 [01:24<02:45,  3.39it/s] 28%|██▊       | 221/780 [01:24<02:44,  3.40it/s] 28%|██▊       | 222/780 [01:24<02:44,  3.40it/s] 29%|██▊       | 223/780 [01:25<02:43,  3.40it/s] 29%|██▊       | 224/780 [01:25<02:44,  3.39it/s] 29%|██▉       | 225/780 [01:25<02:43,  3.40it/s] 29%|██▉       | 226/780 [01:26<02:42,  3.40it/s] 29%|██▉       | 227/780 [01:26<02:42,  3.40it/s] 29%|██▉       | 228/780 [01:26<02:42,  3.40it/s] 29%|██▉       | 229/780 [01:27<02:41,  3.40it/s] 29%|██▉       | 230/780 [01:27<02:42,  3.39it/s] 30%|██▉       | 231/780 [01:27<02:41,  3.39it/s] 30%|██▉       | 232/780 [01:27<02:41,  3.40it/s] 30%|██▉       | 233/780 [01:28<02:40,  3.40it/s] 30%|███       | 234/780 [01:28<02:40,  3.40it/s] 30%|███       | 235/780 [01:28<02:41,  3.37it/s] 30%|███       | 236/780 [01:29<02:41,  3.38it/s] 30%|███       | 237/780 [01:29<02:40,  3.39it/s] 31%|███       | 238/780 [01:29<02:39,  3.39it/s] 31%|███       | 239/780 [01:29<02:39,  3.39it/s] 31%|███       | 240/780 [01:30<02:39,  3.40it/s] 31%|███       | 241/780 [01:30<02:38,  3.40it/s] 31%|███       | 242/780 [01:30<02:38,  3.40it/s] 31%|███       | 243/780 [01:31<02:37,  3.40it/s] 31%|███▏      | 244/780 [01:31<02:37,  3.41it/s] 31%|███▏      | 245/780 [01:31<02:37,  3.41it/s] 32%|███▏      | 246/780 [01:32<02:38,  3.36it/s] 32%|███▏      | 247/780 [01:32<02:38,  3.35it/s] 32%|███▏      | 248/780 [01:32<02:37,  3.37it/s] 32%|███▏      | 249/780 [01:32<02:37,  3.38it/s] 32%|███▏      | 250/780 [01:33<02:36,  3.39it/s] 32%|███▏      | 251/780 [01:33<02:35,  3.39it/s] 32%|███▏      | 252/780 [01:33<02:35,  3.40it/s] 32%|███▏      | 253/780 [01:34<02:35,  3.40it/s] 33%|███▎      | 254/780 [01:34<02:34,  3.40it/s] 33%|███▎      | 255/780 [01:34<02:34,  3.40it/s] 33%|███▎      | 256/780 [01:34<02:33,  3.40it/s] 33%|███▎      | 257/780 [01:35<02:34,  3.38it/s] 33%|███▎      | 258/780 [01:35<02:33,  3.39it/s] 33%|███▎      | 259/780 [01:35<02:33,  3.40it/s] 33%|███▎      | 260/780 [01:36<02:32,  3.40it/s] 33%|███▎      | 261/780 [01:36<02:32,  3.40it/s] 34%|███▎      | 262/780 [01:36<02:32,  3.40it/s] 34%|███▎      | 263/780 [01:37<02:31,  3.40it/s] 34%|███▍      | 264/780 [01:37<02:31,  3.40it/s] 34%|███▍      | 265/780 [01:37<02:31,  3.41it/s] 34%|███▍      | 266/780 [01:37<02:30,  3.41it/s] 34%|███▍      | 267/780 [01:38<02:30,  3.41it/s] 34%|███▍      | 268/780 [01:38<02:30,  3.39it/s] 34%|███▍      | 269/780 [01:38<02:30,  3.40it/s] 35%|███▍      | 270/780 [01:39<02:29,  3.40it/s] 35%|███▍      | 271/780 [01:39<02:29,  3.40it/s] 35%|███▍      | 272/780 [01:39<02:29,  3.40it/s] 35%|███▌      | 273/780 [01:39<02:29,  3.40it/s] 35%|███▌      | 274/780 [01:40<02:28,  3.41it/s] 35%|███▌      | 275/780 [01:40<02:28,  3.40it/s] 35%|███▌      | 276/780 [01:40<02:28,  3.40it/s] 36%|███▌      | 277/780 [01:41<02:27,  3.41it/s] 36%|███▌      | 278/780 [01:41<02:27,  3.40it/s] 36%|███▌      | 279/780 [01:41<02:28,  3.37it/s] 36%|███▌      | 280/780 [01:42<02:27,  3.38it/s] 36%|███▌      | 281/780 [01:42<02:27,  3.39it/s] 36%|███▌      | 282/780 [01:42<02:26,  3.39it/s] 36%|███▋      | 283/780 [01:42<02:26,  3.40it/s] 36%|███▋      | 284/780 [01:43<02:25,  3.40it/s] 37%|███▋      | 285/780 [01:43<02:25,  3.40it/s] 37%|███▋      | 286/780 [01:43<02:25,  3.40it/s] 37%|███▋      | 287/780 [01:44<02:24,  3.40it/s] 37%|███▋      | 288/780 [01:44<02:24,  3.41it/s] 37%|███▋      | 289/780 [01:44<02:24,  3.41it/s] 37%|███▋      | 290/780 [01:44<02:24,  3.38it/s] 37%|███▋      | 291/780 [01:45<02:24,  3.39it/s] 37%|███▋      | 292/780 [01:45<02:23,  3.39it/s] 38%|███▊      | 293/780 [01:45<02:23,  3.40it/s] 38%|███▊      | 294/780 [01:46<02:23,  3.40it/s] 38%|███▊      | 295/780 [01:46<02:22,  3.40it/s] 38%|███▊      | 296/780 [01:46<02:22,  3.40it/s] 38%|███▊      | 297/780 [01:47<02:22,  3.40it/s] 38%|███▊      | 298/780 [01:47<02:21,  3.40it/s] 38%|███▊      | 299/780 [01:47<02:21,  3.40it/s] 38%|███▊      | 300/780 [01:47<02:21,  3.40it/s] 39%|███▊      | 301/780 [01:48<02:21,  3.39it/s] 39%|███▊      | 302/780 [01:48<02:20,  3.39it/s] 39%|███▉      | 303/780 [01:48<02:20,  3.40it/s] 39%|███▉      | 304/780 [01:49<02:20,  3.40it/s] 39%|███▉      | 305/780 [01:49<02:19,  3.40it/s] 39%|███▉      | 306/780 [01:49<02:19,  3.40it/s] 39%|███▉      | 307/780 [01:50<02:30,  3.15it/s] 39%|███▉      | 308/780 [01:50<02:26,  3.23it/s] 40%|███▉      | 309/780 [01:50<02:23,  3.28it/s] 40%|███▉      | 310/780 [01:50<02:21,  3.32it/s] 40%|███▉      | 311/780 [01:51<02:20,  3.34it/s] 40%|████      | 312/780 [01:51<02:19,  3.36it/s][INFO|trainer.py:2140] 2023-08-28 23:22:40,763 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:22:40,763 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 23:22:40,763 >>   Batch size = 8
{'eval_loss': 1.0042188167572021, 'eval_runtime': 9.9776, 'eval_samples_per_second': 345.373, 'eval_steps_per_second': 43.197, 'epoch': 1.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 53.99it/s][A
  3%|▎         | 12/431 [00:00<00:09, 46.53it/s][A
  4%|▍         | 17/431 [00:00<00:09, 44.82it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.18it/s][A
  6%|▋         | 27/431 [00:00<00:09, 43.88it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.73it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.60it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.52it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.55it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.67it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.54it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.30it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.23it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.28it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.41it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.39it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.41it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.48it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.53it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.42it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.23it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.19it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.26it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.41it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 43.47it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.48it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.46it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.50it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.33it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.25it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.24it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.25it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.43it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.44it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.45it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.50it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.41it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.26it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.18it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.28it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.37it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.36it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.44it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.47it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.38it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.31it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.29it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.30it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.34it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.28it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.43it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.49it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.43it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.31it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.28it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.39it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.36it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.37it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.35it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.36it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.12it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.33it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.28it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.38it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.35it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.31it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.43it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.29it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.36it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.33it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.34it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.30it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.37it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.35it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.43it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.38it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.39it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.42it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.28it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.25it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.32it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.34it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.44it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.40it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.31it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.31it/s][A 40%|████      | 312/780 [02:01<02:19,  3.36it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:22:50,760 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 23:22:50,803 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:22:53,491 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:22:53,512 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:22:53,520 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:10<45:54,  5.90s/it] 40%|████      | 314/780 [02:10<32:45,  4.22s/it] 40%|████      | 315/780 [02:11<23:34,  3.04s/it] 41%|████      | 316/780 [02:11<17:09,  2.22s/it] 41%|████      | 317/780 [02:11<12:40,  1.64s/it] 41%|████      | 318/780 [02:11<09:32,  1.24s/it] 41%|████      | 319/780 [02:12<07:20,  1.05it/s] 41%|████      | 320/780 [02:12<05:49,  1.32it/s] 41%|████      | 321/780 [02:12<04:44,  1.61it/s] 41%|████▏     | 322/780 [02:13<03:59,  1.91it/s] 41%|████▏     | 323/780 [02:13<03:28,  2.19it/s] 42%|████▏     | 324/780 [02:13<03:06,  2.45it/s] 42%|████▏     | 325/780 [02:14<02:52,  2.64it/s] 42%|████▏     | 326/780 [02:14<02:41,  2.82it/s] 42%|████▏     | 327/780 [02:14<02:32,  2.96it/s] 42%|████▏     | 328/780 [02:14<02:26,  3.08it/s] 42%|████▏     | 329/780 [02:15<02:21,  3.18it/s] 42%|████▏     | 330/780 [02:15<02:18,  3.24it/s] 42%|████▏     | 331/780 [02:15<02:16,  3.29it/s] 43%|████▎     | 332/780 [02:16<02:14,  3.32it/s] 43%|████▎     | 333/780 [02:16<02:13,  3.35it/s] 43%|████▎     | 334/780 [02:16<02:12,  3.36it/s] 43%|████▎     | 335/780 [02:17<02:11,  3.37it/s] 43%|████▎     | 336/780 [02:17<02:13,  3.33it/s] 43%|████▎     | 337/780 [02:17<02:12,  3.35it/s] 43%|████▎     | 338/780 [02:17<02:11,  3.37it/s] 43%|████▎     | 339/780 [02:18<02:10,  3.38it/s] 44%|████▎     | 340/780 [02:18<02:09,  3.39it/s] 44%|████▎     | 341/780 [02:18<02:09,  3.39it/s] 44%|████▍     | 342/780 [02:19<02:08,  3.40it/s] 44%|████▍     | 343/780 [02:19<02:08,  3.40it/s] 44%|████▍     | 344/780 [02:19<02:08,  3.40it/s] 44%|████▍     | 345/780 [02:19<02:07,  3.40it/s] 44%|████▍     | 346/780 [02:20<02:07,  3.40it/s] 44%|████▍     | 347/780 [02:20<02:08,  3.38it/s] 45%|████▍     | 348/780 [02:20<02:07,  3.39it/s] 45%|████▍     | 349/780 [02:21<02:07,  3.39it/s] 45%|████▍     | 350/780 [02:21<02:06,  3.39it/s] 45%|████▌     | 351/780 [02:21<02:06,  3.40it/s] 45%|████▌     | 352/780 [02:22<02:05,  3.40it/s] 45%|████▌     | 353/780 [02:22<02:05,  3.40it/s] 45%|████▌     | 354/780 [02:22<02:05,  3.40it/s] 46%|████▌     | 355/780 [02:22<02:04,  3.40it/s] 46%|████▌     | 356/780 [02:23<02:04,  3.40it/s] 46%|████▌     | 357/780 [02:23<02:04,  3.40it/s] 46%|████▌     | 358/780 [02:23<02:05,  3.37it/s] 46%|████▌     | 359/780 [02:24<02:04,  3.38it/s] 46%|████▌     | 360/780 [02:24<02:04,  3.37it/s] 46%|████▋     | 361/780 [02:24<02:03,  3.38it/s] 46%|████▋     | 362/780 [02:24<02:03,  3.39it/s] 47%|████▋     | 363/780 [02:25<02:02,  3.39it/s] 47%|████▋     | 364/780 [02:25<02:02,  3.40it/s] 47%|████▋     | 365/780 [02:25<02:02,  3.40it/s] 47%|████▋     | 366/780 [02:26<02:01,  3.40it/s] 47%|████▋     | 367/780 [02:26<02:01,  3.40it/s] 47%|████▋     | 368/780 [02:26<02:01,  3.40it/s] 47%|████▋     | 369/780 [02:27<02:01,  3.39it/s] 47%|████▋     | 370/780 [02:27<02:02,  3.36it/s] 48%|████▊     | 371/780 [02:27<02:01,  3.37it/s] 48%|████▊     | 372/780 [02:27<02:00,  3.38it/s] 48%|████▊     | 373/780 [02:28<02:00,  3.39it/s] 48%|████▊     | 374/780 [02:28<01:59,  3.39it/s] 48%|████▊     | 375/780 [02:28<01:59,  3.39it/s] 48%|████▊     | 376/780 [02:29<01:58,  3.40it/s] 48%|████▊     | 377/780 [02:29<01:58,  3.40it/s] 48%|████▊     | 378/780 [02:29<01:58,  3.40it/s] 49%|████▊     | 379/780 [02:30<01:57,  3.40it/s] 49%|████▊     | 380/780 [02:30<01:58,  3.38it/s] 49%|████▉     | 381/780 [02:30<01:57,  3.39it/s] 49%|████▉     | 382/780 [02:30<01:57,  3.39it/s] 49%|████▉     | 383/780 [02:31<01:57,  3.39it/s] 49%|████▉     | 384/780 [02:31<01:56,  3.40it/s] 49%|████▉     | 385/780 [02:31<01:56,  3.40it/s] 49%|████▉     | 386/780 [02:32<01:55,  3.40it/s] 50%|████▉     | 387/780 [02:32<01:55,  3.40it/s] 50%|████▉     | 388/780 [02:32<01:55,  3.40it/s] 50%|████▉     | 389/780 [02:32<01:55,  3.40it/s] 50%|█████     | 390/780 [02:33<01:54,  3.40it/s] 50%|█████     | 391/780 [02:33<01:55,  3.38it/s] 50%|█████     | 392/780 [02:33<01:54,  3.39it/s] 50%|█████     | 393/780 [02:34<01:54,  3.39it/s] 51%|█████     | 394/780 [02:34<01:53,  3.39it/s] 51%|█████     | 395/780 [02:34<01:53,  3.40it/s] 51%|█████     | 396/780 [02:35<01:52,  3.40it/s] 51%|█████     | 397/780 [02:35<01:52,  3.40it/s] 51%|█████     | 398/780 [02:35<01:52,  3.40it/s] 51%|█████     | 399/780 [02:35<01:52,  3.40it/s] 51%|█████▏    | 400/780 [02:36<01:51,  3.40it/s] 51%|█████▏    | 401/780 [02:36<01:51,  3.40it/s] 52%|█████▏    | 402/780 [02:36<01:52,  3.37it/s] 52%|█████▏    | 403/780 [02:37<01:51,  3.38it/s] 52%|█████▏    | 404/780 [02:37<01:50,  3.39it/s] 52%|█████▏    | 405/780 [02:37<01:50,  3.39it/s] 52%|█████▏    | 406/780 [02:37<01:50,  3.39it/s] 52%|█████▏    | 407/780 [02:38<01:49,  3.40it/s] 52%|█████▏    | 408/780 [02:38<01:49,  3.40it/s] 52%|█████▏    | 409/780 [02:38<01:49,  3.40it/s] 53%|█████▎    | 410/780 [02:39<01:48,  3.40it/s] 53%|█████▎    | 411/780 [02:39<01:48,  3.40it/s] 53%|█████▎    | 412/780 [02:39<01:48,  3.40it/s] 53%|█████▎    | 413/780 [02:40<01:48,  3.39it/s] 53%|█████▎    | 414/780 [02:40<01:47,  3.39it/s] 53%|█████▎    | 415/780 [02:40<01:47,  3.40it/s] 53%|█████▎    | 416/780 [02:40<01:47,  3.40it/s] 53%|█████▎    | 417/780 [02:41<01:46,  3.40it/s] 54%|█████▎    | 418/780 [02:41<01:46,  3.40it/s] 54%|█████▎    | 419/780 [02:41<01:46,  3.40it/s] 54%|█████▍    | 420/780 [02:42<01:45,  3.40it/s] 54%|█████▍    | 421/780 [02:42<01:45,  3.40it/s] 54%|█████▍    | 422/780 [02:42<01:45,  3.40it/s] 54%|█████▍    | 423/780 [02:42<01:44,  3.40it/s] 54%|█████▍    | 424/780 [02:43<01:44,  3.39it/s] 54%|█████▍    | 425/780 [02:43<01:44,  3.40it/s] 55%|█████▍    | 426/780 [02:43<01:44,  3.40it/s] 55%|█████▍    | 427/780 [02:44<01:43,  3.40it/s] 55%|█████▍    | 428/780 [02:44<01:43,  3.40it/s] 55%|█████▌    | 429/780 [02:44<01:43,  3.40it/s] 55%|█████▌    | 430/780 [02:45<01:43,  3.40it/s] 55%|█████▌    | 431/780 [02:45<01:42,  3.40it/s] 55%|█████▌    | 432/780 [02:45<01:42,  3.40it/s] 56%|█████▌    | 433/780 [02:45<01:42,  3.40it/s] 56%|█████▌    | 434/780 [02:46<01:41,  3.40it/s] 56%|█████▌    | 435/780 [02:46<01:45,  3.26it/s] 56%|█████▌    | 436/780 [02:46<01:44,  3.31it/s] 56%|█████▌    | 437/780 [02:47<01:42,  3.33it/s] 56%|█████▌    | 438/780 [02:47<01:42,  3.35it/s] 56%|█████▋    | 439/780 [02:47<01:41,  3.37it/s] 56%|█████▋    | 440/780 [02:48<01:40,  3.38it/s] 57%|█████▋    | 441/780 [02:48<01:40,  3.38it/s] 57%|█████▋    | 442/780 [02:48<01:39,  3.39it/s] 57%|█████▋    | 443/780 [02:48<01:39,  3.39it/s] 57%|█████▋    | 444/780 [02:49<01:38,  3.39it/s] 57%|█████▋    | 445/780 [02:49<01:38,  3.39it/s] 57%|█████▋    | 446/780 [02:49<01:39,  3.35it/s] 57%|█████▋    | 447/780 [02:50<01:43,  3.23it/s] 57%|█████▋    | 448/780 [02:50<01:41,  3.28it/s] 58%|█████▊    | 449/780 [02:50<01:39,  3.32it/s] 58%|█████▊    | 450/780 [02:51<01:38,  3.34it/s] 58%|█████▊    | 451/780 [02:51<01:37,  3.36it/s] 58%|█████▊    | 452/780 [02:51<01:37,  3.37it/s] 58%|█████▊    | 453/780 [02:51<01:36,  3.38it/s] 58%|█████▊    | 454/780 [02:52<01:36,  3.39it/s] 58%|█████▊    | 455/780 [02:52<01:35,  3.39it/s] 58%|█████▊    | 456/780 [02:52<01:35,  3.40it/s] 59%|█████▊    | 457/780 [02:53<01:36,  3.35it/s] 59%|█████▊    | 458/780 [02:53<01:35,  3.37it/s] 59%|█████▉    | 459/780 [02:53<01:35,  3.38it/s] 59%|█████▉    | 460/780 [02:53<01:34,  3.38it/s] 59%|█████▉    | 461/780 [02:54<01:34,  3.39it/s] 59%|█████▉    | 462/780 [02:54<01:33,  3.39it/s] 59%|█████▉    | 463/780 [02:54<01:33,  3.40it/s] 59%|█████▉    | 464/780 [02:55<01:32,  3.40it/s] 60%|█████▉    | 465/780 [02:55<01:32,  3.40it/s] 60%|█████▉    | 466/780 [02:55<01:32,  3.39it/s] 60%|█████▉    | 467/780 [02:56<01:32,  3.40it/s] 60%|██████    | 468/780 [02:56<01:32,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 23:23:45,549 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:23:45,549 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 23:23:45,549 >>   Batch size = 8
{'eval_loss': 1.021996021270752, 'eval_runtime': 9.9699, 'eval_samples_per_second': 345.64, 'eval_steps_per_second': 43.23, 'epoch': 2.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 53.60it/s][A
  3%|▎         | 12/431 [00:00<00:09, 46.48it/s][A
  4%|▍         | 17/431 [00:00<00:09, 44.87it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.30it/s][A
  6%|▋         | 27/431 [00:00<00:09, 43.82it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.58it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.50it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.51it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.55it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.51it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.39it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.32it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.30it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.30it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.29it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.24it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.36it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.48it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.51it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.34it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.34it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.28it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.27it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.28it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.29it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.40it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.41it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.39it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.42it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.37it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.26it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.27it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.29it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.29it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.40it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.36it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.34it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.35it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.33it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.27it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.38it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.31it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.40it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.44it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.37it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.37it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.40it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.31it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.31it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.20it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.38it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.38it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.32it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.37it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.34it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.30it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.22it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.30it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.36it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.34it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.34it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.44it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.42it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.33it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.29it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.23it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.31it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.37it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.32it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.42it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.50it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.37it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.31it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.34it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.34it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.22it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.30it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.34it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.43it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.41it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.33it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.24it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.33it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.31it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.30it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.30it/s][A 60%|██████    | 468/780 [03:06<01:32,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:23:55,546 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 23:23:55,562 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:23:58,652 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:23:58,681 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:23:58,693 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:16<32:46,  6.32s/it] 60%|██████    | 470/780 [03:17<23:20,  4.52s/it] 60%|██████    | 471/780 [03:17<16:44,  3.25s/it] 61%|██████    | 472/780 [03:17<12:08,  2.37s/it] 61%|██████    | 473/780 [03:17<08:55,  1.75s/it] 61%|██████    | 474/780 [03:18<06:41,  1.31s/it] 61%|██████    | 475/780 [03:18<05:07,  1.01s/it] 61%|██████    | 476/780 [03:18<04:01,  1.26it/s] 61%|██████    | 477/780 [03:19<03:15,  1.55it/s] 61%|██████▏   | 478/780 [03:19<02:43,  1.85it/s] 61%|██████▏   | 479/780 [03:19<02:20,  2.14it/s] 62%|██████▏   | 480/780 [03:19<02:05,  2.40it/s] 62%|██████▏   | 481/780 [03:20<01:57,  2.54it/s] 62%|██████▏   | 482/780 [03:20<01:48,  2.74it/s] 62%|██████▏   | 483/780 [03:20<01:42,  2.90it/s] 62%|██████▏   | 484/780 [03:21<01:37,  3.02it/s] 62%|██████▏   | 485/780 [03:21<01:34,  3.12it/s] 62%|██████▏   | 486/780 [03:21<01:32,  3.19it/s] 62%|██████▏   | 487/780 [03:22<01:30,  3.24it/s] 63%|██████▎   | 488/780 [03:22<01:29,  3.27it/s] 63%|██████▎   | 489/780 [03:22<01:28,  3.30it/s] 63%|██████▎   | 490/780 [03:23<01:27,  3.31it/s] 63%|██████▎   | 491/780 [03:23<01:28,  3.28it/s] 63%|██████▎   | 492/780 [03:23<01:27,  3.30it/s] 63%|██████▎   | 493/780 [03:23<01:26,  3.32it/s] 63%|██████▎   | 494/780 [03:24<01:25,  3.33it/s] 63%|██████▎   | 495/780 [03:24<01:25,  3.32it/s] 64%|██████▎   | 496/780 [03:24<01:25,  3.33it/s] 64%|██████▎   | 497/780 [03:25<01:24,  3.34it/s] 64%|██████▍   | 498/780 [03:25<01:24,  3.35it/s] 64%|██████▍   | 499/780 [03:25<01:23,  3.35it/s] 64%|██████▍   | 500/780 [03:26<01:23,  3.36it/s]                                                  64%|██████▍   | 500/780 [03:26<01:23,  3.36it/s] 64%|██████▍   | 501/780 [03:26<01:23,  3.36it/s] 64%|██████▍   | 502/780 [03:26<01:23,  3.31it/s] 64%|██████▍   | 503/780 [03:26<01:23,  3.33it/s] 65%|██████▍   | 504/780 [03:27<01:22,  3.34it/s] 65%|██████▍   | 505/780 [03:27<01:22,  3.33it/s] 65%|██████▍   | 506/780 [03:27<01:22,  3.34it/s] 65%|██████▌   | 507/780 [03:28<01:21,  3.35it/s] 65%|██████▌   | 508/780 [03:28<01:21,  3.36it/s] 65%|██████▌   | 509/780 [03:28<01:20,  3.37it/s] 65%|██████▌   | 510/780 [03:28<01:19,  3.38it/s] 66%|██████▌   | 511/780 [03:29<01:19,  3.39it/s] 66%|██████▌   | 512/780 [03:29<01:18,  3.39it/s] 66%|██████▌   | 513/780 [03:29<01:18,  3.40it/s] 66%|██████▌   | 514/780 [03:30<01:18,  3.40it/s] 66%|██████▌   | 515/780 [03:30<01:17,  3.40it/s] 66%|██████▌   | 516/780 [03:30<01:17,  3.41it/s] 66%|██████▋   | 517/780 [03:31<01:17,  3.41it/s] 66%|██████▋   | 518/780 [03:31<01:16,  3.41it/s] 67%|██████▋   | 519/780 [03:31<01:17,  3.38it/s] 67%|██████▋   | 520/780 [03:31<01:16,  3.39it/s] 67%|██████▋   | 521/780 [03:32<01:16,  3.39it/s] 67%|██████▋   | 522/780 [03:32<01:16,  3.38it/s] 67%|██████▋   | 523/780 [03:32<01:15,  3.38it/s] 67%|██████▋   | 524/780 [03:33<01:15,  3.39it/s] 67%|██████▋   | 525/780 [03:33<01:15,  3.40it/s] 67%|██████▋   | 526/780 [03:33<01:14,  3.40it/s] 68%|██████▊   | 527/780 [03:33<01:14,  3.40it/s] 68%|██████▊   | 528/780 [03:34<01:14,  3.40it/s] 68%|██████▊   | 529/780 [03:34<01:13,  3.40it/s] 68%|██████▊   | 530/780 [03:34<01:13,  3.38it/s] 68%|██████▊   | 531/780 [03:35<01:13,  3.38it/s] 68%|██████▊   | 532/780 [03:35<01:13,  3.39it/s] 68%|██████▊   | 533/780 [03:35<01:12,  3.40it/s] 68%|██████▊   | 534/780 [03:36<01:12,  3.40it/s] 69%|██████▊   | 535/780 [03:36<01:12,  3.40it/s] 69%|██████▊   | 536/780 [03:36<01:11,  3.40it/s] 69%|██████▉   | 537/780 [03:36<01:11,  3.40it/s] 69%|██████▉   | 538/780 [03:37<01:11,  3.40it/s] 69%|██████▉   | 539/780 [03:37<01:10,  3.40it/s] 69%|██████▉   | 540/780 [03:37<01:10,  3.41it/s] 69%|██████▉   | 541/780 [03:38<01:10,  3.39it/s] 69%|██████▉   | 542/780 [03:38<01:10,  3.40it/s] 70%|██████▉   | 543/780 [03:38<01:09,  3.40it/s] 70%|██████▉   | 544/780 [03:38<01:09,  3.40it/s] 70%|██████▉   | 545/780 [03:39<01:09,  3.40it/s] 70%|███████   | 546/780 [03:39<01:08,  3.40it/s] 70%|███████   | 547/780 [03:39<01:08,  3.41it/s] 70%|███████   | 548/780 [03:40<01:08,  3.40it/s] 70%|███████   | 549/780 [03:40<01:07,  3.40it/s] 71%|███████   | 550/780 [03:40<01:07,  3.40it/s] 71%|███████   | 551/780 [03:41<01:07,  3.40it/s] 71%|███████   | 552/780 [03:41<01:07,  3.38it/s] 71%|███████   | 553/780 [03:41<01:06,  3.39it/s] 71%|███████   | 554/780 [03:41<01:06,  3.39it/s] 71%|███████   | 555/780 [03:42<01:06,  3.39it/s] 71%|███████▏  | 556/780 [03:42<01:05,  3.40it/s] 71%|███████▏  | 557/780 [03:42<01:05,  3.40it/s] 72%|███████▏  | 558/780 [03:43<01:05,  3.40it/s] 72%|███████▏  | 559/780 [03:43<01:04,  3.40it/s] 72%|███████▏  | 560/780 [03:43<01:04,  3.40it/s] 72%|███████▏  | 561/780 [03:43<01:04,  3.40it/s] 72%|███████▏  | 562/780 [03:44<01:04,  3.40it/s] 72%|███████▏  | 563/780 [03:44<01:03,  3.39it/s] 72%|███████▏  | 564/780 [03:44<01:03,  3.40it/s] 72%|███████▏  | 565/780 [03:45<01:03,  3.40it/s] 73%|███████▎  | 566/780 [03:45<01:02,  3.40it/s] 73%|███████▎  | 567/780 [03:45<01:02,  3.40it/s] 73%|███████▎  | 568/780 [03:46<01:02,  3.40it/s] 73%|███████▎  | 569/780 [03:46<01:02,  3.40it/s] 73%|███████▎  | 570/780 [03:46<01:01,  3.40it/s] 73%|███████▎  | 571/780 [03:46<01:01,  3.41it/s] 73%|███████▎  | 572/780 [03:47<01:01,  3.40it/s] 73%|███████▎  | 573/780 [03:47<01:00,  3.41it/s] 74%|███████▎  | 574/780 [03:47<01:01,  3.33it/s] 74%|███████▎  | 575/780 [03:48<01:01,  3.35it/s] 74%|███████▍  | 576/780 [03:48<01:00,  3.36it/s] 74%|███████▍  | 577/780 [03:48<01:00,  3.38it/s] 74%|███████▍  | 578/780 [03:49<00:59,  3.39it/s] 74%|███████▍  | 579/780 [03:49<00:59,  3.39it/s] 74%|███████▍  | 580/780 [03:49<00:58,  3.39it/s] 74%|███████▍  | 581/780 [03:49<00:58,  3.40it/s] 75%|███████▍  | 582/780 [03:50<01:01,  3.23it/s] 75%|███████▍  | 583/780 [03:50<01:00,  3.28it/s] 75%|███████▍  | 584/780 [03:50<00:59,  3.30it/s] 75%|███████▌  | 585/780 [03:51<00:58,  3.33it/s] 75%|███████▌  | 586/780 [03:51<00:57,  3.35it/s] 75%|███████▌  | 587/780 [03:51<00:57,  3.37it/s] 75%|███████▌  | 588/780 [03:52<00:56,  3.38it/s] 76%|███████▌  | 589/780 [03:52<00:56,  3.39it/s] 76%|███████▌  | 590/780 [03:52<00:55,  3.39it/s] 76%|███████▌  | 591/780 [03:52<00:55,  3.39it/s] 76%|███████▌  | 592/780 [03:53<00:55,  3.40it/s] 76%|███████▌  | 593/780 [03:53<00:55,  3.40it/s] 76%|███████▌  | 594/780 [03:53<00:54,  3.40it/s] 76%|███████▋  | 595/780 [03:54<00:54,  3.39it/s] 76%|███████▋  | 596/780 [03:54<00:54,  3.39it/s] 77%|███████▋  | 597/780 [03:54<00:53,  3.40it/s] 77%|███████▋  | 598/780 [03:54<00:53,  3.40it/s] 77%|███████▋  | 599/780 [03:55<00:53,  3.40it/s] 77%|███████▋  | 600/780 [03:55<00:52,  3.40it/s] 77%|███████▋  | 601/780 [03:55<00:52,  3.40it/s] 77%|███████▋  | 602/780 [03:56<00:52,  3.40it/s] 77%|███████▋  | 603/780 [03:56<00:52,  3.40it/s] 77%|███████▋  | 604/780 [03:56<00:51,  3.40it/s] 78%|███████▊  | 605/780 [03:57<00:51,  3.40it/s] 78%|███████▊  | 606/780 [03:57<00:51,  3.37it/s] 78%|███████▊  | 607/780 [03:57<00:51,  3.38it/s] 78%|███████▊  | 608/780 [03:57<00:50,  3.39it/s] 78%|███████▊  | 609/780 [03:58<00:50,  3.39it/s] 78%|███████▊  | 610/780 [03:58<00:50,  3.39it/s] 78%|███████▊  | 611/780 [03:58<00:49,  3.40it/s] 78%|███████▊  | 612/780 [03:59<00:49,  3.40it/s] 79%|███████▊  | 613/780 [03:59<00:49,  3.40it/s] 79%|███████▊  | 614/780 [03:59<00:48,  3.40it/s] 79%|███████▉  | 615/780 [03:59<00:48,  3.40it/s] 79%|███████▉  | 616/780 [04:00<00:48,  3.40it/s] 79%|███████▉  | 617/780 [04:00<00:47,  3.40it/s] 79%|███████▉  | 618/780 [04:00<00:47,  3.40it/s] 79%|███████▉  | 619/780 [04:01<00:47,  3.41it/s] 79%|███████▉  | 620/780 [04:01<00:47,  3.40it/s] 80%|███████▉  | 621/780 [04:01<00:46,  3.40it/s] 80%|███████▉  | 622/780 [04:02<00:46,  3.41it/s] 80%|███████▉  | 623/780 [04:02<00:46,  3.39it/s] 80%|████████  | 624/780 [04:02<00:46,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 23:24:51,837 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:24:51,838 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 23:24:51,838 >>   Batch size = 8
{'eval_loss': 1.0317331552505493, 'eval_runtime': 9.9747, 'eval_samples_per_second': 345.473, 'eval_steps_per_second': 43.209, 'epoch': 3.0}
{'loss': 0.5096, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 54.46it/s][A
  3%|▎         | 12/431 [00:00<00:08, 47.03it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.29it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.68it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.04it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.77it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.54it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.51it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.66it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.58it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.49it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.38it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.30it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.25it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.23it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.25it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.35it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.44it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.52it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.36it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.27it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.22it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.19it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.26it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.28it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.34it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.56it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.43it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.40it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.28it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.28it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.21it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.25it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.33it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.40it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.42it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.48it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.37it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.28it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.28it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.21it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.24it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.29it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.41it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.39it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.41it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.28it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.24it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.33it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.29it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.21it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.24it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.44it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.49it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.34it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.29it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.35it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.33it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.23it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.21it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.42it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.54it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.36it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.29it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.36it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.38it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.34it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.16it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.27it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.45it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.40it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.25it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.24it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.34it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.42it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.22it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.29it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.33it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.37it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.39it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.30it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.31it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.47it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.31it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.29it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.29it/s][A 80%|████████  | 624/780 [04:12<00:46,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:25:01,851 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-28 23:25:01,869 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:25:04,802 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:25:04,851 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:25:04,866 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:22<15:43,  6.09s/it] 80%|████████  | 626/780 [04:22<11:10,  4.35s/it] 80%|████████  | 627/780 [04:22<07:59,  3.14s/it] 81%|████████  | 628/780 [04:23<05:47,  2.29s/it] 81%|████████  | 629/780 [04:23<04:14,  1.69s/it] 81%|████████  | 630/780 [04:23<03:10,  1.27s/it] 81%|████████  | 631/780 [04:24<02:25,  1.02it/s] 81%|████████  | 632/780 [04:24<01:54,  1.29it/s] 81%|████████  | 633/780 [04:24<01:32,  1.58it/s] 81%|████████▏ | 634/780 [04:24<01:17,  1.88it/s] 81%|████████▏ | 635/780 [04:25<01:06,  2.17it/s] 82%|████████▏ | 636/780 [04:25<00:59,  2.43it/s] 82%|████████▏ | 637/780 [04:25<00:54,  2.64it/s] 82%|████████▏ | 638/780 [04:26<00:50,  2.82it/s] 82%|████████▏ | 639/780 [04:26<00:47,  2.96it/s] 82%|████████▏ | 640/780 [04:26<00:45,  3.07it/s] 82%|████████▏ | 641/780 [04:26<00:44,  3.15it/s] 82%|████████▏ | 642/780 [04:27<00:42,  3.21it/s] 82%|████████▏ | 643/780 [04:27<00:42,  3.24it/s] 83%|████████▎ | 644/780 [04:27<00:41,  3.29it/s] 83%|████████▎ | 645/780 [04:28<00:40,  3.32it/s] 83%|████████▎ | 646/780 [04:28<00:40,  3.35it/s] 83%|████████▎ | 647/780 [04:28<00:39,  3.37it/s] 83%|████████▎ | 648/780 [04:29<00:39,  3.37it/s] 83%|████████▎ | 649/780 [04:29<00:38,  3.38it/s] 83%|████████▎ | 650/780 [04:29<00:38,  3.39it/s] 83%|████████▎ | 651/780 [04:29<00:38,  3.39it/s] 84%|████████▎ | 652/780 [04:30<00:37,  3.40it/s] 84%|████████▎ | 653/780 [04:30<00:37,  3.40it/s] 84%|████████▍ | 654/780 [04:30<00:37,  3.41it/s] 84%|████████▍ | 655/780 [04:31<00:36,  3.41it/s] 84%|████████▍ | 656/780 [04:31<00:36,  3.41it/s] 84%|████████▍ | 657/780 [04:31<00:36,  3.41it/s] 84%|████████▍ | 658/780 [04:31<00:35,  3.41it/s] 84%|████████▍ | 659/780 [04:32<00:35,  3.41it/s] 85%|████████▍ | 660/780 [04:32<00:35,  3.38it/s] 85%|████████▍ | 661/780 [04:32<00:35,  3.39it/s] 85%|████████▍ | 662/780 [04:33<00:34,  3.39it/s] 85%|████████▌ | 663/780 [04:33<00:34,  3.39it/s] 85%|████████▌ | 664/780 [04:33<00:34,  3.40it/s] 85%|████████▌ | 665/780 [04:34<00:33,  3.40it/s] 85%|████████▌ | 666/780 [04:34<00:33,  3.40it/s] 86%|████████▌ | 667/780 [04:34<00:33,  3.40it/s] 86%|████████▌ | 668/780 [04:34<00:32,  3.41it/s] 86%|████████▌ | 669/780 [04:35<00:32,  3.40it/s] 86%|████████▌ | 670/780 [04:35<00:32,  3.41it/s] 86%|████████▌ | 671/780 [04:35<00:32,  3.41it/s] 86%|████████▌ | 672/780 [04:36<00:31,  3.41it/s] 86%|████████▋ | 673/780 [04:36<00:31,  3.38it/s] 86%|████████▋ | 674/780 [04:36<00:31,  3.39it/s] 87%|████████▋ | 675/780 [04:36<00:30,  3.39it/s] 87%|████████▋ | 676/780 [04:37<00:30,  3.40it/s] 87%|████████▋ | 677/780 [04:37<00:30,  3.40it/s] 87%|████████▋ | 678/780 [04:37<00:29,  3.40it/s] 87%|████████▋ | 679/780 [04:38<00:29,  3.40it/s] 87%|████████▋ | 680/780 [04:38<00:29,  3.40it/s] 87%|████████▋ | 681/780 [04:38<00:29,  3.41it/s] 87%|████████▋ | 682/780 [04:39<00:28,  3.41it/s] 88%|████████▊ | 683/780 [04:39<00:28,  3.40it/s] 88%|████████▊ | 684/780 [04:39<00:28,  3.39it/s] 88%|████████▊ | 685/780 [04:39<00:28,  3.39it/s] 88%|████████▊ | 686/780 [04:40<00:27,  3.39it/s] 88%|████████▊ | 687/780 [04:40<00:27,  3.40it/s] 88%|████████▊ | 688/780 [04:40<00:27,  3.40it/s] 88%|████████▊ | 689/780 [04:41<00:26,  3.40it/s] 88%|████████▊ | 690/780 [04:41<00:26,  3.40it/s] 89%|████████▊ | 691/780 [04:41<00:26,  3.40it/s] 89%|████████▊ | 692/780 [04:41<00:25,  3.40it/s] 89%|████████▉ | 693/780 [04:42<00:25,  3.40it/s] 89%|████████▉ | 694/780 [04:42<00:25,  3.40it/s] 89%|████████▉ | 695/780 [04:42<00:25,  3.39it/s] 89%|████████▉ | 696/780 [04:43<00:24,  3.39it/s] 89%|████████▉ | 697/780 [04:43<00:24,  3.39it/s] 89%|████████▉ | 698/780 [04:43<00:24,  3.39it/s] 90%|████████▉ | 699/780 [04:44<00:23,  3.40it/s] 90%|████████▉ | 700/780 [04:44<00:23,  3.40it/s] 90%|████████▉ | 701/780 [04:44<00:23,  3.40it/s] 90%|█████████ | 702/780 [04:44<00:22,  3.40it/s] 90%|█████████ | 703/780 [04:45<00:22,  3.40it/s] 90%|█████████ | 704/780 [04:45<00:22,  3.40it/s] 90%|█████████ | 705/780 [04:45<00:22,  3.40it/s] 91%|█████████ | 706/780 [04:46<00:21,  3.39it/s] 91%|█████████ | 707/780 [04:46<00:21,  3.40it/s] 91%|█████████ | 708/780 [04:46<00:21,  3.40it/s] 91%|█████████ | 709/780 [04:46<00:20,  3.40it/s] 91%|█████████ | 710/780 [04:47<00:20,  3.40it/s] 91%|█████████ | 711/780 [04:47<00:20,  3.40it/s] 91%|█████████▏| 712/780 [04:47<00:19,  3.40it/s] 91%|█████████▏| 713/780 [04:48<00:19,  3.41it/s] 92%|█████████▏| 714/780 [04:48<00:19,  3.40it/s] 92%|█████████▏| 715/780 [04:48<00:19,  3.40it/s] 92%|█████████▏| 716/780 [04:49<00:18,  3.40it/s] 92%|█████████▏| 717/780 [04:49<00:18,  3.37it/s] 92%|█████████▏| 718/780 [04:49<00:18,  3.38it/s] 92%|█████████▏| 719/780 [04:49<00:18,  3.39it/s] 92%|█████████▏| 720/780 [04:50<00:17,  3.34it/s] 92%|█████████▏| 721/780 [04:50<00:17,  3.36it/s] 93%|█████████▎| 722/780 [04:50<00:17,  3.37it/s] 93%|█████████▎| 723/780 [04:51<00:16,  3.38it/s] 93%|█████████▎| 724/780 [04:51<00:16,  3.39it/s] 93%|█████████▎| 725/780 [04:51<00:16,  3.39it/s] 93%|█████████▎| 726/780 [04:52<00:15,  3.40it/s] 93%|█████████▎| 727/780 [04:52<00:15,  3.40it/s] 93%|█████████▎| 728/780 [04:52<00:15,  3.38it/s] 93%|█████████▎| 729/780 [04:52<00:15,  3.38it/s] 94%|█████████▎| 730/780 [04:53<00:14,  3.39it/s] 94%|█████████▎| 731/780 [04:53<00:14,  3.40it/s] 94%|█████████▍| 732/780 [04:53<00:14,  3.40it/s] 94%|█████████▍| 733/780 [04:54<00:13,  3.40it/s] 94%|█████████▍| 734/780 [04:54<00:13,  3.40it/s] 94%|█████████▍| 735/780 [04:54<00:13,  3.40it/s] 94%|█████████▍| 736/780 [04:54<00:12,  3.40it/s] 94%|█████████▍| 737/780 [04:55<00:12,  3.40it/s] 95%|█████████▍| 738/780 [04:55<00:12,  3.40it/s] 95%|█████████▍| 739/780 [04:55<00:12,  3.39it/s] 95%|█████████▍| 740/780 [04:56<00:11,  3.39it/s] 95%|█████████▌| 741/780 [04:56<00:11,  3.39it/s] 95%|█████████▌| 742/780 [04:56<00:11,  3.40it/s] 95%|█████████▌| 743/780 [04:57<00:10,  3.40it/s] 95%|█████████▌| 744/780 [04:57<00:10,  3.40it/s] 96%|█████████▌| 745/780 [04:57<00:10,  3.40it/s] 96%|█████████▌| 746/780 [04:57<00:09,  3.40it/s] 96%|█████████▌| 747/780 [04:58<00:09,  3.40it/s] 96%|█████████▌| 748/780 [04:58<00:09,  3.40it/s] 96%|█████████▌| 749/780 [04:58<00:09,  3.40it/s] 96%|█████████▌| 750/780 [04:59<00:08,  3.38it/s] 96%|█████████▋| 751/780 [04:59<00:08,  3.38it/s] 96%|█████████▋| 752/780 [04:59<00:08,  3.39it/s] 97%|█████████▋| 753/780 [04:59<00:07,  3.40it/s] 97%|█████████▋| 754/780 [05:00<00:07,  3.40it/s] 97%|█████████▋| 755/780 [05:00<00:07,  3.40it/s] 97%|█████████▋| 756/780 [05:00<00:07,  3.40it/s] 97%|█████████▋| 757/780 [05:01<00:06,  3.40it/s] 97%|█████████▋| 758/780 [05:01<00:06,  3.40it/s] 97%|█████████▋| 759/780 [05:01<00:06,  3.40it/s] 97%|█████████▋| 760/780 [05:02<00:05,  3.40it/s] 98%|█████████▊| 761/780 [05:02<00:05,  3.40it/s] 98%|█████████▊| 762/780 [05:02<00:05,  3.40it/s] 98%|█████████▊| 763/780 [05:02<00:05,  3.40it/s] 98%|█████████▊| 764/780 [05:03<00:04,  3.40it/s] 98%|█████████▊| 765/780 [05:03<00:04,  3.40it/s] 98%|█████████▊| 766/780 [05:03<00:04,  3.33it/s] 98%|█████████▊| 767/780 [05:04<00:03,  3.36it/s] 98%|█████████▊| 768/780 [05:04<00:03,  3.37it/s] 99%|█████████▊| 769/780 [05:04<00:03,  3.38it/s] 99%|█████████▊| 770/780 [05:04<00:02,  3.39it/s] 99%|█████████▉| 771/780 [05:05<00:02,  3.39it/s] 99%|█████████▉| 772/780 [05:05<00:02,  3.39it/s] 99%|█████████▉| 773/780 [05:05<00:02,  3.40it/s] 99%|█████████▉| 774/780 [05:06<00:01,  3.40it/s] 99%|█████████▉| 775/780 [05:06<00:01,  3.40it/s] 99%|█████████▉| 776/780 [05:06<00:01,  3.40it/s]100%|█████████▉| 777/780 [05:07<00:00,  3.38it/s]100%|█████████▉| 778/780 [05:07<00:00,  3.39it/s]100%|█████████▉| 779/780 [05:07<00:00,  3.39it/s]100%|██████████| 780/780 [05:07<00:00,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 23:25:57,134 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:25:57,134 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 23:25:57,134 >>   Batch size = 8
{'eval_loss': 1.0393263101577759, 'eval_runtime': 9.9709, 'eval_samples_per_second': 345.607, 'eval_steps_per_second': 43.226, 'epoch': 4.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 54.30it/s][A
  3%|▎         | 12/431 [00:00<00:08, 46.64it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.00it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.33it/s][A
  6%|▋         | 27/431 [00:00<00:09, 43.93it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.74it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.69it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.49it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.61it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.28it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.27it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.22it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.21it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.18it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.17it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.32it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.41it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.41it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.35it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.32it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.34it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.31it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.23it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.37it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 43.44it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.45it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.41it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.28it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.33it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.23it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.30it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.21it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.29it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.38it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.45it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.40it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.25it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.33it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.34it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.27it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.26it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.34it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.42it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.36it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.18it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.36it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.32it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.27it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.30it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.35it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.43it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.46it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.38it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.33it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.39it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.33it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.29it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.25it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.29it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.44it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.24it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.29it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.27it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.24it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.33it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.36it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.40it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.37it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.30it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.34it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.36it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.38it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.37it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.37it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.34it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.32it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.37it/s][A
 91%|█████████ | 392/431 [00:09<00:00, 43.23it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.41it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.38it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.38it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.28it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.25it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.19it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.27it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.27it/s][A100%|██████████| 780/780 [05:17<00:00,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:26:07,115 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-28 23:26:07,144 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:26:09,450 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:26:09,494 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:26:09,537 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 23:26:15,887 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 23:26:15,893 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156 (score: 1.0042188167572021).
                                                 100%|██████████| 780/780 [05:30<00:00,  3.39it/s]100%|██████████| 780/780 [05:30<00:00,  2.36it/s]
[INFO|trainer.py:1894] 2023-08-28 23:26:19,310 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 23:26:19,334 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:26:22,384 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:26:22,401 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:26:22,410 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:26:22,730 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:22,730 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:22,730 >>   train_loss               =     0.5002
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:22,731 >>   train_runtime            = 0:05:30.09
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:22,731 >>   train_samples            =       9999
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:22,731 >>   train_samples_per_second =    151.454
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:22,731 >>   train_steps_per_second   =      2.363
{'eval_loss': 1.044157862663269, 'eval_runtime': 9.9464, 'eval_samples_per_second': 346.458, 'eval_steps_per_second': 43.332, 'epoch': 5.0}
{'train_runtime': 330.0993, 'train_samples_per_second': 151.454, 'train_steps_per_second': 2.363, 'train_loss': 0.5002162835536859, 'epoch': 5.0}
08/28/2023 23:26:22 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 23:26:22,834 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:26:22,835 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-28 23:26:22,835 >>   Batch size = 8
  0%|          | 0/431 [00:00<?, ?it/s]  1%|▏         | 6/431 [00:00<00:07, 54.41it/s]  3%|▎         | 12/431 [00:00<00:08, 47.61it/s]  4%|▍         | 17/431 [00:00<00:08, 46.13it/s]  5%|▌         | 22/431 [00:00<00:09, 45.28it/s]  6%|▋         | 27/431 [00:00<00:09, 44.84it/s]  7%|▋         | 32/431 [00:00<00:08, 44.61it/s]  9%|▊         | 37/431 [00:00<00:08, 44.42it/s] 10%|▉         | 42/431 [00:00<00:08, 43.67it/s] 11%|█         | 47/431 [00:01<00:08, 43.15it/s] 12%|█▏        | 52/431 [00:01<00:08, 42.94it/s] 13%|█▎        | 57/431 [00:01<00:08, 43.12it/s] 14%|█▍        | 62/431 [00:01<00:08, 43.43it/s] 16%|█▌        | 67/431 [00:01<00:08, 43.51it/s] 17%|█▋        | 72/431 [00:01<00:08, 43.64it/s] 18%|█▊        | 77/431 [00:01<00:08, 43.73it/s] 19%|█▉        | 82/431 [00:01<00:08, 43.61it/s] 20%|██        | 87/431 [00:01<00:07, 43.25it/s] 21%|██▏       | 92/431 [00:02<00:07, 42.90it/s] 23%|██▎       | 97/431 [00:02<00:07, 42.98it/s] 24%|██▎       | 102/431 [00:02<00:07, 43.06it/s] 25%|██▍       | 107/431 [00:02<00:07, 43.35it/s] 26%|██▌       | 112/431 [00:02<00:07, 43.52it/s] 27%|██▋       | 117/431 [00:02<00:07, 43.62it/s] 28%|██▊       | 122/431 [00:02<00:07, 43.78it/s] 29%|██▉       | 127/431 [00:02<00:06, 43.63it/s] 31%|███       | 132/431 [00:03<00:06, 43.32it/s] 32%|███▏      | 137/431 [00:03<00:06, 43.08it/s] 33%|███▎      | 142/431 [00:03<00:06, 43.12it/s] 34%|███▍      | 147/431 [00:03<00:06, 43.41it/s] 35%|███▌      | 152/431 [00:03<00:06, 43.61it/s] 36%|███▋      | 157/431 [00:03<00:06, 43.75it/s] 38%|███▊      | 162/431 [00:03<00:06, 43.72it/s] 39%|███▊      | 167/431 [00:03<00:06, 43.81it/s] 40%|███▉      | 172/431 [00:03<00:05, 43.61it/s] 41%|████      | 177/431 [00:04<00:05, 43.31it/s] 42%|████▏     | 182/431 [00:04<00:05, 43.24it/s] 43%|████▎     | 187/431 [00:04<00:05, 43.22it/s] 45%|████▍     | 192/431 [00:04<00:05, 43.39it/s] 46%|████▌     | 197/431 [00:04<00:05, 43.59it/s] 47%|████▋     | 202/431 [00:04<00:05, 43.69it/s] 48%|████▊     | 207/431 [00:04<00:05, 43.78it/s] 49%|████▉     | 212/431 [00:04<00:05, 43.54it/s] 50%|█████     | 217/431 [00:04<00:04, 43.60it/s] 52%|█████▏    | 222/431 [00:05<00:04, 43.50it/s] 53%|█████▎    | 227/431 [00:05<00:04, 43.25it/s] 54%|█████▍    | 232/431 [00:05<00:04, 43.29it/s] 55%|█████▍    | 237/431 [00:05<00:04, 43.44it/s] 56%|█████▌    | 242/431 [00:05<00:04, 43.63it/s] 57%|█████▋    | 247/431 [00:05<00:04, 43.73it/s] 58%|█████▊    | 252/431 [00:05<00:04, 43.77it/s] 60%|█████▉    | 257/431 [00:05<00:03, 43.64it/s] 61%|██████    | 262/431 [00:05<00:03, 43.56it/s] 62%|██████▏   | 267/431 [00:06<00:03, 43.43it/s] 63%|██████▎   | 272/431 [00:06<00:03, 43.36it/s] 64%|██████▍   | 277/431 [00:06<00:03, 43.31it/s] 65%|██████▌   | 282/431 [00:06<00:03, 43.46it/s] 67%|██████▋   | 287/431 [00:06<00:03, 43.62it/s] 68%|██████▊   | 292/431 [00:06<00:03, 43.67it/s] 69%|██████▉   | 297/431 [00:06<00:03, 43.64it/s] 70%|███████   | 302/431 [00:06<00:02, 43.68it/s] 71%|███████   | 307/431 [00:07<00:02, 43.44it/s] 72%|███████▏  | 312/431 [00:07<00:02, 43.41it/s] 74%|███████▎  | 317/431 [00:07<00:02, 43.34it/s] 75%|███████▍  | 322/431 [00:07<00:02, 43.37it/s] 76%|███████▌  | 327/431 [00:07<00:02, 43.63it/s] 77%|███████▋  | 332/431 [00:07<00:02, 43.56it/s] 78%|███████▊  | 337/431 [00:07<00:02, 43.64it/s] 79%|███████▉  | 342/431 [00:07<00:02, 43.68it/s] 81%|████████  | 347/431 [00:07<00:01, 43.60it/s] 82%|████████▏ | 352/431 [00:08<00:01, 43.42it/s] 83%|████████▎ | 357/431 [00:08<00:01, 43.38it/s] 84%|████████▍ | 362/431 [00:08<00:01, 43.38it/s] 85%|████████▌ | 367/431 [00:08<00:01, 43.53it/s] 86%|████████▋ | 372/431 [00:08<00:01, 43.53it/s] 87%|████████▋ | 377/431 [00:08<00:01, 43.62it/s] 89%|████████▊ | 382/431 [00:08<00:01, 43.64it/s] 90%|████████▉ | 387/431 [00:08<00:01, 43.59it/s] 91%|█████████ | 392/431 [00:08<00:00, 43.47it/s] 92%|█████████▏| 397/431 [00:09<00:00, 43.37it/s] 93%|█████████▎| 402/431 [00:09<00:00, 43.40it/s] 94%|█████████▍| 407/431 [00:09<00:00, 43.43it/s] 96%|█████████▌| 412/431 [00:09<00:00, 43.53it/s] 97%|█████████▋| 417/431 [00:09<00:00, 43.51it/s] 98%|█████████▊| 422/431 [00:09<00:00, 43.70it/s] 99%|█████████▉| 427/431 [00:09<00:00, 43.62it/s]100%|██████████| 431/431 [00:09<00:00, 43.58it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:26:32,742 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:32,742 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:32,742 >>   eval_loss               =     1.0042
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:32,742 >>   eval_runtime            = 0:00:09.90
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:32,742 >>   eval_samples            =       3446
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:32,742 >>   eval_samples_per_second =     347.84
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:32,742 >>   eval_steps_per_second   =     43.505
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:26:32,742 >>   perplexity              =     2.7298
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:26:38,656 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:26:38,662 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:26:38,662 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:26:38,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:26:38,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:26:38,953 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:26:38,954 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:26:39,626 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:26:40,673 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:26:40,673 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:26:42,426 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:26:42,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:26:42,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:26:42,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:26:42,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:26:42,736 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:26:42,737 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:26:42,993 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:26:43,139 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:26:43,139 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:05,  1.47it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.46it/s]Extractor Predicting: 18it [00:12,  1.50it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:14,  1.50it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.49it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:18,  1.46it/s]Extractor Predicting: 28it [00:18,  1.47it/s]Extractor Predicting: 29it [00:19,  1.48it/s]Extractor Predicting: 30it [00:20,  1.47it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:21,  1.48it/s]Extractor Predicting: 33it [00:22,  1.51it/s]Extractor Predicting: 34it [00:22,  1.55it/s]Extractor Predicting: 35it [00:23,  1.57it/s]Extractor Predicting: 36it [00:23,  1.59it/s]Extractor Predicting: 37it [00:24,  1.60it/s]Extractor Predicting: 38it [00:25,  1.56it/s]Extractor Predicting: 39it [00:25,  1.59it/s]Extractor Predicting: 40it [00:26,  1.59it/s]Extractor Predicting: 41it [00:27,  1.55it/s]Extractor Predicting: 42it [00:27,  1.60it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:28,  1.57it/s]Extractor Predicting: 45it [00:29,  1.58it/s]Extractor Predicting: 46it [00:30,  1.58it/s]Extractor Predicting: 47it [00:30,  1.51it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.57it/s]Extractor Predicting: 50it [00:32,  1.56it/s]Extractor Predicting: 51it [00:33,  1.55it/s]Extractor Predicting: 52it [00:34,  1.55it/s]Extractor Predicting: 53it [00:34,  1.53it/s]Extractor Predicting: 54it [00:35,  1.57it/s]Extractor Predicting: 55it [00:36,  1.56it/s]Extractor Predicting: 56it [00:36,  1.59it/s]Extractor Predicting: 57it [00:37,  1.62it/s]Extractor Predicting: 58it [00:37,  1.59it/s]Extractor Predicting: 59it [00:38,  1.64it/s]Extractor Predicting: 60it [00:39,  1.59it/s]Extractor Predicting: 61it [00:39,  1.61it/s]Extractor Predicting: 62it [00:40,  1.67it/s]Extractor Predicting: 63it [00:40,  1.67it/s]Extractor Predicting: 64it [00:41,  1.70it/s]Extractor Predicting: 65it [00:42,  1.71it/s]Extractor Predicting: 66it [00:42,  1.72it/s]Extractor Predicting: 67it [00:43,  1.71it/s]Extractor Predicting: 68it [00:43,  1.70it/s]Extractor Predicting: 69it [00:44,  1.74it/s]Extractor Predicting: 70it [00:44,  1.71it/s]Extractor Predicting: 71it [00:45,  1.68it/s]Extractor Predicting: 72it [00:46,  1.71it/s]Extractor Predicting: 73it [00:46,  1.65it/s]Extractor Predicting: 74it [00:47,  1.65it/s]Extractor Predicting: 75it [00:47,  1.68it/s]Extractor Predicting: 76it [00:48,  1.69it/s]Extractor Predicting: 77it [00:49,  1.71it/s]Extractor Predicting: 78it [00:49,  1.72it/s]Extractor Predicting: 79it [00:50,  1.73it/s]Extractor Predicting: 80it [00:50,  1.70it/s]Extractor Predicting: 81it [00:51,  1.66it/s]Extractor Predicting: 82it [00:52,  1.63it/s]Extractor Predicting: 83it [00:52,  1.65it/s]Extractor Predicting: 84it [00:53,  1.65it/s]Extractor Predicting: 85it [00:54,  1.60it/s]Extractor Predicting: 86it [00:54,  1.59it/s]Extractor Predicting: 87it [00:55,  1.60it/s]Extractor Predicting: 88it [00:55,  1.57it/s]Extractor Predicting: 89it [00:56,  1.57it/s]Extractor Predicting: 90it [00:57,  1.56it/s]Extractor Predicting: 91it [00:57,  1.57it/s]Extractor Predicting: 92it [00:58,  1.55it/s]Extractor Predicting: 93it [00:59,  1.54it/s]Extractor Predicting: 94it [00:59,  1.56it/s]Extractor Predicting: 95it [01:00,  1.55it/s]Extractor Predicting: 96it [01:01,  1.56it/s]Extractor Predicting: 97it [01:01,  1.55it/s]Extractor Predicting: 98it [01:02,  1.55it/s]Extractor Predicting: 99it [01:03,  1.56it/s]Extractor Predicting: 100it [01:03,  1.57it/s]Extractor Predicting: 101it [01:04,  1.57it/s]Extractor Predicting: 102it [01:04,  1.58it/s]Extractor Predicting: 103it [01:05,  1.58it/s]Extractor Predicting: 104it [01:06,  1.60it/s]Extractor Predicting: 105it [01:06,  1.62it/s]Extractor Predicting: 106it [01:07,  1.59it/s]Extractor Predicting: 107it [01:08,  1.58it/s]Extractor Predicting: 108it [01:08,  1.58it/s]Extractor Predicting: 109it [01:09,  1.57it/s]Extractor Predicting: 110it [01:09,  1.56it/s]Extractor Predicting: 111it [01:10,  1.55it/s]Extractor Predicting: 112it [01:11,  1.58it/s]Extractor Predicting: 113it [01:11,  1.59it/s]Extractor Predicting: 114it [01:12,  1.59it/s]Extractor Predicting: 115it [01:13,  1.58it/s]Extractor Predicting: 116it [01:13,  1.60it/s]Extractor Predicting: 117it [01:14,  1.59it/s]Extractor Predicting: 118it [01:15,  1.57it/s]Extractor Predicting: 119it [01:15,  1.58it/s]Extractor Predicting: 120it [01:16,  1.57it/s]Extractor Predicting: 121it [01:16,  1.56it/s]Extractor Predicting: 122it [01:17,  1.55it/s]Extractor Predicting: 123it [01:18,  1.55it/s]Extractor Predicting: 124it [01:18,  1.53it/s]Extractor Predicting: 125it [01:19,  1.56it/s]Extractor Predicting: 126it [01:20,  1.55it/s]Extractor Predicting: 127it [01:20,  1.54it/s]Extractor Predicting: 128it [01:21,  1.52it/s]Extractor Predicting: 129it [01:22,  1.42it/s]Extractor Predicting: 130it [01:22,  1.48it/s]Extractor Predicting: 131it [01:23,  1.47it/s]Extractor Predicting: 132it [01:24,  1.51it/s]Extractor Predicting: 133it [01:24,  1.52it/s]Extractor Predicting: 134it [01:25,  1.52it/s]Extractor Predicting: 135it [01:26,  1.51it/s]Extractor Predicting: 136it [01:26,  1.52it/s]Extractor Predicting: 137it [01:27,  1.55it/s]Extractor Predicting: 138it [01:28,  1.56it/s]Extractor Predicting: 139it [01:28,  1.57it/s]Extractor Predicting: 140it [01:29,  1.59it/s]Extractor Predicting: 141it [01:29,  1.62it/s]Extractor Predicting: 141it [01:29,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:24,170 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:24,172 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:24,172 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:24,172 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:24,173 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:28:24,799 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:28:24,800 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:28:27,982 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:28:29,014 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:28:29,015 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:31,203 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:31,231 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:31,231 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:31,231 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:31,231 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:28:31,576 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:28:31,577 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:28:31,837 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:28:31,986 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:28:31,987 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2652929813264649,
  "recall": 0.1195589088798607,
  "score": 0.16483296659331867,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.67it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.59it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:13,  1.59it/s]Extractor Predicting: 22it [00:13,  1.61it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:14,  1.57it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.57it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:18,  1.53it/s]Extractor Predicting: 31it [00:19,  1.52it/s]Extractor Predicting: 32it [00:20,  1.51it/s]Extractor Predicting: 33it [00:20,  1.52it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:22,  1.43it/s]Extractor Predicting: 36it [00:22,  1.48it/s]Extractor Predicting: 37it [00:23,  1.54it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:24,  1.55it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:26,  1.56it/s]Extractor Predicting: 42it [00:26,  1.56it/s]Extractor Predicting: 43it [00:27,  1.55it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:28,  1.54it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:30,  1.57it/s]Extractor Predicting: 49it [00:31,  1.57it/s]Extractor Predicting: 50it [00:31,  1.55it/s]Extractor Predicting: 51it [00:32,  1.56it/s]Extractor Predicting: 52it [00:33,  1.56it/s]Extractor Predicting: 53it [00:33,  1.56it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:34,  1.54it/s]Extractor Predicting: 56it [00:35,  1.56it/s]Extractor Predicting: 57it [00:36,  1.59it/s]Extractor Predicting: 58it [00:36,  1.62it/s]Extractor Predicting: 59it [00:37,  1.62it/s]Extractor Predicting: 60it [00:38,  1.62it/s]Extractor Predicting: 61it [00:38,  1.60it/s]Extractor Predicting: 62it [00:39,  1.60it/s]Extractor Predicting: 63it [00:39,  1.56it/s]Extractor Predicting: 64it [00:40,  1.59it/s]Extractor Predicting: 65it [00:41,  1.56it/s]Extractor Predicting: 66it [00:41,  1.54it/s]Extractor Predicting: 67it [00:42,  1.58it/s]Extractor Predicting: 68it [00:43,  1.59it/s]Extractor Predicting: 69it [00:43,  1.61it/s]Extractor Predicting: 70it [00:44,  1.61it/s]Extractor Predicting: 71it [00:45,  1.59it/s]Extractor Predicting: 72it [00:45,  1.61it/s]Extractor Predicting: 73it [00:46,  1.56it/s]Extractor Predicting: 74it [00:46,  1.56it/s]Extractor Predicting: 75it [00:47,  1.57it/s]Extractor Predicting: 76it [00:48,  1.58it/s]Extractor Predicting: 77it [00:48,  1.55it/s]Extractor Predicting: 78it [00:49,  1.54it/s]Extractor Predicting: 79it [00:50,  1.56it/s]Extractor Predicting: 80it [00:50,  1.55it/s]Extractor Predicting: 81it [00:51,  1.56it/s]Extractor Predicting: 82it [00:52,  1.56it/s]Extractor Predicting: 83it [00:52,  1.54it/s]Extractor Predicting: 84it [00:53,  1.56it/s]Extractor Predicting: 85it [00:54,  1.56it/s]Extractor Predicting: 86it [00:54,  1.56it/s]Extractor Predicting: 87it [00:55,  1.55it/s]Extractor Predicting: 88it [00:55,  1.56it/s]Extractor Predicting: 89it [00:56,  1.54it/s]Extractor Predicting: 90it [00:57,  1.53it/s]Extractor Predicting: 91it [00:57,  1.55it/s]Extractor Predicting: 92it [00:58,  1.55it/s]Extractor Predicting: 93it [00:59,  1.54it/s]Extractor Predicting: 94it [00:59,  1.51it/s]Extractor Predicting: 95it [01:00,  1.50it/s]Extractor Predicting: 96it [01:01,  1.52it/s]Extractor Predicting: 97it [01:01,  1.52it/s]Extractor Predicting: 98it [01:02,  1.54it/s]Extractor Predicting: 99it [01:03,  1.52it/s]Extractor Predicting: 100it [01:03,  1.52it/s]Extractor Predicting: 101it [01:04,  1.55it/s]Extractor Predicting: 102it [01:05,  1.54it/s]Extractor Predicting: 103it [01:05,  1.54it/s]Extractor Predicting: 104it [01:06,  1.53it/s]Extractor Predicting: 105it [01:07,  1.53it/s]Extractor Predicting: 106it [01:07,  1.54it/s]Extractor Predicting: 107it [01:08,  1.49it/s]Extractor Predicting: 108it [01:09,  1.50it/s]Extractor Predicting: 109it [01:09,  1.52it/s]Extractor Predicting: 110it [01:10,  1.50it/s]Extractor Predicting: 111it [01:11,  1.50it/s]Extractor Predicting: 112it [01:11,  1.52it/s]Extractor Predicting: 113it [01:12,  1.56it/s]Extractor Predicting: 114it [01:12,  1.58it/s]Extractor Predicting: 115it [01:13,  1.56it/s]Extractor Predicting: 116it [01:14,  1.56it/s]Extractor Predicting: 117it [01:14,  1.60it/s]Extractor Predicting: 118it [01:15,  1.58it/s]Extractor Predicting: 119it [01:16,  1.59it/s]Extractor Predicting: 120it [01:16,  1.60it/s]Extractor Predicting: 121it [01:17,  1.60it/s]Extractor Predicting: 122it [01:17,  1.63it/s]Extractor Predicting: 123it [01:18,  1.62it/s]Extractor Predicting: 124it [01:19,  1.42it/s]Extractor Predicting: 125it [01:20,  1.46it/s]Extractor Predicting: 126it [01:20,  1.47it/s]Extractor Predicting: 127it [01:21,  1.51it/s]Extractor Predicting: 128it [01:21,  1.56it/s]Extractor Predicting: 129it [01:22,  1.53it/s]Extractor Predicting: 130it [01:23,  1.54it/s]Extractor Predicting: 131it [01:23,  1.54it/s]Extractor Predicting: 132it [01:24,  1.55it/s]Extractor Predicting: 133it [01:25,  1.55it/s]Extractor Predicting: 134it [01:25,  1.55it/s]Extractor Predicting: 135it [01:26,  1.57it/s]Extractor Predicting: 136it [01:27,  1.58it/s]Extractor Predicting: 137it [01:27,  1.58it/s]Extractor Predicting: 138it [01:28,  1.57it/s]Extractor Predicting: 139it [01:28,  1.61it/s]Extractor Predicting: 140it [01:29,  1.60it/s]Extractor Predicting: 141it [01:30,  1.58it/s]Extractor Predicting: 142it [01:30,  1.60it/s]Extractor Predicting: 143it [01:31,  1.60it/s]Extractor Predicting: 144it [01:32,  1.57it/s]Extractor Predicting: 145it [01:32,  1.59it/s]Extractor Predicting: 146it [01:33,  1.58it/s]Extractor Predicting: 147it [01:34,  1.56it/s]Extractor Predicting: 148it [01:34,  1.55it/s]Extractor Predicting: 149it [01:35,  1.55it/s]Extractor Predicting: 150it [01:35,  1.57it/s]Extractor Predicting: 151it [01:36,  1.60it/s]Extractor Predicting: 152it [01:37,  1.61it/s]Extractor Predicting: 153it [01:37,  1.61it/s]Extractor Predicting: 154it [01:38,  1.57it/s]Extractor Predicting: 155it [01:39,  1.55it/s]Extractor Predicting: 156it [01:39,  1.54it/s]Extractor Predicting: 157it [01:40,  1.54it/s]Extractor Predicting: 158it [01:41,  1.52it/s]Extractor Predicting: 159it [01:41,  1.52it/s]Extractor Predicting: 160it [01:42,  1.53it/s]Extractor Predicting: 161it [01:43,  1.56it/s]Extractor Predicting: 162it [01:43,  1.58it/s]Extractor Predicting: 163it [01:44,  1.57it/s]Extractor Predicting: 164it [01:44,  1.57it/s]Extractor Predicting: 165it [01:45,  1.56it/s]Extractor Predicting: 166it [01:46,  1.57it/s]Extractor Predicting: 167it [01:46,  1.59it/s]Extractor Predicting: 168it [01:47,  1.57it/s]Extractor Predicting: 169it [01:48,  1.56it/s]Extractor Predicting: 170it [01:48,  1.55it/s]Extractor Predicting: 171it [01:49,  1.52it/s]Extractor Predicting: 172it [01:50,  1.51it/s]Extractor Predicting: 173it [01:50,  1.51it/s]Extractor Predicting: 174it [01:51,  1.47it/s]Extractor Predicting: 175it [01:52,  1.45it/s]Extractor Predicting: 176it [01:52,  1.47it/s]Extractor Predicting: 177it [01:53,  1.50it/s]Extractor Predicting: 178it [01:54,  1.51it/s]Extractor Predicting: 179it [01:54,  1.55it/s]Extractor Predicting: 180it [01:55,  1.53it/s]Extractor Predicting: 181it [01:56,  1.56it/s]Extractor Predicting: 182it [01:56,  1.56it/s]Extractor Predicting: 183it [01:57,  1.57it/s]Extractor Predicting: 184it [01:57,  1.59it/s]Extractor Predicting: 185it [01:58,  1.60it/s]Extractor Predicting: 186it [01:59,  1.62it/s]Extractor Predicting: 187it [01:59,  1.61it/s]Extractor Predicting: 188it [02:00,  1.62it/s]Extractor Predicting: 189it [02:01,  1.62it/s]Extractor Predicting: 190it [02:01,  1.62it/s]Extractor Predicting: 191it [02:02,  1.57it/s]Extractor Predicting: 192it [02:03,  1.55it/s]Extractor Predicting: 193it [02:03,  1.56it/s]Extractor Predicting: 194it [02:04,  1.60it/s]Extractor Predicting: 195it [02:04,  1.61it/s]Extractor Predicting: 196it [02:05,  1.61it/s]Extractor Predicting: 197it [02:06,  1.62it/s]Extractor Predicting: 198it [02:06,  1.61it/s]Extractor Predicting: 199it [02:07,  1.60it/s]Extractor Predicting: 200it [02:07,  1.59it/s]Extractor Predicting: 201it [02:08,  1.59it/s]Extractor Predicting: 202it [02:09,  1.61it/s]Extractor Predicting: 203it [02:09,  1.64it/s]Extractor Predicting: 204it [02:10,  1.64it/s]Extractor Predicting: 205it [02:11,  1.63it/s]Extractor Predicting: 206it [02:11,  1.61it/s]Extractor Predicting: 207it [02:12,  1.57it/s]Extractor Predicting: 208it [02:12,  1.57it/s]Extractor Predicting: 209it [02:13,  1.58it/s]Extractor Predicting: 210it [02:14,  1.57it/s]Extractor Predicting: 211it [02:14,  1.59it/s]Extractor Predicting: 212it [02:15,  1.57it/s]Extractor Predicting: 213it [02:16,  1.61it/s]Extractor Predicting: 214it [02:16,  1.60it/s]Extractor Predicting: 215it [02:17,  1.63it/s]Extractor Predicting: 216it [02:17,  1.62it/s]Extractor Predicting: 217it [02:18,  1.55it/s]Extractor Predicting: 218it [02:19,  1.57it/s]Extractor Predicting: 219it [02:19,  1.60it/s]Extractor Predicting: 220it [02:20,  1.62it/s]Extractor Predicting: 221it [02:21,  1.60it/s]Extractor Predicting: 222it [02:21,  1.42it/s]Extractor Predicting: 223it [02:22,  1.49it/s]Extractor Predicting: 224it [02:23,  1.52it/s]Extractor Predicting: 225it [02:23,  1.53it/s]Extractor Predicting: 226it [02:24,  1.51it/s]Extractor Predicting: 227it [02:25,  1.52it/s]Extractor Predicting: 228it [02:25,  1.49it/s]Extractor Predicting: 229it [02:26,  1.50it/s]Extractor Predicting: 230it [02:27,  1.53it/s]Extractor Predicting: 231it [02:27,  1.52it/s]Extractor Predicting: 232it [02:28,  1.52it/s]Extractor Predicting: 233it [02:29,  1.54it/s]Extractor Predicting: 234it [02:29,  1.54it/s]Extractor Predicting: 235it [02:30,  1.51it/s]Extractor Predicting: 236it [02:31,  1.54it/s]Extractor Predicting: 237it [02:31,  1.59it/s]Extractor Predicting: 238it [02:32,  1.56it/s]Extractor Predicting: 239it [02:33,  1.53it/s]Extractor Predicting: 240it [02:33,  1.51it/s]Extractor Predicting: 241it [02:34,  1.53it/s]Extractor Predicting: 242it [02:34,  1.56it/s]Extractor Predicting: 243it [02:35,  1.56it/s]Extractor Predicting: 244it [02:36,  1.53it/s]Extractor Predicting: 245it [02:36,  1.55it/s]Extractor Predicting: 246it [02:37,  1.56it/s]Extractor Predicting: 247it [02:38,  1.58it/s]Extractor Predicting: 248it [02:38,  1.52it/s]Extractor Predicting: 249it [02:39,  1.53it/s]Extractor Predicting: 250it [02:40,  1.54it/s]Extractor Predicting: 251it [02:40,  1.52it/s]Extractor Predicting: 252it [02:41,  1.51it/s]Extractor Predicting: 253it [02:42,  1.52it/s]Extractor Predicting: 254it [02:42,  1.55it/s]Extractor Predicting: 255it [02:43,  1.55it/s]Extractor Predicting: 256it [02:43,  1.59it/s]Extractor Predicting: 257it [02:44,  1.62it/s]Extractor Predicting: 258it [02:45,  1.60it/s]Extractor Predicting: 259it [02:45,  1.59it/s]Extractor Predicting: 260it [02:46,  1.56it/s]Extractor Predicting: 261it [02:47,  1.56it/s]Extractor Predicting: 262it [02:47,  1.56it/s]Extractor Predicting: 263it [02:48,  1.50it/s]Extractor Predicting: 264it [02:49,  1.52it/s]Extractor Predicting: 265it [02:49,  1.53it/s]Extractor Predicting: 266it [02:50,  1.55it/s]Extractor Predicting: 267it [02:51,  1.55it/s]Extractor Predicting: 268it [02:51,  1.58it/s]Extractor Predicting: 269it [02:52,  1.59it/s]Extractor Predicting: 270it [02:52,  1.62it/s]Extractor Predicting: 271it [02:53,  1.59it/s]Extractor Predicting: 272it [02:54,  1.61it/s]Extractor Predicting: 273it [02:54,  1.59it/s]Extractor Predicting: 274it [02:55,  1.59it/s]Extractor Predicting: 275it [02:56,  1.60it/s]Extractor Predicting: 276it [02:56,  1.60it/s]Extractor Predicting: 277it [02:57,  1.57it/s]Extractor Predicting: 278it [02:57,  1.61it/s]Extractor Predicting: 279it [02:58,  1.56it/s]Extractor Predicting: 280it [02:59,  1.56it/s]Extractor Predicting: 281it [02:59,  1.56it/s]Extractor Predicting: 282it [03:00,  1.58it/s]Extractor Predicting: 283it [03:01,  1.58it/s]Extractor Predicting: 284it [03:01,  1.55it/s]Extractor Predicting: 285it [03:02,  1.55it/s]Extractor Predicting: 286it [03:03,  1.56it/s]Extractor Predicting: 287it [03:03,  1.58it/s]Extractor Predicting: 288it [03:04,  1.58it/s]Extractor Predicting: 289it [03:05,  1.54it/s]Extractor Predicting: 290it [03:05,  1.60it/s]Extractor Predicting: 291it [03:06,  1.58it/s]Extractor Predicting: 292it [03:06,  1.56it/s]Extractor Predicting: 293it [03:07,  1.53it/s]Extractor Predicting: 294it [03:08,  1.50it/s]Extractor Predicting: 295it [03:08,  1.55it/s]Extractor Predicting: 296it [03:09,  1.52it/s]Extractor Predicting: 297it [03:10,  1.55it/s]Extractor Predicting: 298it [03:10,  1.51it/s]Extractor Predicting: 299it [03:11,  1.52it/s]Extractor Predicting: 300it [03:12,  1.53it/s]Extractor Predicting: 301it [03:12,  1.54it/s]Extractor Predicting: 302it [03:13,  1.52it/s]Extractor Predicting: 303it [03:14,  1.51it/s]Extractor Predicting: 304it [03:14,  1.48it/s]Extractor Predicting: 305it [03:15,  1.50it/s]Extractor Predicting: 306it [03:16,  1.48it/s]Extractor Predicting: 307it [03:16,  1.48it/s]Extractor Predicting: 308it [03:17,  1.50it/s]Extractor Predicting: 309it [03:18,  1.50it/s]Extractor Predicting: 310it [03:18,  1.48it/s]Extractor Predicting: 311it [03:19,  1.47it/s]Extractor Predicting: 312it [03:20,  1.49it/s]Extractor Predicting: 313it [03:20,  1.49it/s]Extractor Predicting: 314it [03:21,  1.46it/s]Extractor Predicting: 315it [03:22,  1.51it/s]Extractor Predicting: 316it [03:22,  1.50it/s]Extractor Predicting: 317it [03:23,  1.48it/s]Extractor Predicting: 318it [03:24,  1.49it/s]Extractor Predicting: 319it [03:24,  1.49it/s]Extractor Predicting: 320it [03:25,  1.58it/s]Extractor Predicting: 321it [03:26,  1.62it/s]Extractor Predicting: 322it [03:26,  1.70it/s]Extractor Predicting: 323it [03:27,  1.73it/s]Extractor Predicting: 324it [03:27,  1.74it/s]Extractor Predicting: 325it [03:28,  1.51it/s]Extractor Predicting: 326it [03:29,  1.57it/s]Extractor Predicting: 327it [03:29,  1.60it/s]Extractor Predicting: 328it [03:30,  1.66it/s]Extractor Predicting: 329it [03:30,  1.72it/s]Extractor Predicting: 330it [03:31,  1.73it/s]Extractor Predicting: 331it [03:31,  1.78it/s]Extractor Predicting: 332it [03:32,  1.76it/s]Extractor Predicting: 333it [03:33,  1.79it/s]Extractor Predicting: 334it [03:33,  1.80it/s]Extractor Predicting: 335it [03:34,  1.79it/s]Extractor Predicting: 336it [03:34,  1.76it/s]Extractor Predicting: 337it [03:35,  1.81it/s]Extractor Predicting: 338it [03:35,  1.76it/s]Extractor Predicting: 339it [03:36,  1.74it/s]Extractor Predicting: 340it [03:36,  1.80it/s]Extractor Predicting: 341it [03:37,  1.78it/s]Extractor Predicting: 342it [03:38,  1.80it/s]Extractor Predicting: 343it [03:38,  1.79it/s]Extractor Predicting: 344it [03:39,  1.76it/s]Extractor Predicting: 345it [03:39,  1.79it/s]Extractor Predicting: 346it [03:40,  1.78it/s]Extractor Predicting: 347it [03:40,  1.75it/s]Extractor Predicting: 348it [03:41,  1.68it/s]Extractor Predicting: 349it [03:42,  1.64it/s]Extractor Predicting: 350it [03:42,  1.60it/s]Extractor Predicting: 351it [03:43,  1.58it/s]Extractor Predicting: 352it [03:44,  1.59it/s]Extractor Predicting: 353it [03:44,  1.60it/s]Extractor Predicting: 354it [03:45,  1.58it/s]Extractor Predicting: 355it [03:46,  1.57it/s]Extractor Predicting: 356it [03:46,  1.55it/s]Extractor Predicting: 357it [03:47,  1.53it/s]Extractor Predicting: 358it [03:48,  1.55it/s]Extractor Predicting: 359it [03:48,  1.55it/s]Extractor Predicting: 360it [03:49,  1.58it/s]Extractor Predicting: 361it [03:49,  1.57it/s]Extractor Predicting: 362it [03:50,  1.60it/s]Extractor Predicting: 363it [03:51,  1.57it/s]Extractor Predicting: 364it [03:51,  1.56it/s]Extractor Predicting: 365it [03:52,  1.56it/s]Extractor Predicting: 366it [03:53,  1.56it/s]Extractor Predicting: 367it [03:53,  1.55it/s]Extractor Predicting: 368it [03:54,  1.57it/s]Extractor Predicting: 369it [03:55,  1.56it/s]Extractor Predicting: 370it [03:55,  1.55it/s]Extractor Predicting: 371it [03:56,  1.55it/s]Extractor Predicting: 372it [03:57,  1.55it/s]Extractor Predicting: 373it [03:57,  1.54it/s]Extractor Predicting: 374it [03:58,  1.55it/s]Extractor Predicting: 375it [03:58,  1.56it/s]Extractor Predicting: 376it [03:59,  1.54it/s]Extractor Predicting: 377it [04:00,  1.56it/s]Extractor Predicting: 378it [04:00,  1.53it/s]Extractor Predicting: 379it [04:01,  1.52it/s]Extractor Predicting: 380it [04:02,  1.52it/s]Extractor Predicting: 381it [04:02,  1.50it/s]Extractor Predicting: 382it [04:03,  1.49it/s]Extractor Predicting: 383it [04:04,  1.50it/s]Extractor Predicting: 384it [04:04,  1.49it/s]Extractor Predicting: 385it [04:05,  1.49it/s]Extractor Predicting: 386it [04:06,  1.51it/s]Extractor Predicting: 387it [04:06,  1.48it/s]Extractor Predicting: 388it [04:07,  1.48it/s]Extractor Predicting: 389it [04:08,  1.52it/s]Extractor Predicting: 390it [04:08,  1.52it/s]Extractor Predicting: 391it [04:09,  1.52it/s]Extractor Predicting: 392it [04:10,  1.49it/s]Extractor Predicting: 393it [04:10,  1.54it/s]Extractor Predicting: 394it [04:11,  1.52it/s]Extractor Predicting: 395it [04:12,  1.49it/s]Extractor Predicting: 396it [04:12,  1.49it/s]Extractor Predicting: 397it [04:13,  1.47it/s]Extractor Predicting: 398it [04:14,  1.48it/s]Extractor Predicting: 399it [04:15,  1.45it/s]Extractor Predicting: 400it [04:15,  1.45it/s]Extractor Predicting: 401it [04:16,  1.46it/s]Extractor Predicting: 402it [04:17,  1.48it/s]Extractor Predicting: 403it [04:17,  1.48it/s]Extractor Predicting: 404it [04:18,  1.53it/s]Extractor Predicting: 405it [04:18,  1.52it/s]Extractor Predicting: 406it [04:19,  1.56it/s]Extractor Predicting: 407it [04:20,  1.55it/s]Extractor Predicting: 408it [04:20,  1.53it/s]Extractor Predicting: 409it [04:21,  1.56it/s]Extractor Predicting: 410it [04:22,  1.55it/s]Extractor Predicting: 411it [04:22,  1.58it/s]Extractor Predicting: 412it [04:23,  1.55it/s]Extractor Predicting: 413it [04:24,  1.58it/s]Extractor Predicting: 414it [04:24,  1.58it/s]Extractor Predicting: 415it [04:25,  1.57it/s]Extractor Predicting: 416it [04:26,  1.54it/s]Extractor Predicting: 417it [04:26,  1.57it/s]Extractor Predicting: 418it [04:27,  1.56it/s]Extractor Predicting: 419it [04:27,  1.54it/s]Extractor Predicting: 420it [04:28,  1.55it/s]Extractor Predicting: 421it [04:29,  1.55it/s]Extractor Predicting: 422it [04:29,  1.53it/s]Extractor Predicting: 423it [04:30,  1.54it/s]Extractor Predicting: 424it [04:31,  1.51it/s]Extractor Predicting: 425it [04:31,  1.53it/s]Extractor Predicting: 426it [04:32,  1.51it/s]Extractor Predicting: 427it [04:33,  1.53it/s]Extractor Predicting: 428it [04:33,  1.52it/s]Extractor Predicting: 429it [04:34,  1.54it/s]Extractor Predicting: 430it [04:34,  1.90it/s]Extractor Predicting: 430it [04:34,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:15,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:15,427 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:15,427 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:15,427 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:15,427 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:33:15,715 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:33:15,716 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:33:15,969 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:33:17,037 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:33:17,037 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:19,187 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:19,195 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:19,195 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:19,195 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:19,195 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:33:19,923 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:33:19,924 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:33:20,184 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:33:20,352 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:33:20,352 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.25754884547069273,
  "recall": 0.11264323169547485,
  "score": 0.15673557627347656,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.42it/s]Extractor Predicting: 5it [00:03,  1.93it/s]Extractor Predicting: 5it [00:03,  1.66it/s]
[INFO|configuration_utils.py:515] 2023-08-28 23:33:23,802 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:33:23,803 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:33:23,807 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:33:23,808 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 23:33:23,810 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:33:27,985 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 23:33:27,991 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 23:33:28,004 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:33:28,005 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:33:28,014 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:33:28,020 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:33:28,020 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:33:28,020 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:33:28,020 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:33:28,020 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:33:28,020 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.45714285714285713,
  "recall": 0.07920792079207921,
  "score": 0.1350210970464135,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 23:33:28,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:28,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:29,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:30,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:30,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:31,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:32,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:32,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:33,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:33,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:34,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:35,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:35,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:36,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:37,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:37,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:38,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:38,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:39,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:40,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:12<04:00, 12.67s/it][WARNING|generation_utils.py:914] 2023-08-28 23:33:40,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:41,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:42,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:42,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:43,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:44,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:44,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:45,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:46,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:46,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:47,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:47,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:48,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:49,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:49,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:50,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:50,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:51,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:52,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:52,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:25<03:45, 12.51s/it][WARNING|generation_utils.py:914] 2023-08-28 23:33:53,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:53,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:54,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:54,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:55,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:55,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:56,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:56,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:57,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:57,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:58,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:58,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:59,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:59,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:00,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:00,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:01,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:01,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:02,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:02,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:02,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:35<03:14, 11.45s/it][WARNING|generation_utils.py:914] 2023-08-28 23:34:03,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:04,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:04,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:05,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:05,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:06,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:06,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:07,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:08,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:08,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:09,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:09,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:10,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:10,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:11,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:12,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:12,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:13,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:13,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:14,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:46<03:03, 11.46s/it][WARNING|generation_utils.py:914] 2023-08-28 23:34:15,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:15,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:16,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:16,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:17,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:17,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:18,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:18,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:19,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:19,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:20,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:20,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:21,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:21,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:22,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:22,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:23,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:23,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:24,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:24,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [00:57<02:45, 11.07s/it][WARNING|generation_utils.py:914] 2023-08-28 23:34:25,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:25,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:26,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:27,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:27,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:28,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:28,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:29,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:30,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:30,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:31,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:31,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:32,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:33,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:33,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:34,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:34,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:35,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:35,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:36,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:37,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:09<02:40, 11.45s/it][WARNING|generation_utils.py:914] 2023-08-28 23:34:37,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:38,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:38,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:39,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:39,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:40,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:40,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:41,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:41,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:42,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:42,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:43,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:43,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:44,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:44,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:45,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:45,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:46,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:46,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:47,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:19<02:22, 10.98s/it][WARNING|generation_utils.py:914] 2023-08-28 23:34:47,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:48,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:48,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:49,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:50,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:50,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:51,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:51,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:52,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:53,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:53,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:54,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:54,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:55,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:56,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:56,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:57,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:58,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:59,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:59,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:00,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:00,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:33<02:23, 11.92s/it][WARNING|generation_utils.py:914] 2023-08-28 23:35:01,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:02,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:02,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:03,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:04,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:04,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:05,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:05,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:06,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:07,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:07,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:08,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:08,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:09,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:10,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:10,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:11,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:12,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:13,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:13,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:14,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:15,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:15,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:48<02:21, 12.85s/it][WARNING|generation_utils.py:914] 2023-08-28 23:35:16,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:17,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:17,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:18,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:18,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:19,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:20,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:21,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:21,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:22,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:22,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:23,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:24,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:24,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:25,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:26,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:26,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:27,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:27,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:28,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:29,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:01<02:09, 12.95s/it][WARNING|generation_utils.py:914] 2023-08-28 23:35:29,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:30,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:30,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:31,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:32,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:32,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:33,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:33,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:34,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:35,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:35,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:36,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:37,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:38,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:38,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:39,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:39,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:40,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:41,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:41,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:14<01:56, 12.93s/it][WARNING|generation_utils.py:914] 2023-08-28 23:35:42,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:43,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:43,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:44,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:44,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:45,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:45,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:46,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:47,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:47,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:48,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:48,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:49,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:49,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:50,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:51,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:51,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:52,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:52,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:53,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:53,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:25<01:40, 12.59s/it][WARNING|generation_utils.py:914] 2023-08-28 23:35:54,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:54,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:55,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:55,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:56,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:56,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:57,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:57,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:58,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:58,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:59,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:59,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:00,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:00,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:01,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:02,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:02,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:03,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:03,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:03,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:36<01:22, 11.85s/it][WARNING|generation_utils.py:914] 2023-08-28 23:36:04,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:05,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:05,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:06,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:06,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:07,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:08,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:08,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:09,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:09,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:10,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:11,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:11,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:12,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:13,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:13,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:14,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:15,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:15,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:16,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:17,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [02:49<01:13, 12.30s/it][WARNING|generation_utils.py:914] 2023-08-28 23:36:17,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:18,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:18,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:19,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:19,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:20,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:21,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:21,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:22,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:22,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:23,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:24,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:25,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:25,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:26,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:26,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:27,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:27,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:28,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:29,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:29,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:30,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:02<01:02, 12.53s/it][WARNING|generation_utils.py:914] 2023-08-28 23:36:30,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:31,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:32,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:33,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:33,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:34,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:34,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:35,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:36,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:36,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:37,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:38,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:38,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:39,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:40,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:40,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:41,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:42,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:42,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:43,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:15<00:51, 12.78s/it][WARNING|generation_utils.py:914] 2023-08-28 23:36:44,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:44,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:45,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:46,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:46,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:47,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:47,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:48,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:49,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:49,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:50,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:51,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:51,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:52,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:52,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:53,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:54,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:54,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:55,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:55,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:56,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:56,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:29<00:38, 12.94s/it][WARNING|generation_utils.py:914] 2023-08-28 23:36:57,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:58,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:58,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:59,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:59,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:00,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:01,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:01,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:02,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:02,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:03,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:04,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:04,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:05,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:05,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:06,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:07,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:07,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:08,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:08,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:41<00:25, 12.63s/it][WARNING|generation_utils.py:914] 2023-08-28 23:37:09,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:09,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:10,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:11,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:11,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:12,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:13,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:13,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:14,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:14,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:15,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:15,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:16,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:17,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:17,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:18,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:18,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:19,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:19,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:20,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:20,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [03:53<00:12, 12.42s/it][WARNING|generation_utils.py:914] 2023-08-28 23:37:21,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:21,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:22,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:23,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:23,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:24,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:25,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:25,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:26,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:27,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:28,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:29,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:29,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:30,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:31,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:32,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:32,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:33,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:34,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:34,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:37:35,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:08<00:00, 13.19s/it]Generating: 100%|██████████| 20/20 [04:08<00:00, 12.40s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:42,221 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:42,227 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:42,227 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:42,227 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:42,227 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:37:42,956 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:37:42,957 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:37:43,225 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:37:44,324 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:37:44,324 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:46,508 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:46,511 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:46,511 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:46,511 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:46,511 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:37:47,260 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:37:47,261 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:37:47,941 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:37:48,117 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:37:48,117 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : followed by .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : military rank .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 631, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9389880952380952, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9671875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 532, 'raw': 544}
{'target': 600, 'success': 564, 'raw': 576}
{'target': 600, 'success': 595, 'raw': 608}
{'target': 600, 'success': 627, 'raw': 640}
{'prompt': 'Relation : tributary .', 'success_rate': 0.9796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : architect .', 'success_rate': 0.9211309523809523, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 223, 'raw': 224}
{'target': 600, 'success': 254, 'raw': 256}
{'target': 600, 'success': 286, 'raw': 288}
{'target': 600, 'success': 317, 'raw': 320}
{'target': 600, 'success': 349, 'raw': 352}
{'target': 600, 'success': 380, 'raw': 384}
{'target': 600, 'success': 412, 'raw': 416}
{'target': 600, 'success': 443, 'raw': 448}
{'target': 600, 'success': 473, 'raw': 480}
{'target': 600, 'success': 505, 'raw': 512}
{'target': 600, 'success': 533, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : constellation .', 'success_rate': 0.9734375, 'errors': {''}}
['Relation : contains administrative territorial entity . Context : The city of Marchec is under the control of the Marchec Municipal Municipality . Head Entity : Marchec Municipal Municipality , Tail Entity : Marchec , Head Entity : Marchec .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.890625, 'errors': {'too many values to unpack (expected 2)', '', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.8396739130434783, 'errors': {'', '(\'Henry VIII\', \'country of origin\', \'\', "The river is named after Sir John \'s father , Henry VIII .")', '(\'2012 Summer Paralympics\', \'country of origin\', \'\', "In the 1990s , she competed in the Women \'s 100 Mydro 1000 at the 2012 Summer Paralympics in Sochi .")'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : league .', 'success_rate': 0.90625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9671875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8877840909090909, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : notable work .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.8849431818181818, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : position held .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 9848
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9948, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.53it/s]Extractor Estimating: 2it [00:01,  1.38it/s]Extractor Estimating: 3it [00:01,  1.53it/s]Extractor Estimating: 4it [00:02,  1.54it/s]Extractor Estimating: 5it [00:03,  1.39it/s]Extractor Estimating: 6it [00:04,  1.47it/s]Extractor Estimating: 7it [00:04,  1.46it/s]Extractor Estimating: 8it [00:05,  1.53it/s]Extractor Estimating: 9it [00:06,  1.54it/s]Extractor Estimating: 10it [00:06,  1.57it/s]Extractor Estimating: 11it [00:07,  1.58it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.55it/s]Extractor Estimating: 14it [00:09,  1.57it/s]Extractor Estimating: 15it [00:09,  1.56it/s]Extractor Estimating: 16it [00:10,  1.57it/s]Extractor Estimating: 17it [00:11,  1.59it/s]Extractor Estimating: 18it [00:11,  1.64it/s]Extractor Estimating: 19it [00:12,  1.66it/s]Extractor Estimating: 20it [00:12,  1.56it/s]Extractor Estimating: 21it [00:13,  1.57it/s]Extractor Estimating: 22it [00:14,  1.64it/s]Extractor Estimating: 23it [00:14,  1.48it/s]Extractor Estimating: 24it [00:15,  1.48it/s]Extractor Estimating: 25it [00:16,  1.48it/s]Extractor Estimating: 26it [00:16,  1.50it/s]Extractor Estimating: 27it [00:17,  1.51it/s]Extractor Estimating: 28it [00:18,  1.57it/s]Extractor Estimating: 29it [00:18,  1.57it/s]Extractor Estimating: 30it [00:19,  1.61it/s]Extractor Estimating: 31it [00:20,  1.61it/s]Extractor Estimating: 32it [00:20,  1.60it/s]Extractor Estimating: 33it [00:21,  1.62it/s]Extractor Estimating: 34it [00:21,  1.65it/s]Extractor Estimating: 35it [00:22,  1.64it/s]Extractor Estimating: 36it [00:23,  1.61it/s]Extractor Estimating: 37it [00:23,  1.61it/s]Extractor Estimating: 38it [00:24,  1.63it/s]Extractor Estimating: 39it [00:24,  1.67it/s]Extractor Estimating: 40it [00:25,  1.65it/s]Extractor Estimating: 41it [00:26,  1.64it/s]Extractor Estimating: 42it [00:26,  1.62it/s]Extractor Estimating: 43it [00:27,  1.63it/s]Extractor Estimating: 44it [00:27,  1.63it/s]Extractor Estimating: 45it [00:28,  1.68it/s]Extractor Estimating: 46it [00:29,  1.70it/s]Extractor Estimating: 47it [00:29,  1.68it/s]Extractor Estimating: 48it [00:30,  1.72it/s]Extractor Estimating: 49it [00:30,  1.71it/s]Extractor Estimating: 50it [00:31,  1.60it/s]Extractor Estimating: 51it [00:32,  1.76it/s]Extractor Estimating: 52it [00:32,  1.82it/s]Extractor Estimating: 53it [00:32,  1.94it/s]Extractor Estimating: 54it [00:33,  2.07it/s]Extractor Estimating: 55it [00:33,  2.12it/s]Extractor Estimating: 56it [00:34,  2.17it/s]Extractor Estimating: 57it [00:34,  2.14it/s]Extractor Estimating: 58it [00:35,  2.09it/s]Extractor Estimating: 59it [00:35,  2.20it/s]Extractor Estimating: 60it [00:36,  2.21it/s]Extractor Estimating: 61it [00:36,  2.27it/s]Extractor Estimating: 62it [00:36,  2.20it/s]Extractor Estimating: 63it [00:37,  2.14it/s]Extractor Estimating: 64it [00:37,  2.15it/s]Extractor Estimating: 65it [00:38,  2.15it/s]Extractor Estimating: 66it [00:38,  2.18it/s]Extractor Estimating: 67it [00:39,  2.04it/s]Extractor Estimating: 68it [00:39,  2.10it/s]Extractor Estimating: 69it [00:40,  2.14it/s]Extractor Estimating: 70it [00:40,  2.15it/s]Extractor Estimating: 71it [00:41,  2.17it/s]Extractor Estimating: 72it [00:41,  2.25it/s]Extractor Estimating: 73it [00:42,  2.29it/s]Extractor Estimating: 74it [00:42,  2.28it/s]Extractor Estimating: 75it [00:42,  2.24it/s]Extractor Estimating: 76it [00:43,  2.07it/s]Extractor Estimating: 77it [00:44,  1.92it/s]Extractor Estimating: 78it [00:44,  1.87it/s]Extractor Estimating: 79it [00:45,  1.85it/s]Extractor Estimating: 80it [00:45,  1.81it/s]Extractor Estimating: 81it [00:46,  1.79it/s]Extractor Estimating: 82it [00:46,  1.80it/s]Extractor Estimating: 83it [00:47,  1.80it/s]Extractor Estimating: 84it [00:47,  1.84it/s]Extractor Estimating: 85it [00:48,  1.78it/s]Extractor Estimating: 86it [00:49,  1.80it/s]Extractor Estimating: 87it [00:49,  1.78it/s]Extractor Estimating: 88it [00:50,  1.78it/s]Extractor Estimating: 89it [00:50,  1.77it/s]Extractor Estimating: 90it [00:51,  1.77it/s]Extractor Estimating: 91it [00:51,  1.79it/s]Extractor Estimating: 92it [00:52,  1.79it/s]Extractor Estimating: 93it [00:53,  1.79it/s]Extractor Estimating: 94it [00:53,  1.79it/s]Extractor Estimating: 95it [00:54,  1.78it/s]Extractor Estimating: 96it [00:54,  1.79it/s]Extractor Estimating: 97it [00:55,  1.81it/s]Extractor Estimating: 98it [00:55,  1.78it/s]Extractor Estimating: 99it [00:56,  1.58it/s]Extractor Estimating: 100it [00:57,  1.55it/s]Extractor Estimating: 101it [00:57,  1.72it/s]Extractor Estimating: 102it [00:58,  1.89it/s]Extractor Estimating: 103it [00:58,  2.02it/s]Extractor Estimating: 104it [00:59,  2.12it/s]Extractor Estimating: 105it [00:59,  2.09it/s]Extractor Estimating: 106it [00:59,  2.18it/s]Extractor Estimating: 107it [01:00,  2.32it/s]Extractor Estimating: 108it [01:00,  2.26it/s]Extractor Estimating: 109it [01:01,  2.24it/s]Extractor Estimating: 110it [01:01,  2.20it/s]Extractor Estimating: 111it [01:02,  2.24it/s]Extractor Estimating: 112it [01:02,  2.29it/s]Extractor Estimating: 113it [01:02,  2.29it/s]Extractor Estimating: 114it [01:03,  2.29it/s]Extractor Estimating: 115it [01:03,  2.25it/s]Extractor Estimating: 116it [01:04,  2.33it/s]Extractor Estimating: 117it [01:04,  2.29it/s]Extractor Estimating: 118it [01:05,  2.32it/s]Extractor Estimating: 119it [01:05,  2.35it/s]Extractor Estimating: 120it [01:05,  2.33it/s]Extractor Estimating: 121it [01:06,  2.31it/s]Extractor Estimating: 122it [01:06,  2.29it/s]Extractor Estimating: 123it [01:07,  2.24it/s]Extractor Estimating: 124it [01:07,  2.34it/s]Extractor Estimating: 125it [01:08,  2.40it/s]Extractor Estimating: 126it [01:08,  2.21it/s]Extractor Estimating: 127it [01:09,  2.17it/s]Extractor Estimating: 128it [01:09,  2.02it/s]Extractor Estimating: 129it [01:10,  1.95it/s]Extractor Estimating: 130it [01:10,  1.91it/s]Extractor Estimating: 131it [01:11,  1.84it/s]Extractor Estimating: 132it [01:11,  1.82it/s]Extractor Estimating: 133it [01:12,  1.84it/s]Extractor Estimating: 134it [01:12,  1.90it/s]Extractor Estimating: 135it [01:13,  1.90it/s]Extractor Estimating: 136it [01:14,  1.84it/s]Extractor Estimating: 137it [01:14,  1.82it/s]Extractor Estimating: 138it [01:15,  1.84it/s]Extractor Estimating: 139it [01:15,  1.86it/s]Extractor Estimating: 140it [01:16,  1.83it/s]Extractor Estimating: 141it [01:16,  1.84it/s]Extractor Estimating: 142it [01:17,  1.86it/s]Extractor Estimating: 143it [01:17,  1.91it/s]Extractor Estimating: 144it [01:18,  1.94it/s]Extractor Estimating: 145it [01:18,  1.96it/s]Extractor Estimating: 146it [01:19,  1.90it/s]Extractor Estimating: 147it [01:19,  1.91it/s]Extractor Estimating: 148it [01:20,  2.02it/s]Extractor Estimating: 149it [01:20,  2.03it/s]Extractor Estimating: 150it [01:21,  2.01it/s]Extractor Estimating: 151it [01:21,  2.05it/s]Extractor Estimating: 152it [01:22,  2.00it/s]Extractor Estimating: 153it [01:22,  2.02it/s]Extractor Estimating: 154it [01:23,  2.16it/s]Extractor Estimating: 155it [01:23,  2.11it/s]Extractor Estimating: 156it [01:24,  2.10it/s]Extractor Estimating: 157it [01:24,  2.09it/s]Extractor Estimating: 158it [01:25,  2.08it/s]Extractor Estimating: 159it [01:25,  2.06it/s]Extractor Estimating: 160it [01:26,  2.07it/s]Extractor Estimating: 161it [01:26,  1.99it/s]Extractor Estimating: 162it [01:27,  2.01it/s]Extractor Estimating: 163it [01:27,  2.00it/s]Extractor Estimating: 164it [01:28,  2.02it/s]Extractor Estimating: 165it [01:28,  2.04it/s]Extractor Estimating: 166it [01:29,  1.98it/s]Extractor Estimating: 167it [01:29,  2.04it/s]Extractor Estimating: 168it [01:30,  2.04it/s]Extractor Estimating: 169it [01:30,  2.01it/s]Extractor Estimating: 170it [01:31,  2.04it/s]Extractor Estimating: 171it [01:31,  2.11it/s]Extractor Estimating: 172it [01:32,  2.09it/s]Extractor Estimating: 173it [01:32,  2.10it/s]Extractor Estimating: 174it [01:32,  2.13it/s]Extractor Estimating: 175it [01:33,  2.08it/s]Extractor Estimating: 176it [01:33,  1.99it/s]Extractor Estimating: 177it [01:34,  1.93it/s]Extractor Estimating: 178it [01:35,  1.83it/s]Extractor Estimating: 179it [01:35,  1.81it/s]Extractor Estimating: 180it [01:36,  1.85it/s]Extractor Estimating: 181it [01:36,  1.80it/s]Extractor Estimating: 182it [01:37,  1.84it/s]Extractor Estimating: 183it [01:37,  1.88it/s]Extractor Estimating: 184it [01:38,  1.89it/s]Extractor Estimating: 185it [01:38,  1.81it/s]Extractor Estimating: 186it [01:39,  1.85it/s]Extractor Estimating: 187it [01:39,  1.90it/s]Extractor Estimating: 188it [01:40,  1.97it/s]Extractor Estimating: 189it [01:40,  1.94it/s]Extractor Estimating: 190it [01:41,  1.93it/s]Extractor Estimating: 191it [01:42,  1.97it/s]Extractor Estimating: 192it [01:42,  1.90it/s]Extractor Estimating: 193it [01:43,  1.91it/s]Extractor Estimating: 194it [01:43,  1.88it/s]Extractor Estimating: 195it [01:44,  1.83it/s]Extractor Estimating: 196it [01:44,  1.82it/s]Extractor Estimating: 197it [01:45,  1.90it/s]Extractor Estimating: 198it [01:45,  1.94it/s]Extractor Estimating: 199it [01:46,  1.92it/s]Extractor Estimating: 200it [01:46,  1.95it/s]Extractor Estimating: 201it [01:47,  1.91it/s]Extractor Estimating: 202it [01:47,  1.91it/s]Extractor Estimating: 203it [01:48,  1.96it/s]Extractor Estimating: 204it [01:48,  1.93it/s]Extractor Estimating: 205it [01:49,  1.89it/s]Extractor Estimating: 206it [01:49,  1.84it/s]Extractor Estimating: 207it [01:50,  1.83it/s]Extractor Estimating: 208it [01:51,  1.83it/s]Extractor Estimating: 209it [01:51,  1.82it/s]Extractor Estimating: 210it [01:52,  1.84it/s]Extractor Estimating: 211it [01:52,  1.66it/s]Extractor Estimating: 212it [01:53,  1.76it/s]Extractor Estimating: 213it [01:53,  1.78it/s]Extractor Estimating: 214it [01:54,  1.81it/s]Extractor Estimating: 215it [01:55,  1.74it/s]Extractor Estimating: 216it [01:55,  1.72it/s]Extractor Estimating: 217it [01:56,  1.80it/s]Extractor Estimating: 218it [01:56,  1.80it/s]Extractor Estimating: 219it [01:57,  1.79it/s]Extractor Estimating: 220it [01:57,  1.80it/s]Extractor Estimating: 221it [01:58,  1.80it/s]Extractor Estimating: 222it [01:58,  1.79it/s]Extractor Estimating: 223it [01:59,  1.81it/s]Extractor Estimating: 224it [02:00,  1.80it/s]Extractor Estimating: 225it [02:00,  1.78it/s]Extractor Estimating: 226it [02:01,  1.75it/s]Extractor Estimating: 227it [02:01,  1.74it/s]Extractor Estimating: 228it [02:02,  1.70it/s]Extractor Estimating: 229it [02:03,  1.61it/s]Extractor Estimating: 230it [02:03,  1.69it/s]Extractor Estimating: 231it [02:04,  1.68it/s]Extractor Estimating: 232it [02:04,  1.66it/s]Extractor Estimating: 233it [02:05,  1.63it/s]Extractor Estimating: 234it [02:06,  1.60it/s]Extractor Estimating: 235it [02:06,  1.66it/s]Extractor Estimating: 236it [02:07,  1.68it/s]Extractor Estimating: 237it [02:08,  1.60it/s]Extractor Estimating: 238it [02:08,  1.45it/s]Extractor Estimating: 239it [02:09,  1.44it/s]Extractor Estimating: 240it [02:10,  1.55it/s]Extractor Estimating: 241it [02:10,  1.66it/s]Extractor Estimating: 242it [02:11,  1.70it/s]Extractor Estimating: 243it [02:11,  1.73it/s]Extractor Estimating: 244it [02:12,  1.71it/s]Extractor Estimating: 245it [02:12,  1.70it/s]Extractor Estimating: 246it [02:13,  1.64it/s]Extractor Estimating: 247it [02:14,  1.66it/s]Extractor Estimating: 248it [02:14,  1.66it/s]Extractor Estimating: 249it [02:15,  1.55it/s]Extractor Estimating: 250it [02:16,  1.54it/s]Extractor Estimating: 251it [02:16,  1.57it/s]Extractor Estimating: 252it [02:17,  1.53it/s]Extractor Estimating: 253it [02:18,  1.52it/s]Extractor Estimating: 254it [02:18,  1.51it/s]Extractor Estimating: 255it [02:19,  1.54it/s]Extractor Estimating: 256it [02:20,  1.56it/s]Extractor Estimating: 257it [02:20,  1.57it/s]Extractor Estimating: 258it [02:21,  1.58it/s]Extractor Estimating: 259it [02:22,  1.50it/s]Extractor Estimating: 260it [02:22,  1.52it/s]Extractor Estimating: 261it [02:23,  1.51it/s]Extractor Estimating: 262it [02:24,  1.48it/s]Extractor Estimating: 263it [02:24,  1.45it/s]Extractor Estimating: 264it [02:25,  1.46it/s]Extractor Estimating: 265it [02:26,  1.44it/s]Extractor Estimating: 266it [02:26,  1.49it/s]Extractor Estimating: 267it [02:27,  1.48it/s]Extractor Estimating: 268it [02:28,  1.49it/s]Extractor Estimating: 269it [02:28,  1.48it/s]Extractor Estimating: 270it [02:29,  1.48it/s]Extractor Estimating: 271it [02:30,  1.50it/s]Extractor Estimating: 272it [02:30,  1.50it/s]Extractor Estimating: 273it [02:31,  1.52it/s]Extractor Estimating: 274it [02:32,  1.52it/s]Extractor Estimating: 275it [02:32,  1.54it/s]Extractor Estimating: 276it [02:33,  1.58it/s]Extractor Estimating: 277it [02:33,  1.64it/s]Extractor Estimating: 278it [02:34,  1.73it/s]Extractor Estimating: 279it [02:34,  1.73it/s]Extractor Estimating: 280it [02:35,  1.73it/s]Extractor Estimating: 281it [02:36,  1.67it/s]Extractor Estimating: 282it [02:36,  1.71it/s]Extractor Estimating: 283it [02:37,  1.69it/s]Extractor Estimating: 284it [02:37,  1.71it/s]Extractor Estimating: 285it [02:38,  1.76it/s]Extractor Estimating: 286it [02:39,  1.76it/s]Extractor Estimating: 287it [02:39,  1.74it/s]Extractor Estimating: 288it [02:40,  1.78it/s]Extractor Estimating: 289it [02:40,  1.79it/s]Extractor Estimating: 290it [02:41,  1.85it/s]Extractor Estimating: 291it [02:41,  1.88it/s]Extractor Estimating: 292it [02:42,  1.87it/s]Extractor Estimating: 293it [02:42,  1.84it/s]Extractor Estimating: 294it [02:43,  1.88it/s]Extractor Estimating: 295it [02:43,  1.85it/s]Extractor Estimating: 296it [02:44,  1.87it/s]Extractor Estimating: 297it [02:44,  1.87it/s]Extractor Estimating: 298it [02:45,  1.79it/s]Extractor Estimating: 299it [02:46,  1.82it/s]Extractor Estimating: 300it [02:46,  1.83it/s]Extractor Estimating: 301it [02:47,  1.98it/s]Extractor Estimating: 302it [02:47,  2.10it/s]Extractor Estimating: 303it [02:47,  2.12it/s]Extractor Estimating: 304it [02:48,  2.20it/s]Extractor Estimating: 305it [02:48,  2.25it/s]Extractor Estimating: 306it [02:49,  2.28it/s]Extractor Estimating: 307it [02:49,  2.32it/s]Extractor Estimating: 308it [02:49,  2.33it/s]Extractor Estimating: 309it [02:50,  2.29it/s]Extractor Estimating: 310it [02:50,  2.33it/s]Extractor Estimating: 311it [02:51,  2.02it/s]Extractor Estimating: 312it [02:51,  2.06it/s]Extractor Estimating: 313it [02:52,  2.13it/s]Extractor Estimating: 314it [02:52,  2.25it/s]Extractor Estimating: 315it [02:53,  2.25it/s]Extractor Estimating: 316it [02:53,  2.27it/s]Extractor Estimating: 317it [02:54,  2.19it/s]Extractor Estimating: 318it [02:54,  2.24it/s]Extractor Estimating: 319it [02:54,  2.29it/s]Extractor Estimating: 320it [02:55,  2.30it/s]Extractor Estimating: 321it [02:55,  2.23it/s]Extractor Estimating: 322it [02:56,  2.28it/s]Extractor Estimating: 323it [02:56,  2.34it/s]Extractor Estimating: 324it [02:57,  2.38it/s]Extractor Estimating: 325it [02:57,  2.14it/s]Extractor Estimating: 326it [02:58,  1.99it/s]Extractor Estimating: 327it [02:58,  1.91it/s]Extractor Estimating: 328it [02:59,  1.83it/s]Extractor Estimating: 329it [03:00,  1.69it/s]Extractor Estimating: 330it [03:00,  1.63it/s]Extractor Estimating: 331it [03:01,  1.62it/s]Extractor Estimating: 332it [03:02,  1.57it/s]Extractor Estimating: 333it [03:02,  1.63it/s]Extractor Estimating: 334it [03:03,  1.64it/s]Extractor Estimating: 335it [03:03,  1.67it/s]Extractor Estimating: 336it [03:04,  1.64it/s]Extractor Estimating: 337it [03:05,  1.62it/s]Extractor Estimating: 338it [03:05,  1.61it/s]Extractor Estimating: 339it [03:06,  1.61it/s]Extractor Estimating: 340it [03:06,  1.62it/s]Extractor Estimating: 341it [03:07,  1.64it/s]Extractor Estimating: 342it [03:08,  1.64it/s]Extractor Estimating: 343it [03:08,  1.64it/s]Extractor Estimating: 344it [03:09,  1.64it/s]Extractor Estimating: 345it [03:10,  1.62it/s]Extractor Estimating: 346it [03:10,  1.61it/s]Extractor Estimating: 347it [03:11,  1.65it/s]Extractor Estimating: 348it [03:11,  1.63it/s]Extractor Estimating: 349it [03:12,  1.59it/s]Extractor Estimating: 350it [03:13,  1.63it/s]Extractor Estimating: 351it [03:13,  1.69it/s]Extractor Estimating: 352it [03:14,  1.75it/s]Extractor Estimating: 353it [03:14,  1.79it/s]Extractor Estimating: 354it [03:15,  1.76it/s]Extractor Estimating: 355it [03:15,  1.77it/s]Extractor Estimating: 356it [03:16,  1.80it/s]Extractor Estimating: 357it [03:16,  1.84it/s]Extractor Estimating: 358it [03:17,  1.81it/s]Extractor Estimating: 359it [03:18,  1.81it/s]Extractor Estimating: 360it [03:18,  1.78it/s]Extractor Estimating: 361it [03:19,  1.50it/s]Extractor Estimating: 362it [03:20,  1.60it/s]Extractor Estimating: 363it [03:20,  1.60it/s]Extractor Estimating: 364it [03:21,  1.59it/s]Extractor Estimating: 365it [03:21,  1.60it/s]Extractor Estimating: 366it [03:22,  1.66it/s]Extractor Estimating: 367it [03:23,  1.71it/s]Extractor Estimating: 368it [03:23,  1.73it/s]Extractor Estimating: 369it [03:24,  1.81it/s]Extractor Estimating: 370it [03:24,  1.86it/s]Extractor Estimating: 371it [03:25,  1.80it/s]Extractor Estimating: 372it [03:25,  1.79it/s]Extractor Estimating: 373it [03:26,  1.81it/s]Extractor Estimating: 374it [03:26,  1.82it/s]Extractor Estimating: 375it [03:27,  1.78it/s]Extractor Estimating: 376it [03:28,  1.69it/s]Extractor Estimating: 377it [03:28,  1.63it/s]Extractor Estimating: 378it [03:29,  1.59it/s]Extractor Estimating: 379it [03:29,  1.62it/s]Extractor Estimating: 380it [03:30,  1.57it/s]Extractor Estimating: 381it [03:31,  1.58it/s]Extractor Estimating: 382it [03:31,  1.62it/s]Extractor Estimating: 383it [03:32,  1.61it/s]Extractor Estimating: 384it [03:33,  1.56it/s]Extractor Estimating: 385it [03:33,  1.52it/s]Extractor Estimating: 386it [03:34,  1.56it/s]Extractor Estimating: 387it [03:35,  1.56it/s]Extractor Estimating: 388it [03:35,  1.52it/s]Extractor Estimating: 389it [03:36,  1.58it/s]Extractor Estimating: 390it [03:37,  1.57it/s]Extractor Estimating: 391it [03:37,  1.50it/s]Extractor Estimating: 392it [03:38,  1.55it/s]Extractor Estimating: 393it [03:38,  1.60it/s]Extractor Estimating: 394it [03:39,  1.62it/s]Extractor Estimating: 395it [03:40,  1.64it/s]Extractor Estimating: 396it [03:40,  1.58it/s]Extractor Estimating: 397it [03:41,  1.55it/s]Extractor Estimating: 398it [03:42,  1.62it/s]Extractor Estimating: 399it [03:42,  1.52it/s]Extractor Estimating: 400it [03:43,  1.57it/s]Extractor Estimating: 401it [03:43,  1.67it/s]Extractor Estimating: 402it [03:44,  1.68it/s]Extractor Estimating: 403it [03:45,  1.75it/s]Extractor Estimating: 404it [03:45,  1.77it/s]Extractor Estimating: 405it [03:46,  1.75it/s]Extractor Estimating: 406it [03:46,  1.79it/s]Extractor Estimating: 407it [03:47,  1.84it/s]Extractor Estimating: 408it [03:47,  1.89it/s]Extractor Estimating: 409it [03:48,  1.81it/s]Extractor Estimating: 410it [03:48,  1.71it/s]Extractor Estimating: 411it [03:49,  1.75it/s]Extractor Estimating: 412it [03:50,  1.73it/s]Extractor Estimating: 413it [03:50,  1.75it/s]Extractor Estimating: 414it [03:51,  1.84it/s]Extractor Estimating: 415it [03:51,  1.82it/s]Extractor Estimating: 416it [03:52,  1.79it/s]Extractor Estimating: 417it [03:52,  1.82it/s]Extractor Estimating: 418it [03:53,  1.82it/s]Extractor Estimating: 419it [03:53,  1.79it/s]Extractor Estimating: 420it [03:54,  1.83it/s]Extractor Estimating: 421it [03:55,  1.76it/s]Extractor Estimating: 422it [03:55,  1.77it/s]Extractor Estimating: 423it [03:56,  1.73it/s]Extractor Estimating: 424it [03:56,  1.71it/s]Extractor Estimating: 425it [03:57,  1.74it/s]Extractor Estimating: 426it [03:57,  1.79it/s]Extractor Estimating: 427it [03:58,  1.75it/s]Extractor Estimating: 428it [03:59,  1.77it/s]Extractor Estimating: 429it [03:59,  1.80it/s]Extractor Estimating: 430it [04:00,  1.79it/s]Extractor Estimating: 431it [04:00,  1.84it/s]Extractor Estimating: 432it [04:01,  1.82it/s]Extractor Estimating: 433it [04:01,  1.78it/s]Extractor Estimating: 434it [04:02,  1.79it/s]Extractor Estimating: 435it [04:02,  1.76it/s]Extractor Estimating: 436it [04:03,  1.71it/s]Extractor Estimating: 437it [04:04,  1.69it/s]Extractor Estimating: 438it [04:04,  1.72it/s]Extractor Estimating: 439it [04:05,  1.69it/s]Extractor Estimating: 440it [04:05,  1.80it/s]Extractor Estimating: 441it [04:06,  1.75it/s]Extractor Estimating: 442it [04:06,  1.78it/s]Extractor Estimating: 443it [04:07,  1.80it/s]Extractor Estimating: 444it [04:08,  1.80it/s]Extractor Estimating: 445it [04:08,  1.80it/s]Extractor Estimating: 446it [04:09,  1.75it/s]Extractor Estimating: 447it [04:09,  1.76it/s]Extractor Estimating: 448it [04:10,  1.66it/s]Extractor Estimating: 449it [04:11,  1.72it/s]Extractor Estimating: 450it [04:11,  1.80it/s]Extractor Estimating: 451it [04:12,  1.82it/s]Extractor Estimating: 452it [04:12,  1.87it/s]Extractor Estimating: 453it [04:13,  1.83it/s]Extractor Estimating: 454it [04:13,  1.86it/s]Extractor Estimating: 455it [04:14,  1.86it/s]Extractor Estimating: 456it [04:14,  1.90it/s]Extractor Estimating: 457it [04:15,  1.86it/s]Extractor Estimating: 458it [04:15,  1.87it/s]Extractor Estimating: 459it [04:16,  1.85it/s]Extractor Estimating: 460it [04:16,  1.88it/s]Extractor Estimating: 461it [04:17,  1.83it/s]Extractor Estimating: 462it [04:17,  1.83it/s]Extractor Estimating: 463it [04:18,  1.83it/s]Extractor Estimating: 464it [04:19,  1.78it/s]Extractor Estimating: 465it [04:19,  1.75it/s]Extractor Estimating: 466it [04:20,  1.73it/s]Extractor Estimating: 467it [04:20,  1.79it/s]Extractor Estimating: 468it [04:21,  1.84it/s]Extractor Estimating: 469it [04:21,  1.88it/s]Extractor Estimating: 470it [04:22,  1.96it/s]Extractor Estimating: 471it [04:22,  1.91it/s]Extractor Estimating: 472it [04:23,  1.89it/s]Extractor Estimating: 473it [04:23,  1.91it/s]Extractor Estimating: 474it [04:24,  1.97it/s]Extractor Estimating: 475it [04:24,  1.90it/s]Extractor Estimating: 476it [04:25,  1.83it/s]Extractor Estimating: 477it [04:26,  1.81it/s]Extractor Estimating: 478it [04:26,  1.82it/s]Extractor Estimating: 479it [04:27,  1.72it/s]Extractor Estimating: 480it [04:27,  1.70it/s]Extractor Estimating: 481it [04:28,  1.72it/s]Extractor Estimating: 482it [04:29,  1.73it/s]Extractor Estimating: 483it [04:29,  1.65it/s]Extractor Estimating: 484it [04:30,  1.62it/s]Extractor Estimating: 485it [04:30,  1.66it/s]Extractor Estimating: 486it [04:31,  1.67it/s]Extractor Estimating: 487it [04:32,  1.66it/s]Extractor Estimating: 488it [04:32,  1.65it/s]Extractor Estimating: 489it [04:33,  1.65it/s]Extractor Estimating: 490it [04:33,  1.67it/s]Extractor Estimating: 491it [04:34,  1.73it/s]Extractor Estimating: 492it [04:35,  1.70it/s]Extractor Estimating: 493it [04:35,  1.71it/s]Extractor Estimating: 494it [04:36,  1.70it/s]Extractor Estimating: 495it [04:36,  1.75it/s]Extractor Estimating: 496it [04:37,  1.66it/s]Extractor Estimating: 497it [04:37,  1.69it/s]Extractor Estimating: 498it [04:38,  1.72it/s]Extractor Estimating: 499it [04:39,  1.50it/s]Extractor Estimating: 500it [04:39,  1.87it/s]Extractor Estimating: 500it [04:39,  1.79it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:40,770 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:40,774 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:40,774 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:40,775 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:40,775 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:42:41,457 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:42:41,458 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:42:42,039 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:42:43,141 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:42:43,142 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:46,043 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:46,045 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:46,045 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:46,045 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:46,045 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:42:46,692 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:42:46,694 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:42:47,273 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:42:47,444 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:42:47,444 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 02:28:43,579 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 02:28:43,671 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9981 mean pseudo reward: 0.9562415077224341
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 15779
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15879, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15879, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.023, loss:461.4075
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.985, loss:432.6096
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.964, loss:391.6040
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.994, loss:387.2588
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.982, loss:358.5094
>> valid entity prec:0.6050, rec:0.5818, f1:0.5932
>> valid relation prec:0.2308, rec:0.1022, f1:0.1416
>> valid relation with NER prec:0.2308, rec:0.1022, f1:0.1416
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.220, loss:380.0070
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 0.991, loss:378.6021
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 0.969, loss:383.9557
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.977, loss:348.5231
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.984, loss:366.4678
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5779, rec:0.5617, f1:0.5697
>> valid relation prec:0.2110, rec:0.1013, f1:0.1369
>> valid relation with NER prec:0.2110, rec:0.1013, f1:0.1369
g_step 1100, step 268, avg_time 2.218, loss:359.3025
g_step 1200, step 368, avg_time 1.008, loss:363.5553
g_step 1300, step 52, avg_time 0.958, loss:349.2158
g_step 1400, step 152, avg_time 1.012, loss:326.3998
g_step 1500, step 252, avg_time 0.982, loss:350.7685
>> valid entity prec:0.5675, rec:0.5635, f1:0.5655
>> valid relation prec:0.2250, rec:0.1123, f1:0.1499
>> valid relation with NER prec:0.2250, rec:0.1123, f1:0.1499
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 352, avg_time 2.204, loss:348.3936
g_step 1700, step 36, avg_time 0.986, loss:333.9853
g_step 1800, step 136, avg_time 0.973, loss:296.9009
g_step 1900, step 236, avg_time 0.986, loss:336.5118
g_step 2000, step 336, avg_time 0.995, loss:336.1881
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5572, rec:0.5607, f1:0.5589
>> valid relation prec:0.2117, rec:0.0984, f1:0.1344
>> valid relation with NER prec:0.2117, rec:0.0984, f1:0.1344
g_step 2100, step 20, avg_time 2.206, loss:313.4811
g_step 2200, step 120, avg_time 0.985, loss:292.6259
g_step 2300, step 220, avg_time 1.001, loss:294.5497
g_step 2400, step 320, avg_time 0.984, loss:325.2215
g_step 2500, step 4, avg_time 0.973, loss:333.6574
>> valid entity prec:0.5612, rec:0.5353, f1:0.5480
>> valid relation prec:0.2192, rec:0.0949, f1:0.1325
>> valid relation with NER prec:0.2192, rec:0.0949, f1:0.1325
g_step 2600, step 104, avg_time 2.198, loss:275.8541
g_step 2700, step 204, avg_time 0.982, loss:286.2442
g_step 2800, step 304, avg_time 0.970, loss:303.3268
g_step 2900, step 404, avg_time 1.028, loss:299.7134
g_step 3000, step 88, avg_time 0.979, loss:277.4907
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5645, rec:0.5875, f1:0.5758
>> valid relation prec:0.2008, rec:0.1144, f1:0.1457
>> valid relation with NER prec:0.2008, rec:0.1144, f1:0.1457
g_step 3100, step 188, avg_time 2.237, loss:298.5750
g_step 3200, step 288, avg_time 0.976, loss:287.5885
g_step 3300, step 388, avg_time 0.987, loss:276.6540
g_step 3400, step 72, avg_time 0.965, loss:267.0487
g_step 3500, step 172, avg_time 0.971, loss:258.0434
>> valid entity prec:0.5818, rec:0.5232, f1:0.5509
>> valid relation prec:0.2252, rec:0.0940, f1:0.1327
>> valid relation with NER prec:0.2252, rec:0.0940, f1:0.1327
g_step 3600, step 272, avg_time 2.201, loss:278.1257
g_step 3700, step 372, avg_time 0.992, loss:282.0344
g_step 3800, step 56, avg_time 0.982, loss:275.5725
g_step 3900, step 156, avg_time 0.983, loss:257.9816
g_step 4000, step 256, avg_time 0.985, loss:264.3149
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5849, rec:0.5413, f1:0.5623
>> valid relation prec:0.2004, rec:0.0848, f1:0.1191
>> valid relation with NER prec:0.2004, rec:0.0848, f1:0.1191
g_step 4100, step 356, avg_time 2.219, loss:250.8085
g_step 4200, step 40, avg_time 0.974, loss:273.7714
g_step 4300, step 140, avg_time 0.968, loss:235.1730
g_step 4400, step 240, avg_time 0.971, loss:237.6120
g_step 4500, step 340, avg_time 0.992, loss:262.1277
>> valid entity prec:0.5677, rec:0.5906, f1:0.5789
>> valid relation prec:0.2243, rec:0.1077, f1:0.1455
>> valid relation with NER prec:0.2243, rec:0.1077, f1:0.1455
g_step 4600, step 24, avg_time 2.204, loss:246.3590
g_step 4700, step 124, avg_time 0.971, loss:241.7865
g_step 4800, step 224, avg_time 0.977, loss:235.9909
g_step 4900, step 324, avg_time 0.942, loss:260.1996
g_step 5000, step 8, avg_time 0.941, loss:243.6301
learning rate was adjusted to 0.0008
>> valid entity prec:0.5623, rec:0.5805, f1:0.5712
>> valid relation prec:0.2085, rec:0.1062, f1:0.1408
>> valid relation with NER prec:0.2085, rec:0.1062, f1:0.1408
g_step 5100, step 108, avg_time 2.163, loss:233.2733
g_step 5200, step 208, avg_time 0.939, loss:237.0388
g_step 5300, step 308, avg_time 0.927, loss:230.1606
g_step 5400, step 408, avg_time 0.944, loss:231.3559
g_step 5500, step 92, avg_time 0.930, loss:202.3595
>> valid entity prec:0.5651, rec:0.5451, f1:0.5550
>> valid relation prec:0.2084, rec:0.1196, f1:0.1520
>> valid relation with NER prec:0.2084, rec:0.1196, f1:0.1520
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5600, step 192, avg_time 2.132, loss:224.3378
g_step 5700, step 292, avg_time 0.945, loss:239.6306
g_step 5800, step 392, avg_time 0.942, loss:234.4119
g_step 5900, step 76, avg_time 0.931, loss:207.0395
g_step 6000, step 176, avg_time 0.909, loss:224.1124
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5716, rec:0.5303, f1:0.5502
>> valid relation prec:0.2169, rec:0.0981, f1:0.1351
>> valid relation with NER prec:0.2169, rec:0.0981, f1:0.1351
g_step 6100, step 276, avg_time 2.122, loss:227.7620
g_step 6200, step 376, avg_time 0.947, loss:212.0699
g_step 6300, step 60, avg_time 0.948, loss:218.2819
g_step 6400, step 160, avg_time 0.929, loss:209.5959
g_step 6500, step 260, avg_time 0.928, loss:208.5994
>> valid entity prec:0.5780, rec:0.5303, f1:0.5532
>> valid relation prec:0.2169, rec:0.0993, f1:0.1362
>> valid relation with NER prec:0.2169, rec:0.0993, f1:0.1362
g_step 6600, step 360, avg_time 2.125, loss:220.8690
g_step 6700, step 44, avg_time 0.910, loss:206.6076
g_step 6800, step 144, avg_time 0.941, loss:204.6257
g_step 6900, step 244, avg_time 0.935, loss:210.8990
g_step 7000, step 344, avg_time 0.930, loss:207.3910
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5682, rec:0.5320, f1:0.5495
>> valid relation prec:0.2059, rec:0.1007, f1:0.1353
>> valid relation with NER prec:0.2059, rec:0.1007, f1:0.1353
g_step 7100, step 28, avg_time 2.111, loss:191.4872
g_step 7200, step 128, avg_time 0.923, loss:182.6521
g_step 7300, step 228, avg_time 0.954, loss:203.7064
g_step 7400, step 328, avg_time 0.928, loss:181.5238
g_step 7500, step 12, avg_time 0.938, loss:198.1254
>> valid entity prec:0.5700, rec:0.5449, f1:0.5571
>> valid relation prec:0.1911, rec:0.1039, f1:0.1346
>> valid relation with NER prec:0.1911, rec:0.1039, f1:0.1346
g_step 7600, step 112, avg_time 2.130, loss:190.3929
g_step 7700, step 212, avg_time 0.912, loss:191.0929
g_step 7800, step 312, avg_time 0.940, loss:193.6178
g_step 7900, step 412, avg_time 0.942, loss:201.9206
g_step 8000, step 96, avg_time 0.957, loss:174.8886
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5771, rec:0.5331, f1:0.5543
>> valid relation prec:0.2169, rec:0.1109, f1:0.1468
>> valid relation with NER prec:0.2169, rec:0.1109, f1:0.1468
g_step 8100, step 196, avg_time 2.127, loss:178.0922
g_step 8200, step 296, avg_time 0.915, loss:192.8908
g_step 8300, step 396, avg_time 0.938, loss:191.8045
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 02:28:43 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 02:28:43 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_02-28-43_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 02:28:44 - WARNING - datasets.builder -   Using custom data configuration default-be73f0f30fb54360
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-be73f0f30fb54360/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 02:28:44,931 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:28:44,932 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:28:44,933 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:28:44,934 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:28:44,941 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:28:44,944 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:28:44,944 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:28:44,945 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:28:44,945 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:28:44,945 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:28:44,945 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 02:28:45,088 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:28:48,087 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 02:28:48,090 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-be73f0f30fb54360/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.43ba/s] 20%|██        | 2/10 [00:00<00:01,  4.33ba/s] 30%|███       | 3/10 [00:00<00:01,  4.73ba/s] 40%|████      | 4/10 [00:00<00:01,  4.94ba/s] 50%|█████     | 5/10 [00:01<00:00,  5.09ba/s] 60%|██████    | 6/10 [00:01<00:00,  5.16ba/s] 70%|███████   | 7/10 [00:01<00:00,  5.22ba/s] 80%|████████  | 8/10 [00:01<00:00,  5.28ba/s] 90%|█████████ | 9/10 [00:01<00:00,  5.30ba/s]100%|██████████| 10/10 [00:01<00:00,  5.31ba/s]100%|██████████| 10/10 [00:01<00:00,  5.06ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.78ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.66ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.10ba/s]100%|██████████| 4/4 [00:00<00:00,  4.55ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:00,  9.23ba/s] 30%|███       | 3/10 [00:00<00:00, 10.59ba/s] 50%|█████     | 5/10 [00:00<00:00, 10.92ba/s] 70%|███████   | 7/10 [00:00<00:00, 11.04ba/s] 90%|█████████ | 9/10 [00:00<00:00, 11.16ba/s]100%|██████████| 10/10 [00:00<00:00, 11.01ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.77ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.84ba/s]100%|██████████| 4/4 [00:00<00:00, 12.51ba/s]
[INFO|trainer.py:414] 2023-08-29 02:28:52,593 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 02:28:52,606 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 02:28:52,606 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 02:28:52,606 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 02:28:52,606 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 02:28:52,606 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 02:28:52,606 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 02:28:52,606 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<04:53,  2.65it/s]  0%|          | 2/780 [00:00<04:30,  2.88it/s]  0%|          | 3/780 [00:01<04:23,  2.95it/s]  1%|          | 4/780 [00:01<04:16,  3.03it/s]  1%|          | 5/780 [00:01<04:10,  3.09it/s]  1%|          | 6/780 [00:01<04:05,  3.16it/s]  1%|          | 7/780 [00:02<04:02,  3.18it/s]  1%|          | 8/780 [00:02<03:59,  3.22it/s]  1%|          | 9/780 [00:02<03:55,  3.27it/s]  1%|▏         | 10/780 [00:03<03:53,  3.30it/s]  1%|▏         | 11/780 [00:03<03:50,  3.33it/s]  2%|▏         | 12/780 [00:03<03:48,  3.35it/s]  2%|▏         | 13/780 [00:04<03:48,  3.36it/s]  2%|▏         | 14/780 [00:04<03:47,  3.36it/s]  2%|▏         | 15/780 [00:04<03:46,  3.37it/s]  2%|▏         | 16/780 [00:04<03:46,  3.38it/s]  2%|▏         | 17/780 [00:05<03:47,  3.35it/s]  2%|▏         | 18/780 [00:05<03:47,  3.34it/s]  2%|▏         | 19/780 [00:05<03:46,  3.36it/s]  3%|▎         | 20/780 [00:06<03:45,  3.37it/s]  3%|▎         | 21/780 [00:06<03:44,  3.37it/s]  3%|▎         | 22/780 [00:06<03:44,  3.38it/s]  3%|▎         | 23/780 [00:07<03:43,  3.38it/s]  3%|▎         | 24/780 [00:07<03:43,  3.39it/s]  3%|▎         | 25/780 [00:07<03:43,  3.38it/s]  3%|▎         | 26/780 [00:07<03:42,  3.38it/s]  3%|▎         | 27/780 [00:08<03:42,  3.39it/s]  4%|▎         | 28/780 [00:08<03:43,  3.36it/s]  4%|▎         | 29/780 [00:08<03:42,  3.37it/s]  4%|▍         | 30/780 [00:09<03:42,  3.37it/s]  4%|▍         | 31/780 [00:09<03:41,  3.38it/s]  4%|▍         | 32/780 [00:09<03:41,  3.38it/s]  4%|▍         | 33/780 [00:09<03:41,  3.38it/s]  4%|▍         | 34/780 [00:10<03:40,  3.38it/s]  4%|▍         | 35/780 [00:10<03:40,  3.38it/s]  5%|▍         | 36/780 [00:10<03:39,  3.38it/s]  5%|▍         | 37/780 [00:11<03:39,  3.38it/s]  5%|▍         | 38/780 [00:11<03:38,  3.39it/s]  5%|▌         | 39/780 [00:11<03:39,  3.38it/s]  5%|▌         | 40/780 [00:12<03:39,  3.38it/s]  5%|▌         | 41/780 [00:12<03:38,  3.38it/s]  5%|▌         | 42/780 [00:12<03:38,  3.38it/s]  6%|▌         | 43/780 [00:12<03:37,  3.38it/s]  6%|▌         | 44/780 [00:13<03:37,  3.38it/s]  6%|▌         | 45/780 [00:13<03:37,  3.38it/s]  6%|▌         | 46/780 [00:13<03:36,  3.39it/s]  6%|▌         | 47/780 [00:14<03:36,  3.38it/s]  6%|▌         | 48/780 [00:14<03:36,  3.39it/s]  6%|▋         | 49/780 [00:14<03:36,  3.38it/s]  6%|▋         | 50/780 [00:15<03:36,  3.37it/s]  7%|▋         | 51/780 [00:15<03:35,  3.38it/s]  7%|▋         | 52/780 [00:15<03:35,  3.38it/s]  7%|▋         | 53/780 [00:15<03:35,  3.38it/s]  7%|▋         | 54/780 [00:16<03:34,  3.38it/s]  7%|▋         | 55/780 [00:16<03:34,  3.38it/s]  7%|▋         | 56/780 [00:16<03:33,  3.39it/s]  7%|▋         | 57/780 [00:17<03:33,  3.39it/s]  7%|▋         | 58/780 [00:17<03:33,  3.38it/s]  8%|▊         | 59/780 [00:17<03:33,  3.38it/s]  8%|▊         | 60/780 [00:17<03:32,  3.38it/s]  8%|▊         | 61/780 [00:18<03:33,  3.37it/s]  8%|▊         | 62/780 [00:18<03:32,  3.37it/s]  8%|▊         | 63/780 [00:18<03:32,  3.38it/s]  8%|▊         | 64/780 [00:19<03:32,  3.38it/s]  8%|▊         | 65/780 [00:19<03:31,  3.38it/s]  8%|▊         | 66/780 [00:19<03:31,  3.37it/s]  9%|▊         | 67/780 [00:20<03:31,  3.38it/s]  9%|▊         | 68/780 [00:20<03:30,  3.38it/s]  9%|▉         | 69/780 [00:20<03:30,  3.38it/s]  9%|▉         | 70/780 [00:20<03:30,  3.38it/s]  9%|▉         | 71/780 [00:21<03:29,  3.38it/s]  9%|▉         | 72/780 [00:21<03:30,  3.37it/s]  9%|▉         | 73/780 [00:21<03:29,  3.37it/s]  9%|▉         | 74/780 [00:22<03:29,  3.37it/s] 10%|▉         | 75/780 [00:22<03:28,  3.38it/s] 10%|▉         | 76/780 [00:22<03:28,  3.38it/s] 10%|▉         | 77/780 [00:22<03:28,  3.38it/s] 10%|█         | 78/780 [00:23<03:27,  3.38it/s] 10%|█         | 79/780 [00:23<03:27,  3.38it/s] 10%|█         | 80/780 [00:23<03:27,  3.38it/s] 10%|█         | 81/780 [00:24<03:26,  3.39it/s] 11%|█         | 82/780 [00:24<03:26,  3.38it/s] 11%|█         | 83/780 [00:24<03:25,  3.39it/s] 11%|█         | 84/780 [00:25<03:26,  3.37it/s] 11%|█         | 85/780 [00:25<03:25,  3.38it/s] 11%|█         | 86/780 [00:25<03:25,  3.38it/s] 11%|█         | 87/780 [00:25<03:24,  3.38it/s] 11%|█▏        | 88/780 [00:26<03:24,  3.38it/s] 11%|█▏        | 89/780 [00:26<03:24,  3.38it/s] 12%|█▏        | 90/780 [00:26<03:23,  3.38it/s] 12%|█▏        | 91/780 [00:27<03:23,  3.38it/s] 12%|█▏        | 92/780 [00:27<03:23,  3.38it/s] 12%|█▏        | 93/780 [00:27<03:22,  3.39it/s] 12%|█▏        | 94/780 [00:28<03:22,  3.38it/s] 12%|█▏        | 95/780 [00:28<03:23,  3.37it/s] 12%|█▏        | 96/780 [00:28<03:22,  3.37it/s] 12%|█▏        | 97/780 [00:28<03:22,  3.37it/s] 13%|█▎        | 98/780 [00:29<03:22,  3.37it/s] 13%|█▎        | 99/780 [00:29<03:21,  3.37it/s] 13%|█▎        | 100/780 [00:29<03:21,  3.38it/s] 13%|█▎        | 101/780 [00:30<03:21,  3.38it/s] 13%|█▎        | 102/780 [00:30<03:20,  3.38it/s] 13%|█▎        | 103/780 [00:30<03:20,  3.38it/s] 13%|█▎        | 104/780 [00:30<03:19,  3.38it/s] 13%|█▎        | 105/780 [00:31<03:19,  3.38it/s] 14%|█▎        | 106/780 [00:31<03:20,  3.36it/s] 14%|█▎        | 107/780 [00:31<03:20,  3.36it/s] 14%|█▍        | 108/780 [00:32<03:19,  3.37it/s] 14%|█▍        | 109/780 [00:32<03:19,  3.36it/s] 14%|█▍        | 110/780 [00:32<03:19,  3.37it/s] 14%|█▍        | 111/780 [00:33<03:18,  3.37it/s] 14%|█▍        | 112/780 [00:33<03:17,  3.38it/s] 14%|█▍        | 113/780 [00:33<03:17,  3.38it/s] 15%|█▍        | 114/780 [00:33<03:17,  3.38it/s] 15%|█▍        | 115/780 [00:34<03:16,  3.38it/s] 15%|█▍        | 116/780 [00:34<03:16,  3.38it/s] 15%|█▌        | 117/780 [00:34<03:16,  3.37it/s] 15%|█▌        | 118/780 [00:35<03:16,  3.37it/s] 15%|█▌        | 119/780 [00:35<03:15,  3.37it/s] 15%|█▌        | 120/780 [00:35<03:15,  3.38it/s] 16%|█▌        | 121/780 [00:36<03:15,  3.38it/s] 16%|█▌        | 122/780 [00:36<03:14,  3.38it/s] 16%|█▌        | 123/780 [00:36<03:14,  3.38it/s] 16%|█▌        | 124/780 [00:36<03:14,  3.38it/s] 16%|█▌        | 125/780 [00:37<03:12,  3.40it/s] 16%|█▌        | 126/780 [00:37<03:12,  3.41it/s] 16%|█▋        | 127/780 [00:37<03:11,  3.41it/s] 16%|█▋        | 128/780 [00:38<03:11,  3.41it/s] 17%|█▋        | 129/780 [00:38<03:10,  3.41it/s] 17%|█▋        | 130/780 [00:38<03:10,  3.42it/s] 17%|█▋        | 131/780 [00:38<03:09,  3.42it/s] 17%|█▋        | 132/780 [00:39<03:09,  3.42it/s] 17%|█▋        | 133/780 [00:39<03:08,  3.43it/s] 17%|█▋        | 134/780 [00:39<03:08,  3.43it/s] 17%|█▋        | 135/780 [00:40<03:07,  3.43it/s] 17%|█▋        | 136/780 [00:40<03:07,  3.43it/s] 18%|█▊        | 137/780 [00:40<03:07,  3.43it/s] 18%|█▊        | 138/780 [00:41<03:07,  3.43it/s] 18%|█▊        | 139/780 [00:41<03:07,  3.42it/s] 18%|█▊        | 140/780 [00:41<03:06,  3.42it/s] 18%|█▊        | 141/780 [00:41<03:06,  3.42it/s] 18%|█▊        | 142/780 [00:42<03:06,  3.42it/s] 18%|█▊        | 143/780 [00:42<03:05,  3.43it/s] 18%|█▊        | 144/780 [00:42<03:05,  3.43it/s] 19%|█▊        | 145/780 [00:43<03:05,  3.43it/s] 19%|█▊        | 146/780 [00:43<03:04,  3.43it/s] 19%|█▉        | 147/780 [00:43<03:04,  3.43it/s] 19%|█▉        | 148/780 [00:43<03:04,  3.43it/s] 19%|█▉        | 149/780 [00:44<03:04,  3.43it/s] 19%|█▉        | 150/780 [00:44<03:04,  3.41it/s] 19%|█▉        | 151/780 [00:44<03:03,  3.42it/s] 19%|█▉        | 152/780 [00:45<03:03,  3.42it/s] 20%|█▉        | 153/780 [00:45<03:03,  3.42it/s] 20%|█▉        | 154/780 [00:45<03:02,  3.43it/s] 20%|█▉        | 155/780 [00:45<03:02,  3.43it/s] 20%|██        | 156/780 [00:46<03:02,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 02:29:38,901 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:29:38,901 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 02:29:38,901 >>   Batch size = 8

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 54.54it/s][A
  3%|▎         | 12/431 [00:00<00:08, 47.26it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.49it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.80it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.27it/s][A
  7%|▋         | 32/431 [00:00<00:09, 44.00it/s][A
  9%|▊         | 37/431 [00:00<00:09, 43.78it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.74it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.83it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.92it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.73it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.71it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.65it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.53it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.50it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.51it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.63it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.70it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.75it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.66it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.63it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.59it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.54it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.58it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 43.47it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.60it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.72it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.48it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.73it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.48it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.60it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.62it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.65it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.65it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.65it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.69it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.63it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.60it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.58it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.61it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 41.50it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.56it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.65it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.65it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.70it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.72it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.75it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.71it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.63it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.50it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 43.62it/s][A
 61%|██████    | 262/431 [00:05<00:03, 43.61it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.63it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.75it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.75it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.71it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.58it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.50it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.54it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.55it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.68it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.74it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.63it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.62it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.64it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.53it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.49it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.49it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.51it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.69it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.65it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.67it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.62it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.61it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.57it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.50it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.45it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 43.65it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.72it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.76it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.72it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.64it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.63it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.62it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.44it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.44it/s][A 20%|██        | 156/780 [00:56<03:02,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:29:48,858 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 02:29:48,881 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:29:50,639 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:29:50,658 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:29:50,669 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:01<50:49,  4.89s/it] 20%|██        | 158/780 [01:02<36:27,  3.52s/it] 20%|██        | 159/780 [01:02<26:23,  2.55s/it] 21%|██        | 160/780 [01:02<19:21,  1.87s/it] 21%|██        | 161/780 [01:03<14:26,  1.40s/it] 21%|██        | 162/780 [01:03<11:00,  1.07s/it] 21%|██        | 163/780 [01:03<08:36,  1.19it/s] 21%|██        | 164/780 [01:03<06:55,  1.48it/s] 21%|██        | 165/780 [01:04<05:44,  1.78it/s] 21%|██▏       | 166/780 [01:04<04:55,  2.08it/s] 21%|██▏       | 167/780 [01:04<04:20,  2.35it/s] 22%|██▏       | 168/780 [01:05<03:56,  2.59it/s] 22%|██▏       | 169/780 [01:05<03:39,  2.78it/s] 22%|██▏       | 170/780 [01:05<03:27,  2.94it/s] 22%|██▏       | 171/780 [01:06<03:18,  3.07it/s] 22%|██▏       | 172/780 [01:06<03:11,  3.17it/s] 22%|██▏       | 173/780 [01:06<03:07,  3.24it/s] 22%|██▏       | 174/780 [01:06<03:03,  3.30it/s] 22%|██▏       | 175/780 [01:07<03:01,  3.34it/s] 23%|██▎       | 176/780 [01:07<02:59,  3.36it/s] 23%|██▎       | 177/780 [01:07<02:58,  3.38it/s] 23%|██▎       | 178/780 [01:08<02:57,  3.40it/s] 23%|██▎       | 179/780 [01:08<02:55,  3.42it/s] 23%|██▎       | 180/780 [01:08<02:56,  3.41it/s] 23%|██▎       | 181/780 [01:08<02:55,  3.41it/s] 23%|██▎       | 182/780 [01:09<02:54,  3.42it/s] 23%|██▎       | 183/780 [01:09<02:54,  3.42it/s] 24%|██▎       | 184/780 [01:09<02:54,  3.42it/s] 24%|██▎       | 185/780 [01:10<02:53,  3.43it/s] 24%|██▍       | 186/780 [01:10<02:53,  3.43it/s] 24%|██▍       | 187/780 [01:10<02:53,  3.43it/s] 24%|██▍       | 188/780 [01:10<02:52,  3.43it/s] 24%|██▍       | 189/780 [01:11<02:52,  3.43it/s] 24%|██▍       | 190/780 [01:11<02:51,  3.43it/s] 24%|██▍       | 191/780 [01:11<02:52,  3.42it/s] 25%|██▍       | 192/780 [01:12<02:51,  3.43it/s] 25%|██▍       | 193/780 [01:12<02:51,  3.43it/s] 25%|██▍       | 194/780 [01:12<02:50,  3.43it/s] 25%|██▌       | 195/780 [01:13<02:50,  3.42it/s] 25%|██▌       | 196/780 [01:13<02:50,  3.43it/s] 25%|██▌       | 197/780 [01:13<02:49,  3.43it/s] 25%|██▌       | 198/780 [01:13<02:49,  3.43it/s] 26%|██▌       | 199/780 [01:14<02:49,  3.43it/s] 26%|██▌       | 200/780 [01:14<02:49,  3.42it/s] 26%|██▌       | 201/780 [01:14<02:48,  3.43it/s] 26%|██▌       | 202/780 [01:15<02:49,  3.41it/s] 26%|██▌       | 203/780 [01:15<02:48,  3.42it/s] 26%|██▌       | 204/780 [01:15<02:48,  3.42it/s] 26%|██▋       | 205/780 [01:15<02:47,  3.43it/s] 26%|██▋       | 206/780 [01:16<02:47,  3.43it/s] 27%|██▋       | 207/780 [01:16<02:46,  3.43it/s] 27%|██▋       | 208/780 [01:16<02:46,  3.43it/s] 27%|██▋       | 209/780 [01:17<02:46,  3.43it/s] 27%|██▋       | 210/780 [01:17<02:46,  3.43it/s] 27%|██▋       | 211/780 [01:17<02:45,  3.43it/s] 27%|██▋       | 212/780 [01:17<02:45,  3.43it/s] 27%|██▋       | 213/780 [01:18<02:46,  3.42it/s] 27%|██▋       | 214/780 [01:18<02:45,  3.42it/s] 28%|██▊       | 215/780 [01:18<02:45,  3.41it/s] 28%|██▊       | 216/780 [01:19<02:45,  3.41it/s] 28%|██▊       | 217/780 [01:19<02:45,  3.41it/s] 28%|██▊       | 218/780 [01:19<02:44,  3.41it/s] 28%|██▊       | 219/780 [01:20<02:44,  3.41it/s] 28%|██▊       | 220/780 [01:20<02:44,  3.41it/s] 28%|██▊       | 221/780 [01:20<02:43,  3.41it/s] 28%|██▊       | 222/780 [01:20<02:43,  3.41it/s] 29%|██▊       | 223/780 [01:21<02:43,  3.41it/s] 29%|██▊       | 224/780 [01:21<02:43,  3.41it/s] 29%|██▉       | 225/780 [01:21<02:42,  3.41it/s] 29%|██▉       | 226/780 [01:22<02:42,  3.41it/s] 29%|██▉       | 227/780 [01:22<02:41,  3.42it/s] 29%|██▉       | 228/780 [01:22<02:41,  3.42it/s] 29%|██▉       | 229/780 [01:22<02:41,  3.42it/s] 29%|██▉       | 230/780 [01:23<02:40,  3.42it/s] 30%|██▉       | 231/780 [01:23<02:40,  3.42it/s] 30%|██▉       | 232/780 [01:23<02:40,  3.42it/s] 30%|██▉       | 233/780 [01:24<02:39,  3.42it/s] 30%|███       | 234/780 [01:24<02:39,  3.42it/s] 30%|███       | 235/780 [01:24<02:39,  3.43it/s] 30%|███       | 236/780 [01:25<02:38,  3.43it/s] 30%|███       | 237/780 [01:25<02:38,  3.43it/s] 31%|███       | 238/780 [01:25<02:37,  3.43it/s] 31%|███       | 239/780 [01:25<02:37,  3.43it/s] 31%|███       | 240/780 [01:26<02:37,  3.43it/s] 31%|███       | 241/780 [01:26<02:37,  3.42it/s] 31%|███       | 242/780 [01:26<02:36,  3.43it/s] 31%|███       | 243/780 [01:27<02:36,  3.43it/s] 31%|███▏      | 244/780 [01:27<02:36,  3.43it/s] 31%|███▏      | 245/780 [01:27<02:35,  3.43it/s] 32%|███▏      | 246/780 [01:27<02:35,  3.43it/s] 32%|███▏      | 247/780 [01:28<02:35,  3.43it/s] 32%|███▏      | 248/780 [01:28<02:35,  3.43it/s] 32%|███▏      | 249/780 [01:28<02:34,  3.43it/s] 32%|███▏      | 250/780 [01:29<02:34,  3.43it/s] 32%|███▏      | 251/780 [01:29<02:34,  3.43it/s] 32%|███▏      | 252/780 [01:29<02:34,  3.42it/s] 32%|███▏      | 253/780 [01:29<02:33,  3.43it/s] 33%|███▎      | 254/780 [01:30<02:33,  3.43it/s] 33%|███▎      | 255/780 [01:30<02:33,  3.42it/s] 33%|███▎      | 256/780 [01:30<02:32,  3.43it/s] 33%|███▎      | 257/780 [01:31<02:32,  3.43it/s] 33%|███▎      | 258/780 [01:31<02:32,  3.42it/s] 33%|███▎      | 259/780 [01:31<02:31,  3.43it/s] 33%|███▎      | 260/780 [01:32<02:31,  3.43it/s] 33%|███▎      | 261/780 [01:32<02:31,  3.44it/s] 34%|███▎      | 262/780 [01:32<02:30,  3.43it/s] 34%|███▎      | 263/780 [01:32<02:32,  3.39it/s] 34%|███▍      | 264/780 [01:33<02:31,  3.41it/s] 34%|███▍      | 265/780 [01:33<02:30,  3.41it/s] 34%|███▍      | 266/780 [01:33<02:30,  3.42it/s] 34%|███▍      | 267/780 [01:34<02:29,  3.42it/s] 34%|███▍      | 268/780 [01:34<02:29,  3.42it/s] 34%|███▍      | 269/780 [01:34<02:29,  3.42it/s] 35%|███▍      | 270/780 [01:34<02:28,  3.43it/s] 35%|███▍      | 271/780 [01:35<02:28,  3.43it/s] 35%|███▍      | 272/780 [01:35<02:27,  3.43it/s] 35%|███▌      | 273/780 [01:35<02:27,  3.43it/s] 35%|███▌      | 274/780 [01:36<02:28,  3.42it/s] 35%|███▌      | 275/780 [01:36<02:27,  3.42it/s] 35%|███▌      | 276/780 [01:36<02:27,  3.43it/s] 36%|███▌      | 277/780 [01:36<02:26,  3.43it/s] 36%|███▌      | 278/780 [01:37<02:26,  3.43it/s] 36%|███▌      | 279/780 [01:37<02:25,  3.43it/s] 36%|███▌      | 280/780 [01:37<02:25,  3.43it/s] 36%|███▌      | 281/780 [01:38<02:25,  3.43it/s] 36%|███▌      | 282/780 [01:38<02:25,  3.43it/s] 36%|███▋      | 283/780 [01:38<02:24,  3.43it/s] 36%|███▋      | 284/780 [01:39<02:24,  3.43it/s] 37%|███▋      | 285/780 [01:39<02:24,  3.42it/s] 37%|███▋      | 286/780 [01:39<02:24,  3.42it/s] 37%|███▋      | 287/780 [01:39<02:23,  3.43it/s] 37%|███▋      | 288/780 [01:40<02:23,  3.43it/s] 37%|███▋      | 289/780 [01:40<02:23,  3.43it/s] 37%|███▋      | 290/780 [01:40<02:23,  3.43it/s] 37%|███▋      | 291/780 [01:41<02:22,  3.43it/s] 37%|███▋      | 292/780 [01:41<02:22,  3.43it/s] 38%|███▊      | 293/780 [01:41<02:21,  3.43it/s] 38%|███▊      | 294/780 [01:41<02:21,  3.43it/s] 38%|███▊      | 295/780 [01:42<02:21,  3.44it/s] 38%|███▊      | 296/780 [01:42<02:22,  3.41it/s] 38%|███▊      | 297/780 [01:42<02:21,  3.41it/s] 38%|███▊      | 298/780 [01:43<02:21,  3.42it/s] 38%|███▊      | 299/780 [01:43<02:20,  3.42it/s] 38%|███▊      | 300/780 [01:43<02:20,  3.42it/s] 39%|███▊      | 301/780 [01:43<02:20,  3.42it/s] 39%|███▊      | 302/780 [01:44<02:19,  3.42it/s] 39%|███▉      | 303/780 [01:44<02:19,  3.43it/s] 39%|███▉      | 304/780 [01:44<02:18,  3.43it/s] 39%|███▉      | 305/780 [01:45<02:18,  3.43it/s] 39%|███▉      | 306/780 [01:45<02:18,  3.43it/s] 39%|███▉      | 307/780 [01:45<02:18,  3.42it/s] 39%|███▉      | 308/780 [01:46<02:18,  3.42it/s] 40%|███▉      | 309/780 [01:46<02:17,  3.42it/s] 40%|███▉      | 310/780 [01:46<02:17,  3.42it/s] 40%|███▉      | 311/780 [01:46<02:17,  3.42it/s] 40%|████      | 312/780 [01:47<02:16,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 02:30:39,850 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:30:39,850 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 02:30:39,850 >>   Batch size = 8
{'eval_loss': 1.0592540502548218, 'eval_runtime': 9.9219, 'eval_samples_per_second': 347.312, 'eval_steps_per_second': 43.439, 'epoch': 1.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 54.49it/s][A
  3%|▎         | 12/431 [00:00<00:08, 46.98it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.14it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.53it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.14it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.95it/s][A
  9%|▊         | 37/431 [00:00<00:08, 43.83it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.77it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.80it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.79it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.74it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.67it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.56it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.63it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.50it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.49it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.53it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.66it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.73it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.68it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.60it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.61it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.60it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.49it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.36it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.57it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.59it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.65it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.65it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.60it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.61it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.53it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 41.70it/s][A
 40%|███▉      | 172/431 [00:03<00:06, 42.83it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.24it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.37it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.43it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.46it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.53it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.46it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.45it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.33it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.49it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.67it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.61it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.64it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.59it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.56it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.53it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.43it/s][A
 60%|█████▉    | 257/431 [00:05<00:04, 43.42it/s][A
 61%|██████    | 262/431 [00:06<00:03, 43.56it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.68it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.62it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.65it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.60it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.53it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.59it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.48it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.51it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.60it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.72it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.62it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.62it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.65it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.60it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.56it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.58it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.60it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.58it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.60it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.59it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.66it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.61it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.56it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.54it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.48it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 43.59it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.57it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.59it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.64it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.66it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.57it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.45it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.56it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.56it/s][A 40%|████      | 312/780 [01:57<02:16,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:30:49,811 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 02:30:49,832 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:30:51,383 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:30:51,393 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:30:51,404 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:02<38:24,  4.93s/it] 40%|████      | 314/780 [02:03<27:31,  3.54s/it] 40%|████      | 315/780 [02:03<19:54,  2.57s/it] 41%|████      | 316/780 [02:03<14:35,  1.89s/it] 41%|████      | 317/780 [02:04<10:52,  1.41s/it] 41%|████      | 318/780 [02:04<08:16,  1.08s/it] 41%|████      | 319/780 [02:04<06:27,  1.19it/s] 41%|████      | 320/780 [02:05<05:11,  1.48it/s] 41%|████      | 321/780 [02:05<04:18,  1.78it/s] 41%|████▏     | 322/780 [02:05<03:41,  2.07it/s] 41%|████▏     | 323/780 [02:05<03:14,  2.35it/s] 42%|████▏     | 324/780 [02:06<02:56,  2.58it/s] 42%|████▏     | 325/780 [02:06<02:43,  2.78it/s] 42%|████▏     | 326/780 [02:06<02:34,  2.93it/s] 42%|████▏     | 327/780 [02:07<02:28,  3.06it/s] 42%|████▏     | 328/780 [02:07<02:23,  3.14it/s] 42%|████▏     | 329/780 [02:07<02:20,  3.21it/s] 42%|████▏     | 330/780 [02:08<02:17,  3.26it/s] 42%|████▏     | 331/780 [02:08<02:16,  3.30it/s] 43%|████▎     | 332/780 [02:08<02:14,  3.33it/s] 43%|████▎     | 333/780 [02:08<02:13,  3.34it/s] 43%|████▎     | 334/780 [02:09<02:12,  3.36it/s] 43%|████▎     | 335/780 [02:09<02:12,  3.36it/s] 43%|████▎     | 336/780 [02:09<02:12,  3.36it/s] 43%|████▎     | 337/780 [02:10<02:11,  3.36it/s] 43%|████▎     | 338/780 [02:10<02:11,  3.37it/s] 43%|████▎     | 339/780 [02:10<02:10,  3.39it/s] 44%|████▎     | 340/780 [02:10<02:09,  3.40it/s] 44%|████▎     | 341/780 [02:11<02:08,  3.41it/s] 44%|████▍     | 342/780 [02:11<02:08,  3.42it/s] 44%|████▍     | 343/780 [02:11<02:07,  3.42it/s] 44%|████▍     | 344/780 [02:12<02:07,  3.42it/s] 44%|████▍     | 345/780 [02:12<02:06,  3.43it/s] 44%|████▍     | 346/780 [02:12<02:06,  3.43it/s] 44%|████▍     | 347/780 [02:12<02:06,  3.42it/s] 45%|████▍     | 348/780 [02:13<02:06,  3.42it/s] 45%|████▍     | 349/780 [02:13<02:05,  3.43it/s] 45%|████▍     | 350/780 [02:13<02:05,  3.43it/s] 45%|████▌     | 351/780 [02:14<02:05,  3.42it/s] 45%|████▌     | 352/780 [02:14<02:04,  3.43it/s] 45%|████▌     | 353/780 [02:14<02:04,  3.43it/s] 45%|████▌     | 354/780 [02:15<02:04,  3.43it/s] 46%|████▌     | 355/780 [02:15<02:03,  3.43it/s] 46%|████▌     | 356/780 [02:15<02:03,  3.43it/s] 46%|████▌     | 357/780 [02:15<02:03,  3.43it/s] 46%|████▌     | 358/780 [02:16<02:03,  3.42it/s] 46%|████▌     | 359/780 [02:16<02:02,  3.43it/s] 46%|████▌     | 360/780 [02:16<02:02,  3.43it/s] 46%|████▋     | 361/780 [02:17<02:02,  3.43it/s] 46%|████▋     | 362/780 [02:17<02:01,  3.43it/s] 47%|████▋     | 363/780 [02:17<02:01,  3.43it/s] 47%|████▋     | 364/780 [02:17<02:01,  3.43it/s] 47%|████▋     | 365/780 [02:18<02:01,  3.42it/s] 47%|████▋     | 366/780 [02:18<02:00,  3.43it/s] 47%|████▋     | 367/780 [02:18<02:00,  3.43it/s] 47%|████▋     | 368/780 [02:19<02:00,  3.43it/s] 47%|████▋     | 369/780 [02:19<02:00,  3.42it/s] 47%|████▋     | 370/780 [02:19<01:59,  3.42it/s] 48%|████▊     | 371/780 [02:19<01:59,  3.42it/s] 48%|████▊     | 372/780 [02:20<01:58,  3.43it/s] 48%|████▊     | 373/780 [02:20<01:58,  3.43it/s] 48%|████▊     | 374/780 [02:20<01:58,  3.43it/s] 48%|████▊     | 375/780 [02:21<01:58,  3.43it/s] 48%|████▊     | 376/780 [02:21<01:57,  3.43it/s] 48%|████▊     | 377/780 [02:21<01:57,  3.43it/s] 48%|████▊     | 378/780 [02:22<01:57,  3.43it/s] 49%|████▊     | 379/780 [02:22<01:56,  3.43it/s] 49%|████▊     | 380/780 [02:22<01:57,  3.42it/s] 49%|████▉     | 381/780 [02:22<01:56,  3.42it/s] 49%|████▉     | 382/780 [02:23<01:56,  3.43it/s] 49%|████▉     | 383/780 [02:23<01:55,  3.43it/s] 49%|████▉     | 384/780 [02:23<01:55,  3.43it/s] 49%|████▉     | 385/780 [02:24<01:55,  3.43it/s] 49%|████▉     | 386/780 [02:24<01:54,  3.43it/s] 50%|████▉     | 387/780 [02:24<01:54,  3.42it/s] 50%|████▉     | 388/780 [02:24<01:54,  3.43it/s] 50%|████▉     | 389/780 [02:25<01:54,  3.43it/s] 50%|█████     | 390/780 [02:25<01:53,  3.43it/s] 50%|█████     | 391/780 [02:25<01:53,  3.43it/s] 50%|█████     | 392/780 [02:26<01:53,  3.43it/s] 50%|█████     | 393/780 [02:26<01:53,  3.42it/s] 51%|█████     | 394/780 [02:26<01:52,  3.43it/s] 51%|█████     | 395/780 [02:26<01:52,  3.43it/s] 51%|█████     | 396/780 [02:27<01:52,  3.43it/s] 51%|█████     | 397/780 [02:27<01:51,  3.43it/s] 51%|█████     | 398/780 [02:27<01:51,  3.42it/s] 51%|█████     | 399/780 [02:28<01:51,  3.42it/s] 51%|█████▏    | 400/780 [02:28<01:50,  3.42it/s] 51%|█████▏    | 401/780 [02:28<01:50,  3.43it/s] 52%|█████▏    | 402/780 [02:29<01:50,  3.43it/s] 52%|█████▏    | 403/780 [02:29<01:50,  3.43it/s] 52%|█████▏    | 404/780 [02:29<01:49,  3.43it/s] 52%|█████▏    | 405/780 [02:29<01:49,  3.43it/s] 52%|█████▏    | 406/780 [02:30<01:48,  3.43it/s] 52%|█████▏    | 407/780 [02:30<01:48,  3.43it/s] 52%|█████▏    | 408/780 [02:30<01:48,  3.43it/s] 52%|█████▏    | 409/780 [02:31<01:48,  3.42it/s] 53%|█████▎    | 410/780 [02:31<01:48,  3.42it/s] 53%|█████▎    | 411/780 [02:31<01:47,  3.43it/s] 53%|█████▎    | 412/780 [02:31<01:47,  3.42it/s] 53%|█████▎    | 413/780 [02:32<01:47,  3.43it/s] 53%|█████▎    | 414/780 [02:32<01:46,  3.42it/s] 53%|█████▎    | 415/780 [02:32<01:46,  3.42it/s] 53%|█████▎    | 416/780 [02:33<01:46,  3.43it/s] 53%|█████▎    | 417/780 [02:33<01:45,  3.43it/s] 54%|█████▎    | 418/780 [02:33<01:45,  3.43it/s] 54%|█████▎    | 419/780 [02:34<01:45,  3.43it/s] 54%|█████▍    | 420/780 [02:34<01:45,  3.42it/s] 54%|█████▍    | 421/780 [02:34<01:44,  3.43it/s] 54%|█████▍    | 422/780 [02:34<01:44,  3.43it/s] 54%|█████▍    | 423/780 [02:35<01:44,  3.43it/s] 54%|█████▍    | 424/780 [02:35<01:43,  3.43it/s] 54%|█████▍    | 425/780 [02:35<01:43,  3.43it/s] 55%|█████▍    | 426/780 [02:36<01:43,  3.43it/s] 55%|█████▍    | 427/780 [02:36<01:42,  3.43it/s] 55%|█████▍    | 428/780 [02:36<01:42,  3.43it/s] 55%|█████▌    | 429/780 [02:36<01:42,  3.44it/s] 55%|█████▌    | 430/780 [02:37<01:41,  3.43it/s] 55%|█████▌    | 431/780 [02:37<01:41,  3.42it/s] 55%|█████▌    | 432/780 [02:37<01:41,  3.43it/s] 56%|█████▌    | 433/780 [02:38<01:41,  3.43it/s] 56%|█████▌    | 434/780 [02:38<01:40,  3.43it/s] 56%|█████▌    | 435/780 [02:38<01:40,  3.43it/s] 56%|█████▌    | 436/780 [02:38<01:40,  3.43it/s] 56%|█████▌    | 437/780 [02:39<01:39,  3.43it/s] 56%|█████▌    | 438/780 [02:39<01:39,  3.43it/s] 56%|█████▋    | 439/780 [02:39<01:39,  3.43it/s] 56%|█████▋    | 440/780 [02:40<01:38,  3.44it/s] 57%|█████▋    | 441/780 [02:40<01:38,  3.43it/s] 57%|█████▋    | 442/780 [02:40<01:38,  3.42it/s] 57%|█████▋    | 443/780 [02:41<01:38,  3.43it/s] 57%|█████▋    | 444/780 [02:41<01:38,  3.43it/s] 57%|█████▋    | 445/780 [02:41<01:37,  3.43it/s] 57%|█████▋    | 446/780 [02:41<01:37,  3.43it/s] 57%|█████▋    | 447/780 [02:42<01:37,  3.43it/s] 57%|█████▋    | 448/780 [02:42<01:36,  3.43it/s] 58%|█████▊    | 449/780 [02:42<01:36,  3.43it/s] 58%|█████▊    | 450/780 [02:43<01:36,  3.43it/s] 58%|█████▊    | 451/780 [02:43<01:35,  3.43it/s] 58%|█████▊    | 452/780 [02:43<01:35,  3.43it/s] 58%|█████▊    | 453/780 [02:43<01:35,  3.42it/s] 58%|█████▊    | 454/780 [02:44<01:35,  3.42it/s] 58%|█████▊    | 455/780 [02:44<01:35,  3.42it/s] 58%|█████▊    | 456/780 [02:44<01:34,  3.42it/s] 59%|█████▊    | 457/780 [02:45<01:34,  3.42it/s] 59%|█████▊    | 458/780 [02:45<01:34,  3.42it/s] 59%|█████▉    | 459/780 [02:45<01:33,  3.42it/s] 59%|█████▉    | 460/780 [02:45<01:33,  3.43it/s] 59%|█████▉    | 461/780 [02:46<01:33,  3.42it/s] 59%|█████▉    | 462/780 [02:46<01:32,  3.43it/s] 59%|█████▉    | 463/780 [02:46<01:32,  3.43it/s] 59%|█████▉    | 464/780 [02:47<01:32,  3.42it/s] 60%|█████▉    | 465/780 [02:47<01:32,  3.42it/s] 60%|█████▉    | 466/780 [02:47<01:31,  3.42it/s] 60%|█████▉    | 467/780 [02:48<01:31,  3.43it/s] 60%|██████    | 468/780 [02:48<01:31,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 02:31:40,946 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:31:40,946 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 02:31:40,946 >>   Batch size = 8
{'eval_loss': 1.0812879800796509, 'eval_runtime': 9.9349, 'eval_samples_per_second': 346.859, 'eval_steps_per_second': 43.383, 'epoch': 2.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 54.64it/s][A
  3%|▎         | 12/431 [00:00<00:08, 47.08it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.52it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.71it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.33it/s][A
  7%|▋         | 32/431 [00:00<00:09, 44.12it/s][A
  9%|▊         | 37/431 [00:00<00:08, 43.89it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.68it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.74it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.80it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.76it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.67it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.65it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.62it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.55it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.44it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.51it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.56it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.57it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.68it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.67it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.58it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.56it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 42.96it/s][A
 29%|██▉       | 127/431 [00:02<00:07, 43.20it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.31it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.49it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.62it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.57it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.59it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.57it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.48it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.45it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.44it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.66it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.73it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.79it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.65it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.61it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.49it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.49it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.57it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.59it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.66it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.69it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.70it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.57it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.59it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.49it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.54it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 43.59it/s][A
 61%|██████    | 262/431 [00:05<00:03, 43.69it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.67it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.73it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.69it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.71it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.53it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.44it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.44it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.62it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.65it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.73it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.73it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.63it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.58it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.59it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.62it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.60it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.61it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.66it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.69it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.59it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.59it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.60it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.64it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.57it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.56it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 43.61it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.63it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.62it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.60it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.65it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.56it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.58it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.53it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.53it/s][A 60%|██████    | 468/780 [02:58<01:31,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:31:50,898 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 02:31:50,919 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:31:52,648 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:31:52,663 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:31:52,675 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:04<25:33,  4.93s/it] 60%|██████    | 470/780 [03:04<18:17,  3.54s/it] 60%|██████    | 471/780 [03:04<13:13,  2.57s/it] 61%|██████    | 472/780 [03:04<09:41,  1.89s/it] 61%|██████    | 473/780 [03:05<07:12,  1.41s/it] 61%|██████    | 474/780 [03:05<05:28,  1.08s/it] 61%|██████    | 475/780 [03:05<04:16,  1.19it/s] 61%|██████    | 476/780 [03:06<03:26,  1.48it/s] 61%|██████    | 477/780 [03:06<02:50,  1.78it/s] 61%|██████▏   | 478/780 [03:06<02:25,  2.07it/s] 61%|██████▏   | 479/780 [03:07<02:08,  2.34it/s] 62%|██████▏   | 480/780 [03:07<01:56,  2.58it/s] 62%|██████▏   | 481/780 [03:07<01:48,  2.77it/s] 62%|██████▏   | 482/780 [03:07<01:41,  2.93it/s] 62%|██████▏   | 483/780 [03:08<01:37,  3.05it/s] 62%|██████▏   | 484/780 [03:08<01:34,  3.14it/s] 62%|██████▏   | 485/780 [03:08<01:31,  3.21it/s] 62%|██████▏   | 486/780 [03:09<01:30,  3.26it/s] 62%|██████▏   | 487/780 [03:09<01:29,  3.29it/s] 63%|██████▎   | 488/780 [03:09<01:28,  3.32it/s] 63%|██████▎   | 489/780 [03:09<01:27,  3.33it/s] 63%|██████▎   | 490/780 [03:10<01:26,  3.35it/s] 63%|██████▎   | 491/780 [03:10<01:25,  3.36it/s] 63%|██████▎   | 492/780 [03:10<01:25,  3.35it/s] 63%|██████▎   | 493/780 [03:11<01:25,  3.36it/s] 63%|██████▎   | 494/780 [03:11<01:24,  3.37it/s] 63%|██████▎   | 495/780 [03:11<01:24,  3.37it/s] 64%|██████▎   | 496/780 [03:12<01:24,  3.38it/s] 64%|██████▎   | 497/780 [03:12<01:23,  3.38it/s] 64%|██████▍   | 498/780 [03:12<01:23,  3.38it/s] 64%|██████▍   | 499/780 [03:12<01:23,  3.38it/s] 64%|██████▍   | 500/780 [03:13<01:22,  3.37it/s]                                                  64%|██████▍   | 500/780 [03:13<01:22,  3.37it/s] 64%|██████▍   | 501/780 [03:13<01:22,  3.37it/s] 64%|██████▍   | 502/780 [03:13<01:22,  3.38it/s] 64%|██████▍   | 503/780 [03:14<01:22,  3.36it/s] 65%|██████▍   | 504/780 [03:14<01:22,  3.36it/s] 65%|██████▍   | 505/780 [03:14<01:21,  3.37it/s] 65%|██████▍   | 506/780 [03:15<01:21,  3.38it/s] 65%|██████▌   | 507/780 [03:15<01:20,  3.38it/s] 65%|██████▌   | 508/780 [03:15<01:20,  3.38it/s] 65%|██████▌   | 509/780 [03:15<01:20,  3.38it/s] 65%|██████▌   | 510/780 [03:16<01:19,  3.38it/s] 66%|██████▌   | 511/780 [03:16<01:19,  3.38it/s] 66%|██████▌   | 512/780 [03:16<01:19,  3.38it/s] 66%|██████▌   | 513/780 [03:17<01:19,  3.38it/s] 66%|██████▌   | 514/780 [03:17<01:19,  3.37it/s] 66%|██████▌   | 515/780 [03:17<01:18,  3.37it/s] 66%|██████▌   | 516/780 [03:17<01:18,  3.37it/s] 66%|██████▋   | 517/780 [03:18<01:17,  3.37it/s] 66%|██████▋   | 518/780 [03:18<01:17,  3.38it/s] 67%|██████▋   | 519/780 [03:18<01:17,  3.38it/s] 67%|██████▋   | 520/780 [03:19<01:17,  3.38it/s] 67%|██████▋   | 521/780 [03:19<01:16,  3.38it/s] 67%|██████▋   | 522/780 [03:19<01:16,  3.38it/s] 67%|██████▋   | 523/780 [03:20<01:15,  3.38it/s] 67%|██████▋   | 524/780 [03:20<01:15,  3.38it/s] 67%|██████▋   | 525/780 [03:20<01:15,  3.37it/s] 67%|██████▋   | 526/780 [03:20<01:15,  3.38it/s] 68%|██████▊   | 527/780 [03:21<01:14,  3.38it/s] 68%|██████▊   | 528/780 [03:21<01:14,  3.38it/s] 68%|██████▊   | 529/780 [03:21<01:14,  3.38it/s] 68%|██████▊   | 530/780 [03:22<01:13,  3.38it/s] 68%|██████▊   | 531/780 [03:22<01:13,  3.38it/s] 68%|██████▊   | 532/780 [03:22<01:13,  3.38it/s] 68%|██████▊   | 533/780 [03:23<01:13,  3.38it/s] 68%|██████▊   | 534/780 [03:23<01:12,  3.38it/s] 69%|██████▊   | 535/780 [03:23<01:12,  3.38it/s] 69%|██████▊   | 536/780 [03:23<01:12,  3.37it/s] 69%|██████▉   | 537/780 [03:24<01:12,  3.37it/s] 69%|██████▉   | 538/780 [03:24<01:11,  3.37it/s] 69%|██████▉   | 539/780 [03:24<01:11,  3.38it/s] 69%|██████▉   | 540/780 [03:25<01:11,  3.38it/s] 69%|██████▉   | 541/780 [03:25<01:10,  3.38it/s] 69%|██████▉   | 542/780 [03:25<01:10,  3.38it/s] 70%|██████▉   | 543/780 [03:25<01:10,  3.38it/s] 70%|██████▉   | 544/780 [03:26<01:09,  3.38it/s] 70%|██████▉   | 545/780 [03:26<01:09,  3.38it/s] 70%|███████   | 546/780 [03:26<01:09,  3.38it/s] 70%|███████   | 547/780 [03:27<01:08,  3.38it/s] 70%|███████   | 548/780 [03:27<01:08,  3.38it/s] 70%|███████   | 549/780 [03:27<01:08,  3.38it/s] 71%|███████   | 550/780 [03:28<01:08,  3.38it/s] 71%|███████   | 551/780 [03:28<01:07,  3.38it/s] 71%|███████   | 552/780 [03:28<01:07,  3.38it/s] 71%|███████   | 553/780 [03:28<01:07,  3.38it/s] 71%|███████   | 554/780 [03:29<01:06,  3.38it/s] 71%|███████   | 555/780 [03:29<01:06,  3.37it/s] 71%|███████▏  | 556/780 [03:29<01:06,  3.37it/s] 71%|███████▏  | 557/780 [03:30<01:06,  3.38it/s] 72%|███████▏  | 558/780 [03:30<01:05,  3.38it/s] 72%|███████▏  | 559/780 [03:30<01:05,  3.38it/s] 72%|███████▏  | 560/780 [03:31<01:05,  3.38it/s] 72%|███████▏  | 561/780 [03:31<01:04,  3.38it/s] 72%|███████▏  | 562/780 [03:31<01:04,  3.38it/s] 72%|███████▏  | 563/780 [03:31<01:04,  3.38it/s] 72%|███████▏  | 564/780 [03:32<01:03,  3.38it/s] 72%|███████▏  | 565/780 [03:32<01:03,  3.38it/s] 73%|███████▎  | 566/780 [03:32<01:03,  3.37it/s] 73%|███████▎  | 567/780 [03:33<01:03,  3.37it/s] 73%|███████▎  | 568/780 [03:33<01:02,  3.38it/s] 73%|███████▎  | 569/780 [03:33<01:02,  3.38it/s] 73%|███████▎  | 570/780 [03:33<01:02,  3.38it/s] 73%|███████▎  | 571/780 [03:34<01:01,  3.38it/s] 73%|███████▎  | 572/780 [03:34<01:01,  3.38it/s] 73%|███████▎  | 573/780 [03:34<01:01,  3.38it/s] 74%|███████▎  | 574/780 [03:35<01:00,  3.38it/s] 74%|███████▎  | 575/780 [03:35<01:00,  3.38it/s] 74%|███████▍  | 576/780 [03:35<01:00,  3.38it/s] 74%|███████▍  | 577/780 [03:36<01:00,  3.37it/s] 74%|███████▍  | 578/780 [03:36<00:59,  3.37it/s] 74%|███████▍  | 579/780 [03:36<00:59,  3.37it/s] 74%|███████▍  | 580/780 [03:36<00:59,  3.38it/s] 74%|███████▍  | 581/780 [03:37<00:58,  3.39it/s] 75%|███████▍  | 582/780 [03:37<00:58,  3.40it/s] 75%|███████▍  | 583/780 [03:37<00:57,  3.41it/s] 75%|███████▍  | 584/780 [03:38<00:57,  3.42it/s] 75%|███████▌  | 585/780 [03:38<00:57,  3.42it/s] 75%|███████▌  | 586/780 [03:38<00:56,  3.42it/s] 75%|███████▌  | 587/780 [03:38<00:56,  3.43it/s] 75%|███████▌  | 588/780 [03:39<00:56,  3.41it/s] 76%|███████▌  | 589/780 [03:39<00:56,  3.41it/s] 76%|███████▌  | 590/780 [03:39<00:55,  3.42it/s] 76%|███████▌  | 591/780 [03:40<00:55,  3.43it/s] 76%|███████▌  | 592/780 [03:40<00:54,  3.42it/s] 76%|███████▌  | 593/780 [03:40<00:54,  3.43it/s] 76%|███████▌  | 594/780 [03:41<00:54,  3.43it/s] 76%|███████▋  | 595/780 [03:41<00:53,  3.43it/s] 76%|███████▋  | 596/780 [03:41<00:53,  3.43it/s] 77%|███████▋  | 597/780 [03:41<00:53,  3.43it/s] 77%|███████▋  | 598/780 [03:42<00:53,  3.43it/s] 77%|███████▋  | 599/780 [03:42<00:53,  3.41it/s] 77%|███████▋  | 600/780 [03:42<00:52,  3.42it/s] 77%|███████▋  | 601/780 [03:43<00:52,  3.42it/s] 77%|███████▋  | 602/780 [03:43<00:51,  3.42it/s] 77%|███████▋  | 603/780 [03:43<00:51,  3.42it/s] 77%|███████▋  | 604/780 [03:43<00:51,  3.43it/s] 78%|███████▊  | 605/780 [03:44<00:51,  3.43it/s] 78%|███████▊  | 606/780 [03:44<00:50,  3.43it/s] 78%|███████▊  | 607/780 [03:44<00:50,  3.43it/s] 78%|███████▊  | 608/780 [03:45<00:50,  3.42it/s] 78%|███████▊  | 609/780 [03:45<00:49,  3.43it/s] 78%|███████▊  | 610/780 [03:45<00:49,  3.41it/s] 78%|███████▊  | 611/780 [03:45<00:49,  3.42it/s] 78%|███████▊  | 612/780 [03:46<00:49,  3.42it/s] 79%|███████▊  | 613/780 [03:46<00:48,  3.43it/s] 79%|███████▊  | 614/780 [03:46<00:48,  3.41it/s] 79%|███████▉  | 615/780 [03:47<00:48,  3.40it/s] 79%|███████▉  | 616/780 [03:47<00:48,  3.39it/s] 79%|███████▉  | 617/780 [03:47<00:48,  3.39it/s] 79%|███████▉  | 618/780 [03:48<00:47,  3.39it/s] 79%|███████▉  | 619/780 [03:48<00:47,  3.39it/s] 79%|███████▉  | 620/780 [03:48<00:47,  3.38it/s] 80%|███████▉  | 621/780 [03:48<00:47,  3.38it/s] 80%|███████▉  | 622/780 [03:49<00:46,  3.38it/s] 80%|███████▉  | 623/780 [03:49<00:46,  3.38it/s] 80%|████████  | 624/780 [03:49<00:46,  3.37it/s][INFO|trainer.py:2140] 2023-08-29 02:32:42,474 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:32:42,474 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 02:32:42,474 >>   Batch size = 8
{'eval_loss': 1.0885376930236816, 'eval_runtime': 9.9217, 'eval_samples_per_second': 347.319, 'eval_steps_per_second': 43.44, 'epoch': 3.0}
{'loss': 0.3911, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 53.89it/s][A
  3%|▎         | 12/431 [00:00<00:08, 47.01it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.85it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.88it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.42it/s][A
  7%|▋         | 32/431 [00:00<00:09, 44.05it/s][A
  9%|▊         | 37/431 [00:00<00:08, 43.85it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.63it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.61it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.11it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 42.34it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 42.85it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.18it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.23it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.22it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.19it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.33it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.47it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.36it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.45it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.63it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.75it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.70it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.61it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 43.50it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.53it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.50it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.52it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.53it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.71it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.72it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.71it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.61it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.51it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.51it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.53it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.55it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.61it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.70it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.74it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.59it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.58it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.58it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.55it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.48it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.42it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.59it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.68it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.71it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.63it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 43.66it/s][A
 61%|██████    | 262/431 [00:05<00:03, 43.54it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.49it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.47it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.58it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.65it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.76it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.66it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.58it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.56it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.53it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.53it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.57it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.61it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.67it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.73it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.64it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.56it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.52it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.52it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.53it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.53it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.67it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.69it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.70it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.68it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.48it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 43.65it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.50it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.42it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.54it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.66it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.71it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.68it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.70it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.70it/s][A 80%|████████  | 624/780 [03:59<00:46,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:32:52,434 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 02:32:52,454 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:32:54,283 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:32:54,295 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:32:54,305 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:05<12:44,  4.93s/it] 80%|████████  | 626/780 [04:05<09:05,  3.54s/it] 80%|████████  | 627/780 [04:06<06:33,  2.57s/it] 81%|████████  | 628/780 [04:06<04:46,  1.89s/it] 81%|████████  | 629/780 [04:06<03:32,  1.41s/it] 81%|████████  | 630/780 [04:07<02:41,  1.07s/it] 81%|████████  | 631/780 [04:07<02:05,  1.19it/s] 81%|████████  | 632/780 [04:07<01:39,  1.48it/s] 81%|████████  | 633/780 [04:07<01:22,  1.79it/s] 81%|████████▏ | 634/780 [04:08<01:09,  2.09it/s] 81%|████████▏ | 635/780 [04:08<01:01,  2.37it/s] 82%|████████▏ | 636/780 [04:08<00:55,  2.61it/s] 82%|████████▏ | 637/780 [04:09<00:50,  2.81it/s] 82%|████████▏ | 638/780 [04:09<00:47,  2.97it/s] 82%|████████▏ | 639/780 [04:09<00:45,  3.10it/s] 82%|████████▏ | 640/780 [04:09<00:43,  3.19it/s] 82%|████████▏ | 641/780 [04:10<00:42,  3.26it/s] 82%|████████▏ | 642/780 [04:10<00:41,  3.31it/s] 82%|████████▏ | 643/780 [04:10<00:40,  3.35it/s] 83%|████████▎ | 644/780 [04:11<00:40,  3.37it/s] 83%|████████▎ | 645/780 [04:11<00:39,  3.39it/s] 83%|████████▎ | 646/780 [04:11<00:39,  3.41it/s] 83%|████████▎ | 647/780 [04:12<00:38,  3.41it/s] 83%|████████▎ | 648/780 [04:12<00:38,  3.40it/s] 83%|████████▎ | 649/780 [04:12<00:38,  3.41it/s] 83%|████████▎ | 650/780 [04:12<00:37,  3.42it/s] 83%|████████▎ | 651/780 [04:13<00:37,  3.43it/s] 84%|████████▎ | 652/780 [04:13<00:37,  3.42it/s] 84%|████████▎ | 653/780 [04:13<00:37,  3.42it/s] 84%|████████▍ | 654/780 [04:14<00:36,  3.43it/s] 84%|████████▍ | 655/780 [04:14<00:36,  3.43it/s] 84%|████████▍ | 656/780 [04:14<00:36,  3.42it/s] 84%|████████▍ | 657/780 [04:14<00:35,  3.43it/s] 84%|████████▍ | 658/780 [04:15<00:35,  3.43it/s] 84%|████████▍ | 659/780 [04:15<00:35,  3.42it/s] 85%|████████▍ | 660/780 [04:15<00:35,  3.42it/s] 85%|████████▍ | 661/780 [04:16<00:34,  3.43it/s] 85%|████████▍ | 662/780 [04:16<00:34,  3.43it/s] 85%|████████▌ | 663/780 [04:16<00:34,  3.43it/s] 85%|████████▌ | 664/780 [04:16<00:33,  3.43it/s] 85%|████████▌ | 665/780 [04:17<00:33,  3.43it/s] 85%|████████▌ | 666/780 [04:17<00:33,  3.43it/s] 86%|████████▌ | 667/780 [04:17<00:32,  3.43it/s] 86%|████████▌ | 668/780 [04:18<00:32,  3.43it/s] 86%|████████▌ | 669/780 [04:18<00:32,  3.43it/s] 86%|████████▌ | 670/780 [04:18<00:32,  3.43it/s] 86%|████████▌ | 671/780 [04:19<00:31,  3.43it/s] 86%|████████▌ | 672/780 [04:19<00:31,  3.43it/s] 86%|████████▋ | 673/780 [04:19<00:31,  3.43it/s] 86%|████████▋ | 674/780 [04:19<00:30,  3.43it/s] 87%|████████▋ | 675/780 [04:20<00:30,  3.43it/s] 87%|████████▋ | 676/780 [04:20<00:30,  3.43it/s] 87%|████████▋ | 677/780 [04:20<00:30,  3.43it/s] 87%|████████▋ | 678/780 [04:21<00:29,  3.43it/s] 87%|████████▋ | 679/780 [04:21<00:29,  3.43it/s] 87%|████████▋ | 680/780 [04:21<00:29,  3.43it/s] 87%|████████▋ | 681/780 [04:21<00:28,  3.41it/s] 87%|████████▋ | 682/780 [04:22<00:28,  3.41it/s] 88%|████████▊ | 683/780 [04:22<00:28,  3.42it/s] 88%|████████▊ | 684/780 [04:22<00:28,  3.43it/s] 88%|████████▊ | 685/780 [04:23<00:27,  3.43it/s] 88%|████████▊ | 686/780 [04:23<00:27,  3.43it/s] 88%|████████▊ | 687/780 [04:23<00:27,  3.43it/s] 88%|████████▊ | 688/780 [04:23<00:26,  3.41it/s] 88%|████████▊ | 689/780 [04:24<00:26,  3.40it/s] 88%|████████▊ | 690/780 [04:24<00:26,  3.39it/s] 89%|████████▊ | 691/780 [04:24<00:26,  3.39it/s] 89%|████████▊ | 692/780 [04:25<00:26,  3.38it/s] 89%|████████▉ | 693/780 [04:25<00:25,  3.38it/s] 89%|████████▉ | 694/780 [04:25<00:25,  3.38it/s] 89%|████████▉ | 695/780 [04:26<00:25,  3.38it/s] 89%|████████▉ | 696/780 [04:26<00:24,  3.38it/s] 89%|████████▉ | 697/780 [04:26<00:24,  3.38it/s] 89%|████████▉ | 698/780 [04:26<00:24,  3.38it/s] 90%|████████▉ | 699/780 [04:27<00:23,  3.38it/s] 90%|████████▉ | 700/780 [04:27<00:23,  3.38it/s] 90%|████████▉ | 701/780 [04:27<00:23,  3.38it/s] 90%|█████████ | 702/780 [04:28<00:23,  3.37it/s] 90%|█████████ | 703/780 [04:28<00:22,  3.37it/s] 90%|█████████ | 704/780 [04:28<00:22,  3.38it/s] 90%|█████████ | 705/780 [04:29<00:22,  3.38it/s] 91%|█████████ | 706/780 [04:29<00:21,  3.38it/s] 91%|█████████ | 707/780 [04:29<00:21,  3.38it/s] 91%|█████████ | 708/780 [04:29<00:21,  3.37it/s] 91%|█████████ | 709/780 [04:30<00:21,  3.38it/s] 91%|█████████ | 710/780 [04:30<00:20,  3.38it/s] 91%|█████████ | 711/780 [04:30<00:20,  3.37it/s] 91%|█████████▏| 712/780 [04:31<00:20,  3.37it/s] 91%|█████████▏| 713/780 [04:31<00:19,  3.37it/s] 92%|█████████▏| 714/780 [04:31<00:19,  3.37it/s] 92%|█████████▏| 715/780 [04:31<00:19,  3.39it/s] 92%|█████████▏| 716/780 [04:32<00:18,  3.41it/s] 92%|█████████▏| 717/780 [04:32<00:18,  3.41it/s] 92%|█████████▏| 718/780 [04:32<00:18,  3.42it/s] 92%|█████████▏| 719/780 [04:33<00:17,  3.42it/s] 92%|█████████▏| 720/780 [04:33<00:17,  3.43it/s] 92%|█████████▏| 721/780 [04:33<00:17,  3.43it/s] 93%|█████████▎| 722/780 [04:34<00:16,  3.42it/s] 93%|█████████▎| 723/780 [04:34<00:16,  3.43it/s] 93%|█████████▎| 724/780 [04:34<00:16,  3.43it/s] 93%|█████████▎| 725/780 [04:34<00:16,  3.43it/s] 93%|█████████▎| 726/780 [04:35<00:15,  3.43it/s] 93%|█████████▎| 727/780 [04:35<00:15,  3.43it/s] 93%|█████████▎| 728/780 [04:35<00:15,  3.43it/s] 93%|█████████▎| 729/780 [04:36<00:14,  3.43it/s] 94%|█████████▎| 730/780 [04:36<00:14,  3.43it/s] 94%|█████████▎| 731/780 [04:36<00:14,  3.44it/s] 94%|█████████▍| 732/780 [04:36<00:13,  3.44it/s] 94%|█████████▍| 733/780 [04:37<00:13,  3.41it/s] 94%|█████████▍| 734/780 [04:37<00:13,  3.42it/s] 94%|█████████▍| 735/780 [04:37<00:13,  3.42it/s] 94%|█████████▍| 736/780 [04:38<00:12,  3.42it/s] 94%|█████████▍| 737/780 [04:38<00:12,  3.43it/s] 95%|█████████▍| 738/780 [04:38<00:12,  3.43it/s] 95%|█████████▍| 739/780 [04:38<00:11,  3.42it/s] 95%|█████████▍| 740/780 [04:39<00:11,  3.43it/s] 95%|█████████▌| 741/780 [04:39<00:11,  3.43it/s] 95%|█████████▌| 742/780 [04:39<00:11,  3.43it/s] 95%|█████████▌| 743/780 [04:40<00:10,  3.43it/s] 95%|█████████▌| 744/780 [04:40<00:10,  3.40it/s] 96%|█████████▌| 745/780 [04:40<00:10,  3.41it/s] 96%|█████████▌| 746/780 [04:41<00:09,  3.42it/s] 96%|█████████▌| 747/780 [04:41<00:09,  3.42it/s] 96%|█████████▌| 748/780 [04:41<00:09,  3.42it/s] 96%|█████████▌| 749/780 [04:41<00:09,  3.42it/s] 96%|█████████▌| 750/780 [04:42<00:08,  3.42it/s] 96%|█████████▋| 751/780 [04:42<00:08,  3.43it/s] 96%|█████████▋| 752/780 [04:42<00:08,  3.42it/s] 97%|█████████▋| 753/780 [04:43<00:07,  3.43it/s] 97%|█████████▋| 754/780 [04:43<00:07,  3.43it/s] 97%|█████████▋| 755/780 [04:43<00:07,  3.42it/s] 97%|█████████▋| 756/780 [04:43<00:07,  3.42it/s] 97%|█████████▋| 757/780 [04:44<00:06,  3.42it/s] 97%|█████████▋| 758/780 [04:44<00:06,  3.43it/s] 97%|█████████▋| 759/780 [04:44<00:06,  3.42it/s] 97%|█████████▋| 760/780 [04:45<00:05,  3.42it/s] 98%|█████████▊| 761/780 [04:45<00:05,  3.42it/s] 98%|█████████▊| 762/780 [04:45<00:05,  3.43it/s] 98%|█████████▊| 763/780 [04:45<00:04,  3.43it/s] 98%|█████████▊| 764/780 [04:46<00:04,  3.43it/s] 98%|█████████▊| 765/780 [04:46<00:04,  3.43it/s] 98%|█████████▊| 766/780 [04:46<00:04,  3.41it/s] 98%|█████████▊| 767/780 [04:47<00:03,  3.41it/s] 98%|█████████▊| 768/780 [04:47<00:03,  3.42it/s] 99%|█████████▊| 769/780 [04:47<00:03,  3.42it/s] 99%|█████████▊| 770/780 [04:48<00:02,  3.42it/s] 99%|█████████▉| 771/780 [04:48<00:02,  3.43it/s] 99%|█████████▉| 772/780 [04:48<00:02,  3.43it/s] 99%|█████████▉| 773/780 [04:48<00:02,  3.43it/s] 99%|█████████▉| 774/780 [04:49<00:01,  3.43it/s] 99%|█████████▉| 775/780 [04:49<00:01,  3.43it/s] 99%|█████████▉| 776/780 [04:49<00:01,  3.43it/s]100%|█████████▉| 777/780 [04:50<00:00,  3.41it/s]100%|█████████▉| 778/780 [04:50<00:00,  3.41it/s]100%|█████████▉| 779/780 [04:50<00:00,  3.42it/s]100%|██████████| 780/780 [04:50<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 02:33:43,563 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:33:43,563 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 02:33:43,563 >>   Batch size = 8
{'eval_loss': 1.0953699350357056, 'eval_runtime': 9.9298, 'eval_samples_per_second': 347.037, 'eval_steps_per_second': 43.405, 'epoch': 4.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 55.07it/s][A
  3%|▎         | 12/431 [00:00<00:08, 47.17it/s][A
  4%|▍         | 17/431 [00:00<00:09, 45.34it/s][A
  5%|▌         | 22/431 [00:00<00:09, 44.59it/s][A
  6%|▋         | 27/431 [00:00<00:09, 44.30it/s][A
  7%|▋         | 32/431 [00:00<00:09, 43.91it/s][A
  9%|▊         | 37/431 [00:00<00:08, 43.78it/s][A
 10%|▉         | 42/431 [00:00<00:08, 43.70it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.80it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 43.88it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 43.81it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 43.66it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 43.54it/s][A
 17%|█▋        | 72/431 [00:01<00:08, 43.51it/s][A
 18%|█▊        | 77/431 [00:01<00:08, 43.51it/s][A
 19%|█▉        | 82/431 [00:01<00:08, 43.49it/s][A
 20%|██        | 87/431 [00:01<00:07, 43.62it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 43.70it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 43.74it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 43.72it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 43.64it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 43.46it/s][A
 27%|██▋       | 117/431 [00:02<00:07, 43.43it/s][A
 28%|██▊       | 122/431 [00:02<00:07, 43.49it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 43.51it/s][A
 31%|███       | 132/431 [00:03<00:06, 43.56it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 43.78it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 43.81it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 43.71it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 43.44it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 43.57it/s][A
 38%|███▊      | 162/431 [00:03<00:06, 43.58it/s][A
 39%|███▊      | 167/431 [00:03<00:06, 43.48it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 43.51it/s][A
 41%|████      | 177/431 [00:04<00:05, 43.61it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 43.74it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 43.78it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 43.59it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 43.47it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 43.63it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 43.61it/s][A
 49%|████▉     | 212/431 [00:04<00:05, 43.53it/s][A
 50%|█████     | 217/431 [00:04<00:04, 43.49it/s][A
 52%|█████▏    | 222/431 [00:05<00:04, 43.57it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 43.64it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 43.73it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 43.60it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 43.60it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 43.59it/s][A
 58%|█████▊    | 252/431 [00:05<00:04, 43.50it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 43.53it/s][A
 61%|██████    | 262/431 [00:05<00:03, 43.60it/s][A
 62%|██████▏   | 267/431 [00:06<00:03, 43.62it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 43.70it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 43.69it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 43.53it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 43.52it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 43.53it/s][A
 69%|██████▉   | 297/431 [00:06<00:03, 43.52it/s][A
 70%|███████   | 302/431 [00:06<00:02, 43.57it/s][A
 71%|███████   | 307/431 [00:07<00:02, 43.54it/s][A
 72%|███████▏  | 312/431 [00:07<00:02, 43.66it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 43.70it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 43.66it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 43.59it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 43.59it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.53it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 43.54it/s][A
 81%|████████  | 347/431 [00:07<00:01, 43.58it/s][A
 82%|████████▏ | 352/431 [00:08<00:01, 43.35it/s][A
 83%|████████▎ | 357/431 [00:08<00:01, 43.52it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 43.72it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 43.67it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 43.65it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 43.62it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 43.51it/s][A
 90%|████████▉ | 387/431 [00:08<00:01, 43.54it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 43.62it/s][A
 92%|█████████▏| 397/431 [00:09<00:00, 43.66it/s][A
 93%|█████████▎| 402/431 [00:09<00:00, 43.66it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 43.65it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 43.56it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 43.62it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 43.55it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 43.57it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 43.57it/s][A100%|██████████| 780/780 [05:00<00:00,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:33:53,480 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 02:33:53,499 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:33:55,178 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:33:55,193 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:33:55,204 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 02:33:58,488 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 02:33:58,491 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156 (score: 1.0592540502548218).
                                                 100%|██████████| 780/780 [05:07<00:00,  3.42it/s]100%|██████████| 780/780 [05:07<00:00,  2.53it/s]
[INFO|trainer.py:1894] 2023-08-29 02:34:00,337 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 02:34:00,363 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:34:02,131 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:34:02,151 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:34:02,165 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:34:02,352 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:02,352 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:02,352 >>   train_loss               =      0.384
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:02,352 >>   train_runtime            = 0:05:07.72
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:02,352 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:02,352 >>   train_samples_per_second =    162.483
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:02,352 >>   train_steps_per_second   =      2.535
{'eval_loss': 1.1063368320465088, 'eval_runtime': 9.884, 'eval_samples_per_second': 348.646, 'eval_steps_per_second': 43.606, 'epoch': 5.0}
{'train_runtime': 307.7245, 'train_samples_per_second': 162.483, 'train_steps_per_second': 2.535, 'train_loss': 0.3840260236691206, 'epoch': 5.0}
08/29/2023 02:34:02 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 02:34:02,396 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:34:02,396 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 02:34:02,396 >>   Batch size = 8
  0%|          | 0/431 [00:00<?, ?it/s]  1%|▏         | 6/431 [00:00<00:07, 55.74it/s]  3%|▎         | 12/431 [00:00<00:08, 48.25it/s]  4%|▍         | 17/431 [00:00<00:08, 46.36it/s]  5%|▌         | 22/431 [00:00<00:08, 45.72it/s]  6%|▋         | 27/431 [00:00<00:08, 45.18it/s]  7%|▋         | 32/431 [00:00<00:08, 44.81it/s]  9%|▊         | 37/431 [00:00<00:08, 44.59it/s] 10%|▉         | 42/431 [00:00<00:08, 43.98it/s] 11%|█         | 47/431 [00:01<00:08, 43.35it/s] 12%|█▏        | 52/431 [00:01<00:08, 43.18it/s] 13%|█▎        | 57/431 [00:01<00:08, 43.35it/s] 14%|█▍        | 62/431 [00:01<00:08, 43.56it/s] 16%|█▌        | 67/431 [00:01<00:08, 43.72it/s] 17%|█▋        | 72/431 [00:01<00:08, 43.74it/s] 18%|█▊        | 77/431 [00:01<00:08, 43.79it/s] 19%|█▉        | 82/431 [00:01<00:07, 43.86it/s] 20%|██        | 87/431 [00:01<00:07, 43.48it/s] 21%|██▏       | 92/431 [00:02<00:07, 43.17it/s] 23%|██▎       | 97/431 [00:02<00:07, 43.14it/s] 24%|██▎       | 102/431 [00:02<00:07, 43.40it/s] 25%|██▍       | 107/431 [00:02<00:07, 43.49it/s] 26%|██▌       | 112/431 [00:02<00:07, 43.62it/s] 27%|██▋       | 117/431 [00:02<00:07, 43.71it/s] 28%|██▊       | 122/431 [00:02<00:07, 43.82it/s] 29%|██▉       | 127/431 [00:02<00:06, 43.72it/s] 31%|███       | 132/431 [00:02<00:06, 43.42it/s] 32%|███▏      | 137/431 [00:03<00:06, 43.18it/s] 33%|███▎      | 142/431 [00:03<00:06, 43.25it/s] 34%|███▍      | 147/431 [00:03<00:06, 43.40it/s] 35%|███▌      | 152/431 [00:03<00:06, 43.58it/s] 36%|███▋      | 157/431 [00:03<00:06, 43.56it/s] 38%|███▊      | 162/431 [00:03<00:06, 43.77it/s] 39%|███▊      | 167/431 [00:03<00:06, 43.77it/s] 40%|███▉      | 172/431 [00:03<00:05, 43.50it/s] 41%|████      | 177/431 [00:04<00:05, 43.33it/s] 42%|████▏     | 182/431 [00:04<00:05, 43.15it/s] 43%|████▎     | 187/431 [00:04<00:05, 43.23it/s] 45%|████▍     | 192/431 [00:04<00:05, 43.33it/s] 46%|████▌     | 197/431 [00:04<00:05, 43.62it/s] 47%|████▋     | 202/431 [00:04<00:05, 43.63it/s] 48%|████▊     | 207/431 [00:04<00:05, 43.72it/s] 49%|████▉     | 212/431 [00:04<00:05, 43.66it/s] 50%|█████     | 217/431 [00:04<00:04, 43.53it/s] 52%|█████▏    | 222/431 [00:05<00:04, 43.39it/s] 53%|█████▎    | 227/431 [00:05<00:04, 43.31it/s] 54%|█████▍    | 232/431 [00:05<00:04, 43.33it/s] 55%|█████▍    | 237/431 [00:05<00:04, 43.36it/s] 56%|█████▌    | 242/431 [00:05<00:04, 43.63it/s] 57%|█████▋    | 247/431 [00:05<00:04, 43.75it/s] 58%|█████▊    | 252/431 [00:05<00:04, 43.70it/s] 60%|█████▉    | 257/431 [00:05<00:03, 43.50it/s] 61%|██████    | 262/431 [00:05<00:03, 43.42it/s] 62%|██████▏   | 267/431 [00:06<00:03, 43.37it/s] 63%|██████▎   | 272/431 [00:06<00:03, 43.34it/s] 64%|██████▍   | 277/431 [00:06<00:03, 43.32it/s] 65%|██████▌   | 282/431 [00:06<00:03, 43.33it/s] 67%|██████▋   | 287/431 [00:06<00:03, 43.49it/s] 68%|██████▊   | 292/431 [00:06<00:03, 43.65it/s] 69%|██████▉   | 297/431 [00:06<00:03, 43.67it/s] 70%|███████   | 302/431 [00:06<00:02, 43.53it/s] 71%|███████   | 307/431 [00:07<00:02, 43.41it/s] 72%|███████▏  | 312/431 [00:07<00:02, 43.38it/s] 74%|███████▎  | 317/431 [00:07<00:02, 43.41it/s] 75%|███████▍  | 322/431 [00:07<00:02, 43.21it/s] 76%|███████▌  | 327/431 [00:07<00:02, 43.45it/s] 77%|███████▋  | 332/431 [00:07<00:02, 43.54it/s] 78%|███████▊  | 337/431 [00:07<00:02, 43.60it/s] 79%|███████▉  | 342/431 [00:07<00:02, 43.65it/s] 81%|████████  | 347/431 [00:07<00:01, 43.49it/s] 82%|████████▏ | 352/431 [00:08<00:01, 43.50it/s] 83%|████████▎ | 357/431 [00:08<00:01, 43.41it/s] 84%|████████▍ | 362/431 [00:08<00:01, 43.42it/s] 85%|████████▌ | 367/431 [00:08<00:01, 43.36it/s] 86%|████████▋ | 372/431 [00:08<00:01, 43.35it/s] 87%|████████▋ | 377/431 [00:08<00:01, 43.53it/s] 89%|████████▊ | 382/431 [00:08<00:01, 43.65it/s] 90%|████████▉ | 387/431 [00:08<00:01, 43.57it/s] 91%|█████████ | 392/431 [00:08<00:00, 43.50it/s] 92%|█████████▏| 397/431 [00:09<00:00, 43.44it/s] 93%|█████████▎| 402/431 [00:09<00:00, 43.45it/s] 94%|█████████▍| 407/431 [00:09<00:00, 43.44it/s] 96%|█████████▌| 412/431 [00:09<00:00, 43.33it/s] 97%|█████████▋| 417/431 [00:09<00:00, 43.44it/s] 98%|█████████▊| 422/431 [00:09<00:00, 43.42it/s] 99%|█████████▉| 427/431 [00:09<00:00, 43.64it/s]100%|██████████| 431/431 [00:09<00:00, 43.61it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:34:12,296 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:12,296 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:12,296 >>   eval_loss               =     1.0593
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:12,296 >>   eval_runtime            = 0:00:09.90
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:12,296 >>   eval_samples            =       3446
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:12,296 >>   eval_samples_per_second =    348.075
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:12,296 >>   eval_steps_per_second   =     43.535
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:34:12,296 >>   perplexity              =     2.8842
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:34:17,642 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:34:17,647 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:34:17,648 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:34:17,648 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:34:17,648 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:34:17,944 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:34:17,945 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:34:18,613 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:34:19,634 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:34:19,634 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:34:21,318 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:34:21,324 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:34:21,324 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:34:21,324 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:34:21,324 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:34:21,648 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:34:21,649 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:34:22,313 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:34:22,472 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:34:22,472 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:06,  1.57it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:08,  1.52it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:10,  1.51it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.59it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.51it/s]Extractor Predicting: 27it [00:17,  1.49it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.57it/s]Extractor Predicting: 37it [00:23,  1.59it/s]Extractor Predicting: 38it [00:24,  1.56it/s]Extractor Predicting: 39it [00:25,  1.61it/s]Extractor Predicting: 40it [00:25,  1.61it/s]Extractor Predicting: 41it [00:26,  1.59it/s]Extractor Predicting: 42it [00:26,  1.64it/s]Extractor Predicting: 43it [00:27,  1.60it/s]Extractor Predicting: 44it [00:28,  1.60it/s]Extractor Predicting: 45it [00:28,  1.61it/s]Extractor Predicting: 46it [00:29,  1.62it/s]Extractor Predicting: 47it [00:30,  1.67it/s]Extractor Predicting: 48it [00:30,  1.68it/s]Extractor Predicting: 49it [00:31,  1.69it/s]Extractor Predicting: 50it [00:31,  1.66it/s]Extractor Predicting: 51it [00:32,  1.63it/s]Extractor Predicting: 52it [00:33,  1.62it/s]Extractor Predicting: 53it [00:33,  1.59it/s]Extractor Predicting: 54it [00:34,  1.63it/s]Extractor Predicting: 55it [00:34,  1.63it/s]Extractor Predicting: 56it [00:35,  1.65it/s]Extractor Predicting: 57it [00:36,  1.68it/s]Extractor Predicting: 58it [00:36,  1.64it/s]Extractor Predicting: 59it [00:37,  1.69it/s]Extractor Predicting: 60it [00:37,  1.63it/s]Extractor Predicting: 61it [00:38,  1.65it/s]Extractor Predicting: 62it [00:39,  1.71it/s]Extractor Predicting: 63it [00:39,  1.71it/s]Extractor Predicting: 64it [00:40,  1.75it/s]Extractor Predicting: 65it [00:40,  1.76it/s]Extractor Predicting: 66it [00:41,  1.77it/s]Extractor Predicting: 67it [00:41,  1.75it/s]Extractor Predicting: 68it [00:42,  1.75it/s]Extractor Predicting: 69it [00:42,  1.78it/s]Extractor Predicting: 70it [00:43,  1.75it/s]Extractor Predicting: 71it [00:44,  1.72it/s]Extractor Predicting: 72it [00:44,  1.75it/s]Extractor Predicting: 73it [00:45,  1.69it/s]Extractor Predicting: 74it [00:45,  1.69it/s]Extractor Predicting: 75it [00:46,  1.73it/s]Extractor Predicting: 76it [00:47,  1.74it/s]Extractor Predicting: 77it [00:47,  1.75it/s]Extractor Predicting: 78it [00:48,  1.76it/s]Extractor Predicting: 79it [00:48,  1.75it/s]Extractor Predicting: 80it [00:49,  1.73it/s]Extractor Predicting: 81it [00:50,  1.69it/s]Extractor Predicting: 82it [00:50,  1.66it/s]Extractor Predicting: 83it [00:51,  1.69it/s]Extractor Predicting: 84it [00:51,  1.69it/s]Extractor Predicting: 85it [00:52,  1.63it/s]Extractor Predicting: 86it [00:53,  1.62it/s]Extractor Predicting: 87it [00:53,  1.63it/s]Extractor Predicting: 88it [00:54,  1.60it/s]Extractor Predicting: 89it [00:54,  1.60it/s]Extractor Predicting: 90it [00:55,  1.59it/s]Extractor Predicting: 91it [00:56,  1.60it/s]Extractor Predicting: 92it [00:56,  1.59it/s]Extractor Predicting: 93it [00:57,  1.58it/s]Extractor Predicting: 94it [00:58,  1.60it/s]Extractor Predicting: 95it [00:58,  1.57it/s]Extractor Predicting: 96it [00:59,  1.59it/s]Extractor Predicting: 97it [01:00,  1.58it/s]Extractor Predicting: 98it [01:00,  1.59it/s]Extractor Predicting: 99it [01:01,  1.60it/s]Extractor Predicting: 100it [01:01,  1.62it/s]Extractor Predicting: 101it [01:02,  1.61it/s]Extractor Predicting: 102it [01:03,  1.62it/s]Extractor Predicting: 103it [01:03,  1.62it/s]Extractor Predicting: 104it [01:04,  1.65it/s]Extractor Predicting: 105it [01:04,  1.66it/s]Extractor Predicting: 106it [01:05,  1.64it/s]Extractor Predicting: 107it [01:06,  1.63it/s]Extractor Predicting: 108it [01:06,  1.62it/s]Extractor Predicting: 109it [01:07,  1.61it/s]Extractor Predicting: 110it [01:08,  1.60it/s]Extractor Predicting: 111it [01:08,  1.59it/s]Extractor Predicting: 112it [01:09,  1.60it/s]Extractor Predicting: 113it [01:09,  1.61it/s]Extractor Predicting: 114it [01:10,  1.62it/s]Extractor Predicting: 115it [01:11,  1.61it/s]Extractor Predicting: 116it [01:11,  1.64it/s]Extractor Predicting: 117it [01:12,  1.64it/s]Extractor Predicting: 118it [01:12,  1.62it/s]Extractor Predicting: 119it [01:13,  1.63it/s]Extractor Predicting: 120it [01:14,  1.48it/s]Extractor Predicting: 121it [01:15,  1.51it/s]Extractor Predicting: 122it [01:15,  1.52it/s]Extractor Predicting: 123it [01:16,  1.54it/s]Extractor Predicting: 124it [01:16,  1.53it/s]Extractor Predicting: 125it [01:17,  1.57it/s]Extractor Predicting: 126it [01:18,  1.57it/s]Extractor Predicting: 127it [01:18,  1.56it/s]Extractor Predicting: 128it [01:19,  1.54it/s]Extractor Predicting: 129it [01:20,  1.57it/s]Extractor Predicting: 130it [01:20,  1.60it/s]Extractor Predicting: 131it [01:21,  1.56it/s]Extractor Predicting: 132it [01:21,  1.59it/s]Extractor Predicting: 133it [01:22,  1.61it/s]Extractor Predicting: 134it [01:23,  1.60it/s]Extractor Predicting: 135it [01:23,  1.58it/s]Extractor Predicting: 136it [01:24,  1.59it/s]Extractor Predicting: 137it [01:25,  1.61it/s]Extractor Predicting: 138it [01:25,  1.62it/s]Extractor Predicting: 139it [01:26,  1.63it/s]Extractor Predicting: 140it [01:26,  1.63it/s]Extractor Predicting: 141it [01:27,  1.67it/s]Extractor Predicting: 141it [01:27,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:59,075 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:59,082 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:59,082 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:59,082 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:59,082 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:35:59,680 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:35:59,681 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:36:00,249 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:36:01,262 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:36:01,262 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:04,274 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:04,279 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:04,279 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:04,279 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:04,279 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:36:04,914 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:36:04,915 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:36:05,471 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:36:05,629 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:36:05,630 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.26609442060085836,
  "recall": 0.10795124782356355,
  "score": 0.1535920726672172,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:05,  1.73it/s]Extractor Predicting: 11it [00:06,  1.69it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.54it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:10,  1.61it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:13,  1.63it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.62it/s]Extractor Predicting: 28it [00:17,  1.64it/s]Extractor Predicting: 29it [00:17,  1.60it/s]Extractor Predicting: 30it [00:18,  1.56it/s]Extractor Predicting: 31it [00:19,  1.56it/s]Extractor Predicting: 32it [00:19,  1.55it/s]Extractor Predicting: 33it [00:20,  1.56it/s]Extractor Predicting: 34it [00:20,  1.57it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:23,  1.64it/s]Extractor Predicting: 39it [00:23,  1.63it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.63it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.61it/s]Extractor Predicting: 44it [00:27,  1.61it/s]Extractor Predicting: 45it [00:27,  1.58it/s]Extractor Predicting: 46it [00:28,  1.61it/s]Extractor Predicting: 47it [00:28,  1.60it/s]Extractor Predicting: 48it [00:29,  1.60it/s]Extractor Predicting: 49it [00:30,  1.59it/s]Extractor Predicting: 50it [00:30,  1.58it/s]Extractor Predicting: 51it [00:31,  1.58it/s]Extractor Predicting: 52it [00:32,  1.59it/s]Extractor Predicting: 53it [00:32,  1.59it/s]Extractor Predicting: 54it [00:33,  1.61it/s]Extractor Predicting: 55it [00:34,  1.57it/s]Extractor Predicting: 56it [00:34,  1.58it/s]Extractor Predicting: 57it [00:35,  1.62it/s]Extractor Predicting: 58it [00:35,  1.64it/s]Extractor Predicting: 59it [00:36,  1.65it/s]Extractor Predicting: 60it [00:37,  1.65it/s]Extractor Predicting: 61it [00:37,  1.65it/s]Extractor Predicting: 62it [00:38,  1.65it/s]Extractor Predicting: 63it [00:38,  1.62it/s]Extractor Predicting: 64it [00:39,  1.63it/s]Extractor Predicting: 65it [00:40,  1.61it/s]Extractor Predicting: 66it [00:40,  1.59it/s]Extractor Predicting: 67it [00:41,  1.63it/s]Extractor Predicting: 68it [00:41,  1.65it/s]Extractor Predicting: 69it [00:42,  1.67it/s]Extractor Predicting: 70it [00:43,  1.66it/s]Extractor Predicting: 71it [00:43,  1.64it/s]Extractor Predicting: 72it [00:44,  1.65it/s]Extractor Predicting: 73it [00:45,  1.60it/s]Extractor Predicting: 74it [00:45,  1.61it/s]Extractor Predicting: 75it [00:46,  1.62it/s]Extractor Predicting: 76it [00:46,  1.63it/s]Extractor Predicting: 77it [00:47,  1.60it/s]Extractor Predicting: 78it [00:48,  1.59it/s]Extractor Predicting: 79it [00:48,  1.61it/s]Extractor Predicting: 80it [00:49,  1.60it/s]Extractor Predicting: 81it [00:49,  1.60it/s]Extractor Predicting: 82it [00:50,  1.60it/s]Extractor Predicting: 83it [00:51,  1.58it/s]Extractor Predicting: 84it [00:51,  1.59it/s]Extractor Predicting: 85it [00:52,  1.60it/s]Extractor Predicting: 86it [00:53,  1.59it/s]Extractor Predicting: 87it [00:53,  1.59it/s]Extractor Predicting: 88it [00:54,  1.60it/s]Extractor Predicting: 89it [00:55,  1.58it/s]Extractor Predicting: 90it [00:55,  1.58it/s]Extractor Predicting: 91it [00:56,  1.59it/s]Extractor Predicting: 92it [00:56,  1.59it/s]Extractor Predicting: 93it [00:57,  1.58it/s]Extractor Predicting: 94it [00:58,  1.55it/s]Extractor Predicting: 95it [00:58,  1.54it/s]Extractor Predicting: 96it [00:59,  1.56it/s]Extractor Predicting: 97it [01:00,  1.56it/s]Extractor Predicting: 98it [01:00,  1.58it/s]Extractor Predicting: 99it [01:01,  1.56it/s]Extractor Predicting: 100it [01:02,  1.56it/s]Extractor Predicting: 101it [01:02,  1.41it/s]Extractor Predicting: 102it [01:03,  1.45it/s]Extractor Predicting: 103it [01:04,  1.47it/s]Extractor Predicting: 104it [01:04,  1.49it/s]Extractor Predicting: 105it [01:05,  1.51it/s]Extractor Predicting: 106it [01:06,  1.53it/s]Extractor Predicting: 107it [01:06,  1.50it/s]Extractor Predicting: 108it [01:07,  1.52it/s]Extractor Predicting: 109it [01:08,  1.53it/s]Extractor Predicting: 110it [01:08,  1.52it/s]Extractor Predicting: 111it [01:09,  1.52it/s]Extractor Predicting: 112it [01:10,  1.54it/s]Extractor Predicting: 113it [01:10,  1.57it/s]Extractor Predicting: 114it [01:11,  1.60it/s]Extractor Predicting: 115it [01:11,  1.57it/s]Extractor Predicting: 116it [01:12,  1.59it/s]Extractor Predicting: 117it [01:13,  1.63it/s]Extractor Predicting: 118it [01:13,  1.61it/s]Extractor Predicting: 119it [01:14,  1.62it/s]Extractor Predicting: 120it [01:15,  1.62it/s]Extractor Predicting: 121it [01:15,  1.63it/s]Extractor Predicting: 122it [01:16,  1.67it/s]Extractor Predicting: 123it [01:16,  1.66it/s]Extractor Predicting: 124it [01:17,  1.66it/s]Extractor Predicting: 125it [01:18,  1.64it/s]Extractor Predicting: 126it [01:18,  1.62it/s]Extractor Predicting: 127it [01:19,  1.63it/s]Extractor Predicting: 128it [01:19,  1.66it/s]Extractor Predicting: 129it [01:20,  1.61it/s]Extractor Predicting: 130it [01:21,  1.62it/s]Extractor Predicting: 131it [01:21,  1.60it/s]Extractor Predicting: 132it [01:22,  1.61it/s]Extractor Predicting: 133it [01:23,  1.61it/s]Extractor Predicting: 134it [01:23,  1.61it/s]Extractor Predicting: 135it [01:24,  1.62it/s]Extractor Predicting: 136it [01:24,  1.63it/s]Extractor Predicting: 137it [01:25,  1.63it/s]Extractor Predicting: 138it [01:26,  1.62it/s]Extractor Predicting: 139it [01:26,  1.66it/s]Extractor Predicting: 140it [01:27,  1.64it/s]Extractor Predicting: 141it [01:27,  1.63it/s]Extractor Predicting: 142it [01:28,  1.65it/s]Extractor Predicting: 143it [01:29,  1.64it/s]Extractor Predicting: 144it [01:29,  1.61it/s]Extractor Predicting: 145it [01:30,  1.63it/s]Extractor Predicting: 146it [01:30,  1.62it/s]Extractor Predicting: 147it [01:31,  1.60it/s]Extractor Predicting: 148it [01:32,  1.59it/s]Extractor Predicting: 149it [01:32,  1.59it/s]Extractor Predicting: 150it [01:33,  1.61it/s]Extractor Predicting: 151it [01:34,  1.64it/s]Extractor Predicting: 152it [01:34,  1.65it/s]Extractor Predicting: 153it [01:35,  1.64it/s]Extractor Predicting: 154it [01:35,  1.61it/s]Extractor Predicting: 155it [01:36,  1.60it/s]Extractor Predicting: 156it [01:37,  1.59it/s]Extractor Predicting: 157it [01:37,  1.59it/s]Extractor Predicting: 158it [01:38,  1.58it/s]Extractor Predicting: 159it [01:39,  1.58it/s]Extractor Predicting: 160it [01:39,  1.59it/s]Extractor Predicting: 161it [01:40,  1.61it/s]Extractor Predicting: 162it [01:40,  1.63it/s]Extractor Predicting: 163it [01:41,  1.63it/s]Extractor Predicting: 164it [01:42,  1.63it/s]Extractor Predicting: 165it [01:42,  1.61it/s]Extractor Predicting: 166it [01:43,  1.62it/s]Extractor Predicting: 167it [01:43,  1.64it/s]Extractor Predicting: 168it [01:44,  1.62it/s]Extractor Predicting: 169it [01:45,  1.61it/s]Extractor Predicting: 170it [01:45,  1.60it/s]Extractor Predicting: 171it [01:46,  1.57it/s]Extractor Predicting: 172it [01:47,  1.55it/s]Extractor Predicting: 173it [01:47,  1.55it/s]Extractor Predicting: 174it [01:48,  1.52it/s]Extractor Predicting: 175it [01:49,  1.49it/s]Extractor Predicting: 176it [01:49,  1.51it/s]Extractor Predicting: 177it [01:50,  1.54it/s]Extractor Predicting: 178it [01:51,  1.56it/s]Extractor Predicting: 179it [01:51,  1.58it/s]Extractor Predicting: 180it [01:52,  1.57it/s]Extractor Predicting: 181it [01:53,  1.59it/s]Extractor Predicting: 182it [01:53,  1.60it/s]Extractor Predicting: 183it [01:54,  1.61it/s]Extractor Predicting: 184it [01:54,  1.63it/s]Extractor Predicting: 185it [01:55,  1.64it/s]Extractor Predicting: 186it [01:56,  1.67it/s]Extractor Predicting: 187it [01:56,  1.66it/s]Extractor Predicting: 188it [01:57,  1.66it/s]Extractor Predicting: 189it [01:57,  1.66it/s]Extractor Predicting: 190it [01:58,  1.66it/s]Extractor Predicting: 191it [01:59,  1.62it/s]Extractor Predicting: 192it [01:59,  1.59it/s]Extractor Predicting: 193it [02:00,  1.61it/s]Extractor Predicting: 194it [02:00,  1.65it/s]Extractor Predicting: 195it [02:01,  1.65it/s]Extractor Predicting: 196it [02:02,  1.66it/s]Extractor Predicting: 197it [02:02,  1.67it/s]Extractor Predicting: 198it [02:03,  1.47it/s]Extractor Predicting: 199it [02:04,  1.51it/s]Extractor Predicting: 200it [02:04,  1.53it/s]Extractor Predicting: 201it [02:05,  1.55it/s]Extractor Predicting: 202it [02:06,  1.59it/s]Extractor Predicting: 203it [02:06,  1.63it/s]Extractor Predicting: 204it [02:07,  1.65it/s]Extractor Predicting: 205it [02:07,  1.65it/s]Extractor Predicting: 206it [02:08,  1.64it/s]Extractor Predicting: 207it [02:09,  1.60it/s]Extractor Predicting: 208it [02:09,  1.61it/s]Extractor Predicting: 209it [02:10,  1.62it/s]Extractor Predicting: 210it [02:10,  1.61it/s]Extractor Predicting: 211it [02:11,  1.63it/s]Extractor Predicting: 212it [02:12,  1.61it/s]Extractor Predicting: 213it [02:12,  1.65it/s]Extractor Predicting: 214it [02:13,  1.63it/s]Extractor Predicting: 215it [02:13,  1.67it/s]Extractor Predicting: 216it [02:14,  1.65it/s]Extractor Predicting: 217it [02:15,  1.58it/s]Extractor Predicting: 218it [02:15,  1.60it/s]Extractor Predicting: 219it [02:16,  1.64it/s]Extractor Predicting: 220it [02:17,  1.65it/s]Extractor Predicting: 221it [02:17,  1.63it/s]Extractor Predicting: 222it [02:18,  1.65it/s]Extractor Predicting: 223it [02:18,  1.66it/s]Extractor Predicting: 224it [02:19,  1.66it/s]Extractor Predicting: 225it [02:20,  1.64it/s]Extractor Predicting: 226it [02:20,  1.59it/s]Extractor Predicting: 227it [02:21,  1.59it/s]Extractor Predicting: 228it [02:22,  1.55it/s]Extractor Predicting: 229it [02:22,  1.56it/s]Extractor Predicting: 230it [02:23,  1.58it/s]Extractor Predicting: 231it [02:23,  1.56it/s]Extractor Predicting: 232it [02:24,  1.57it/s]Extractor Predicting: 233it [02:25,  1.58it/s]Extractor Predicting: 234it [02:25,  1.58it/s]Extractor Predicting: 235it [02:26,  1.55it/s]Extractor Predicting: 236it [02:27,  1.58it/s]Extractor Predicting: 237it [02:27,  1.63it/s]Extractor Predicting: 238it [02:28,  1.59it/s]Extractor Predicting: 239it [02:29,  1.56it/s]Extractor Predicting: 240it [02:29,  1.54it/s]Extractor Predicting: 241it [02:30,  1.55it/s]Extractor Predicting: 242it [02:30,  1.58it/s]Extractor Predicting: 243it [02:31,  1.59it/s]Extractor Predicting: 244it [02:32,  1.55it/s]Extractor Predicting: 245it [02:32,  1.58it/s]Extractor Predicting: 246it [02:33,  1.59it/s]Extractor Predicting: 247it [02:34,  1.61it/s]Extractor Predicting: 248it [02:34,  1.54it/s]Extractor Predicting: 249it [02:35,  1.56it/s]Extractor Predicting: 250it [02:36,  1.57it/s]Extractor Predicting: 251it [02:36,  1.54it/s]Extractor Predicting: 252it [02:37,  1.54it/s]Extractor Predicting: 253it [02:37,  1.54it/s]Extractor Predicting: 254it [02:38,  1.58it/s]Extractor Predicting: 255it [02:39,  1.59it/s]Extractor Predicting: 256it [02:39,  1.63it/s]Extractor Predicting: 257it [02:40,  1.66it/s]Extractor Predicting: 258it [02:40,  1.63it/s]Extractor Predicting: 259it [02:41,  1.62it/s]Extractor Predicting: 260it [02:42,  1.60it/s]Extractor Predicting: 261it [02:42,  1.60it/s]Extractor Predicting: 262it [02:43,  1.60it/s]Extractor Predicting: 263it [02:44,  1.54it/s]Extractor Predicting: 264it [02:44,  1.55it/s]Extractor Predicting: 265it [02:45,  1.58it/s]Extractor Predicting: 266it [02:46,  1.60it/s]Extractor Predicting: 267it [02:46,  1.60it/s]Extractor Predicting: 268it [02:47,  1.62it/s]Extractor Predicting: 269it [02:47,  1.66it/s]Extractor Predicting: 270it [02:48,  1.68it/s]Extractor Predicting: 271it [02:49,  1.64it/s]Extractor Predicting: 272it [02:49,  1.66it/s]Extractor Predicting: 273it [02:50,  1.64it/s]Extractor Predicting: 274it [02:50,  1.63it/s]Extractor Predicting: 275it [02:51,  1.65it/s]Extractor Predicting: 276it [02:52,  1.64it/s]Extractor Predicting: 277it [02:52,  1.62it/s]Extractor Predicting: 278it [02:53,  1.65it/s]Extractor Predicting: 279it [02:53,  1.61it/s]Extractor Predicting: 280it [02:54,  1.61it/s]Extractor Predicting: 281it [02:55,  1.61it/s]Extractor Predicting: 282it [02:55,  1.63it/s]Extractor Predicting: 283it [02:56,  1.63it/s]Extractor Predicting: 284it [02:57,  1.60it/s]Extractor Predicting: 285it [02:57,  1.60it/s]Extractor Predicting: 286it [02:58,  1.62it/s]Extractor Predicting: 287it [02:58,  1.64it/s]Extractor Predicting: 288it [02:59,  1.63it/s]Extractor Predicting: 289it [03:00,  1.58it/s]Extractor Predicting: 290it [03:00,  1.64it/s]Extractor Predicting: 291it [03:01,  1.62it/s]Extractor Predicting: 292it [03:02,  1.61it/s]Extractor Predicting: 293it [03:02,  1.58it/s]Extractor Predicting: 294it [03:03,  1.55it/s]Extractor Predicting: 295it [03:03,  1.59it/s]Extractor Predicting: 296it [03:04,  1.57it/s]Extractor Predicting: 297it [03:05,  1.60it/s]Extractor Predicting: 298it [03:05,  1.55it/s]Extractor Predicting: 299it [03:06,  1.56it/s]Extractor Predicting: 300it [03:07,  1.57it/s]Extractor Predicting: 301it [03:07,  1.58it/s]Extractor Predicting: 302it [03:08,  1.57it/s]Extractor Predicting: 303it [03:09,  1.56it/s]Extractor Predicting: 304it [03:09,  1.53it/s]Extractor Predicting: 305it [03:10,  1.55it/s]Extractor Predicting: 306it [03:11,  1.53it/s]Extractor Predicting: 307it [03:11,  1.53it/s]Extractor Predicting: 308it [03:12,  1.55it/s]Extractor Predicting: 309it [03:12,  1.55it/s]Extractor Predicting: 310it [03:13,  1.52it/s]Extractor Predicting: 311it [03:14,  1.51it/s]Extractor Predicting: 312it [03:14,  1.52it/s]Extractor Predicting: 313it [03:15,  1.53it/s]Extractor Predicting: 314it [03:16,  1.51it/s]Extractor Predicting: 315it [03:16,  1.56it/s]Extractor Predicting: 316it [03:17,  1.55it/s]Extractor Predicting: 317it [03:18,  1.53it/s]Extractor Predicting: 318it [03:18,  1.54it/s]Extractor Predicting: 319it [03:19,  1.54it/s]Extractor Predicting: 320it [03:20,  1.63it/s]Extractor Predicting: 321it [03:20,  1.67it/s]Extractor Predicting: 322it [03:21,  1.75it/s]Extractor Predicting: 323it [03:21,  1.56it/s]Extractor Predicting: 324it [03:22,  1.62it/s]Extractor Predicting: 325it [03:23,  1.68it/s]Extractor Predicting: 326it [03:23,  1.71it/s]Extractor Predicting: 327it [03:24,  1.72it/s]Extractor Predicting: 328it [03:24,  1.76it/s]Extractor Predicting: 329it [03:25,  1.80it/s]Extractor Predicting: 330it [03:25,  1.81it/s]Extractor Predicting: 331it [03:26,  1.86it/s]Extractor Predicting: 332it [03:26,  1.82it/s]Extractor Predicting: 333it [03:27,  1.86it/s]Extractor Predicting: 334it [03:27,  1.86it/s]Extractor Predicting: 335it [03:28,  1.85it/s]Extractor Predicting: 336it [03:29,  1.83it/s]Extractor Predicting: 337it [03:29,  1.85it/s]Extractor Predicting: 338it [03:30,  1.81it/s]Extractor Predicting: 339it [03:30,  1.79it/s]Extractor Predicting: 340it [03:31,  1.86it/s]Extractor Predicting: 341it [03:31,  1.83it/s]Extractor Predicting: 342it [03:32,  1.84it/s]Extractor Predicting: 343it [03:32,  1.84it/s]Extractor Predicting: 344it [03:33,  1.81it/s]Extractor Predicting: 345it [03:33,  1.83it/s]Extractor Predicting: 346it [03:34,  1.83it/s]Extractor Predicting: 347it [03:35,  1.79it/s]Extractor Predicting: 348it [03:35,  1.75it/s]Extractor Predicting: 349it [03:36,  1.70it/s]Extractor Predicting: 350it [03:36,  1.66it/s]Extractor Predicting: 351it [03:37,  1.64it/s]Extractor Predicting: 352it [03:38,  1.63it/s]Extractor Predicting: 353it [03:38,  1.64it/s]Extractor Predicting: 354it [03:39,  1.62it/s]Extractor Predicting: 355it [03:40,  1.61it/s]Extractor Predicting: 356it [03:40,  1.59it/s]Extractor Predicting: 357it [03:41,  1.56it/s]Extractor Predicting: 358it [03:41,  1.58it/s]Extractor Predicting: 359it [03:42,  1.59it/s]Extractor Predicting: 360it [03:43,  1.61it/s]Extractor Predicting: 361it [03:43,  1.59it/s]Extractor Predicting: 362it [03:44,  1.61it/s]Extractor Predicting: 363it [03:45,  1.59it/s]Extractor Predicting: 364it [03:45,  1.58it/s]Extractor Predicting: 365it [03:46,  1.58it/s]Extractor Predicting: 366it [03:47,  1.58it/s]Extractor Predicting: 367it [03:47,  1.57it/s]Extractor Predicting: 368it [03:48,  1.59it/s]Extractor Predicting: 369it [03:48,  1.58it/s]Extractor Predicting: 370it [03:49,  1.57it/s]Extractor Predicting: 371it [03:50,  1.58it/s]Extractor Predicting: 372it [03:50,  1.59it/s]Extractor Predicting: 373it [03:51,  1.57it/s]Extractor Predicting: 374it [03:52,  1.59it/s]Extractor Predicting: 375it [03:52,  1.60it/s]Extractor Predicting: 376it [03:53,  1.58it/s]Extractor Predicting: 377it [03:53,  1.60it/s]Extractor Predicting: 378it [03:54,  1.57it/s]Extractor Predicting: 379it [03:55,  1.56it/s]Extractor Predicting: 380it [03:55,  1.56it/s]Extractor Predicting: 381it [03:56,  1.54it/s]Extractor Predicting: 382it [03:57,  1.54it/s]Extractor Predicting: 383it [03:57,  1.54it/s]Extractor Predicting: 384it [03:58,  1.54it/s]Extractor Predicting: 385it [03:59,  1.53it/s]Extractor Predicting: 386it [03:59,  1.55it/s]Extractor Predicting: 387it [04:00,  1.53it/s]Extractor Predicting: 388it [04:01,  1.53it/s]Extractor Predicting: 389it [04:01,  1.57it/s]Extractor Predicting: 390it [04:02,  1.57it/s]Extractor Predicting: 391it [04:02,  1.57it/s]Extractor Predicting: 392it [04:03,  1.54it/s]Extractor Predicting: 393it [04:04,  1.58it/s]Extractor Predicting: 394it [04:04,  1.55it/s]Extractor Predicting: 395it [04:05,  1.52it/s]Extractor Predicting: 396it [04:06,  1.52it/s]Extractor Predicting: 397it [04:06,  1.50it/s]Extractor Predicting: 398it [04:07,  1.52it/s]Extractor Predicting: 399it [04:08,  1.49it/s]Extractor Predicting: 400it [04:08,  1.50it/s]Extractor Predicting: 401it [04:09,  1.50it/s]Extractor Predicting: 402it [04:10,  1.53it/s]Extractor Predicting: 403it [04:10,  1.53it/s]Extractor Predicting: 404it [04:11,  1.58it/s]Extractor Predicting: 405it [04:12,  1.57it/s]Extractor Predicting: 406it [04:12,  1.61it/s]Extractor Predicting: 407it [04:13,  1.60it/s]Extractor Predicting: 408it [04:14,  1.57it/s]Extractor Predicting: 409it [04:14,  1.61it/s]Extractor Predicting: 410it [04:15,  1.60it/s]Extractor Predicting: 411it [04:15,  1.63it/s]Extractor Predicting: 412it [04:16,  1.60it/s]Extractor Predicting: 413it [04:17,  1.63it/s]Extractor Predicting: 414it [04:17,  1.64it/s]Extractor Predicting: 415it [04:18,  1.63it/s]Extractor Predicting: 416it [04:18,  1.60it/s]Extractor Predicting: 417it [04:19,  1.62it/s]Extractor Predicting: 418it [04:20,  1.61it/s]Extractor Predicting: 419it [04:20,  1.59it/s]Extractor Predicting: 420it [04:21,  1.60it/s]Extractor Predicting: 421it [04:22,  1.60it/s]Extractor Predicting: 422it [04:22,  1.58it/s]Extractor Predicting: 423it [04:23,  1.59it/s]Extractor Predicting: 424it [04:24,  1.55it/s]Extractor Predicting: 425it [04:24,  1.57it/s]Extractor Predicting: 426it [04:25,  1.56it/s]Extractor Predicting: 427it [04:25,  1.58it/s]Extractor Predicting: 428it [04:26,  1.57it/s]Extractor Predicting: 429it [04:27,  1.39it/s]Extractor Predicting: 430it [04:27,  1.75it/s]Extractor Predicting: 430it [04:27,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:41,393 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:41,400 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:41,401 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:41,401 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:41,401 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:40:42,119 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:40:42,120 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:40:42,378 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:40:43,419 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:40:43,419 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:45,995 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:46,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:46,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:46,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:40:46,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:40:46,327 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:40:46,328 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:40:47,001 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:40:47,170 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:40:47,170 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.27383924113829256,
  "recall": 0.10652553893959993,
  "score": 0.15338366890380314,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:02,  2.01it/s]Extractor Predicting: 5it [00:02,  1.73it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4838709677419355,
  "recall": 0.07425742574257425,
  "score": 0.12875536480686695,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_15_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_15_seed_4', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_15_seed_4/generator/model', data_dir='outputs/wrapper/wiki/unseen_15_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:18<06:00, 18.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:32<04:40, 15.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:47<04:25, 15.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:02<04:01, 15.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:16<03:40, 14.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:30<03:21, 14.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:45<03:10, 14.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:00<02:58, 14.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:13<02:38, 14.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:27<02:22, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:43<02:11, 14.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:57<01:56, 14.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:10<01:38, 14.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:24<01:23, 13.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:36<01:07, 13.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:50<00:54, 13.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:05<00:41, 13.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:21<00:29, 14.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:35<00:14, 14.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:50<00:00, 14.44s/it]Generating: 100%|██████████| 20/20 [04:50<00:00, 14.50s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : inception .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8152173913043478, 'errors': {'', "('Sydney Aquatic Centre', 'located on terrain feature', '', 'He was appointed to the Sydney Aquatic Centre in 1987 by the Australian Government and had a staff of about 200 and has lived there ever since .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 364, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 410, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 474, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 519, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 568, 'raw': 768}
{'target': 600, 'success': 588, 'raw': 800}
{'target': 600, 'success': 614, 'raw': 832}
{'prompt': 'Relation : military branch .', 'success_rate': 0.7379807692307693, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in Life he studied at the Lydden School of Art in Jerusalem and at the Royal Academy of Fine Arts in London . Head Entity : King , Tail Entity : Lydden School of Art .\n']
['Relation : occupant . Context : Later in Life he studied at the Lydden School of Art in Jerusalem and at the Royal Academy of Fine Arts in London . Head Entity : King , Tail Entity : Lydden School of Art .\n', 'Relation : occupant . Context : After he had recovered from pneumonia and was offered a job with the Air Force , he returned to his native New York after two months at sea . Head Entity : Air Force , Tail Entity : hispanic .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8138020833333334, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 346, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 466, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('Häcklenberg', 'occupation', '', 'For his part , a friend of Häcklenberg used an oil vat on a bench .')"}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 552, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : applies to jurisdiction .', 'success_rate': 0.7877604166666666, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : family name .', 'success_rate': 0.8522727272727273, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : father .', 'success_rate': 0.8355978260869565, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8247282608695652, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 385, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 492, 'raw': 640}
{'target': 600, 'success': 520, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 600, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.78125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.8664772727272727, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 556, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.7890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('', 'located on astronomical body', 'Earth', 'Located in the Kuiper Belt region of the Milky Way , it is approximately 7 light years ( 25,000 km ) from the Earth and 12 light years ( 40,000 km ) from the Sun .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('The Girl on the Train', 'lyrics by', '', 'The album reached number five on the Billboard Hot Country Singles chart along with the soundtrack of the hit 1967 studio album , The Girl on the Train .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : member of . Context : Later in the year he came in under the tutelage of Thomas L. Bourgeois , Jr. , who had also been given the name Walter , and led the charge , alongside Sir John R. N. Wootton , who would go on to become Minister of Arts . Head Entity : Thomas L. Bourgeois , Jr. , Tail Entity : Thomas Wootton , Jr. .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 408, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 482, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 551, 'raw': 736}
{'target': 600, 'success': 574, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : part of the series .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : place of birth . Context : Later in Life he studied at the Conservatoire de Paris and at other centres at different universities in Paris , Paris , and London . Head Entity : Felix , Tail Entity : Paris .\n']
['Relation : place of birth . Context : Later in Life he studied at the Conservatoire de Paris and at other centres at different universities in Paris , Paris , and London . Head Entity : Felix , Tail Entity : Paris .\n', 'Relation : place of birth . Context : James Sippy ( 1837 1901 ) , ( born Henry James Sippy , Jr. ) was an American chemist chemist and chemist who specialized in the discovery of the atomic bombs and atomic bombs precursor s . Head Entity : 1837 , Tail Entity : New York .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 131, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 199, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 269, 'raw': 384}
{'target': 600, 'success': 285, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 330, 'raw': 480}
{'target': 600, 'success': 355, 'raw': 512}
{'target': 600, 'success': 380, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 423, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 473, 'raw': 672}
{'target': 600, 'success': 500, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 541, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 582, 'raw': 832}
{'target': 600, 'success': 605, 'raw': 864}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.7002314814814815, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : production company .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : use . Context : Later in Life he studied at the Conservatoire de Paris and at other centres at different universities in Paris , Paris , and London . Head Entity : conservatoire , Tail Entity : Paris .\n']
['Relation : use . Context : Later in Life he studied at the Conservatoire de Paris and at other centres at different universities in Paris , Paris , and London . Head Entity : conservatoire , Tail Entity : Paris .\n', 'Relation : use . Context : After he gained the position of Vice Chancellor his elder sister , sister , cousin , and other relatives visited the Royal Danish Embassy in Oslo . Head Entity : Norwegian Embassy , Tail Entity : Oslo .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 466, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : use .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 18868
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18968, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:15, 15.60s/it]Extractor Estimating: 2it [00:17,  7.46s/it]Extractor Estimating: 3it [00:18,  4.60s/it]Extractor Estimating: 4it [00:19,  3.02s/it]Extractor Estimating: 5it [00:19,  2.17s/it]Extractor Estimating: 6it [00:20,  1.63s/it]Extractor Estimating: 7it [00:21,  1.30s/it]Extractor Estimating: 8it [00:21,  1.08s/it]Extractor Estimating: 9it [00:22,  1.06it/s]Extractor Estimating: 10it [00:22,  1.18it/s]Extractor Estimating: 11it [00:23,  1.32it/s]Extractor Estimating: 12it [00:24,  1.44it/s]Extractor Estimating: 13it [00:26,  1.18s/it]Extractor Estimating: 14it [00:26,  1.02s/it]Extractor Estimating: 15it [00:27,  1.12it/s]Extractor Estimating: 16it [00:28,  1.24it/s]Extractor Estimating: 17it [00:28,  1.32it/s]Extractor Estimating: 18it [00:29,  1.39it/s]Extractor Estimating: 19it [00:30,  1.46it/s]Extractor Estimating: 20it [00:30,  1.52it/s]Extractor Estimating: 21it [00:31,  1.55it/s]Extractor Estimating: 22it [00:31,  1.55it/s]Extractor Estimating: 23it [00:32,  1.60it/s]Extractor Estimating: 24it [00:33,  1.58it/s]Extractor Estimating: 25it [00:33,  1.58it/s]Extractor Estimating: 26it [00:34,  1.63it/s]Extractor Estimating: 27it [00:34,  1.68it/s]Extractor Estimating: 28it [00:35,  1.65it/s]Extractor Estimating: 29it [00:36,  1.65it/s]Extractor Estimating: 30it [00:36,  1.69it/s]Extractor Estimating: 31it [00:37,  1.65it/s]Extractor Estimating: 32it [00:37,  1.64it/s]Extractor Estimating: 33it [00:38,  1.68it/s]Extractor Estimating: 34it [00:39,  1.65it/s]Extractor Estimating: 35it [00:39,  1.68it/s]Extractor Estimating: 36it [00:40,  1.72it/s]Extractor Estimating: 37it [00:40,  1.74it/s]Extractor Estimating: 38it [00:41,  1.74it/s]Extractor Estimating: 39it [00:41,  1.72it/s]Extractor Estimating: 40it [00:42,  1.72it/s]Extractor Estimating: 41it [00:43,  1.66it/s]Extractor Estimating: 42it [00:43,  1.67it/s]Extractor Estimating: 43it [00:44,  1.72it/s]Extractor Estimating: 44it [00:44,  1.72it/s]Extractor Estimating: 45it [00:45,  1.80it/s]Extractor Estimating: 46it [00:46,  1.77it/s]Extractor Estimating: 47it [00:46,  1.79it/s]Extractor Estimating: 48it [00:47,  1.72it/s]Extractor Estimating: 49it [00:47,  1.70it/s]Extractor Estimating: 50it [00:48,  1.71it/s]Extractor Estimating: 51it [00:49,  1.66it/s]Extractor Estimating: 52it [00:49,  1.66it/s]Extractor Estimating: 53it [00:50,  1.64it/s]Extractor Estimating: 54it [00:50,  1.64it/s]Extractor Estimating: 55it [00:51,  1.68it/s]Extractor Estimating: 56it [00:52,  1.66it/s]Extractor Estimating: 57it [00:52,  1.68it/s]Extractor Estimating: 58it [00:53,  1.67it/s]Extractor Estimating: 59it [00:53,  1.59it/s]Extractor Estimating: 60it [00:54,  1.68it/s]Extractor Estimating: 61it [00:55,  1.67it/s]Extractor Estimating: 62it [00:55,  1.69it/s]Extractor Estimating: 63it [00:56,  1.72it/s]Extractor Estimating: 64it [00:56,  1.73it/s]Extractor Estimating: 65it [00:57,  1.70it/s]Extractor Estimating: 66it [00:57,  1.68it/s]Extractor Estimating: 67it [00:58,  1.68it/s]Extractor Estimating: 68it [00:59,  1.72it/s]Extractor Estimating: 69it [00:59,  1.66it/s]Extractor Estimating: 70it [01:00,  1.71it/s]Extractor Estimating: 71it [01:00,  1.70it/s]Extractor Estimating: 72it [01:01,  1.69it/s]Extractor Estimating: 73it [01:02,  1.59it/s]Extractor Estimating: 74it [01:02,  1.65it/s]Extractor Estimating: 75it [01:03,  1.52it/s]Extractor Estimating: 76it [01:04,  1.57it/s]Extractor Estimating: 77it [01:04,  1.61it/s]Extractor Estimating: 78it [01:05,  1.63it/s]Extractor Estimating: 79it [01:05,  1.63it/s]Extractor Estimating: 80it [01:06,  1.63it/s]Extractor Estimating: 81it [01:07,  1.59it/s]Extractor Estimating: 82it [01:07,  1.62it/s]Extractor Estimating: 83it [01:08,  1.67it/s]Extractor Estimating: 84it [01:08,  1.66it/s]Extractor Estimating: 85it [01:09,  1.67it/s]Extractor Estimating: 86it [01:10,  1.67it/s]Extractor Estimating: 87it [01:10,  1.64it/s]Extractor Estimating: 88it [01:11,  1.65it/s]Extractor Estimating: 89it [01:11,  1.67it/s]Extractor Estimating: 90it [01:12,  1.63it/s]Extractor Estimating: 91it [01:13,  1.65it/s]Extractor Estimating: 92it [01:13,  1.56it/s]Extractor Estimating: 93it [01:14,  1.60it/s]Extractor Estimating: 94it [01:15,  1.59it/s]Extractor Estimating: 95it [01:15,  1.60it/s]Extractor Estimating: 96it [01:16,  1.63it/s]Extractor Estimating: 97it [01:17,  1.60it/s]Extractor Estimating: 98it [01:17,  1.62it/s]Extractor Estimating: 99it [01:18,  1.66it/s]Extractor Estimating: 100it [01:18,  1.66it/s]Extractor Estimating: 101it [01:19,  1.72it/s]Extractor Estimating: 102it [01:19,  1.69it/s]Extractor Estimating: 103it [01:20,  1.69it/s]Extractor Estimating: 104it [01:21,  1.71it/s]Extractor Estimating: 105it [01:21,  1.73it/s]Extractor Estimating: 106it [01:22,  1.70it/s]Extractor Estimating: 107it [01:22,  1.69it/s]Extractor Estimating: 108it [01:23,  1.68it/s]Extractor Estimating: 109it [01:24,  1.67it/s]Extractor Estimating: 110it [01:24,  1.69it/s]Extractor Estimating: 111it [01:25,  1.74it/s]Extractor Estimating: 112it [01:25,  1.71it/s]Extractor Estimating: 113it [01:26,  1.68it/s]Extractor Estimating: 114it [01:27,  1.69it/s]Extractor Estimating: 115it [01:27,  1.67it/s]Extractor Estimating: 116it [01:28,  1.68it/s]Extractor Estimating: 117it [01:28,  1.65it/s]Extractor Estimating: 118it [01:29,  1.66it/s]Extractor Estimating: 119it [01:30,  1.67it/s]Extractor Estimating: 120it [01:30,  1.65it/s]Extractor Estimating: 121it [01:31,  1.67it/s]Extractor Estimating: 122it [01:31,  1.67it/s]Extractor Estimating: 123it [01:32,  1.60it/s]Extractor Estimating: 124it [01:33,  1.63it/s]Extractor Estimating: 125it [01:33,  1.62it/s]Extractor Estimating: 126it [01:34,  1.66it/s]Extractor Estimating: 127it [01:34,  1.71it/s]Extractor Estimating: 128it [01:35,  1.68it/s]Extractor Estimating: 129it [01:36,  1.70it/s]Extractor Estimating: 130it [01:36,  1.71it/s]Extractor Estimating: 131it [01:37,  1.73it/s]Extractor Estimating: 132it [01:37,  1.64it/s]Extractor Estimating: 133it [01:38,  1.67it/s]Extractor Estimating: 134it [01:39,  1.68it/s]Extractor Estimating: 135it [01:39,  1.67it/s]Extractor Estimating: 136it [01:40,  1.65it/s]Extractor Estimating: 137it [01:40,  1.63it/s]Extractor Estimating: 138it [01:41,  1.65it/s]Extractor Estimating: 139it [01:42,  1.64it/s]Extractor Estimating: 140it [01:42,  1.64it/s]Extractor Estimating: 141it [01:43,  1.67it/s]Extractor Estimating: 142it [01:43,  1.68it/s]Extractor Estimating: 143it [01:44,  1.73it/s]Extractor Estimating: 144it [01:44,  1.74it/s]Extractor Estimating: 145it [01:45,  1.66it/s]Extractor Estimating: 146it [01:46,  1.64it/s]Extractor Estimating: 147it [01:46,  1.64it/s]Extractor Estimating: 148it [01:47,  1.63it/s]Extractor Estimating: 149it [01:48,  1.69it/s]Extractor Estimating: 150it [01:48,  1.69it/s]Extractor Estimating: 151it [01:49,  1.49it/s]Extractor Estimating: 152it [01:50,  1.48it/s]Extractor Estimating: 153it [01:50,  1.49it/s]Extractor Estimating: 154it [01:51,  1.53it/s]Extractor Estimating: 155it [01:52,  1.53it/s]Extractor Estimating: 156it [01:52,  1.50it/s]Extractor Estimating: 157it [01:53,  1.53it/s]Extractor Estimating: 158it [01:54,  1.52it/s]Extractor Estimating: 159it [01:54,  1.52it/s]Extractor Estimating: 160it [01:55,  1.50it/s]Extractor Estimating: 161it [01:56,  1.46it/s]Extractor Estimating: 162it [01:56,  1.40it/s]Extractor Estimating: 163it [01:57,  1.46it/s]Extractor Estimating: 164it [01:58,  1.48it/s]Extractor Estimating: 165it [01:58,  1.47it/s]Extractor Estimating: 166it [01:59,  1.49it/s]Extractor Estimating: 167it [02:00,  1.50it/s]Extractor Estimating: 168it [02:00,  1.50it/s]Extractor Estimating: 169it [02:01,  1.53it/s]Extractor Estimating: 170it [02:02,  1.57it/s]Extractor Estimating: 171it [02:02,  1.60it/s]Extractor Estimating: 172it [02:03,  1.58it/s]Extractor Estimating: 173it [02:04,  1.55it/s]Extractor Estimating: 174it [02:04,  1.57it/s]Extractor Estimating: 175it [02:05,  1.52it/s]Extractor Estimating: 176it [02:06,  1.50it/s]Extractor Estimating: 177it [02:06,  1.56it/s]Extractor Estimating: 178it [02:07,  1.60it/s]Extractor Estimating: 179it [02:07,  1.55it/s]Extractor Estimating: 180it [02:08,  1.53it/s]Extractor Estimating: 181it [02:09,  1.55it/s]Extractor Estimating: 182it [02:09,  1.56it/s]Extractor Estimating: 183it [02:10,  1.52it/s]Extractor Estimating: 184it [02:11,  1.51it/s]Extractor Estimating: 185it [02:11,  1.51it/s]Extractor Estimating: 186it [02:12,  1.55it/s]Extractor Estimating: 187it [02:13,  1.55it/s]Extractor Estimating: 188it [02:13,  1.46it/s]Extractor Estimating: 189it [02:14,  1.48it/s]Extractor Estimating: 190it [02:15,  1.49it/s]Extractor Estimating: 191it [02:15,  1.46it/s]Extractor Estimating: 192it [02:16,  1.46it/s]Extractor Estimating: 193it [02:17,  1.50it/s]Extractor Estimating: 194it [02:17,  1.51it/s]Extractor Estimating: 195it [02:18,  1.48it/s]Extractor Estimating: 196it [02:19,  1.52it/s]Extractor Estimating: 197it [02:19,  1.56it/s]Extractor Estimating: 198it [02:20,  1.59it/s]Extractor Estimating: 199it [02:21,  1.53it/s]Extractor Estimating: 200it [02:21,  1.55it/s]Extractor Estimating: 201it [02:22,  1.52it/s]Extractor Estimating: 202it [02:22,  1.58it/s]Extractor Estimating: 203it [02:23,  1.63it/s]Extractor Estimating: 204it [02:24,  1.61it/s]Extractor Estimating: 205it [02:24,  1.66it/s]Extractor Estimating: 206it [02:25,  1.66it/s]Extractor Estimating: 207it [02:25,  1.66it/s]Extractor Estimating: 208it [02:26,  1.65it/s]Extractor Estimating: 209it [02:27,  1.64it/s]Extractor Estimating: 210it [02:27,  1.66it/s]Extractor Estimating: 211it [02:28,  1.59it/s]Extractor Estimating: 212it [02:29,  1.58it/s]Extractor Estimating: 213it [02:29,  1.60it/s]Extractor Estimating: 214it [02:30,  1.63it/s]Extractor Estimating: 215it [02:30,  1.65it/s]Extractor Estimating: 216it [02:31,  1.70it/s]Extractor Estimating: 217it [02:32,  1.66it/s]Extractor Estimating: 218it [02:32,  1.67it/s]Extractor Estimating: 219it [02:33,  1.65it/s]Extractor Estimating: 220it [02:33,  1.64it/s]Extractor Estimating: 221it [02:34,  1.65it/s]Extractor Estimating: 222it [02:35,  1.63it/s]Extractor Estimating: 223it [02:35,  1.64it/s]Extractor Estimating: 224it [02:36,  1.67it/s]Extractor Estimating: 225it [02:36,  1.67it/s]Extractor Estimating: 226it [02:37,  1.70it/s]Extractor Estimating: 227it [02:38,  1.53it/s]Extractor Estimating: 228it [02:38,  1.58it/s]Extractor Estimating: 229it [02:39,  1.61it/s]Extractor Estimating: 230it [02:40,  1.62it/s]Extractor Estimating: 231it [02:40,  1.60it/s]Extractor Estimating: 232it [02:41,  1.66it/s]Extractor Estimating: 233it [02:41,  1.64it/s]Extractor Estimating: 234it [02:42,  1.64it/s]Extractor Estimating: 235it [02:43,  1.66it/s]Extractor Estimating: 236it [02:43,  1.62it/s]Extractor Estimating: 237it [02:44,  1.61it/s]Extractor Estimating: 238it [02:44,  1.65it/s]Extractor Estimating: 239it [02:45,  1.64it/s]Extractor Estimating: 240it [02:46,  1.69it/s]Extractor Estimating: 241it [02:46,  1.70it/s]Extractor Estimating: 242it [02:47,  1.75it/s]Extractor Estimating: 243it [02:47,  1.78it/s]Extractor Estimating: 244it [02:48,  1.82it/s]Extractor Estimating: 245it [02:48,  1.83it/s]Extractor Estimating: 246it [02:49,  1.75it/s]Extractor Estimating: 247it [02:50,  1.72it/s]Extractor Estimating: 248it [02:50,  1.78it/s]Extractor Estimating: 249it [02:51,  1.73it/s]Extractor Estimating: 250it [02:51,  1.72it/s]Extractor Estimating: 251it [02:52,  1.75it/s]Extractor Estimating: 252it [02:52,  1.72it/s]Extractor Estimating: 253it [02:53,  1.72it/s]Extractor Estimating: 254it [02:54,  1.70it/s]Extractor Estimating: 255it [02:54,  1.68it/s]Extractor Estimating: 256it [02:55,  1.64it/s]Extractor Estimating: 257it [02:55,  1.62it/s]Extractor Estimating: 258it [02:56,  1.69it/s]Extractor Estimating: 259it [02:57,  1.67it/s]Extractor Estimating: 260it [02:57,  1.69it/s]Extractor Estimating: 261it [02:58,  1.70it/s]Extractor Estimating: 262it [02:58,  1.61it/s]Extractor Estimating: 263it [02:59,  1.59it/s]Extractor Estimating: 264it [03:00,  1.57it/s]Extractor Estimating: 265it [03:00,  1.62it/s]Extractor Estimating: 266it [03:01,  1.63it/s]Extractor Estimating: 267it [03:02,  1.64it/s]Extractor Estimating: 268it [03:02,  1.68it/s]Extractor Estimating: 269it [03:03,  1.66it/s]Extractor Estimating: 270it [03:03,  1.65it/s]Extractor Estimating: 271it [03:04,  1.61it/s]Extractor Estimating: 272it [03:05,  1.63it/s]Extractor Estimating: 273it [03:05,  1.64it/s]Extractor Estimating: 274it [03:06,  1.68it/s]Extractor Estimating: 275it [03:06,  1.76it/s]Extractor Estimating: 276it [03:07,  1.82it/s]Extractor Estimating: 277it [03:07,  1.84it/s]Extractor Estimating: 278it [03:08,  1.82it/s]Extractor Estimating: 279it [03:08,  1.80it/s]Extractor Estimating: 280it [03:09,  1.78it/s]Extractor Estimating: 281it [03:10,  1.71it/s]Extractor Estimating: 282it [03:10,  1.75it/s]Extractor Estimating: 283it [03:11,  1.77it/s]Extractor Estimating: 284it [03:11,  1.78it/s]Extractor Estimating: 285it [03:12,  1.83it/s]Extractor Estimating: 286it [03:12,  1.77it/s]Extractor Estimating: 287it [03:13,  1.81it/s]Extractor Estimating: 288it [03:13,  1.81it/s]Extractor Estimating: 289it [03:14,  1.79it/s]Extractor Estimating: 290it [03:15,  1.76it/s]Extractor Estimating: 291it [03:15,  1.77it/s]Extractor Estimating: 292it [03:16,  1.75it/s]Extractor Estimating: 293it [03:16,  1.82it/s]Extractor Estimating: 294it [03:17,  1.83it/s]Extractor Estimating: 295it [03:17,  1.80it/s]Extractor Estimating: 296it [03:18,  1.74it/s]Extractor Estimating: 297it [03:19,  1.76it/s]Extractor Estimating: 298it [03:19,  1.78it/s]Extractor Estimating: 299it [03:20,  1.77it/s]Extractor Estimating: 300it [03:20,  1.75it/s]Extractor Estimating: 301it [03:21,  1.79it/s]Extractor Estimating: 302it [03:21,  1.75it/s]Extractor Estimating: 303it [03:22,  1.79it/s]Extractor Estimating: 304it [03:23,  1.78it/s]Extractor Estimating: 305it [03:23,  1.83it/s]Extractor Estimating: 306it [03:24,  1.84it/s]Extractor Estimating: 307it [03:24,  1.75it/s]Extractor Estimating: 308it [03:25,  1.79it/s]Extractor Estimating: 309it [03:25,  1.79it/s]Extractor Estimating: 310it [03:26,  1.74it/s]Extractor Estimating: 311it [03:26,  1.77it/s]Extractor Estimating: 312it [03:27,  1.76it/s]Extractor Estimating: 313it [03:28,  1.77it/s]Extractor Estimating: 314it [03:28,  1.70it/s]Extractor Estimating: 315it [03:29,  1.77it/s]Extractor Estimating: 316it [03:29,  1.82it/s]Extractor Estimating: 317it [03:30,  1.80it/s]Extractor Estimating: 318it [03:30,  1.75it/s]Extractor Estimating: 319it [03:31,  1.72it/s]Extractor Estimating: 320it [03:32,  1.73it/s]Extractor Estimating: 321it [03:32,  1.77it/s]Extractor Estimating: 322it [03:33,  1.78it/s]Extractor Estimating: 323it [03:33,  1.78it/s]Extractor Estimating: 324it [03:34,  1.76it/s]Extractor Estimating: 325it [03:34,  1.69it/s]Extractor Estimating: 326it [03:35,  1.62it/s]Extractor Estimating: 327it [03:36,  1.60it/s]Extractor Estimating: 328it [03:36,  1.56it/s]Extractor Estimating: 329it [03:37,  1.61it/s]Extractor Estimating: 330it [03:38,  1.57it/s]Extractor Estimating: 331it [03:38,  1.59it/s]Extractor Estimating: 332it [03:39,  1.52it/s]Extractor Estimating: 333it [03:40,  1.57it/s]Extractor Estimating: 334it [03:41,  1.44it/s]Extractor Estimating: 335it [03:41,  1.50it/s]Extractor Estimating: 336it [03:42,  1.51it/s]Extractor Estimating: 337it [03:42,  1.49it/s]Extractor Estimating: 338it [03:43,  1.50it/s]Extractor Estimating: 339it [03:44,  1.52it/s]Extractor Estimating: 340it [03:44,  1.53it/s]Extractor Estimating: 341it [03:45,  1.57it/s]Extractor Estimating: 342it [03:46,  1.58it/s]Extractor Estimating: 343it [03:46,  1.61it/s]Extractor Estimating: 344it [03:47,  1.58it/s]Extractor Estimating: 345it [03:47,  1.63it/s]Extractor Estimating: 346it [03:48,  1.66it/s]Extractor Estimating: 347it [03:49,  1.65it/s]Extractor Estimating: 348it [03:49,  1.66it/s]Extractor Estimating: 349it [03:50,  1.65it/s]Extractor Estimating: 350it [03:50,  1.65it/s]Extractor Estimating: 351it [03:51,  1.70it/s]Extractor Estimating: 352it [03:52,  1.74it/s]Extractor Estimating: 353it [03:52,  1.71it/s]Extractor Estimating: 354it [03:53,  1.71it/s]Extractor Estimating: 355it [03:53,  1.70it/s]Extractor Estimating: 356it [03:54,  1.74it/s]Extractor Estimating: 357it [03:54,  1.74it/s]Extractor Estimating: 358it [03:55,  1.73it/s]Extractor Estimating: 359it [03:56,  1.75it/s]Extractor Estimating: 360it [03:56,  1.79it/s]Extractor Estimating: 361it [03:57,  1.78it/s]Extractor Estimating: 362it [03:57,  1.78it/s]Extractor Estimating: 363it [03:58,  1.77it/s]Extractor Estimating: 364it [03:58,  1.74it/s]Extractor Estimating: 365it [03:59,  1.71it/s]Extractor Estimating: 366it [04:00,  1.63it/s]Extractor Estimating: 367it [04:00,  1.64it/s]Extractor Estimating: 368it [04:01,  1.63it/s]Extractor Estimating: 369it [04:01,  1.70it/s]Extractor Estimating: 370it [04:02,  1.64it/s]Extractor Estimating: 371it [04:03,  1.67it/s]Extractor Estimating: 372it [04:03,  1.68it/s]Extractor Estimating: 373it [04:04,  1.66it/s]Extractor Estimating: 374it [04:04,  1.68it/s]Extractor Estimating: 375it [04:05,  1.70it/s]Extractor Estimating: 376it [04:06,  1.68it/s]Extractor Estimating: 377it [04:06,  1.71it/s]Extractor Estimating: 378it [04:07,  1.66it/s]Extractor Estimating: 379it [04:07,  1.68it/s]Extractor Estimating: 380it [04:08,  1.70it/s]Extractor Estimating: 381it [04:09,  1.70it/s]Extractor Estimating: 382it [04:09,  1.72it/s]Extractor Estimating: 383it [04:10,  1.70it/s]Extractor Estimating: 384it [04:10,  1.61it/s]Extractor Estimating: 385it [04:11,  1.59it/s]Extractor Estimating: 386it [04:12,  1.59it/s]Extractor Estimating: 387it [04:12,  1.60it/s]Extractor Estimating: 388it [04:13,  1.62it/s]Extractor Estimating: 389it [04:14,  1.64it/s]Extractor Estimating: 390it [04:14,  1.57it/s]Extractor Estimating: 391it [04:15,  1.56it/s]Extractor Estimating: 392it [04:15,  1.58it/s]Extractor Estimating: 393it [04:16,  1.58it/s]Extractor Estimating: 394it [04:17,  1.61it/s]Extractor Estimating: 395it [04:17,  1.65it/s]Extractor Estimating: 396it [04:18,  1.59it/s]Extractor Estimating: 397it [04:19,  1.57it/s]Extractor Estimating: 398it [04:19,  1.60it/s]Extractor Estimating: 399it [04:20,  1.61it/s]Extractor Estimating: 400it [04:20,  1.64it/s]Extractor Estimating: 401it [04:21,  1.67it/s]Extractor Estimating: 402it [04:22,  1.63it/s]Extractor Estimating: 403it [04:22,  1.61it/s]Extractor Estimating: 404it [04:23,  1.62it/s]Extractor Estimating: 405it [04:24,  1.62it/s]Extractor Estimating: 406it [04:24,  1.64it/s]Extractor Estimating: 407it [04:25,  1.65it/s]Extractor Estimating: 408it [04:25,  1.64it/s]Extractor Estimating: 409it [04:26,  1.62it/s]Extractor Estimating: 410it [04:27,  1.62it/s]Extractor Estimating: 411it [04:27,  1.64it/s]Extractor Estimating: 412it [04:28,  1.68it/s]Extractor Estimating: 413it [04:28,  1.71it/s]Extractor Estimating: 414it [04:29,  1.71it/s]Extractor Estimating: 415it [04:29,  1.73it/s]Extractor Estimating: 416it [04:30,  1.69it/s]Extractor Estimating: 417it [04:31,  1.71it/s]Extractor Estimating: 418it [04:31,  1.70it/s]Extractor Estimating: 419it [04:32,  1.72it/s]Extractor Estimating: 420it [04:32,  1.68it/s]Extractor Estimating: 421it [04:33,  1.70it/s]Extractor Estimating: 422it [04:34,  1.70it/s]Extractor Estimating: 423it [04:34,  1.68it/s]Extractor Estimating: 424it [04:35,  1.71it/s]Extractor Estimating: 425it [04:36,  1.50it/s]Extractor Estimating: 426it [04:36,  1.56it/s]Extractor Estimating: 427it [04:37,  1.56it/s]Extractor Estimating: 428it [04:37,  1.53it/s]Extractor Estimating: 429it [04:38,  1.49it/s]Extractor Estimating: 430it [04:39,  1.52it/s]Extractor Estimating: 431it [04:39,  1.56it/s]Extractor Estimating: 432it [04:40,  1.55it/s]Extractor Estimating: 433it [04:41,  1.58it/s]Extractor Estimating: 434it [04:41,  1.54it/s]Extractor Estimating: 435it [04:42,  1.56it/s]Extractor Estimating: 436it [04:43,  1.58it/s]Extractor Estimating: 437it [04:43,  1.55it/s]Extractor Estimating: 438it [04:44,  1.56it/s]Extractor Estimating: 439it [04:45,  1.55it/s]Extractor Estimating: 440it [04:45,  1.59it/s]Extractor Estimating: 441it [04:46,  1.60it/s]Extractor Estimating: 442it [04:46,  1.61it/s]Extractor Estimating: 443it [04:47,  1.61it/s]Extractor Estimating: 444it [04:48,  1.61it/s]Extractor Estimating: 445it [04:48,  1.65it/s]Extractor Estimating: 446it [04:49,  1.61it/s]Extractor Estimating: 447it [04:50,  1.58it/s]Extractor Estimating: 448it [04:50,  1.57it/s]Extractor Estimating: 449it [04:51,  1.58it/s]Extractor Estimating: 450it [04:51,  1.60it/s]Extractor Estimating: 451it [04:52,  1.64it/s]Extractor Estimating: 452it [04:53,  1.22it/s]Extractor Estimating: 453it [04:54,  1.30it/s]Extractor Estimating: 454it [04:55,  1.37it/s]Extractor Estimating: 455it [04:55,  1.42it/s]Extractor Estimating: 456it [04:56,  1.47it/s]Extractor Estimating: 457it [04:56,  1.54it/s]Extractor Estimating: 458it [04:57,  1.60it/s]Extractor Estimating: 459it [04:58,  1.62it/s]Extractor Estimating: 460it [04:58,  1.63it/s]Extractor Estimating: 461it [04:59,  1.61it/s]Extractor Estimating: 462it [04:59,  1.68it/s]Extractor Estimating: 463it [05:00,  1.70it/s]Extractor Estimating: 464it [05:01,  1.66it/s]Extractor Estimating: 465it [05:01,  1.65it/s]Extractor Estimating: 466it [05:02,  1.65it/s]Extractor Estimating: 467it [05:02,  1.69it/s]Extractor Estimating: 468it [05:03,  1.66it/s]Extractor Estimating: 469it [05:04,  1.67it/s]Extractor Estimating: 470it [05:04,  1.66it/s]Extractor Estimating: 471it [05:05,  1.67it/s]Extractor Estimating: 472it [05:05,  1.65it/s]Extractor Estimating: 473it [05:06,  1.72it/s]Extractor Estimating: 474it [05:07,  1.72it/s]Extractor Estimating: 475it [05:07,  1.70it/s]Extractor Estimating: 476it [05:08,  1.70it/s]Extractor Estimating: 477it [05:08,  1.67it/s]Extractor Estimating: 478it [05:09,  1.66it/s]Extractor Estimating: 479it [05:10,  1.65it/s]Extractor Estimating: 480it [05:10,  1.67it/s]Extractor Estimating: 481it [05:11,  1.70it/s]Extractor Estimating: 482it [05:11,  1.72it/s]Extractor Estimating: 483it [05:12,  1.65it/s]Extractor Estimating: 484it [05:13,  1.64it/s]Extractor Estimating: 485it [05:13,  1.63it/s]Extractor Estimating: 486it [05:14,  1.61it/s]Extractor Estimating: 487it [05:14,  1.63it/s]Extractor Estimating: 488it [05:15,  1.58it/s]Extractor Estimating: 489it [05:16,  1.66it/s]Extractor Estimating: 490it [05:16,  1.65it/s]Extractor Estimating: 491it [05:17,  1.67it/s]Extractor Estimating: 492it [05:17,  1.68it/s]Extractor Estimating: 493it [05:18,  1.66it/s]Extractor Estimating: 494it [05:19,  1.67it/s]Extractor Estimating: 495it [05:19,  1.65it/s]Extractor Estimating: 496it [05:20,  1.64it/s]Extractor Estimating: 497it [05:20,  1.68it/s]Extractor Estimating: 498it [05:21,  1.73it/s]Extractor Estimating: 499it [05:22,  1.72it/s]Extractor Estimating: 500it [05:22,  1.79it/s]Extractor Estimating: 500it [05:22,  1.55it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 9836 mean pseudo reward: 0.9470097825398706
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl'}
train vocab size: 31899
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31999, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31999, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.181, loss:3063.0455
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.916, loss:2182.5177
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.958, loss:1778.0628
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.920, loss:1613.1593
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 90, avg_time 0.914, loss:1580.2344
>> valid entity prec:0.5092, rec:0.4136, f1:0.4564
>> valid relation prec:0.3529, rec:0.0051, f1:0.0101
>> valid relation with NER prec:0.3529, rec:0.0051, f1:0.0101
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 190, avg_time 2.754, loss:1539.1585
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 290, avg_time 0.933, loss:1454.0749
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 390, avg_time 0.929, loss:1398.1191
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 80, avg_time 0.917, loss:1334.3745
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 180, avg_time 0.927, loss:1290.8056
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5009, rec:0.3349, f1:0.4014
>> valid relation prec:0.4468, rec:0.0143, f1:0.0278
>> valid relation with NER prec:0.4468, rec:0.0143, f1:0.0278
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 280, avg_time 2.748, loss:1250.9181
g_step 1200, step 380, avg_time 0.935, loss:1186.1874
g_step 1300, step 70, avg_time 0.925, loss:1091.3819
g_step 1400, step 170, avg_time 0.919, loss:1118.7039
g_step 1500, step 270, avg_time 0.929, loss:1103.3155
>> valid entity prec:0.5096, rec:0.3663, f1:0.4262
>> valid relation prec:0.3582, rec:0.0172, f1:0.0329
>> valid relation with NER prec:0.3582, rec:0.0172, f1:0.0329
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 370, avg_time 2.753, loss:1146.3118
g_step 1700, step 60, avg_time 0.916, loss:1041.3336
g_step 1800, step 160, avg_time 0.917, loss:1010.3096
g_step 1900, step 260, avg_time 0.932, loss:1082.3807
g_step 2000, step 360, avg_time 0.923, loss:1034.4518
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4505, rec:0.4391, f1:0.4448
>> valid relation prec:0.2619, rec:0.0150, f1:0.0284
>> valid relation with NER prec:0.2619, rec:0.0150, f1:0.0284
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 50, avg_time 2.774, loss:1003.7533
g_step 2200, step 150, avg_time 0.925, loss:959.6776
g_step 2300, step 250, avg_time 0.913, loss:1005.4843
g_step 2400, step 350, avg_time 0.926, loss:966.3414
g_step 2500, step 40, avg_time 0.918, loss:935.2012
>> valid entity prec:0.4478, rec:0.4887, f1:0.4674
>> valid relation prec:0.2627, rec:0.0327, f1:0.0582
>> valid relation with NER prec:0.2627, rec:0.0327, f1:0.0582
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 140, avg_time 2.764, loss:903.7096
g_step 2700, step 240, avg_time 0.913, loss:969.6276
g_step 2800, step 340, avg_time 0.916, loss:926.1466
g_step 2900, step 30, avg_time 0.922, loss:968.0607
g_step 3000, step 130, avg_time 0.921, loss:909.1823
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4596, rec:0.4129, f1:0.4350
>> valid relation prec:0.1409, rec:0.0107, f1:0.0200
>> valid relation with NER prec:0.1409, rec:0.0107, f1:0.0200
g_step 3100, step 230, avg_time 2.751, loss:876.0771
g_step 3200, step 330, avg_time 0.925, loss:908.3072
g_step 3300, step 20, avg_time 0.932, loss:862.5244
g_step 3400, step 120, avg_time 0.925, loss:844.9625
g_step 3500, step 220, avg_time 0.923, loss:856.9054
>> valid entity prec:0.4940, rec:0.3803, f1:0.4298
>> valid relation prec:0.1516, rec:0.0140, f1:0.0256
>> valid relation with NER prec:0.1516, rec:0.0140, f1:0.0256
g_step 3600, step 320, avg_time 2.754, loss:881.9775
g_step 3700, step 10, avg_time 0.914, loss:870.6790
g_step 3800, step 110, avg_time 0.925, loss:809.6439
g_step 3900, step 210, avg_time 0.916, loss:811.3006
g_step 4000, step 310, avg_time 0.924, loss:869.4366
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4555, rec:0.3324, f1:0.3843
>> valid relation prec:0.1897, rec:0.0176, f1:0.0322
>> valid relation with NER prec:0.1897, rec:0.0176, f1:0.0322
g_step 4100, step 410, avg_time 2.741, loss:842.0939
g_step 4200, step 100, avg_time 0.923, loss:812.7278
g_step 4300, step 200, avg_time 0.920, loss:801.7816
g_step 4400, step 300, avg_time 0.934, loss:804.8393
g_step 4500, step 400, avg_time 0.914, loss:798.3655
>> valid entity prec:0.4110, rec:0.3084, f1:0.3524
>> valid relation prec:0.1857, rec:0.0155, f1:0.0286
>> valid relation with NER prec:0.1857, rec:0.0155, f1:0.0286
g_step 4600, step 90, avg_time 2.764, loss:757.2323
g_step 4700, step 190, avg_time 0.914, loss:745.3690
g_step 4800, step 290, avg_time 0.921, loss:781.5116
g_step 4900, step 390, avg_time 0.929, loss:812.8267
g_step 5000, step 80, avg_time 0.910, loss:729.7455
learning rate was adjusted to 0.0008
>> valid entity prec:0.4478, rec:0.4374, f1:0.4425
>> valid relation prec:0.2382, rec:0.0406, f1:0.0694
>> valid relation with NER prec:0.2382, rec:0.0406, f1:0.0694
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 180, avg_time 2.750, loss:750.4025
g_step 5200, step 280, avg_time 0.931, loss:755.1679
g_step 5300, step 380, avg_time 0.925, loss:768.0528
g_step 5400, step 70, avg_time 0.927, loss:733.6246
g_step 5500, step 170, avg_time 0.915, loss:734.3630
>> valid entity prec:0.4580, rec:0.4593, f1:0.4587
>> valid relation prec:0.1350, rec:0.0213, f1:0.0368
>> valid relation with NER prec:0.1350, rec:0.0213, f1:0.0368
g_step 5600, step 270, avg_time 2.762, loss:726.7876
g_step 5700, step 370, avg_time 0.925, loss:751.1472
g_step 5800, step 60, avg_time 0.912, loss:707.2384
g_step 5900, step 160, avg_time 0.922, loss:681.7961
g_step 6000, step 260, avg_time 0.927, loss:721.4143
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4424, rec:0.4151, f1:0.4283
>> valid relation prec:0.1298, rec:0.0184, f1:0.0323
>> valid relation with NER prec:0.1298, rec:0.0184, f1:0.0323
g_step 6100, step 360, avg_time 2.754, loss:705.4891
g_step 6200, step 50, avg_time 0.928, loss:718.3131
g_step 6300, step 150, avg_time 0.936, loss:673.1288
g_step 6400, step 250, avg_time 0.928, loss:677.1635
g_step 6500, step 350, avg_time 0.921, loss:678.9391
>> valid entity prec:0.4592, rec:0.3928, f1:0.4234
>> valid relation prec:0.1671, rec:0.0218, f1:0.0386
>> valid relation with NER prec:0.1671, rec:0.0218, f1:0.0386
g_step 6600, step 40, avg_time 2.748, loss:681.7728
g_step 6700, step 140, avg_time 0.929, loss:631.8610
g_step 6800, step 240, avg_time 0.927, loss:653.8235
g_step 6900, step 340, avg_time 0.927, loss:698.9494
g_step 7000, step 30, avg_time 0.914, loss:651.3206
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4338, rec:0.4400, f1:0.4369
>> valid relation prec:0.1690, rec:0.0307, f1:0.0520
>> valid relation with NER prec:0.1690, rec:0.0307, f1:0.0520
g_step 7100, step 130, avg_time 2.757, loss:635.9717
g_step 7200, step 230, avg_time 0.941, loss:636.9751
g_step 7300, step 330, avg_time 0.919, loss:657.2122
g_step 7400, step 20, avg_time 0.922, loss:655.6736
g_step 7500, step 120, avg_time 0.922, loss:618.1775
>> valid entity prec:0.4601, rec:0.3678, f1:0.4088
>> valid relation prec:0.1533, rec:0.0225, f1:0.0393
>> valid relation with NER prec:0.1533, rec:0.0225, f1:0.0393
g_step 7600, step 220, avg_time 2.755, loss:620.1004
g_step 7700, step 320, avg_time 0.917, loss:653.4256
g_step 7800, step 10, avg_time 0.925, loss:626.4166
g_step 7900, step 110, avg_time 0.919, loss:616.7213
g_step 8000, step 210, avg_time 0.929, loss:593.2450
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4409, rec:0.4418, f1:0.4414
>> valid relation prec:0.1577, rec:0.0321, f1:0.0533
>> valid relation with NER prec:0.1577, rec:0.0321, f1:0.0533
g_step 8100, step 310, avg_time 2.765, loss:606.4503
g_step 8200, step 410, avg_time 0.920, loss:596.8328
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 05:48:04 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 05:48:04 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_05-48-04_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 05:48:05 - WARNING - datasets.builder -   Using custom data configuration default-f7ecd06c0010fa9a
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f7ecd06c0010fa9a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 05:48:05,628 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:48:05,629 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:48:05,629 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:48:05,630 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:48:05,645 >> Didn't find file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:48:05,649 >> loading file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:48:05,649 >> loading file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:48:05,650 >> loading file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:48:05,650 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:48:05,650 >> loading file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:48:05,650 >> loading file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 05:48:05,808 >> loading weights file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:48:08,808 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 05:48:08,813 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_15_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f7ecd06c0010fa9a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_15_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 05:48:08 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14fa225a2e60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.83ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.74ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.13ba/s] 36%|███▋      | 4/11 [00:00<00:01,  4.35ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.45ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.53ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.59ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  3.81ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.05ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.24ba/s]100%|██████████| 11/11 [00:02<00:00,  4.54ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  4.34ba/s] 33%|███▎      | 2/6 [00:00<00:00,  4.51ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.56ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.60ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.64ba/s]100%|██████████| 6/6 [00:01<00:00,  4.88ba/s]100%|██████████| 6/6 [00:01<00:00,  4.71ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  9.43ba/s] 27%|██▋       | 3/11 [00:00<00:00, 10.82ba/s] 45%|████▌     | 5/11 [00:00<00:00, 11.20ba/s] 64%|██████▎   | 7/11 [00:00<00:00, 11.42ba/s] 82%|████████▏ | 9/11 [00:00<00:00, 11.46ba/s]100%|██████████| 11/11 [00:00<00:00, 12.40ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  8.97ba/s] 50%|█████     | 3/6 [00:00<00:00, 10.73ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 11.08ba/s]100%|██████████| 6/6 [00:00<00:00, 11.21ba/s]
[INFO|trainer.py:414] 2023-08-29 05:48:14,340 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 05:48:14,353 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 05:48:14,353 >>   Num examples = 10044
[INFO|trainer.py:1149] 2023-08-29 05:48:14,353 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 05:48:14,353 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 05:48:14,353 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 05:48:14,353 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 05:48:14,353 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:53,  3.36it/s]  0%|          | 2/785 [00:00<03:49,  3.42it/s]  0%|          | 3/785 [00:00<03:48,  3.43it/s]  1%|          | 4/785 [00:01<03:48,  3.42it/s]  1%|          | 5/785 [00:01<03:47,  3.43it/s]  1%|          | 6/785 [00:01<03:46,  3.43it/s]  1%|          | 7/785 [00:02<03:47,  3.42it/s]  1%|          | 8/785 [00:02<03:47,  3.41it/s]  1%|          | 9/785 [00:02<03:48,  3.40it/s]  1%|▏         | 10/785 [00:02<03:47,  3.40it/s]  1%|▏         | 11/785 [00:03<03:47,  3.40it/s]  2%|▏         | 12/785 [00:03<03:47,  3.40it/s]  2%|▏         | 13/785 [00:03<03:47,  3.40it/s]  2%|▏         | 14/785 [00:04<03:47,  3.39it/s]  2%|▏         | 15/785 [00:04<03:47,  3.39it/s]  2%|▏         | 16/785 [00:04<03:47,  3.39it/s]  2%|▏         | 17/785 [00:04<03:46,  3.39it/s]  2%|▏         | 18/785 [00:05<03:46,  3.39it/s]  2%|▏         | 19/785 [00:05<03:45,  3.39it/s]  3%|▎         | 20/785 [00:05<03:45,  3.39it/s]  3%|▎         | 21/785 [00:06<03:45,  3.39it/s]  3%|▎         | 22/785 [00:06<03:45,  3.39it/s]  3%|▎         | 23/785 [00:06<03:44,  3.39it/s]  3%|▎         | 24/785 [00:07<03:44,  3.39it/s]  3%|▎         | 25/785 [00:07<03:44,  3.39it/s]  3%|▎         | 26/785 [00:07<03:44,  3.38it/s]  3%|▎         | 27/785 [00:07<03:44,  3.38it/s]  4%|▎         | 28/785 [00:08<03:43,  3.39it/s]  4%|▎         | 29/785 [00:08<03:43,  3.39it/s]  4%|▍         | 30/785 [00:08<03:42,  3.39it/s]  4%|▍         | 31/785 [00:09<03:42,  3.39it/s]  4%|▍         | 32/785 [00:09<03:42,  3.39it/s]  4%|▍         | 33/785 [00:09<03:42,  3.39it/s]  4%|▍         | 34/785 [00:10<03:42,  3.38it/s]  4%|▍         | 35/785 [00:10<03:41,  3.39it/s]  5%|▍         | 36/785 [00:10<03:41,  3.39it/s]  5%|▍         | 37/785 [00:10<03:40,  3.39it/s]  5%|▍         | 38/785 [00:11<03:40,  3.39it/s]  5%|▍         | 39/785 [00:11<03:40,  3.38it/s]  5%|▌         | 40/785 [00:11<03:39,  3.39it/s]  5%|▌         | 41/785 [00:12<03:39,  3.39it/s]  5%|▌         | 42/785 [00:12<03:39,  3.39it/s]  5%|▌         | 43/785 [00:12<03:39,  3.39it/s]  6%|▌         | 44/785 [00:12<03:38,  3.39it/s]  6%|▌         | 45/785 [00:13<03:39,  3.38it/s]  6%|▌         | 46/785 [00:13<03:38,  3.38it/s]  6%|▌         | 47/785 [00:13<03:38,  3.38it/s]  6%|▌         | 48/785 [00:14<03:38,  3.38it/s]  6%|▌         | 49/785 [00:14<03:37,  3.38it/s]  6%|▋         | 50/785 [00:14<03:37,  3.38it/s]  6%|▋         | 51/785 [00:15<03:36,  3.39it/s]  7%|▋         | 52/785 [00:15<03:36,  3.39it/s]  7%|▋         | 53/785 [00:15<03:36,  3.39it/s]  7%|▋         | 54/785 [00:15<03:35,  3.39it/s]  7%|▋         | 55/785 [00:16<03:34,  3.40it/s]  7%|▋         | 56/785 [00:16<03:33,  3.41it/s]  7%|▋         | 57/785 [00:16<03:32,  3.42it/s]  7%|▋         | 58/785 [00:17<03:32,  3.42it/s]  8%|▊         | 59/785 [00:17<03:32,  3.42it/s]  8%|▊         | 60/785 [00:17<03:31,  3.43it/s]  8%|▊         | 61/785 [00:17<03:30,  3.43it/s]  8%|▊         | 62/785 [00:18<03:30,  3.43it/s]  8%|▊         | 63/785 [00:18<03:30,  3.44it/s]  8%|▊         | 64/785 [00:18<03:29,  3.43it/s]  8%|▊         | 65/785 [00:19<03:29,  3.43it/s]  8%|▊         | 66/785 [00:19<03:29,  3.43it/s]  9%|▊         | 67/785 [00:19<03:28,  3.44it/s]  9%|▊         | 68/785 [00:19<03:28,  3.43it/s]  9%|▉         | 69/785 [00:20<03:29,  3.43it/s]  9%|▉         | 70/785 [00:20<03:28,  3.43it/s]  9%|▉         | 71/785 [00:20<03:28,  3.43it/s]  9%|▉         | 72/785 [00:21<03:27,  3.43it/s]  9%|▉         | 73/785 [00:21<03:27,  3.43it/s]  9%|▉         | 74/785 [00:21<03:27,  3.43it/s] 10%|▉         | 75/785 [00:22<03:26,  3.44it/s] 10%|▉         | 76/785 [00:22<03:26,  3.43it/s] 10%|▉         | 77/785 [00:22<03:26,  3.43it/s] 10%|▉         | 78/785 [00:22<03:25,  3.43it/s] 10%|█         | 79/785 [00:23<03:25,  3.44it/s] 10%|█         | 80/785 [00:23<03:25,  3.43it/s] 10%|█         | 81/785 [00:23<03:25,  3.43it/s] 10%|█         | 82/785 [00:24<03:25,  3.43it/s] 11%|█         | 83/785 [00:24<03:24,  3.43it/s] 11%|█         | 84/785 [00:24<03:24,  3.43it/s] 11%|█         | 85/785 [00:24<03:24,  3.43it/s] 11%|█         | 86/785 [00:25<03:23,  3.43it/s] 11%|█         | 87/785 [00:25<03:23,  3.43it/s] 11%|█         | 88/785 [00:25<03:22,  3.43it/s] 11%|█▏        | 89/785 [00:26<03:22,  3.43it/s] 11%|█▏        | 90/785 [00:26<03:22,  3.43it/s] 12%|█▏        | 91/785 [00:26<03:22,  3.42it/s] 12%|█▏        | 92/785 [00:26<03:22,  3.43it/s] 12%|█▏        | 93/785 [00:27<03:21,  3.43it/s] 12%|█▏        | 94/785 [00:27<03:21,  3.43it/s] 12%|█▏        | 95/785 [00:27<03:21,  3.43it/s] 12%|█▏        | 96/785 [00:28<03:20,  3.44it/s] 12%|█▏        | 97/785 [00:28<03:20,  3.43it/s] 12%|█▏        | 98/785 [00:28<03:20,  3.43it/s] 13%|█▎        | 99/785 [00:29<03:19,  3.43it/s] 13%|█▎        | 100/785 [00:29<03:19,  3.44it/s] 13%|█▎        | 101/785 [00:29<03:19,  3.44it/s] 13%|█▎        | 102/785 [00:29<03:19,  3.43it/s] 13%|█▎        | 103/785 [00:30<03:18,  3.43it/s] 13%|█▎        | 104/785 [00:30<03:18,  3.43it/s] 13%|█▎        | 105/785 [00:30<03:18,  3.43it/s] 14%|█▎        | 106/785 [00:31<03:17,  3.44it/s] 14%|█▎        | 107/785 [00:31<03:17,  3.43it/s] 14%|█▍        | 108/785 [00:31<03:17,  3.43it/s] 14%|█▍        | 109/785 [00:31<03:17,  3.43it/s] 14%|█▍        | 110/785 [00:32<03:16,  3.43it/s] 14%|█▍        | 111/785 [00:32<03:16,  3.43it/s] 14%|█▍        | 112/785 [00:32<03:16,  3.43it/s] 14%|█▍        | 113/785 [00:33<03:16,  3.43it/s] 15%|█▍        | 114/785 [00:33<03:15,  3.42it/s] 15%|█▍        | 115/785 [00:33<03:15,  3.43it/s] 15%|█▍        | 116/785 [00:33<03:15,  3.43it/s] 15%|█▍        | 117/785 [00:34<03:14,  3.43it/s] 15%|█▌        | 118/785 [00:34<03:14,  3.43it/s] 15%|█▌        | 119/785 [00:34<03:13,  3.43it/s] 15%|█▌        | 120/785 [00:35<03:13,  3.43it/s] 15%|█▌        | 121/785 [00:35<03:13,  3.43it/s] 16%|█▌        | 122/785 [00:35<03:13,  3.43it/s] 16%|█▌        | 123/785 [00:36<03:12,  3.43it/s] 16%|█▌        | 124/785 [00:36<03:13,  3.42it/s] 16%|█▌        | 125/785 [00:36<03:12,  3.43it/s] 16%|█▌        | 126/785 [00:36<03:12,  3.43it/s] 16%|█▌        | 127/785 [00:37<03:11,  3.43it/s] 16%|█▋        | 128/785 [00:37<03:11,  3.43it/s] 16%|█▋        | 129/785 [00:37<03:11,  3.43it/s] 17%|█▋        | 130/785 [00:38<03:10,  3.43it/s] 17%|█▋        | 131/785 [00:38<03:10,  3.43it/s] 17%|█▋        | 132/785 [00:38<03:10,  3.43it/s] 17%|█▋        | 133/785 [00:38<03:09,  3.43it/s] 17%|█▋        | 134/785 [00:39<03:09,  3.44it/s] 17%|█▋        | 135/785 [00:39<03:09,  3.43it/s] 17%|█▋        | 136/785 [00:39<03:09,  3.43it/s] 17%|█▋        | 137/785 [00:40<03:09,  3.43it/s] 18%|█▊        | 138/785 [00:40<03:08,  3.43it/s] 18%|█▊        | 139/785 [00:40<03:08,  3.43it/s] 18%|█▊        | 140/785 [00:40<03:08,  3.43it/s] 18%|█▊        | 141/785 [00:41<03:07,  3.43it/s] 18%|█▊        | 142/785 [00:41<03:07,  3.43it/s] 18%|█▊        | 143/785 [00:41<03:07,  3.43it/s] 18%|█▊        | 144/785 [00:42<03:06,  3.43it/s] 18%|█▊        | 145/785 [00:42<03:06,  3.44it/s] 19%|█▊        | 146/785 [00:42<03:06,  3.43it/s] 19%|█▊        | 147/785 [00:43<03:05,  3.43it/s] 19%|█▉        | 148/785 [00:43<03:05,  3.43it/s] 19%|█▉        | 149/785 [00:43<03:05,  3.43it/s] 19%|█▉        | 150/785 [00:43<03:05,  3.43it/s] 19%|█▉        | 151/785 [00:44<03:04,  3.43it/s] 19%|█▉        | 152/785 [00:44<03:05,  3.42it/s] 19%|█▉        | 153/785 [00:44<03:04,  3.42it/s] 20%|█▉        | 154/785 [00:45<03:04,  3.43it/s] 20%|█▉        | 155/785 [00:45<03:03,  3.43it/s] 20%|█▉        | 156/785 [00:45<03:03,  3.43it/s] 20%|██        | 157/785 [00:45<03:00,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 05:49:00,283 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:49:00,283 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 05:49:00,283 >>   Batch size = 8

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 55.42it/s][A
  2%|▏         | 12/733 [00:00<00:15, 47.32it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.58it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.65it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.26it/s][A
  4%|▍         | 32/733 [00:00<00:15, 43.97it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.82it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.71it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.78it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.89it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.82it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.57it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.52it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.56it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.54it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.48it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.46it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.64it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.65it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.59it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.52it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.55it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.48it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.50it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.43it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.48it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.50it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.65it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.64it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.59it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.60it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.48it/s][A
 23%|██▎       | 167/733 [00:03<00:13, 43.44it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.54it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.51it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.04it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.28it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.39it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.41it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.44it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.44it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.48it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.49it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.42it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.54it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.64it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.57it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.54it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.52it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.52it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.50it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.48it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.56it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.61it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.52it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.51it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.50it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.62it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.51it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.55it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.57it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.61it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.57it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.49it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.56it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.58it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.42it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.54it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.56it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.49it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.59it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.54it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.54it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.56it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.45it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.53it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.56it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.56it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.50it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.49it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.45it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.60it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.61it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.49it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.61it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.55it/s][A
 60%|█████▉    | 437/733 [00:10<00:06, 43.58it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.50it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.25it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.50it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.52it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.59it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.56it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.61it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.60it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.57it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.51it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.55it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.53it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.47it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.58it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.63it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.57it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.50it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.58it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.51it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.55it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.45it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.43it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.59it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.58it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.54it/s][A
 77%|███████▋  | 567/733 [00:13<00:03, 43.60it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.52it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.48it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.40it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.49it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.44it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.49it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.51it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.59it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.48it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.48it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.52it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.37it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.49it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.59it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.57it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.58it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.56it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.52it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.54it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.53it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.46it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.48it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.58it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.57it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.57it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.52it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.52it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.43it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.44it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.45it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.49it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.60it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.63it/s][A                                                 
                                                 [A 20%|██        | 157/785 [01:02<03:00,  3.48it/s]
100%|██████████| 733/733 [00:16<00:00, 43.63it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:49:17,157 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-29 05:49:17,176 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:49:18,880 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:49:18,897 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:49:18,911 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:08<1:13:33,  7.04s/it] 20%|██        | 159/785 [01:09<52:20,  5.02s/it]   20%|██        | 160/785 [01:09<37:30,  3.60s/it] 21%|██        | 161/785 [01:09<27:07,  2.61s/it] 21%|██        | 162/785 [01:09<19:52,  1.91s/it] 21%|██        | 163/785 [01:10<14:48,  1.43s/it] 21%|██        | 164/785 [01:10<11:16,  1.09s/it] 21%|██        | 165/785 [01:10<08:47,  1.18it/s] 21%|██        | 166/785 [01:11<07:03,  1.46it/s] 21%|██▏       | 167/785 [01:11<05:50,  1.76it/s] 21%|██▏       | 168/785 [01:11<04:59,  2.06it/s] 22%|██▏       | 169/785 [01:11<04:23,  2.33it/s] 22%|██▏       | 170/785 [01:12<03:58,  2.58it/s] 22%|██▏       | 171/785 [01:12<03:41,  2.77it/s] 22%|██▏       | 172/785 [01:12<03:29,  2.93it/s] 22%|██▏       | 173/785 [01:13<03:20,  3.06it/s] 22%|██▏       | 174/785 [01:13<03:14,  3.15it/s] 22%|██▏       | 175/785 [01:13<03:09,  3.21it/s] 22%|██▏       | 176/785 [01:14<03:06,  3.27it/s] 23%|██▎       | 177/785 [01:14<03:04,  3.30it/s] 23%|██▎       | 178/785 [01:14<03:02,  3.33it/s] 23%|██▎       | 179/785 [01:14<03:01,  3.34it/s] 23%|██▎       | 180/785 [01:15<03:00,  3.35it/s] 23%|██▎       | 181/785 [01:15<02:59,  3.36it/s] 23%|██▎       | 182/785 [01:15<02:59,  3.36it/s] 23%|██▎       | 183/785 [01:16<02:58,  3.37it/s] 23%|██▎       | 184/785 [01:16<02:58,  3.38it/s] 24%|██▎       | 185/785 [01:16<02:57,  3.38it/s] 24%|██▎       | 186/785 [01:16<02:57,  3.38it/s] 24%|██▍       | 187/785 [01:17<02:57,  3.38it/s] 24%|██▍       | 188/785 [01:17<02:56,  3.38it/s] 24%|██▍       | 189/785 [01:17<02:56,  3.38it/s] 24%|██▍       | 190/785 [01:18<02:55,  3.38it/s] 24%|██▍       | 191/785 [01:18<02:55,  3.38it/s] 24%|██▍       | 192/785 [01:18<02:55,  3.38it/s] 25%|██▍       | 193/785 [01:19<02:55,  3.38it/s] 25%|██▍       | 194/785 [01:19<02:54,  3.38it/s] 25%|██▍       | 195/785 [01:19<02:54,  3.38it/s] 25%|██▍       | 196/785 [01:19<02:54,  3.38it/s] 25%|██▌       | 197/785 [01:20<02:53,  3.38it/s] 25%|██▌       | 198/785 [01:20<02:53,  3.38it/s] 25%|██▌       | 199/785 [01:20<02:53,  3.38it/s] 25%|██▌       | 200/785 [01:21<02:53,  3.38it/s] 26%|██▌       | 201/785 [01:21<02:52,  3.38it/s] 26%|██▌       | 202/785 [01:21<02:52,  3.37it/s] 26%|██▌       | 203/785 [01:22<02:52,  3.38it/s] 26%|██▌       | 204/785 [01:22<02:52,  3.36it/s] 26%|██▌       | 205/785 [01:22<02:52,  3.37it/s] 26%|██▌       | 206/785 [01:22<02:51,  3.37it/s] 26%|██▋       | 207/785 [01:23<02:51,  3.37it/s] 26%|██▋       | 208/785 [01:23<02:50,  3.37it/s] 27%|██▋       | 209/785 [01:23<02:50,  3.38it/s] 27%|██▋       | 210/785 [01:24<02:50,  3.38it/s] 27%|██▋       | 211/785 [01:24<02:49,  3.38it/s] 27%|██▋       | 212/785 [01:24<02:49,  3.38it/s] 27%|██▋       | 213/785 [01:24<02:50,  3.36it/s] 27%|██▋       | 214/785 [01:25<02:49,  3.37it/s] 27%|██▋       | 215/785 [01:25<02:48,  3.37it/s] 28%|██▊       | 216/785 [01:25<02:48,  3.38it/s] 28%|██▊       | 217/785 [01:26<02:48,  3.38it/s] 28%|██▊       | 218/785 [01:26<02:47,  3.38it/s] 28%|██▊       | 219/785 [01:26<02:47,  3.38it/s] 28%|██▊       | 220/785 [01:27<02:47,  3.38it/s] 28%|██▊       | 221/785 [01:27<02:46,  3.38it/s] 28%|██▊       | 222/785 [01:27<02:46,  3.38it/s] 28%|██▊       | 223/785 [01:27<02:46,  3.38it/s] 29%|██▊       | 224/785 [01:28<02:46,  3.37it/s] 29%|██▊       | 225/785 [01:28<02:45,  3.38it/s] 29%|██▉       | 226/785 [01:28<02:45,  3.38it/s] 29%|██▉       | 227/785 [01:29<02:45,  3.38it/s] 29%|██▉       | 228/785 [01:29<02:44,  3.38it/s] 29%|██▉       | 229/785 [01:29<02:44,  3.38it/s] 29%|██▉       | 230/785 [01:30<02:44,  3.38it/s] 29%|██▉       | 231/785 [01:30<02:43,  3.38it/s] 30%|██▉       | 232/785 [01:30<02:43,  3.38it/s] 30%|██▉       | 233/785 [01:30<02:43,  3.38it/s] 30%|██▉       | 234/785 [01:31<02:42,  3.38it/s] 30%|██▉       | 235/785 [01:31<02:43,  3.37it/s] 30%|███       | 236/785 [01:31<02:42,  3.37it/s] 30%|███       | 237/785 [01:32<02:42,  3.38it/s] 30%|███       | 238/785 [01:32<02:42,  3.38it/s] 30%|███       | 239/785 [01:32<02:41,  3.38it/s] 31%|███       | 240/785 [01:32<02:41,  3.38it/s] 31%|███       | 241/785 [01:33<02:40,  3.38it/s] 31%|███       | 242/785 [01:33<02:40,  3.38it/s] 31%|███       | 243/785 [01:33<02:40,  3.38it/s] 31%|███       | 244/785 [01:34<02:40,  3.38it/s] 31%|███       | 245/785 [01:34<02:39,  3.38it/s] 31%|███▏      | 246/785 [01:34<02:39,  3.37it/s] 31%|███▏      | 247/785 [01:35<02:39,  3.36it/s] 32%|███▏      | 248/785 [01:35<02:39,  3.36it/s] 32%|███▏      | 249/785 [01:35<02:39,  3.37it/s] 32%|███▏      | 250/785 [01:35<02:38,  3.37it/s] 32%|███▏      | 251/785 [01:36<02:38,  3.37it/s] 32%|███▏      | 252/785 [01:36<02:37,  3.38it/s] 32%|███▏      | 253/785 [01:36<02:37,  3.38it/s] 32%|███▏      | 254/785 [01:37<02:37,  3.38it/s] 32%|███▏      | 255/785 [01:37<02:37,  3.37it/s] 33%|███▎      | 256/785 [01:37<02:36,  3.37it/s] 33%|███▎      | 257/785 [01:38<02:36,  3.36it/s] 33%|███▎      | 258/785 [01:38<02:36,  3.37it/s] 33%|███▎      | 259/785 [01:38<02:36,  3.37it/s] 33%|███▎      | 260/785 [01:38<02:35,  3.37it/s] 33%|███▎      | 261/785 [01:39<02:35,  3.37it/s] 33%|███▎      | 262/785 [01:39<02:34,  3.38it/s] 34%|███▎      | 263/785 [01:39<02:33,  3.40it/s] 34%|███▎      | 264/785 [01:40<02:33,  3.40it/s] 34%|███▍      | 265/785 [01:40<02:32,  3.41it/s] 34%|███▍      | 266/785 [01:40<02:31,  3.42it/s] 34%|███▍      | 267/785 [01:40<02:31,  3.42it/s] 34%|███▍      | 268/785 [01:41<02:31,  3.42it/s] 34%|███▍      | 269/785 [01:41<02:30,  3.42it/s] 34%|███▍      | 270/785 [01:41<02:30,  3.42it/s] 35%|███▍      | 271/785 [01:42<02:29,  3.43it/s] 35%|███▍      | 272/785 [01:42<02:29,  3.43it/s] 35%|███▍      | 273/785 [01:42<02:29,  3.43it/s] 35%|███▍      | 274/785 [01:43<02:29,  3.43it/s] 35%|███▌      | 275/785 [01:43<02:28,  3.43it/s] 35%|███▌      | 276/785 [01:43<02:28,  3.43it/s] 35%|███▌      | 277/785 [01:43<02:28,  3.43it/s] 35%|███▌      | 278/785 [01:44<02:27,  3.43it/s] 36%|███▌      | 279/785 [01:44<02:27,  3.43it/s] 36%|███▌      | 280/785 [01:44<02:27,  3.42it/s] 36%|███▌      | 281/785 [01:45<02:27,  3.42it/s] 36%|███▌      | 282/785 [01:45<02:26,  3.43it/s] 36%|███▌      | 283/785 [01:45<02:26,  3.43it/s] 36%|███▌      | 284/785 [01:45<02:26,  3.41it/s] 36%|███▋      | 285/785 [01:46<02:26,  3.42it/s] 36%|███▋      | 286/785 [01:46<02:25,  3.42it/s] 37%|███▋      | 287/785 [01:46<02:25,  3.43it/s] 37%|███▋      | 288/785 [01:47<02:25,  3.43it/s] 37%|███▋      | 289/785 [01:47<02:24,  3.43it/s] 37%|███▋      | 290/785 [01:47<02:24,  3.43it/s] 37%|███▋      | 291/785 [01:47<02:24,  3.43it/s] 37%|███▋      | 292/785 [01:48<02:23,  3.43it/s] 37%|███▋      | 293/785 [01:48<02:23,  3.42it/s] 37%|███▋      | 294/785 [01:48<02:23,  3.42it/s] 38%|███▊      | 295/785 [01:49<02:23,  3.42it/s] 38%|███▊      | 296/785 [01:49<02:23,  3.42it/s] 38%|███▊      | 297/785 [01:49<02:22,  3.42it/s] 38%|███▊      | 298/785 [01:50<02:22,  3.43it/s] 38%|███▊      | 299/785 [01:50<02:29,  3.25it/s] 38%|███▊      | 300/785 [01:50<02:26,  3.30it/s] 38%|███▊      | 301/785 [01:50<02:24,  3.34it/s] 38%|███▊      | 302/785 [01:51<02:23,  3.37it/s] 39%|███▊      | 303/785 [01:51<02:22,  3.38it/s] 39%|███▊      | 304/785 [01:51<02:21,  3.40it/s] 39%|███▉      | 305/785 [01:52<02:20,  3.41it/s] 39%|███▉      | 306/785 [01:52<02:21,  3.39it/s] 39%|███▉      | 307/785 [01:52<02:20,  3.41it/s] 39%|███▉      | 308/785 [01:52<02:19,  3.41it/s] 39%|███▉      | 309/785 [01:53<02:19,  3.41it/s] 39%|███▉      | 310/785 [01:53<02:18,  3.42it/s] 40%|███▉      | 311/785 [01:53<02:18,  3.42it/s] 40%|███▉      | 312/785 [01:54<02:18,  3.41it/s] 40%|███▉      | 313/785 [01:54<02:18,  3.40it/s] 40%|████      | 314/785 [01:54<02:16,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 05:50:09,091 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:50:09,092 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 05:50:09,092 >>   Batch size = 8
{'eval_loss': 0.9204018115997314, 'eval_runtime': 16.842, 'eval_samples_per_second': 348.177, 'eval_steps_per_second': 43.522, 'epoch': 1.0}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 55.35it/s][A
  2%|▏         | 12/733 [00:00<00:15, 47.47it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.85it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.90it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.43it/s][A
  4%|▍         | 32/733 [00:00<00:15, 44.02it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.68it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.61it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.76it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.80it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.79it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.70it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.64it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.51it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.40it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.42it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.46it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.37it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.60it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.69it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.66it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.56it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.05it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.42it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.30it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.35it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.37it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.50it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.70it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.50it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.42it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.47it/s][A
 23%|██▎       | 167/733 [00:03<00:13, 43.41it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.37it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.51it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.65it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.66it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.65it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.55it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.45it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.39it/s][A
 29%|██▉       | 212/733 [00:04<00:12, 43.39it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.52it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.63it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.63it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.59it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.58it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.48it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.38it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.41it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.37it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.55it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.63it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.63it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.58it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.57it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.43it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.41it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.36it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.53it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.55it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.68it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.68it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.64it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.55it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.52it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.37it/s][A
 47%|████▋     | 342/733 [00:07<00:09, 43.40it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.51it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.54it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.61it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.63it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.60it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.52it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.42it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.49it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.45it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.54it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.55it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.61it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.58it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.43it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.53it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.50it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.40it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.49it/s][A
 60%|█████▉    | 437/733 [00:10<00:06, 43.56it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.61it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.70it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.56it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.57it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.54it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.53it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.54it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.53it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.59it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.62it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.58it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.59it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.52it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.57it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.50it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.60it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.57it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.61it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.60it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.53it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.26it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.61it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.52it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.58it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.65it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.65it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.69it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.61it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.46it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.53it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.54it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.49it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.62it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.64it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.64it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.21it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.50it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.57it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.55it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.57it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.58it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.58it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.66it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.62it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.43it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.38it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.53it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.50it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.55it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.59it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.62it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.66it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.46it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.51it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.58it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.55it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.45it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.46it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.68it/s][A                                                 
                                                 [A 40%|████      | 314/785 [02:11<02:16,  3.44it/s]
100%|██████████| 733/733 [00:16<00:00, 43.68it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:50:25,950 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-29 05:50:25,972 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:50:27,679 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:50:27,692 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:50:27,704 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:17<54:09,  6.91s/it] 40%|████      | 316/785 [02:17<38:31,  4.93s/it] 40%|████      | 317/785 [02:17<27:36,  3.54s/it] 41%|████      | 318/785 [02:17<19:58,  2.57s/it] 41%|████      | 319/785 [02:18<14:38,  1.88s/it] 41%|████      | 320/785 [02:18<10:54,  1.41s/it] 41%|████      | 321/785 [02:18<08:18,  1.07s/it] 41%|████      | 322/785 [02:19<06:29,  1.19it/s] 41%|████      | 323/785 [02:19<05:13,  1.48it/s] 41%|████▏     | 324/785 [02:19<04:19,  1.78it/s] 41%|████▏     | 325/785 [02:20<03:42,  2.07it/s] 42%|████▏     | 326/785 [02:20<03:15,  2.34it/s] 42%|████▏     | 327/785 [02:20<02:57,  2.58it/s] 42%|████▏     | 328/785 [02:20<02:44,  2.78it/s] 42%|████▏     | 329/785 [02:21<02:35,  2.94it/s] 42%|████▏     | 330/785 [02:21<02:28,  3.06it/s] 42%|████▏     | 331/785 [02:21<02:24,  3.15it/s] 42%|████▏     | 332/785 [02:22<02:20,  3.22it/s] 42%|████▏     | 333/785 [02:22<02:18,  3.27it/s] 43%|████▎     | 334/785 [02:22<02:16,  3.30it/s] 43%|████▎     | 335/785 [02:23<02:15,  3.32it/s] 43%|████▎     | 336/785 [02:23<02:14,  3.35it/s] 43%|████▎     | 337/785 [02:23<02:13,  3.36it/s] 43%|████▎     | 338/785 [02:23<02:13,  3.35it/s] 43%|████▎     | 339/785 [02:24<02:12,  3.36it/s] 43%|████▎     | 340/785 [02:24<02:12,  3.37it/s] 43%|████▎     | 341/785 [02:24<02:11,  3.37it/s] 44%|████▎     | 342/785 [02:25<02:11,  3.38it/s] 44%|████▎     | 343/785 [02:25<02:10,  3.38it/s] 44%|████▍     | 344/785 [02:25<02:10,  3.38it/s] 44%|████▍     | 345/785 [02:25<02:10,  3.38it/s] 44%|████▍     | 346/785 [02:26<02:09,  3.38it/s] 44%|████▍     | 347/785 [02:26<02:09,  3.38it/s] 44%|████▍     | 348/785 [02:26<02:09,  3.38it/s] 44%|████▍     | 349/785 [02:27<02:09,  3.37it/s] 45%|████▍     | 350/785 [02:27<02:09,  3.37it/s] 45%|████▍     | 351/785 [02:27<02:08,  3.38it/s] 45%|████▍     | 352/785 [02:28<02:08,  3.38it/s] 45%|████▍     | 353/785 [02:28<02:07,  3.39it/s] 45%|████▌     | 354/785 [02:28<02:06,  3.40it/s] 45%|████▌     | 355/785 [02:28<02:05,  3.42it/s] 45%|████▌     | 356/785 [02:29<02:05,  3.42it/s] 45%|████▌     | 357/785 [02:29<02:04,  3.43it/s] 46%|████▌     | 358/785 [02:29<02:04,  3.43it/s] 46%|████▌     | 359/785 [02:30<02:04,  3.43it/s] 46%|████▌     | 360/785 [02:30<02:04,  3.42it/s] 46%|████▌     | 361/785 [02:30<02:03,  3.42it/s] 46%|████▌     | 362/785 [02:30<02:03,  3.43it/s] 46%|████▌     | 363/785 [02:31<02:02,  3.43it/s] 46%|████▋     | 364/785 [02:31<02:02,  3.43it/s] 46%|████▋     | 365/785 [02:31<02:02,  3.43it/s] 47%|████▋     | 366/785 [02:32<02:01,  3.44it/s] 47%|████▋     | 367/785 [02:32<02:01,  3.43it/s] 47%|████▋     | 368/785 [02:32<02:01,  3.43it/s] 47%|████▋     | 369/785 [02:32<02:01,  3.44it/s] 47%|████▋     | 370/785 [02:33<02:00,  3.43it/s] 47%|████▋     | 371/785 [02:33<02:00,  3.42it/s] 47%|████▋     | 372/785 [02:33<02:00,  3.43it/s] 48%|████▊     | 373/785 [02:34<02:00,  3.43it/s] 48%|████▊     | 374/785 [02:34<01:59,  3.43it/s] 48%|████▊     | 375/785 [02:34<01:59,  3.43it/s] 48%|████▊     | 376/785 [02:35<01:59,  3.43it/s] 48%|████▊     | 377/785 [02:35<01:58,  3.43it/s] 48%|████▊     | 378/785 [02:35<01:58,  3.43it/s] 48%|████▊     | 379/785 [02:35<01:58,  3.43it/s] 48%|████▊     | 380/785 [02:36<01:57,  3.43it/s] 49%|████▊     | 381/785 [02:36<01:57,  3.43it/s] 49%|████▊     | 382/785 [02:36<01:57,  3.42it/s] 49%|████▉     | 383/785 [02:37<01:57,  3.42it/s] 49%|████▉     | 384/785 [02:37<01:57,  3.43it/s] 49%|████▉     | 385/785 [02:37<01:56,  3.43it/s] 49%|████▉     | 386/785 [02:37<01:56,  3.43it/s] 49%|████▉     | 387/785 [02:38<01:56,  3.43it/s] 49%|████▉     | 388/785 [02:38<01:55,  3.43it/s] 50%|████▉     | 389/785 [02:38<01:55,  3.43it/s] 50%|████▉     | 390/785 [02:39<01:55,  3.43it/s] 50%|████▉     | 391/785 [02:39<01:54,  3.43it/s] 50%|████▉     | 392/785 [02:39<01:54,  3.43it/s] 50%|█████     | 393/785 [02:39<01:54,  3.43it/s] 50%|█████     | 394/785 [02:40<01:53,  3.43it/s] 50%|█████     | 395/785 [02:40<01:53,  3.43it/s] 50%|█████     | 396/785 [02:40<01:53,  3.43it/s] 51%|█████     | 397/785 [02:41<01:53,  3.42it/s] 51%|█████     | 398/785 [02:41<01:52,  3.43it/s] 51%|█████     | 399/785 [02:41<01:52,  3.43it/s] 51%|█████     | 400/785 [02:42<01:52,  3.43it/s] 51%|█████     | 401/785 [02:42<01:51,  3.43it/s] 51%|█████     | 402/785 [02:42<01:51,  3.43it/s] 51%|█████▏    | 403/785 [02:42<01:51,  3.43it/s] 51%|█████▏    | 404/785 [02:43<01:51,  3.43it/s] 52%|█████▏    | 405/785 [02:43<01:50,  3.43it/s] 52%|█████▏    | 406/785 [02:43<01:50,  3.43it/s] 52%|█████▏    | 407/785 [02:44<01:50,  3.43it/s] 52%|█████▏    | 408/785 [02:44<01:49,  3.43it/s] 52%|█████▏    | 409/785 [02:44<01:49,  3.43it/s] 52%|█████▏    | 410/785 [02:44<01:49,  3.43it/s] 52%|█████▏    | 411/785 [02:45<01:48,  3.43it/s] 52%|█████▏    | 412/785 [02:45<01:48,  3.43it/s] 53%|█████▎    | 413/785 [02:45<01:48,  3.43it/s] 53%|█████▎    | 414/785 [02:46<01:48,  3.43it/s] 53%|█████▎    | 415/785 [02:46<01:47,  3.43it/s] 53%|█████▎    | 416/785 [02:46<01:47,  3.43it/s] 53%|█████▎    | 417/785 [02:46<01:47,  3.43it/s] 53%|█████▎    | 418/785 [02:47<01:47,  3.42it/s] 53%|█████▎    | 419/785 [02:47<01:46,  3.42it/s] 54%|█████▎    | 420/785 [02:47<01:46,  3.43it/s] 54%|█████▎    | 421/785 [02:48<01:46,  3.43it/s] 54%|█████▍    | 422/785 [02:48<01:45,  3.43it/s] 54%|█████▍    | 423/785 [02:48<01:45,  3.42it/s] 54%|█████▍    | 424/785 [02:49<01:45,  3.42it/s] 54%|█████▍    | 425/785 [02:49<01:45,  3.43it/s] 54%|█████▍    | 426/785 [02:49<01:44,  3.43it/s] 54%|█████▍    | 427/785 [02:49<01:44,  3.43it/s] 55%|█████▍    | 428/785 [02:50<01:44,  3.43it/s] 55%|█████▍    | 429/785 [02:50<01:46,  3.33it/s] 55%|█████▍    | 430/785 [02:50<01:45,  3.36it/s] 55%|█████▍    | 431/785 [02:51<01:44,  3.38it/s] 55%|█████▌    | 432/785 [02:51<01:43,  3.40it/s] 55%|█████▌    | 433/785 [02:51<01:43,  3.41it/s] 55%|█████▌    | 434/785 [02:51<01:42,  3.42it/s] 55%|█████▌    | 435/785 [02:52<01:42,  3.42it/s] 56%|█████▌    | 436/785 [02:52<01:41,  3.42it/s] 56%|█████▌    | 437/785 [02:52<01:41,  3.42it/s] 56%|█████▌    | 438/785 [02:53<01:41,  3.43it/s] 56%|█████▌    | 439/785 [02:53<01:40,  3.43it/s] 56%|█████▌    | 440/785 [02:53<01:40,  3.42it/s] 56%|█████▌    | 441/785 [02:54<01:40,  3.42it/s] 56%|█████▋    | 442/785 [02:54<01:40,  3.43it/s] 56%|█████▋    | 443/785 [02:54<01:39,  3.43it/s] 57%|█████▋    | 444/785 [02:54<01:39,  3.43it/s] 57%|█████▋    | 445/785 [02:55<01:39,  3.43it/s] 57%|█████▋    | 446/785 [02:55<01:38,  3.43it/s] 57%|█████▋    | 447/785 [02:55<01:38,  3.43it/s] 57%|█████▋    | 448/785 [02:56<01:38,  3.43it/s] 57%|█████▋    | 449/785 [02:56<01:37,  3.43it/s] 57%|█████▋    | 450/785 [02:56<01:37,  3.43it/s] 57%|█████▋    | 451/785 [02:56<01:38,  3.40it/s] 58%|█████▊    | 452/785 [02:57<01:37,  3.41it/s] 58%|█████▊    | 453/785 [02:57<01:37,  3.42it/s] 58%|█████▊    | 454/785 [02:57<01:36,  3.41it/s] 58%|█████▊    | 455/785 [02:58<01:36,  3.42it/s] 58%|█████▊    | 456/785 [02:58<01:36,  3.42it/s] 58%|█████▊    | 457/785 [02:58<01:35,  3.43it/s] 58%|█████▊    | 458/785 [02:58<01:35,  3.43it/s] 58%|█████▊    | 459/785 [02:59<01:35,  3.42it/s] 59%|█████▊    | 460/785 [02:59<01:34,  3.43it/s] 59%|█████▊    | 461/785 [02:59<01:34,  3.43it/s] 59%|█████▉    | 462/785 [03:00<01:34,  3.42it/s] 59%|█████▉    | 463/785 [03:00<01:34,  3.42it/s] 59%|█████▉    | 464/785 [03:00<01:33,  3.42it/s] 59%|█████▉    | 465/785 [03:01<01:33,  3.43it/s] 59%|█████▉    | 466/785 [03:01<01:33,  3.43it/s] 59%|█████▉    | 467/785 [03:01<01:32,  3.43it/s] 60%|█████▉    | 468/785 [03:01<01:32,  3.43it/s] 60%|█████▉    | 469/785 [03:02<01:32,  3.43it/s] 60%|█████▉    | 470/785 [03:02<01:31,  3.43it/s] 60%|██████    | 471/785 [03:02<01:30,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 05:51:17,128 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:51:17,128 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 05:51:17,128 >>   Batch size = 8
{'eval_loss': 0.9128285646438599, 'eval_runtime': 16.84, 'eval_samples_per_second': 348.218, 'eval_steps_per_second': 43.527, 'epoch': 2.0}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 55.59it/s][A
  2%|▏         | 12/733 [00:00<00:15, 47.70it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.54it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.74it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.22it/s][A
  4%|▍         | 32/733 [00:00<00:15, 43.91it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.76it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.67it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.71it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.87it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.80it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.66it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.53it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.43it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.32it/s][A
 11%|█         | 82/733 [00:01<00:15, 43.39it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.47it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.66it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.67it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.65it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.57it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.43it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.41it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.46it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.46it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.60it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.66it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.64it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.59it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.47it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.43it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.37it/s][A
 23%|██▎       | 167/733 [00:03<00:13, 43.45it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.53it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.60it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.65it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.52it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.53it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.35it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.40it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.38it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.52it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.66it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.65it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.57it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.49it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.56it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.48it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.35it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.50it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.62it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.70it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.60it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.57it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.65it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.51it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.37it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.41it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.59it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.69it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.64it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.61it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.68it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.49it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.45it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.35it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.46it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.54it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.62it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.68it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.63it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.60it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.52it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.43it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.47it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.42it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.59it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.67it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.57it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.59it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.50it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.48it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.48it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.40it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.20it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.60it/s][A
 60%|█████▉    | 437/733 [00:10<00:06, 43.71it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.69it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.56it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.52it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.40it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.42it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.46it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.59it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.74it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.58it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.59it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.55it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.46it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.40it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.33it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.51it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.67it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.63it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.66it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.57it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.50it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.49it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.36it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.44it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.60it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.57it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.52it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.66it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.57it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.52it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.40it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.33it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.51it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.59it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.62it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.59it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.66it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.52it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.48it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.43it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.37it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.32it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.29it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.73it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.68it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.55it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.55it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.54it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.40it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.41it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.52it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.62it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.64it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.59it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.57it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.43it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.41it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.40it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.37it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.59it/s][A                                                 
                                                 [A 60%|██████    | 471/785 [03:19<01:30,  3.47it/s]
100%|██████████| 733/733 [00:16<00:00, 43.59it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:51:34,000 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-29 05:51:34,022 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:51:35,757 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:51:35,783 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:51:35,802 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:24<35:38,  6.83s/it] 60%|██████    | 473/785 [03:25<25:19,  4.87s/it] 60%|██████    | 474/785 [03:25<18:07,  3.50s/it] 61%|██████    | 475/785 [03:25<13:06,  2.54s/it] 61%|██████    | 476/785 [03:26<09:36,  1.86s/it] 61%|██████    | 477/785 [03:26<07:09,  1.39s/it] 61%|██████    | 478/785 [03:26<05:26,  1.06s/it] 61%|██████    | 479/785 [03:26<04:15,  1.20it/s] 61%|██████    | 480/785 [03:27<03:25,  1.49it/s] 61%|██████▏   | 481/785 [03:27<02:49,  1.79it/s] 61%|██████▏   | 482/785 [03:27<02:25,  2.08it/s] 62%|██████▏   | 483/785 [03:28<02:08,  2.36it/s] 62%|██████▏   | 484/785 [03:28<01:56,  2.58it/s] 62%|██████▏   | 485/785 [03:28<01:47,  2.78it/s] 62%|██████▏   | 486/785 [03:29<01:41,  2.94it/s] 62%|██████▏   | 487/785 [03:29<01:37,  3.06it/s] 62%|██████▏   | 488/785 [03:29<01:34,  3.15it/s] 62%|██████▏   | 489/785 [03:29<01:31,  3.22it/s] 62%|██████▏   | 490/785 [03:30<01:30,  3.27it/s] 63%|██████▎   | 491/785 [03:30<01:29,  3.30it/s] 63%|██████▎   | 492/785 [03:30<01:28,  3.32it/s] 63%|██████▎   | 493/785 [03:31<01:27,  3.34it/s] 63%|██████▎   | 494/785 [03:31<01:26,  3.36it/s] 63%|██████▎   | 495/785 [03:31<01:26,  3.35it/s] 63%|██████▎   | 496/785 [03:31<01:25,  3.36it/s] 63%|██████▎   | 497/785 [03:32<01:25,  3.37it/s] 63%|██████▎   | 498/785 [03:32<01:25,  3.38it/s] 64%|██████▎   | 499/785 [03:32<01:24,  3.38it/s] 64%|██████▎   | 500/785 [03:33<01:24,  3.38it/s]                                                  64%|██████▎   | 500/785 [03:33<01:24,  3.38it/s] 64%|██████▍   | 501/785 [03:33<01:24,  3.38it/s] 64%|██████▍   | 502/785 [03:33<01:23,  3.38it/s] 64%|██████▍   | 503/785 [03:34<01:23,  3.38it/s] 64%|██████▍   | 504/785 [03:34<01:23,  3.38it/s] 64%|██████▍   | 505/785 [03:34<01:22,  3.38it/s] 64%|██████▍   | 506/785 [03:34<01:22,  3.37it/s] 65%|██████▍   | 507/785 [03:35<01:22,  3.38it/s] 65%|██████▍   | 508/785 [03:35<01:22,  3.38it/s] 65%|██████▍   | 509/785 [03:35<01:21,  3.38it/s] 65%|██████▍   | 510/785 [03:36<01:21,  3.38it/s] 65%|██████▌   | 511/785 [03:36<01:20,  3.39it/s] 65%|██████▌   | 512/785 [03:36<01:20,  3.38it/s] 65%|██████▌   | 513/785 [03:36<01:20,  3.38it/s] 65%|██████▌   | 514/785 [03:37<01:20,  3.38it/s] 66%|██████▌   | 515/785 [03:37<01:19,  3.38it/s] 66%|██████▌   | 516/785 [03:37<01:19,  3.38it/s] 66%|██████▌   | 517/785 [03:38<01:19,  3.38it/s] 66%|██████▌   | 518/785 [03:38<01:19,  3.38it/s] 66%|██████▌   | 519/785 [03:38<01:18,  3.38it/s] 66%|██████▌   | 520/785 [03:39<01:18,  3.38it/s] 66%|██████▋   | 521/785 [03:39<01:18,  3.38it/s] 66%|██████▋   | 522/785 [03:39<01:17,  3.38it/s] 67%|██████▋   | 523/785 [03:39<01:17,  3.38it/s] 67%|██████▋   | 524/785 [03:40<01:17,  3.38it/s] 67%|██████▋   | 525/785 [03:40<01:16,  3.38it/s] 67%|██████▋   | 526/785 [03:40<01:16,  3.38it/s] 67%|██████▋   | 527/785 [03:41<01:16,  3.38it/s] 67%|██████▋   | 528/785 [03:41<01:16,  3.37it/s] 67%|██████▋   | 529/785 [03:41<01:15,  3.37it/s] 68%|██████▊   | 530/785 [03:42<01:15,  3.37it/s] 68%|██████▊   | 531/785 [03:42<01:15,  3.38it/s] 68%|██████▊   | 532/785 [03:42<01:14,  3.38it/s] 68%|██████▊   | 533/785 [03:42<01:14,  3.38it/s] 68%|██████▊   | 534/785 [03:43<01:14,  3.38it/s] 68%|██████▊   | 535/785 [03:43<01:13,  3.38it/s] 68%|██████▊   | 536/785 [03:43<01:13,  3.38it/s] 68%|██████▊   | 537/785 [03:44<01:13,  3.38it/s] 69%|██████▊   | 538/785 [03:44<01:13,  3.38it/s] 69%|██████▊   | 539/785 [03:44<01:12,  3.37it/s] 69%|██████▉   | 540/785 [03:44<01:12,  3.37it/s] 69%|██████▉   | 541/785 [03:45<01:12,  3.38it/s] 69%|██████▉   | 542/785 [03:45<01:11,  3.38it/s] 69%|██████▉   | 543/785 [03:45<01:11,  3.38it/s] 69%|██████▉   | 544/785 [03:46<01:11,  3.38it/s] 69%|██████▉   | 545/785 [03:46<01:10,  3.38it/s] 70%|██████▉   | 546/785 [03:46<01:10,  3.38it/s] 70%|██████▉   | 547/785 [03:47<01:10,  3.38it/s] 70%|██████▉   | 548/785 [03:47<01:10,  3.38it/s] 70%|██████▉   | 549/785 [03:47<01:09,  3.38it/s] 70%|███████   | 550/785 [03:47<01:09,  3.38it/s] 70%|███████   | 551/785 [03:48<01:09,  3.38it/s] 70%|███████   | 552/785 [03:48<01:08,  3.38it/s] 70%|███████   | 553/785 [03:48<01:08,  3.36it/s] 71%|███████   | 554/785 [03:49<01:08,  3.37it/s] 71%|███████   | 555/785 [03:49<01:08,  3.37it/s] 71%|███████   | 556/785 [03:49<01:07,  3.38it/s] 71%|███████   | 557/785 [03:50<01:07,  3.38it/s] 71%|███████   | 558/785 [03:50<01:07,  3.38it/s] 71%|███████   | 559/785 [03:50<01:08,  3.30it/s] 71%|███████▏  | 560/785 [03:50<01:07,  3.32it/s] 71%|███████▏  | 561/785 [03:51<01:07,  3.34it/s] 72%|███████▏  | 562/785 [03:51<01:06,  3.35it/s] 72%|███████▏  | 563/785 [03:51<01:06,  3.36it/s] 72%|███████▏  | 564/785 [03:52<01:05,  3.36it/s] 72%|███████▏  | 565/785 [03:52<01:05,  3.36it/s] 72%|███████▏  | 566/785 [03:52<01:05,  3.36it/s] 72%|███████▏  | 567/785 [03:53<01:04,  3.37it/s] 72%|███████▏  | 568/785 [03:53<01:04,  3.37it/s] 72%|███████▏  | 569/785 [03:53<01:03,  3.38it/s] 73%|███████▎  | 570/785 [03:53<01:03,  3.38it/s] 73%|███████▎  | 571/785 [03:54<01:03,  3.38it/s] 73%|███████▎  | 572/785 [03:54<01:02,  3.38it/s] 73%|███████▎  | 573/785 [03:54<01:02,  3.38it/s] 73%|███████▎  | 574/785 [03:55<01:02,  3.38it/s] 73%|███████▎  | 575/785 [03:55<01:02,  3.37it/s] 73%|███████▎  | 576/785 [03:55<01:01,  3.38it/s] 74%|███████▎  | 577/785 [03:55<01:01,  3.38it/s] 74%|███████▎  | 578/785 [03:56<01:01,  3.38it/s] 74%|███████▍  | 579/785 [03:56<01:00,  3.38it/s] 74%|███████▍  | 580/785 [03:56<01:00,  3.38it/s] 74%|███████▍  | 581/785 [03:57<01:00,  3.38it/s] 74%|███████▍  | 582/785 [03:57<00:59,  3.38it/s] 74%|███████▍  | 583/785 [03:57<00:59,  3.38it/s] 74%|███████▍  | 584/785 [03:58<00:59,  3.38it/s] 75%|███████▍  | 585/785 [03:58<00:59,  3.38it/s] 75%|███████▍  | 586/785 [03:58<00:59,  3.35it/s] 75%|███████▍  | 587/785 [03:58<00:58,  3.36it/s] 75%|███████▍  | 588/785 [03:59<00:58,  3.37it/s] 75%|███████▌  | 589/785 [03:59<00:58,  3.37it/s] 75%|███████▌  | 590/785 [03:59<00:57,  3.38it/s] 75%|███████▌  | 591/785 [04:00<00:57,  3.37it/s] 75%|███████▌  | 592/785 [04:00<00:57,  3.38it/s] 76%|███████▌  | 593/785 [04:00<00:56,  3.38it/s] 76%|███████▌  | 594/785 [04:00<00:56,  3.38it/s] 76%|███████▌  | 595/785 [04:01<00:56,  3.38it/s] 76%|███████▌  | 596/785 [04:01<00:55,  3.38it/s] 76%|███████▌  | 597/785 [04:01<00:56,  3.35it/s] 76%|███████▌  | 598/785 [04:02<00:55,  3.36it/s] 76%|███████▋  | 599/785 [04:02<00:55,  3.37it/s] 76%|███████▋  | 600/785 [04:02<00:54,  3.37it/s] 77%|███████▋  | 601/785 [04:03<00:54,  3.38it/s] 77%|███████▋  | 602/785 [04:03<00:54,  3.38it/s] 77%|███████▋  | 603/785 [04:03<00:53,  3.38it/s] 77%|███████▋  | 604/785 [04:03<00:53,  3.38it/s] 77%|███████▋  | 605/785 [04:04<00:53,  3.38it/s] 77%|███████▋  | 606/785 [04:04<00:52,  3.38it/s] 77%|███████▋  | 607/785 [04:04<00:52,  3.38it/s] 77%|███████▋  | 608/785 [04:05<00:52,  3.37it/s] 78%|███████▊  | 609/785 [04:05<00:52,  3.38it/s] 78%|███████▊  | 610/785 [04:05<00:51,  3.38it/s] 78%|███████▊  | 611/785 [04:06<00:51,  3.38it/s] 78%|███████▊  | 612/785 [04:06<00:51,  3.38it/s] 78%|███████▊  | 613/785 [04:06<00:50,  3.38it/s] 78%|███████▊  | 614/785 [04:06<00:50,  3.38it/s] 78%|███████▊  | 615/785 [04:07<00:50,  3.38it/s] 78%|███████▊  | 616/785 [04:07<00:49,  3.38it/s] 79%|███████▊  | 617/785 [04:07<00:49,  3.39it/s] 79%|███████▊  | 618/785 [04:08<00:49,  3.41it/s] 79%|███████▉  | 619/785 [04:08<00:48,  3.40it/s] 79%|███████▉  | 620/785 [04:08<00:48,  3.41it/s] 79%|███████▉  | 621/785 [04:08<00:47,  3.42it/s] 79%|███████▉  | 622/785 [04:09<00:47,  3.42it/s] 79%|███████▉  | 623/785 [04:09<00:47,  3.42it/s] 79%|███████▉  | 624/785 [04:09<00:46,  3.43it/s] 80%|███████▉  | 625/785 [04:10<00:46,  3.43it/s] 80%|███████▉  | 626/785 [04:10<00:46,  3.43it/s] 80%|███████▉  | 627/785 [04:10<00:46,  3.43it/s] 80%|████████  | 628/785 [04:10<00:45,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 05:52:25,353 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:52:25,353 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 05:52:25,353 >>   Batch size = 8
{'eval_loss': 0.9138461947441101, 'eval_runtime': 16.841, 'eval_samples_per_second': 348.198, 'eval_steps_per_second': 43.525, 'epoch': 3.0}
{'loss': 0.7578, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 55.16it/s][A
  2%|▏         | 12/733 [00:00<00:15, 46.98it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.27it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.48it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.22it/s][A
  4%|▍         | 32/733 [00:00<00:15, 43.98it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.84it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.73it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.78it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.83it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.61it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.56it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.55it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.48it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.59it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.59it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.52it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.69it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.71it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.66it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.66it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.51it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.48it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.41it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.59it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.64it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.61it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.64it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.64it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.57it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.46it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.38it/s][A
 23%|██▎       | 167/733 [00:03<00:12, 43.54it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.56it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.44it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.70it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.68it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.61it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.50it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.49it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.46it/s][A
 29%|██▉       | 212/733 [00:04<00:12, 43.41it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.51it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.55it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.65it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.56it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.52it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.58it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.61it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.56it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.52it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.58it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.57it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.64it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.59it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.51it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.55it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.55it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.51it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.54it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.60it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.56it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.55it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.57it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.51it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.47it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.58it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.56it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.58it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.61it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.63it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.65it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.54it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.46it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.56it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.61it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.60it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.50it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.52it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.58it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.62it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.53it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.58it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.53it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.53it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.54it/s][A
 60%|█████▉    | 437/733 [00:10<00:06, 43.49it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.56it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.58it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.57it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.53it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.55it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.58it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.64it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.61it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.46it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.49it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.49it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.57it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.51it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.55it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.54it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.63it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.56it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.50it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.55it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.50it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.52it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.58it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.64it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.61it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.57it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.55it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.56it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.55it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.39it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.37it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.62it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.65it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.61it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.61it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.56it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.57it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.58it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.54it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.55it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.52it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.47it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.61it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.61it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.55it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.52it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.55it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.52it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.58it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.60it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.62it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.60it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.56it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.53it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.58it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.56it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.51it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.50it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.60it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.54it/s][A                                                 
                                                 [A 80%|████████  | 628/785 [04:27<00:45,  3.48it/s]
100%|██████████| 733/733 [00:16<00:00, 43.54it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:52:42,212 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-29 05:52:42,230 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:52:43,853 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:52:43,876 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:52:43,883 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:33<17:56,  6.90s/it] 80%|████████  | 630/785 [04:33<12:42,  4.92s/it] 80%|████████  | 631/785 [04:33<09:03,  3.53s/it] 81%|████████  | 632/785 [04:34<06:31,  2.56s/it] 81%|████████  | 633/785 [04:34<04:45,  1.88s/it] 81%|████████  | 634/785 [04:34<03:32,  1.41s/it] 81%|████████  | 635/785 [04:35<02:40,  1.07s/it] 81%|████████  | 636/785 [04:35<02:05,  1.19it/s] 81%|████████  | 637/785 [04:35<01:40,  1.48it/s] 81%|████████▏ | 638/785 [04:35<01:22,  1.78it/s] 81%|████████▏ | 639/785 [04:36<01:10,  2.07it/s] 82%|████████▏ | 640/785 [04:36<01:01,  2.35it/s] 82%|████████▏ | 641/785 [04:36<00:55,  2.58it/s] 82%|████████▏ | 642/785 [04:37<00:51,  2.78it/s] 82%|████████▏ | 643/785 [04:37<00:48,  2.93it/s] 82%|████████▏ | 644/785 [04:37<00:46,  3.05it/s] 82%|████████▏ | 645/785 [04:38<00:44,  3.15it/s] 82%|████████▏ | 646/785 [04:38<00:43,  3.21it/s] 82%|████████▏ | 647/785 [04:38<00:42,  3.26it/s] 83%|████████▎ | 648/785 [04:38<00:41,  3.30it/s] 83%|████████▎ | 649/785 [04:39<00:40,  3.33it/s] 83%|████████▎ | 650/785 [04:39<00:40,  3.34it/s] 83%|████████▎ | 651/785 [04:39<00:39,  3.36it/s] 83%|████████▎ | 652/785 [04:40<00:39,  3.35it/s] 83%|████████▎ | 653/785 [04:40<00:39,  3.36it/s] 83%|████████▎ | 654/785 [04:40<00:38,  3.37it/s] 83%|████████▎ | 655/785 [04:41<00:38,  3.37it/s] 84%|████████▎ | 656/785 [04:41<00:38,  3.37it/s] 84%|████████▎ | 657/785 [04:41<00:37,  3.38it/s] 84%|████████▍ | 658/785 [04:41<00:37,  3.38it/s] 84%|████████▍ | 659/785 [04:42<00:37,  3.38it/s] 84%|████████▍ | 660/785 [04:42<00:37,  3.37it/s] 84%|████████▍ | 661/785 [04:42<00:36,  3.37it/s] 84%|████████▍ | 662/785 [04:43<00:36,  3.38it/s] 84%|████████▍ | 663/785 [04:43<00:36,  3.37it/s] 85%|████████▍ | 664/785 [04:43<00:35,  3.38it/s] 85%|████████▍ | 665/785 [04:43<00:35,  3.38it/s] 85%|████████▍ | 666/785 [04:44<00:35,  3.38it/s] 85%|████████▍ | 667/785 [04:44<00:34,  3.38it/s] 85%|████████▌ | 668/785 [04:44<00:34,  3.38it/s] 85%|████████▌ | 669/785 [04:45<00:34,  3.38it/s] 85%|████████▌ | 670/785 [04:45<00:33,  3.38it/s] 85%|████████▌ | 671/785 [04:45<00:33,  3.38it/s] 86%|████████▌ | 672/785 [04:46<00:33,  3.38it/s] 86%|████████▌ | 673/785 [04:46<00:33,  3.38it/s] 86%|████████▌ | 674/785 [04:46<00:32,  3.38it/s] 86%|████████▌ | 675/785 [04:46<00:32,  3.38it/s] 86%|████████▌ | 676/785 [04:47<00:32,  3.38it/s] 86%|████████▌ | 677/785 [04:47<00:31,  3.38it/s] 86%|████████▋ | 678/785 [04:47<00:31,  3.38it/s] 86%|████████▋ | 679/785 [04:48<00:31,  3.38it/s] 87%|████████▋ | 680/785 [04:48<00:31,  3.38it/s] 87%|████████▋ | 681/785 [04:48<00:30,  3.38it/s] 87%|████████▋ | 682/785 [04:49<00:30,  3.38it/s] 87%|████████▋ | 683/785 [04:49<00:30,  3.38it/s] 87%|████████▋ | 684/785 [04:49<00:29,  3.38it/s] 87%|████████▋ | 685/785 [04:49<00:29,  3.38it/s] 87%|████████▋ | 686/785 [04:50<00:29,  3.37it/s] 88%|████████▊ | 687/785 [04:50<00:29,  3.37it/s] 88%|████████▊ | 688/785 [04:50<00:29,  3.27it/s] 88%|████████▊ | 689/785 [04:51<00:29,  3.30it/s] 88%|████████▊ | 690/785 [04:51<00:28,  3.32it/s] 88%|████████▊ | 691/785 [04:51<00:28,  3.34it/s] 88%|████████▊ | 692/785 [04:51<00:27,  3.36it/s] 88%|████████▊ | 693/785 [04:52<00:27,  3.36it/s] 88%|████████▊ | 694/785 [04:52<00:27,  3.37it/s] 89%|████████▊ | 695/785 [04:52<00:26,  3.37it/s] 89%|████████▊ | 696/785 [04:53<00:26,  3.38it/s] 89%|████████▉ | 697/785 [04:53<00:26,  3.37it/s] 89%|████████▉ | 698/785 [04:53<00:25,  3.38it/s] 89%|████████▉ | 699/785 [04:54<00:25,  3.38it/s] 89%|████████▉ | 700/785 [04:54<00:25,  3.38it/s] 89%|████████▉ | 701/785 [04:54<00:24,  3.38it/s] 89%|████████▉ | 702/785 [04:54<00:24,  3.38it/s] 90%|████████▉ | 703/785 [04:55<00:24,  3.38it/s] 90%|████████▉ | 704/785 [04:55<00:23,  3.38it/s] 90%|████████▉ | 705/785 [04:55<00:23,  3.38it/s] 90%|████████▉ | 706/785 [04:56<00:23,  3.38it/s] 90%|█████████ | 707/785 [04:56<00:23,  3.38it/s] 90%|█████████ | 708/785 [04:56<00:22,  3.37it/s] 90%|█████████ | 709/785 [04:57<00:22,  3.37it/s] 90%|█████████ | 710/785 [04:57<00:22,  3.37it/s] 91%|█████████ | 711/785 [04:57<00:21,  3.38it/s] 91%|█████████ | 712/785 [04:57<00:21,  3.38it/s] 91%|█████████ | 713/785 [04:58<00:21,  3.38it/s] 91%|█████████ | 714/785 [04:58<00:21,  3.38it/s] 91%|█████████ | 715/785 [04:58<00:20,  3.38it/s] 91%|█████████ | 716/785 [04:59<00:20,  3.38it/s] 91%|█████████▏| 717/785 [04:59<00:20,  3.38it/s] 91%|█████████▏| 718/785 [04:59<00:19,  3.38it/s] 92%|█████████▏| 719/785 [04:59<00:19,  3.37it/s] 92%|█████████▏| 720/785 [05:00<00:19,  3.37it/s] 92%|█████████▏| 721/785 [05:00<00:18,  3.37it/s] 92%|█████████▏| 722/785 [05:00<00:18,  3.38it/s] 92%|█████████▏| 723/785 [05:01<00:18,  3.38it/s] 92%|█████████▏| 724/785 [05:01<00:18,  3.38it/s] 92%|█████████▏| 725/785 [05:01<00:17,  3.38it/s] 92%|█████████▏| 726/785 [05:02<00:17,  3.39it/s] 93%|█████████▎| 727/785 [05:02<00:17,  3.38it/s] 93%|█████████▎| 728/785 [05:02<00:16,  3.38it/s] 93%|█████████▎| 729/785 [05:02<00:16,  3.39it/s] 93%|█████████▎| 730/785 [05:03<00:16,  3.37it/s] 93%|█████████▎| 731/785 [05:03<00:16,  3.37it/s] 93%|█████████▎| 732/785 [05:03<00:15,  3.38it/s] 93%|█████████▎| 733/785 [05:04<00:15,  3.38it/s] 94%|█████████▎| 734/785 [05:04<00:15,  3.38it/s] 94%|█████████▎| 735/785 [05:04<00:14,  3.38it/s] 94%|█████████▍| 736/785 [05:05<00:14,  3.38it/s] 94%|█████████▍| 737/785 [05:05<00:14,  3.38it/s] 94%|█████████▍| 738/785 [05:05<00:13,  3.39it/s] 94%|█████████▍| 739/785 [05:05<00:13,  3.39it/s] 94%|█████████▍| 740/785 [05:06<00:13,  3.39it/s] 94%|█████████▍| 741/785 [05:06<00:13,  3.38it/s] 95%|█████████▍| 742/785 [05:06<00:12,  3.38it/s] 95%|█████████▍| 743/785 [05:07<00:12,  3.38it/s] 95%|█████████▍| 744/785 [05:07<00:12,  3.38it/s] 95%|█████████▍| 745/785 [05:07<00:11,  3.38it/s] 95%|█████████▌| 746/785 [05:07<00:11,  3.38it/s] 95%|█████████▌| 747/785 [05:08<00:11,  3.38it/s] 95%|█████████▌| 748/785 [05:08<00:10,  3.38it/s] 95%|█████████▌| 749/785 [05:08<00:10,  3.38it/s] 96%|█████████▌| 750/785 [05:09<00:10,  3.38it/s] 96%|█████████▌| 751/785 [05:09<00:10,  3.38it/s] 96%|█████████▌| 752/785 [05:09<00:09,  3.37it/s] 96%|█████████▌| 753/785 [05:10<00:09,  3.37it/s] 96%|█████████▌| 754/785 [05:10<00:09,  3.38it/s] 96%|█████████▌| 755/785 [05:10<00:08,  3.38it/s] 96%|█████████▋| 756/785 [05:10<00:08,  3.38it/s] 96%|█████████▋| 757/785 [05:11<00:08,  3.38it/s] 97%|█████████▋| 758/785 [05:11<00:07,  3.38it/s] 97%|█████████▋| 759/785 [05:11<00:07,  3.38it/s] 97%|█████████▋| 760/785 [05:12<00:07,  3.38it/s] 97%|█████████▋| 761/785 [05:12<00:07,  3.38it/s] 97%|█████████▋| 762/785 [05:12<00:06,  3.38it/s] 97%|█████████▋| 763/785 [05:13<00:06,  3.37it/s] 97%|█████████▋| 764/785 [05:13<00:06,  3.37it/s] 97%|█████████▋| 765/785 [05:13<00:05,  3.38it/s] 98%|█████████▊| 766/785 [05:13<00:05,  3.38it/s] 98%|█████████▊| 767/785 [05:14<00:05,  3.38it/s] 98%|█████████▊| 768/785 [05:14<00:05,  3.38it/s] 98%|█████████▊| 769/785 [05:14<00:04,  3.38it/s] 98%|█████████▊| 770/785 [05:15<00:04,  3.38it/s] 98%|█████████▊| 771/785 [05:15<00:04,  3.39it/s] 98%|█████████▊| 772/785 [05:15<00:03,  3.38it/s] 98%|█████████▊| 773/785 [05:15<00:03,  3.37it/s] 99%|█████████▊| 774/785 [05:16<00:03,  3.36it/s] 99%|█████████▊| 775/785 [05:16<00:02,  3.37it/s] 99%|█████████▉| 776/785 [05:16<00:02,  3.37it/s] 99%|█████████▉| 777/785 [05:17<00:02,  3.38it/s] 99%|█████████▉| 778/785 [05:17<00:02,  3.38it/s] 99%|█████████▉| 779/785 [05:17<00:01,  3.38it/s] 99%|█████████▉| 780/785 [05:18<00:01,  3.38it/s] 99%|█████████▉| 781/785 [05:18<00:01,  3.39it/s]100%|█████████▉| 782/785 [05:18<00:00,  3.38it/s]100%|█████████▉| 783/785 [05:18<00:00,  3.38it/s]100%|█████████▉| 784/785 [05:19<00:00,  3.38it/s]100%|██████████| 785/785 [05:19<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 05:53:33,860 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:53:33,860 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 05:53:33,860 >>   Batch size = 8
{'eval_loss': 0.9163805842399597, 'eval_runtime': 16.8313, 'eval_samples_per_second': 348.399, 'eval_steps_per_second': 43.55, 'epoch': 4.0}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 55.01it/s][A
  2%|▏         | 12/733 [00:00<00:15, 47.11it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.73it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.88it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.45it/s][A
  4%|▍         | 32/733 [00:00<00:15, 43.97it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.73it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.71it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.75it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.55it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.61it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.49it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.55it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.50it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.40it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.52it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.55it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.64it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.65it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.69it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.66it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.58it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.23it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.58it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.54it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.65it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.61it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.73it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.65it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.62it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.64it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.40it/s][A
 23%|██▎       | 167/733 [00:03<00:13, 43.46it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.48it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.69it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.65it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.58it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.60it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.66it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.57it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.50it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.53it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.48it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.52it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.63it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.67it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.64it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.61it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.58it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.56it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.52it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.48it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.47it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.64it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.66it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.57it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.50it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.56it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.56it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.46it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.53it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.63it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.60it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.57it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.61it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.58it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.60it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.55it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.45it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.63it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.60it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.30it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.50it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.46it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.49it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.55it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.53it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.56it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.49it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.50it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.61it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.62it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.51it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.57it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.60it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.38it/s][A
 60%|█████▉    | 437/733 [00:10<00:06, 43.53it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.57it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.63it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.65it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.61it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.52it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.53it/s][A
 64%|██████▍   | 472/733 [00:10<00:06, 43.49it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.49it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.48it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.58it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.57it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.60it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.62it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.63it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.53it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.54it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.54it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.53it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.54it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.57it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.65it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.66it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.54it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.63it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.63it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.55it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.49it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.58it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.57it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.62it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.65it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.48it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.57it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.51it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.52it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.59it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.51it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.57it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.64it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.74it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.63it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.52it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.52it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.51it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.51it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.52it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.51it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.62it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.63it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.60it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.67it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.62it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.49it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.53it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.60it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.57it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.62it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.57it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.65it/s][A                                                 
                                                 [A100%|██████████| 785/785 [05:36<00:00,  3.43it/s]
100%|██████████| 733/733 [00:16<00:00, 43.65it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:53:50,723 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-29 05:53:50,744 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:53:52,396 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:53:52,413 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:53:52,430 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 05:53:55,763 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 05:53:55,765 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-314 (score: 0.9128285646438599).
                                                 100%|██████████| 785/785 [05:43<00:00,  3.43it/s]100%|██████████| 785/785 [05:43<00:00,  2.29it/s]
[INFO|trainer.py:1894] 2023-08-29 05:53:57,562 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-29 05:53:57,577 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:53:59,401 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:53:59,428 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:53:59,440 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:53:59,624 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:53:59,624 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:53:59,624 >>   train_loss               =     0.7449
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:53:59,625 >>   train_runtime            = 0:05:43.20
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:53:59,625 >>   train_samples            =      10044
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:53:59,625 >>   train_samples_per_second =    146.327
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:53:59,625 >>   train_steps_per_second   =      2.287
{'eval_loss': 0.9192246198654175, 'eval_runtime': 16.8292, 'eval_samples_per_second': 348.441, 'eval_steps_per_second': 43.555, 'epoch': 5.0}
{'train_runtime': 343.2049, 'train_samples_per_second': 146.327, 'train_steps_per_second': 2.287, 'train_loss': 0.7448856815411027, 'epoch': 5.0}
08/29/2023 05:53:59 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 05:53:59,668 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:53:59,668 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 05:53:59,668 >>   Batch size = 8
  0%|          | 0/733 [00:00<?, ?it/s]  1%|          | 6/733 [00:00<00:13, 54.87it/s]  2%|▏         | 12/733 [00:00<00:15, 47.72it/s]  2%|▏         | 17/733 [00:00<00:15, 46.19it/s]  3%|▎         | 22/733 [00:00<00:15, 45.36it/s]  4%|▎         | 27/733 [00:00<00:15, 44.96it/s]  4%|▍         | 32/733 [00:00<00:15, 44.79it/s]  5%|▌         | 37/733 [00:00<00:15, 44.58it/s]  6%|▌         | 42/733 [00:00<00:15, 43.96it/s]  6%|▋         | 47/733 [00:01<00:15, 43.34it/s]  7%|▋         | 52/733 [00:01<00:15, 43.03it/s]  8%|▊         | 57/733 [00:01<00:15, 43.48it/s]  8%|▊         | 62/733 [00:01<00:15, 43.55it/s]  9%|▉         | 67/733 [00:01<00:15, 43.77it/s] 10%|▉         | 72/733 [00:01<00:15, 43.87it/s] 11%|█         | 77/733 [00:01<00:14, 43.97it/s] 11%|█         | 82/733 [00:01<00:14, 43.79it/s] 12%|█▏        | 87/733 [00:01<00:14, 43.49it/s] 13%|█▎        | 92/733 [00:02<00:14, 43.20it/s] 13%|█▎        | 97/733 [00:02<00:14, 43.15it/s] 14%|█▍        | 102/733 [00:02<00:14, 43.26it/s] 15%|█▍        | 107/733 [00:02<00:14, 43.47it/s] 15%|█▌        | 112/733 [00:02<00:14, 43.61it/s] 16%|█▌        | 117/733 [00:02<00:14, 43.71it/s] 17%|█▋        | 122/733 [00:02<00:13, 43.68it/s] 17%|█▋        | 127/733 [00:02<00:13, 43.74it/s] 18%|█▊        | 132/733 [00:02<00:13, 43.48it/s] 19%|█▊        | 137/733 [00:03<00:13, 43.29it/s] 19%|█▉        | 142/733 [00:03<00:13, 43.25it/s] 20%|██        | 147/733 [00:03<00:13, 43.41it/s] 21%|██        | 152/733 [00:03<00:13, 43.51it/s] 21%|██▏       | 157/733 [00:03<00:13, 43.63it/s] 22%|██▏       | 162/733 [00:03<00:13, 43.68it/s] 23%|██▎       | 167/733 [00:03<00:12, 43.75it/s] 23%|██▎       | 172/733 [00:03<00:12, 43.53it/s] 24%|██▍       | 177/733 [00:04<00:12, 43.39it/s] 25%|██▍       | 182/733 [00:04<00:12, 43.15it/s] 26%|██▌       | 187/733 [00:04<00:12, 43.29it/s] 26%|██▌       | 192/733 [00:04<00:12, 43.46it/s] 27%|██▋       | 197/733 [00:04<00:12, 43.61it/s] 28%|██▊       | 202/733 [00:04<00:12, 43.66it/s] 28%|██▊       | 207/733 [00:04<00:12, 43.78it/s] 29%|██▉       | 212/733 [00:04<00:11, 43.82it/s] 30%|██▉       | 217/733 [00:04<00:11, 43.61it/s] 30%|███       | 222/733 [00:05<00:11, 43.53it/s] 31%|███       | 227/733 [00:05<00:11, 43.53it/s] 32%|███▏      | 232/733 [00:05<00:11, 43.59it/s] 32%|███▏      | 237/733 [00:05<00:11, 43.67it/s] 33%|███▎      | 242/733 [00:05<00:11, 42.14it/s] 34%|███▎      | 247/733 [00:05<00:11, 42.75it/s] 34%|███▍      | 252/733 [00:05<00:11, 43.22it/s] 35%|███▌      | 257/733 [00:05<00:10, 43.46it/s] 36%|███▌      | 262/733 [00:05<00:10, 43.43it/s] 36%|███▋      | 267/733 [00:06<00:10, 43.26it/s] 37%|███▋      | 272/733 [00:06<00:10, 43.48it/s] 38%|███▊      | 277/733 [00:06<00:10, 43.59it/s] 38%|███▊      | 282/733 [00:06<00:10, 43.50it/s] 39%|███▉      | 287/733 [00:06<00:10, 43.44it/s] 40%|███▉      | 292/733 [00:06<00:10, 43.64it/s] 41%|████      | 297/733 [00:06<00:09, 43.89it/s] 41%|████      | 302/733 [00:06<00:09, 43.83it/s] 42%|████▏     | 307/733 [00:07<00:09, 43.81it/s] 43%|████▎     | 312/733 [00:07<00:09, 43.58it/s] 43%|████▎     | 317/733 [00:07<00:09, 43.53it/s] 44%|████▍     | 322/733 [00:07<00:09, 43.60it/s] 45%|████▍     | 327/733 [00:07<00:09, 43.46it/s] 45%|████▌     | 332/733 [00:07<00:09, 43.69it/s] 46%|████▌     | 337/733 [00:07<00:09, 43.78it/s] 47%|████▋     | 342/733 [00:07<00:08, 43.84it/s] 47%|████▋     | 347/733 [00:07<00:08, 43.88it/s] 48%|████▊     | 352/733 [00:08<00:08, 43.78it/s] 49%|████▊     | 357/733 [00:08<00:08, 43.74it/s] 49%|████▉     | 362/733 [00:08<00:08, 43.72it/s] 50%|█████     | 367/733 [00:08<00:08, 43.61it/s] 51%|█████     | 372/733 [00:08<00:08, 43.53it/s] 51%|█████▏    | 377/733 [00:08<00:08, 43.70it/s] 52%|█████▏    | 382/733 [00:08<00:08, 43.83it/s] 53%|█████▎    | 387/733 [00:08<00:07, 43.85it/s] 53%|█████▎    | 392/733 [00:08<00:07, 43.85it/s] 54%|█████▍    | 397/733 [00:09<00:07, 43.83it/s] 55%|█████▍    | 402/733 [00:09<00:07, 43.70it/s] 56%|█████▌    | 407/733 [00:09<00:07, 43.66it/s] 56%|█████▌    | 412/733 [00:09<00:07, 43.50it/s] 57%|█████▋    | 417/733 [00:09<00:07, 43.66it/s] 58%|█████▊    | 422/733 [00:09<00:07, 43.75it/s] 58%|█████▊    | 427/733 [00:09<00:07, 43.70it/s] 59%|█████▉    | 432/733 [00:09<00:06, 43.74it/s] 60%|█████▉    | 437/733 [00:09<00:06, 43.79it/s] 60%|██████    | 442/733 [00:10<00:06, 43.59it/s] 61%|██████    | 447/733 [00:10<00:06, 43.69it/s] 62%|██████▏   | 452/733 [00:10<00:06, 43.64it/s] 62%|██████▏   | 457/733 [00:10<00:06, 43.53it/s] 63%|██████▎   | 462/733 [00:10<00:06, 43.66it/s] 64%|██████▎   | 467/733 [00:10<00:06, 43.65it/s] 64%|██████▍   | 472/733 [00:10<00:05, 43.74it/s] 65%|██████▌   | 477/733 [00:10<00:05, 43.76it/s] 66%|██████▌   | 482/733 [00:11<00:05, 43.72it/s] 66%|██████▋   | 487/733 [00:11<00:05, 43.68it/s] 67%|██████▋   | 492/733 [00:11<00:05, 43.68it/s] 68%|██████▊   | 497/733 [00:11<00:05, 43.69it/s] 68%|██████▊   | 502/733 [00:11<00:05, 43.52it/s] 69%|██████▉   | 507/733 [00:11<00:05, 43.69it/s] 70%|██████▉   | 512/733 [00:11<00:05, 43.74it/s] 71%|███████   | 517/733 [00:11<00:04, 43.79it/s] 71%|███████   | 522/733 [00:11<00:04, 43.81it/s] 72%|███████▏  | 527/733 [00:12<00:04, 43.71it/s] 73%|███████▎  | 532/733 [00:12<00:04, 43.64it/s] 73%|███████▎  | 537/733 [00:12<00:04, 43.58it/s] 74%|███████▍  | 542/733 [00:12<00:04, 43.62it/s] 75%|███████▍  | 547/733 [00:12<00:04, 43.58it/s] 75%|███████▌  | 552/733 [00:12<00:04, 43.62it/s] 76%|███████▌  | 557/733 [00:12<00:04, 43.69it/s] 77%|███████▋  | 562/733 [00:12<00:03, 43.81it/s] 77%|███████▋  | 567/733 [00:12<00:03, 43.77it/s] 78%|███████▊  | 572/733 [00:13<00:03, 43.60it/s] 79%|███████▊  | 577/733 [00:13<00:03, 43.68it/s] 79%|███████▉  | 582/733 [00:13<00:03, 43.70it/s] 80%|████████  | 587/733 [00:13<00:03, 43.69it/s] 81%|████████  | 592/733 [00:13<00:03, 43.57it/s] 81%|████████▏ | 597/733 [00:13<00:03, 43.71it/s] 82%|████████▏ | 602/733 [00:13<00:02, 43.79it/s] 83%|████████▎ | 607/733 [00:13<00:02, 43.67it/s] 83%|████████▎ | 612/733 [00:14<00:02, 43.69it/s] 84%|████████▍ | 617/733 [00:14<00:02, 43.65it/s] 85%|████████▍ | 622/733 [00:14<00:02, 43.71it/s] 86%|████████▌ | 627/733 [00:14<00:02, 43.68it/s] 86%|████████▌ | 632/733 [00:14<00:02, 43.58it/s] 87%|████████▋ | 637/733 [00:14<00:02, 43.70it/s] 88%|████████▊ | 642/733 [00:14<00:02, 43.74it/s] 88%|████████▊ | 647/733 [00:14<00:01, 43.71it/s] 89%|████████▉ | 652/733 [00:14<00:01, 43.71it/s] 90%|████████▉ | 657/733 [00:15<00:01, 43.76it/s] 90%|█████████ | 662/733 [00:15<00:01, 43.65it/s] 91%|█████████ | 667/733 [00:15<00:01, 43.51it/s] 92%|█████████▏| 672/733 [00:15<00:01, 43.64it/s] 92%|█████████▏| 677/733 [00:15<00:01, 43.69it/s] 93%|█████████▎| 682/733 [00:15<00:01, 43.78it/s] 94%|█████████▎| 687/733 [00:15<00:01, 43.73it/s] 94%|█████████▍| 692/733 [00:15<00:00, 43.76it/s] 95%|█████████▌| 697/733 [00:15<00:00, 43.75it/s] 96%|█████████▌| 702/733 [00:16<00:00, 43.67it/s] 96%|█████████▋| 707/733 [00:16<00:00, 43.51it/s] 97%|█████████▋| 712/733 [00:16<00:00, 43.68it/s] 98%|█████████▊| 717/733 [00:16<00:00, 43.60it/s] 98%|█████████▊| 722/733 [00:16<00:00, 43.64it/s] 99%|█████████▉| 727/733 [00:16<00:00, 43.71it/s]100%|█████████▉| 732/733 [00:16<00:00, 43.77it/s]100%|██████████| 733/733 [00:16<00:00, 43.66it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:54:16,474 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:54:16,474 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:54:16,474 >>   eval_loss               =     0.9128
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:54:16,474 >>   eval_runtime            = 0:00:16.80
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:54:16,474 >>   eval_samples            =       5864
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:54:16,474 >>   eval_samples_per_second =    348.928
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:54:16,474 >>   eval_steps_per_second   =     43.616
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:54:16,474 >>   perplexity              =     2.4914
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:23,429 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:23,437 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:23,437 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:23,437 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:23,437 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:54:24,041 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:54:24,042 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:54:24,614 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:54:25,696 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:54:25,696 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:28,634 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:28,638 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:28,638 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:28,638 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:28,638 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:54:29,269 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:54:29,270 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:54:29,855 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:54:30,005 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:54:30,005 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-157
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-314
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-628
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-785
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-471
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl', 'labels': ['inception', 'located on terrain feature', 'military branch', 'occupant', 'occupation'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14932
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15032, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.75it/s]Extractor Predicting: 6it [00:03,  1.79it/s]Extractor Predicting: 7it [00:04,  1.80it/s]Extractor Predicting: 8it [00:04,  1.80it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.79it/s]Extractor Predicting: 11it [00:06,  1.78it/s]Extractor Predicting: 12it [00:06,  1.76it/s]Extractor Predicting: 13it [00:07,  1.76it/s]Extractor Predicting: 14it [00:07,  1.75it/s]Extractor Predicting: 15it [00:08,  1.74it/s]Extractor Predicting: 16it [00:09,  1.72it/s]Extractor Predicting: 17it [00:09,  1.70it/s]Extractor Predicting: 18it [00:10,  1.74it/s]Extractor Predicting: 19it [00:10,  1.76it/s]Extractor Predicting: 20it [00:11,  1.77it/s]Extractor Predicting: 21it [00:11,  1.78it/s]Extractor Predicting: 22it [00:12,  1.69it/s]Extractor Predicting: 23it [00:13,  1.74it/s]Extractor Predicting: 24it [00:13,  1.78it/s]Extractor Predicting: 25it [00:14,  1.81it/s]Extractor Predicting: 26it [00:14,  1.77it/s]Extractor Predicting: 27it [00:15,  1.80it/s]Extractor Predicting: 28it [00:15,  1.78it/s]Extractor Predicting: 29it [00:16,  1.79it/s]Extractor Predicting: 30it [00:17,  1.77it/s]Extractor Predicting: 31it [00:17,  1.78it/s]Extractor Predicting: 32it [00:18,  1.79it/s]Extractor Predicting: 33it [00:18,  1.76it/s]Extractor Predicting: 34it [00:19,  1.78it/s]Extractor Predicting: 35it [00:19,  1.73it/s]Extractor Predicting: 36it [00:20,  1.72it/s]Extractor Predicting: 37it [00:21,  1.71it/s]Extractor Predicting: 38it [00:21,  1.74it/s]Extractor Predicting: 39it [00:22,  1.77it/s]Extractor Predicting: 40it [00:22,  1.84it/s]Extractor Predicting: 41it [00:23,  1.77it/s]Extractor Predicting: 42it [00:23,  1.71it/s]Extractor Predicting: 43it [00:24,  1.64it/s]Extractor Predicting: 44it [00:25,  1.59it/s]Extractor Predicting: 45it [00:25,  1.58it/s]Extractor Predicting: 46it [00:26,  1.62it/s]Extractor Predicting: 47it [00:27,  1.57it/s]Extractor Predicting: 48it [00:27,  1.61it/s]Extractor Predicting: 49it [00:28,  1.60it/s]Extractor Predicting: 50it [00:29,  1.54it/s]Extractor Predicting: 51it [00:29,  1.60it/s]Extractor Predicting: 52it [00:30,  1.58it/s]Extractor Predicting: 53it [00:30,  1.61it/s]Extractor Predicting: 54it [00:31,  1.61it/s]Extractor Predicting: 55it [00:32,  1.63it/s]Extractor Predicting: 56it [00:32,  1.62it/s]Extractor Predicting: 57it [00:33,  1.64it/s]Extractor Predicting: 58it [00:33,  1.64it/s]Extractor Predicting: 59it [00:34,  1.63it/s]Extractor Predicting: 60it [00:35,  1.64it/s]Extractor Predicting: 61it [00:35,  1.64it/s]Extractor Predicting: 62it [00:36,  1.64it/s]Extractor Predicting: 63it [00:36,  1.68it/s]Extractor Predicting: 64it [00:37,  1.64it/s]Extractor Predicting: 65it [00:38,  1.65it/s]Extractor Predicting: 66it [00:38,  1.64it/s]Extractor Predicting: 67it [00:39,  1.66it/s]Extractor Predicting: 68it [00:40,  1.67it/s]Extractor Predicting: 69it [00:40,  1.64it/s]Extractor Predicting: 70it [00:41,  1.65it/s]Extractor Predicting: 71it [00:41,  1.64it/s]Extractor Predicting: 72it [00:42,  1.66it/s]Extractor Predicting: 73it [00:43,  1.66it/s]Extractor Predicting: 74it [00:43,  1.68it/s]Extractor Predicting: 75it [00:44,  1.64it/s]Extractor Predicting: 76it [00:44,  1.68it/s]Extractor Predicting: 77it [00:45,  1.69it/s]Extractor Predicting: 78it [00:46,  1.67it/s]Extractor Predicting: 79it [00:46,  1.65it/s]Extractor Predicting: 80it [00:47,  1.66it/s]Extractor Predicting: 81it [00:47,  1.68it/s]Extractor Predicting: 82it [00:48,  1.67it/s]Extractor Predicting: 83it [00:49,  1.66it/s]Extractor Predicting: 84it [00:49,  1.66it/s]Extractor Predicting: 85it [00:50,  1.68it/s]Extractor Predicting: 86it [00:50,  1.67it/s]Extractor Predicting: 87it [00:51,  1.63it/s]Extractor Predicting: 88it [00:52,  1.64it/s]Extractor Predicting: 89it [00:52,  1.65it/s]Extractor Predicting: 90it [00:53,  1.67it/s]Extractor Predicting: 91it [00:53,  1.68it/s]Extractor Predicting: 92it [00:54,  1.68it/s]Extractor Predicting: 93it [00:55,  1.66it/s]Extractor Predicting: 94it [00:55,  1.67it/s]Extractor Predicting: 95it [00:56,  1.66it/s]Extractor Predicting: 96it [00:56,  1.65it/s]Extractor Predicting: 97it [00:57,  1.64it/s]Extractor Predicting: 98it [00:58,  1.61it/s]Extractor Predicting: 99it [00:58,  1.63it/s]Extractor Predicting: 100it [00:59,  1.62it/s]Extractor Predicting: 101it [00:59,  1.63it/s]Extractor Predicting: 102it [01:00,  1.63it/s]Extractor Predicting: 103it [01:01,  1.62it/s]Extractor Predicting: 104it [01:01,  1.62it/s]Extractor Predicting: 105it [01:02,  1.63it/s]Extractor Predicting: 106it [01:03,  1.51it/s]Extractor Predicting: 107it [01:03,  1.55it/s]Extractor Predicting: 108it [01:04,  1.60it/s]Extractor Predicting: 109it [01:04,  1.62it/s]Extractor Predicting: 110it [01:05,  1.64it/s]Extractor Predicting: 111it [01:06,  1.67it/s]Extractor Predicting: 112it [01:06,  1.66it/s]Extractor Predicting: 113it [01:07,  1.67it/s]Extractor Predicting: 114it [01:07,  1.70it/s]Extractor Predicting: 115it [01:08,  1.67it/s]Extractor Predicting: 116it [01:09,  1.69it/s]Extractor Predicting: 117it [01:09,  1.68it/s]Extractor Predicting: 118it [01:10,  1.68it/s]Extractor Predicting: 119it [01:10,  1.66it/s]Extractor Predicting: 120it [01:11,  1.65it/s]Extractor Predicting: 121it [01:12,  1.68it/s]Extractor Predicting: 122it [01:12,  1.70it/s]Extractor Predicting: 123it [01:13,  1.69it/s]Extractor Predicting: 124it [01:13,  1.71it/s]Extractor Predicting: 125it [01:14,  1.66it/s]Extractor Predicting: 126it [01:15,  1.65it/s]Extractor Predicting: 127it [01:15,  1.66it/s]Extractor Predicting: 128it [01:16,  1.68it/s]Extractor Predicting: 129it [01:16,  1.69it/s]Extractor Predicting: 130it [01:17,  1.68it/s]Extractor Predicting: 131it [01:18,  1.68it/s]Extractor Predicting: 132it [01:18,  1.68it/s]Extractor Predicting: 133it [01:19,  1.72it/s]Extractor Predicting: 134it [01:19,  1.66it/s]Extractor Predicting: 135it [01:20,  1.65it/s]Extractor Predicting: 136it [01:21,  1.65it/s]Extractor Predicting: 137it [01:21,  1.66it/s]Extractor Predicting: 138it [01:22,  1.61it/s]Extractor Predicting: 139it [01:22,  1.62it/s]Extractor Predicting: 140it [01:23,  1.61it/s]Extractor Predicting: 141it [01:24,  1.60it/s]Extractor Predicting: 142it [01:24,  1.56it/s]Extractor Predicting: 143it [01:25,  1.59it/s]Extractor Predicting: 144it [01:26,  1.60it/s]Extractor Predicting: 145it [01:26,  1.61it/s]Extractor Predicting: 146it [01:27,  1.58it/s]Extractor Predicting: 147it [01:28,  1.59it/s]Extractor Predicting: 148it [01:28,  1.59it/s]Extractor Predicting: 149it [01:29,  1.57it/s]Extractor Predicting: 150it [01:29,  1.59it/s]Extractor Predicting: 151it [01:30,  1.58it/s]Extractor Predicting: 152it [01:31,  1.59it/s]Extractor Predicting: 153it [01:31,  1.62it/s]Extractor Predicting: 154it [01:32,  1.63it/s]Extractor Predicting: 155it [01:33,  1.59it/s]Extractor Predicting: 156it [01:33,  1.62it/s]Extractor Predicting: 157it [01:34,  1.61it/s]Extractor Predicting: 158it [01:34,  1.63it/s]Extractor Predicting: 159it [01:35,  1.59it/s]Extractor Predicting: 160it [01:36,  1.61it/s]Extractor Predicting: 161it [01:36,  1.63it/s]Extractor Predicting: 162it [01:37,  1.56it/s]Extractor Predicting: 163it [01:38,  1.55it/s]Extractor Predicting: 164it [01:38,  1.55it/s]Extractor Predicting: 165it [01:39,  1.56it/s]Extractor Predicting: 166it [01:39,  1.59it/s]Extractor Predicting: 167it [01:40,  1.59it/s]Extractor Predicting: 168it [01:41,  1.58it/s]Extractor Predicting: 169it [01:41,  1.58it/s]Extractor Predicting: 170it [01:42,  1.53it/s]Extractor Predicting: 171it [01:43,  1.59it/s]Extractor Predicting: 172it [01:43,  1.59it/s]Extractor Predicting: 173it [01:44,  1.57it/s]Extractor Predicting: 174it [01:45,  1.56it/s]Extractor Predicting: 175it [01:45,  1.58it/s]Extractor Predicting: 176it [01:46,  1.58it/s]Extractor Predicting: 177it [01:46,  1.55it/s]Extractor Predicting: 178it [01:47,  1.55it/s]Extractor Predicting: 179it [01:48,  1.55it/s]Extractor Predicting: 180it [01:48,  1.54it/s]Extractor Predicting: 181it [01:49,  1.52it/s]Extractor Predicting: 182it [01:50,  1.52it/s]Extractor Predicting: 183it [01:50,  1.56it/s]Extractor Predicting: 184it [01:51,  1.62it/s]Extractor Predicting: 185it [01:52,  1.62it/s]Extractor Predicting: 186it [01:52,  1.63it/s]Extractor Predicting: 187it [01:53,  1.44it/s]Extractor Predicting: 188it [01:54,  1.49it/s]Extractor Predicting: 189it [01:54,  1.56it/s]Extractor Predicting: 190it [01:55,  1.61it/s]Extractor Predicting: 191it [01:55,  1.63it/s]Extractor Predicting: 192it [01:56,  1.62it/s]Extractor Predicting: 193it [01:57,  1.63it/s]Extractor Predicting: 194it [01:57,  1.62it/s]Extractor Predicting: 195it [01:58,  1.64it/s]Extractor Predicting: 196it [01:58,  1.65it/s]Extractor Predicting: 197it [01:59,  1.67it/s]Extractor Predicting: 198it [02:00,  1.63it/s]Extractor Predicting: 199it [02:00,  1.62it/s]Extractor Predicting: 200it [02:01,  1.64it/s]Extractor Predicting: 201it [02:01,  1.65it/s]Extractor Predicting: 202it [02:02,  1.70it/s]Extractor Predicting: 203it [02:03,  1.71it/s]Extractor Predicting: 204it [02:03,  1.69it/s]Extractor Predicting: 205it [02:04,  1.67it/s]Extractor Predicting: 206it [02:04,  1.67it/s]Extractor Predicting: 207it [02:05,  1.67it/s]Extractor Predicting: 208it [02:06,  1.67it/s]Extractor Predicting: 209it [02:06,  1.69it/s]Extractor Predicting: 210it [02:07,  1.64it/s]Extractor Predicting: 211it [02:07,  1.65it/s]Extractor Predicting: 212it [02:08,  1.64it/s]Extractor Predicting: 213it [02:09,  1.62it/s]Extractor Predicting: 214it [02:09,  1.62it/s]Extractor Predicting: 215it [02:10,  1.60it/s]Extractor Predicting: 216it [02:11,  1.58it/s]Extractor Predicting: 217it [02:11,  1.60it/s]Extractor Predicting: 218it [02:12,  1.61it/s]Extractor Predicting: 219it [02:12,  1.63it/s]Extractor Predicting: 220it [02:13,  1.67it/s]Extractor Predicting: 221it [02:14,  1.69it/s]Extractor Predicting: 222it [02:14,  1.68it/s]Extractor Predicting: 223it [02:15,  1.62it/s]Extractor Predicting: 224it [02:15,  1.85it/s]Extractor Predicting: 224it [02:15,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:55,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:55,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:55,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:55,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:55,482 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:56:55,794 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:56:55,795 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:56:56,062 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:56:57,135 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:56:57,135 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:58,820 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:58,825 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:58,825 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:58,825 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:58,825 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:56:59,553 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:56:59,554 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:57:00,230 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:57:00,402 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:57:00,402 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.33613445378151263,
  "recall": 0.034106412005457026,
  "score": 0.06192909119058677,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'father', 'follows', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'part of the series', 'place of birth', 'production company', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 30214
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 30314, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.69it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.73it/s]Extractor Predicting: 14it [00:08,  1.71it/s]Extractor Predicting: 15it [00:08,  1.72it/s]Extractor Predicting: 16it [00:09,  1.72it/s]Extractor Predicting: 17it [00:10,  1.70it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:11,  1.66it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:13,  1.69it/s]Extractor Predicting: 24it [00:14,  1.71it/s]Extractor Predicting: 25it [00:14,  1.72it/s]Extractor Predicting: 26it [00:15,  1.73it/s]Extractor Predicting: 27it [00:15,  1.72it/s]Extractor Predicting: 28it [00:16,  1.75it/s]Extractor Predicting: 29it [00:17,  1.72it/s]Extractor Predicting: 30it [00:17,  1.71it/s]Extractor Predicting: 31it [00:18,  1.74it/s]Extractor Predicting: 32it [00:18,  1.69it/s]Extractor Predicting: 33it [00:19,  1.66it/s]Extractor Predicting: 34it [00:20,  1.64it/s]Extractor Predicting: 35it [00:20,  1.65it/s]Extractor Predicting: 36it [00:21,  1.60it/s]Extractor Predicting: 37it [00:21,  1.68it/s]Extractor Predicting: 38it [00:22,  1.65it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:23,  1.66it/s]Extractor Predicting: 41it [00:24,  1.64it/s]Extractor Predicting: 42it [00:24,  1.66it/s]Extractor Predicting: 43it [00:25,  1.65it/s]Extractor Predicting: 44it [00:26,  1.65it/s]Extractor Predicting: 45it [00:26,  1.61it/s]Extractor Predicting: 46it [00:27,  1.61it/s]Extractor Predicting: 47it [00:28,  1.60it/s]Extractor Predicting: 48it [00:28,  1.59it/s]Extractor Predicting: 49it [00:29,  1.64it/s]Extractor Predicting: 50it [00:29,  1.71it/s]Extractor Predicting: 51it [00:30,  1.72it/s]Extractor Predicting: 52it [00:30,  1.73it/s]Extractor Predicting: 53it [00:31,  1.68it/s]Extractor Predicting: 54it [00:32,  1.68it/s]Extractor Predicting: 55it [00:32,  1.70it/s]Extractor Predicting: 56it [00:33,  1.71it/s]Extractor Predicting: 57it [00:33,  1.70it/s]Extractor Predicting: 58it [00:34,  1.73it/s]Extractor Predicting: 59it [00:34,  1.79it/s]Extractor Predicting: 60it [00:35,  1.82it/s]Extractor Predicting: 61it [00:36,  1.76it/s]Extractor Predicting: 62it [00:36,  1.79it/s]Extractor Predicting: 63it [00:37,  1.77it/s]Extractor Predicting: 64it [00:37,  1.84it/s]Extractor Predicting: 65it [00:38,  1.83it/s]Extractor Predicting: 66it [00:38,  1.79it/s]Extractor Predicting: 67it [00:39,  1.75it/s]Extractor Predicting: 68it [00:39,  1.80it/s]Extractor Predicting: 69it [00:40,  1.78it/s]Extractor Predicting: 70it [00:41,  1.75it/s]Extractor Predicting: 71it [00:41,  1.79it/s]Extractor Predicting: 72it [00:42,  1.76it/s]Extractor Predicting: 73it [00:42,  1.81it/s]Extractor Predicting: 74it [00:43,  1.80it/s]Extractor Predicting: 75it [00:43,  1.86it/s]Extractor Predicting: 76it [00:44,  1.84it/s]Extractor Predicting: 77it [00:45,  1.76it/s]Extractor Predicting: 78it [00:45,  1.77it/s]Extractor Predicting: 79it [00:46,  1.78it/s]Extractor Predicting: 80it [00:46,  1.72it/s]Extractor Predicting: 81it [00:47,  1.71it/s]Extractor Predicting: 82it [00:47,  1.72it/s]Extractor Predicting: 83it [00:48,  1.70it/s]Extractor Predicting: 84it [00:49,  1.72it/s]Extractor Predicting: 85it [00:49,  1.76it/s]Extractor Predicting: 86it [00:50,  1.58it/s]Extractor Predicting: 87it [00:50,  1.65it/s]Extractor Predicting: 88it [00:51,  1.69it/s]Extractor Predicting: 89it [00:52,  1.70it/s]Extractor Predicting: 90it [00:52,  1.66it/s]Extractor Predicting: 91it [00:53,  1.65it/s]Extractor Predicting: 92it [00:53,  1.69it/s]Extractor Predicting: 93it [00:54,  1.78it/s]Extractor Predicting: 94it [00:54,  1.79it/s]Extractor Predicting: 95it [00:55,  1.79it/s]Extractor Predicting: 96it [00:56,  1.80it/s]Extractor Predicting: 97it [00:56,  1.88it/s]Extractor Predicting: 98it [00:57,  1.88it/s]Extractor Predicting: 99it [00:57,  1.88it/s]Extractor Predicting: 100it [00:58,  1.88it/s]Extractor Predicting: 101it [00:58,  1.90it/s]Extractor Predicting: 102it [00:59,  1.84it/s]Extractor Predicting: 103it [00:59,  1.77it/s]Extractor Predicting: 104it [01:00,  1.81it/s]Extractor Predicting: 105it [01:00,  1.82it/s]Extractor Predicting: 106it [01:01,  1.84it/s]Extractor Predicting: 107it [01:01,  1.86it/s]Extractor Predicting: 108it [01:02,  1.85it/s]Extractor Predicting: 109it [01:03,  1.86it/s]Extractor Predicting: 110it [01:03,  1.83it/s]Extractor Predicting: 111it [01:04,  1.86it/s]Extractor Predicting: 112it [01:04,  1.87it/s]Extractor Predicting: 113it [01:05,  1.87it/s]Extractor Predicting: 114it [01:05,  1.86it/s]Extractor Predicting: 115it [01:06,  1.88it/s]Extractor Predicting: 116it [01:06,  1.85it/s]Extractor Predicting: 117it [01:07,  1.88it/s]Extractor Predicting: 118it [01:07,  1.82it/s]Extractor Predicting: 119it [01:08,  1.80it/s]Extractor Predicting: 120it [01:09,  1.82it/s]Extractor Predicting: 121it [01:09,  1.79it/s]Extractor Predicting: 122it [01:10,  1.79it/s]Extractor Predicting: 123it [01:10,  1.78it/s]Extractor Predicting: 124it [01:11,  1.75it/s]Extractor Predicting: 125it [01:11,  1.77it/s]Extractor Predicting: 126it [01:12,  1.78it/s]Extractor Predicting: 127it [01:12,  1.82it/s]Extractor Predicting: 128it [01:13,  1.77it/s]Extractor Predicting: 129it [01:14,  1.62it/s]Extractor Predicting: 130it [01:14,  1.61it/s]Extractor Predicting: 131it [01:15,  1.63it/s]Extractor Predicting: 132it [01:16,  1.63it/s]Extractor Predicting: 133it [01:16,  1.65it/s]Extractor Predicting: 134it [01:17,  1.68it/s]Extractor Predicting: 135it [01:17,  1.71it/s]Extractor Predicting: 136it [01:18,  1.72it/s]Extractor Predicting: 137it [01:18,  1.74it/s]Extractor Predicting: 138it [01:19,  1.72it/s]Extractor Predicting: 139it [01:20,  1.72it/s]Extractor Predicting: 140it [01:20,  1.73it/s]Extractor Predicting: 141it [01:21,  1.64it/s]Extractor Predicting: 142it [01:21,  1.67it/s]Extractor Predicting: 143it [01:22,  1.55it/s]Extractor Predicting: 144it [01:23,  1.59it/s]Extractor Predicting: 145it [01:23,  1.61it/s]Extractor Predicting: 146it [01:24,  1.58it/s]Extractor Predicting: 147it [01:25,  1.59it/s]Extractor Predicting: 148it [01:25,  1.62it/s]Extractor Predicting: 149it [01:26,  1.63it/s]Extractor Predicting: 150it [01:27,  1.64it/s]Extractor Predicting: 151it [01:27,  1.63it/s]Extractor Predicting: 152it [01:28,  1.63it/s]Extractor Predicting: 153it [01:28,  1.66it/s]Extractor Predicting: 154it [01:29,  1.67it/s]Extractor Predicting: 155it [01:30,  1.67it/s]Extractor Predicting: 156it [01:30,  1.65it/s]Extractor Predicting: 157it [01:31,  1.67it/s]Extractor Predicting: 158it [01:31,  1.69it/s]Extractor Predicting: 159it [01:32,  1.69it/s]Extractor Predicting: 160it [01:33,  1.60it/s]Extractor Predicting: 161it [01:33,  1.62it/s]Extractor Predicting: 162it [01:34,  1.61it/s]Extractor Predicting: 163it [01:34,  1.61it/s]Extractor Predicting: 164it [01:35,  1.63it/s]Extractor Predicting: 165it [01:36,  1.61it/s]Extractor Predicting: 166it [01:36,  1.63it/s]Extractor Predicting: 167it [01:37,  1.62it/s]Extractor Predicting: 168it [01:38,  1.58it/s]Extractor Predicting: 169it [01:38,  1.58it/s]Extractor Predicting: 170it [01:39,  1.62it/s]Extractor Predicting: 171it [01:39,  1.55it/s]Extractor Predicting: 172it [01:40,  1.56it/s]Extractor Predicting: 173it [01:41,  1.58it/s]Extractor Predicting: 174it [01:41,  1.59it/s]Extractor Predicting: 175it [01:42,  1.62it/s]Extractor Predicting: 176it [01:43,  1.65it/s]Extractor Predicting: 177it [01:43,  1.67it/s]Extractor Predicting: 178it [01:44,  1.70it/s]Extractor Predicting: 179it [01:44,  1.71it/s]Extractor Predicting: 180it [01:45,  1.71it/s]Extractor Predicting: 181it [01:45,  1.72it/s]Extractor Predicting: 182it [01:46,  1.77it/s]Extractor Predicting: 183it [01:47,  1.72it/s]Extractor Predicting: 184it [01:47,  1.75it/s]Extractor Predicting: 185it [01:48,  1.71it/s]Extractor Predicting: 186it [01:48,  1.72it/s]Extractor Predicting: 187it [01:49,  1.73it/s]Extractor Predicting: 188it [01:49,  1.74it/s]Extractor Predicting: 189it [01:50,  1.70it/s]Extractor Predicting: 190it [01:51,  1.67it/s]Extractor Predicting: 191it [01:51,  1.69it/s]Extractor Predicting: 192it [01:52,  1.73it/s]Extractor Predicting: 193it [01:52,  1.69it/s]Extractor Predicting: 194it [01:53,  1.70it/s]Extractor Predicting: 195it [01:54,  1.74it/s]Extractor Predicting: 196it [01:54,  1.75it/s]Extractor Predicting: 197it [01:55,  1.73it/s]Extractor Predicting: 198it [01:55,  1.76it/s]Extractor Predicting: 199it [01:56,  1.78it/s]Extractor Predicting: 200it [01:56,  1.73it/s]Extractor Predicting: 201it [01:57,  1.75it/s]Extractor Predicting: 202it [01:58,  1.72it/s]Extractor Predicting: 203it [01:58,  1.73it/s]Extractor Predicting: 204it [01:59,  1.70it/s]Extractor Predicting: 205it [01:59,  1.72it/s]Extractor Predicting: 206it [02:00,  1.70it/s]Extractor Predicting: 207it [02:00,  1.72it/s]Extractor Predicting: 208it [02:01,  1.73it/s]Extractor Predicting: 209it [02:02,  1.70it/s]Extractor Predicting: 210it [02:02,  1.71it/s]Extractor Predicting: 211it [02:03,  1.66it/s]Extractor Predicting: 212it [02:03,  1.68it/s]Extractor Predicting: 213it [02:04,  1.73it/s]Extractor Predicting: 214it [02:05,  1.68it/s]Extractor Predicting: 215it [02:05,  1.67it/s]Extractor Predicting: 216it [02:06,  1.66it/s]Extractor Predicting: 217it [02:06,  1.71it/s]Extractor Predicting: 218it [02:07,  1.71it/s]Extractor Predicting: 219it [02:08,  1.67it/s]Extractor Predicting: 220it [02:08,  1.71it/s]Extractor Predicting: 221it [02:09,  1.73it/s]Extractor Predicting: 222it [02:09,  1.74it/s]Extractor Predicting: 223it [02:10,  1.49it/s]Extractor Predicting: 224it [02:11,  1.58it/s]Extractor Predicting: 225it [02:11,  1.59it/s]Extractor Predicting: 226it [02:12,  1.58it/s]Extractor Predicting: 227it [02:13,  1.66it/s]Extractor Predicting: 228it [02:13,  1.68it/s]Extractor Predicting: 229it [02:14,  1.66it/s]Extractor Predicting: 230it [02:14,  1.69it/s]Extractor Predicting: 231it [02:15,  1.66it/s]Extractor Predicting: 232it [02:15,  1.68it/s]Extractor Predicting: 233it [02:16,  1.68it/s]Extractor Predicting: 234it [02:17,  1.68it/s]Extractor Predicting: 235it [02:17,  1.68it/s]Extractor Predicting: 236it [02:18,  1.67it/s]Extractor Predicting: 237it [02:18,  1.71it/s]Extractor Predicting: 238it [02:19,  1.69it/s]Extractor Predicting: 239it [02:20,  1.72it/s]Extractor Predicting: 240it [02:20,  1.69it/s]Extractor Predicting: 241it [02:21,  1.67it/s]Extractor Predicting: 242it [02:21,  1.71it/s]Extractor Predicting: 243it [02:22,  1.72it/s]Extractor Predicting: 244it [02:23,  1.72it/s]Extractor Predicting: 245it [02:23,  1.69it/s]Extractor Predicting: 246it [02:24,  1.68it/s]Extractor Predicting: 247it [02:24,  1.71it/s]Extractor Predicting: 248it [02:25,  1.70it/s]Extractor Predicting: 249it [02:25,  1.72it/s]Extractor Predicting: 250it [02:26,  1.70it/s]Extractor Predicting: 251it [02:27,  1.72it/s]Extractor Predicting: 252it [02:27,  1.70it/s]Extractor Predicting: 253it [02:28,  1.69it/s]Extractor Predicting: 254it [02:28,  1.67it/s]Extractor Predicting: 255it [02:29,  1.70it/s]Extractor Predicting: 256it [02:30,  1.69it/s]Extractor Predicting: 257it [02:30,  1.71it/s]Extractor Predicting: 258it [02:31,  1.69it/s]Extractor Predicting: 259it [02:31,  1.68it/s]Extractor Predicting: 260it [02:32,  1.67it/s]Extractor Predicting: 261it [02:33,  1.66it/s]Extractor Predicting: 262it [02:33,  1.63it/s]Extractor Predicting: 263it [02:34,  1.67it/s]Extractor Predicting: 264it [02:34,  1.64it/s]Extractor Predicting: 265it [02:35,  1.62it/s]Extractor Predicting: 266it [02:36,  1.61it/s]Extractor Predicting: 267it [02:36,  1.61it/s]Extractor Predicting: 268it [02:37,  1.59it/s]Extractor Predicting: 269it [02:38,  1.62it/s]Extractor Predicting: 270it [02:38,  1.63it/s]Extractor Predicting: 271it [02:39,  1.70it/s]Extractor Predicting: 272it [02:39,  1.65it/s]Extractor Predicting: 273it [02:40,  1.64it/s]Extractor Predicting: 274it [02:41,  1.66it/s]Extractor Predicting: 275it [02:41,  1.66it/s]Extractor Predicting: 276it [02:42,  1.65it/s]Extractor Predicting: 277it [02:42,  1.64it/s]Extractor Predicting: 278it [02:43,  1.64it/s]Extractor Predicting: 279it [02:44,  1.64it/s]Extractor Predicting: 280it [02:44,  1.63it/s]Extractor Predicting: 281it [02:45,  1.65it/s]Extractor Predicting: 282it [02:45,  1.66it/s]Extractor Predicting: 283it [02:46,  1.66it/s]Extractor Predicting: 284it [02:47,  1.67it/s]Extractor Predicting: 285it [02:47,  1.65it/s]Extractor Predicting: 286it [02:48,  1.64it/s]Extractor Predicting: 287it [02:48,  1.62it/s]Extractor Predicting: 288it [02:49,  1.60it/s]Extractor Predicting: 289it [02:50,  1.61it/s]Extractor Predicting: 290it [02:50,  1.61it/s]Extractor Predicting: 291it [02:51,  1.63it/s]Extractor Predicting: 292it [02:52,  1.60it/s]Extractor Predicting: 293it [02:52,  1.62it/s]Extractor Predicting: 294it [02:53,  1.61it/s]Extractor Predicting: 295it [02:53,  1.61it/s]Extractor Predicting: 296it [02:54,  1.61it/s]Extractor Predicting: 297it [02:55,  1.60it/s]Extractor Predicting: 298it [02:55,  1.57it/s]Extractor Predicting: 299it [02:56,  1.53it/s]Extractor Predicting: 300it [02:57,  1.51it/s]Extractor Predicting: 301it [02:57,  1.50it/s]Extractor Predicting: 302it [02:58,  1.50it/s]Extractor Predicting: 303it [02:59,  1.49it/s]Extractor Predicting: 304it [02:59,  1.56it/s]Extractor Predicting: 305it [03:00,  1.60it/s]Extractor Predicting: 306it [03:01,  1.62it/s]Extractor Predicting: 307it [03:01,  1.61it/s]Extractor Predicting: 308it [03:02,  1.63it/s]Extractor Predicting: 309it [03:02,  1.60it/s]Extractor Predicting: 310it [03:03,  1.57it/s]Extractor Predicting: 311it [03:04,  1.56it/s]Extractor Predicting: 312it [03:04,  1.57it/s]Extractor Predicting: 313it [03:05,  1.55it/s]Extractor Predicting: 314it [03:06,  1.52it/s]Extractor Predicting: 315it [03:06,  1.55it/s]Extractor Predicting: 316it [03:07,  1.54it/s]Extractor Predicting: 317it [03:08,  1.56it/s]Extractor Predicting: 318it [03:08,  1.55it/s]Extractor Predicting: 319it [03:09,  1.54it/s]Extractor Predicting: 320it [03:10,  1.54it/s]Extractor Predicting: 321it [03:10,  1.51it/s]Extractor Predicting: 322it [03:11,  1.46it/s]Extractor Predicting: 323it [03:12,  1.49it/s]Extractor Predicting: 324it [03:12,  1.48it/s]Extractor Predicting: 325it [03:13,  1.49it/s]Extractor Predicting: 326it [03:14,  1.49it/s]Extractor Predicting: 327it [03:14,  1.49it/s]Extractor Predicting: 328it [03:15,  1.53it/s]Extractor Predicting: 329it [03:16,  1.53it/s]Extractor Predicting: 330it [03:16,  1.56it/s]Extractor Predicting: 331it [03:17,  1.57it/s]Extractor Predicting: 332it [03:18,  1.53it/s]Extractor Predicting: 333it [03:18,  1.54it/s]Extractor Predicting: 334it [03:19,  1.55it/s]Extractor Predicting: 335it [03:19,  1.56it/s]Extractor Predicting: 336it [03:20,  1.50it/s]Extractor Predicting: 337it [03:21,  1.50it/s]Extractor Predicting: 338it [03:21,  1.53it/s]Extractor Predicting: 339it [03:22,  1.52it/s]Extractor Predicting: 340it [03:23,  1.51it/s]Extractor Predicting: 341it [03:23,  1.49it/s]Extractor Predicting: 342it [03:24,  1.50it/s]Extractor Predicting: 343it [03:25,  1.46it/s]Extractor Predicting: 344it [03:26,  1.47it/s]Extractor Predicting: 345it [03:26,  1.32it/s]Extractor Predicting: 346it [03:27,  1.40it/s]Extractor Predicting: 347it [03:28,  1.47it/s]Extractor Predicting: 348it [03:28,  1.53it/s]Extractor Predicting: 349it [03:29,  1.53it/s]Extractor Predicting: 350it [03:30,  1.54it/s]Extractor Predicting: 351it [03:30,  1.55it/s]Extractor Predicting: 352it [03:31,  1.59it/s]Extractor Predicting: 353it [03:31,  1.65it/s]Extractor Predicting: 354it [03:32,  1.68it/s]Extractor Predicting: 355it [03:33,  1.67it/s]Extractor Predicting: 356it [03:33,  1.69it/s]Extractor Predicting: 357it [03:34,  1.71it/s]Extractor Predicting: 358it [03:34,  1.71it/s]Extractor Predicting: 359it [03:35,  1.70it/s]Extractor Predicting: 360it [03:35,  1.73it/s]Extractor Predicting: 361it [03:36,  1.69it/s]Extractor Predicting: 362it [03:37,  1.68it/s]Extractor Predicting: 363it [03:37,  1.70it/s]Extractor Predicting: 364it [03:38,  1.74it/s]Extractor Predicting: 365it [03:38,  1.72it/s]Extractor Predicting: 366it [03:39,  1.74it/s]Extractor Predicting: 367it [03:39,  1.75it/s]Extractor Predicting: 368it [03:40,  1.73it/s]Extractor Predicting: 369it [03:41,  1.70it/s]Extractor Predicting: 370it [03:41,  1.67it/s]Extractor Predicting: 371it [03:42,  1.62it/s]Extractor Predicting: 372it [03:43,  1.67it/s]Extractor Predicting: 373it [03:43,  1.68it/s]Extractor Predicting: 374it [03:44,  1.66it/s]Extractor Predicting: 375it [03:44,  1.64it/s]Extractor Predicting: 376it [03:45,  1.63it/s]Extractor Predicting: 377it [03:46,  1.63it/s]Extractor Predicting: 378it [03:46,  1.70it/s]Extractor Predicting: 379it [03:47,  1.77it/s]Extractor Predicting: 380it [03:47,  1.80it/s]Extractor Predicting: 381it [03:48,  1.75it/s]Extractor Predicting: 382it [03:48,  1.72it/s]Extractor Predicting: 383it [03:49,  1.71it/s]Extractor Predicting: 384it [03:49,  1.75it/s]Extractor Predicting: 385it [03:50,  1.80it/s]Extractor Predicting: 386it [03:51,  1.76it/s]Extractor Predicting: 387it [03:51,  1.81it/s]Extractor Predicting: 388it [03:52,  1.79it/s]Extractor Predicting: 389it [03:52,  1.74it/s]Extractor Predicting: 390it [03:53,  1.75it/s]Extractor Predicting: 391it [03:53,  1.77it/s]Extractor Predicting: 392it [03:54,  1.72it/s]Extractor Predicting: 393it [03:55,  1.75it/s]Extractor Predicting: 394it [03:55,  1.77it/s]Extractor Predicting: 395it [03:56,  1.73it/s]Extractor Predicting: 396it [03:56,  1.72it/s]Extractor Predicting: 397it [03:57,  1.74it/s]Extractor Predicting: 398it [03:57,  1.73it/s]Extractor Predicting: 399it [03:58,  1.72it/s]Extractor Predicting: 400it [03:59,  1.74it/s]Extractor Predicting: 401it [03:59,  1.77it/s]Extractor Predicting: 402it [04:00,  1.75it/s]Extractor Predicting: 403it [04:00,  1.75it/s]Extractor Predicting: 404it [04:01,  1.71it/s]Extractor Predicting: 405it [04:02,  1.71it/s]Extractor Predicting: 406it [04:02,  1.66it/s]Extractor Predicting: 407it [04:03,  1.60it/s]Extractor Predicting: 408it [04:03,  1.64it/s]Extractor Predicting: 409it [04:04,  1.67it/s]Extractor Predicting: 410it [04:05,  1.66it/s]Extractor Predicting: 411it [04:05,  1.66it/s]Extractor Predicting: 412it [04:06,  1.64it/s]Extractor Predicting: 413it [04:06,  1.68it/s]Extractor Predicting: 414it [04:07,  1.70it/s]Extractor Predicting: 415it [04:08,  1.71it/s]Extractor Predicting: 416it [04:08,  1.67it/s]Extractor Predicting: 417it [04:09,  1.65it/s]Extractor Predicting: 418it [04:09,  1.67it/s]Extractor Predicting: 419it [04:10,  1.70it/s]Extractor Predicting: 420it [04:10,  1.76it/s]Extractor Predicting: 421it [04:11,  1.77it/s]Extractor Predicting: 422it [04:12,  1.84it/s]Extractor Predicting: 423it [04:12,  1.85it/s]Extractor Predicting: 424it [04:13,  1.82it/s]Extractor Predicting: 425it [04:13,  1.83it/s]Extractor Predicting: 426it [04:14,  1.80it/s]Extractor Predicting: 427it [04:14,  1.73it/s]Extractor Predicting: 428it [04:15,  1.67it/s]Extractor Predicting: 429it [04:16,  1.67it/s]Extractor Predicting: 430it [04:16,  1.69it/s]Extractor Predicting: 431it [04:17,  1.72it/s]Extractor Predicting: 432it [04:17,  1.74it/s]Extractor Predicting: 433it [04:18,  1.73it/s]Extractor Predicting: 434it [04:19,  1.68it/s]Extractor Predicting: 435it [04:19,  1.69it/s]Extractor Predicting: 436it [04:20,  1.69it/s]Extractor Predicting: 437it [04:20,  1.66it/s]Extractor Predicting: 438it [04:21,  1.64it/s]Extractor Predicting: 439it [04:22,  1.63it/s]Extractor Predicting: 440it [04:22,  1.61it/s]Extractor Predicting: 441it [04:23,  1.65it/s]Extractor Predicting: 442it [04:23,  1.65it/s]Extractor Predicting: 443it [04:24,  1.68it/s]Extractor Predicting: 444it [04:25,  1.66it/s]Extractor Predicting: 445it [04:25,  1.64it/s]Extractor Predicting: 446it [04:26,  1.62it/s]Extractor Predicting: 447it [04:26,  1.62it/s]Extractor Predicting: 448it [04:27,  1.63it/s]Extractor Predicting: 449it [04:28,  1.63it/s]Extractor Predicting: 450it [04:28,  1.68it/s]Extractor Predicting: 451it [04:29,  1.64it/s]Extractor Predicting: 452it [04:30,  1.63it/s]Extractor Predicting: 453it [04:30,  1.63it/s]Extractor Predicting: 454it [04:31,  1.62it/s]Extractor Predicting: 455it [04:31,  1.63it/s]Extractor Predicting: 456it [04:32,  1.62it/s]Extractor Predicting: 457it [04:33,  1.60it/s]Extractor Predicting: 458it [04:33,  1.62it/s]Extractor Predicting: 459it [04:34,  1.65it/s]Extractor Predicting: 460it [04:34,  1.64it/s]Extractor Predicting: 461it [04:35,  1.43it/s]Extractor Predicting: 462it [04:36,  1.48it/s]Extractor Predicting: 463it [04:37,  1.53it/s]Extractor Predicting: 464it [04:37,  1.57it/s]Extractor Predicting: 465it [04:38,  1.61it/s]Extractor Predicting: 466it [04:38,  1.62it/s]Extractor Predicting: 467it [04:39,  1.62it/s]Extractor Predicting: 468it [04:40,  1.63it/s]Extractor Predicting: 469it [04:40,  1.61it/s]Extractor Predicting: 470it [04:41,  1.60it/s]Extractor Predicting: 471it [04:41,  1.61it/s]Extractor Predicting: 472it [04:42,  1.65it/s]Extractor Predicting: 473it [04:43,  1.64it/s]Extractor Predicting: 474it [04:43,  1.63it/s]Extractor Predicting: 475it [04:44,  1.66it/s]Extractor Predicting: 476it [04:44,  1.68it/s]Extractor Predicting: 477it [04:45,  1.72it/s]Extractor Predicting: 478it [04:46,  1.75it/s]Extractor Predicting: 479it [04:46,  1.75it/s]Extractor Predicting: 480it [04:47,  1.67it/s]Extractor Predicting: 481it [04:47,  1.62it/s]Extractor Predicting: 482it [04:48,  1.58it/s]Extractor Predicting: 483it [04:49,  1.52it/s]Extractor Predicting: 484it [04:49,  1.54it/s]Extractor Predicting: 485it [04:50,  1.54it/s]Extractor Predicting: 486it [04:51,  1.58it/s]Extractor Predicting: 487it [04:51,  1.64it/s]Extractor Predicting: 488it [04:52,  1.69it/s]Extractor Predicting: 489it [04:52,  1.64it/s]Extractor Predicting: 490it [04:53,  1.64it/s]Extractor Predicting: 491it [04:54,  1.64it/s]Extractor Predicting: 492it [04:54,  1.64it/s]Extractor Predicting: 493it [04:55,  1.65it/s]Extractor Predicting: 494it [04:55,  1.63it/s]Extractor Predicting: 495it [04:56,  1.65it/s]Extractor Predicting: 496it [04:57,  1.70it/s]Extractor Predicting: 497it [04:57,  1.71it/s]Extractor Predicting: 498it [04:58,  1.67it/s]Extractor Predicting: 499it [04:58,  1.65it/s]Extractor Predicting: 500it [04:59,  1.66it/s]Extractor Predicting: 501it [05:00,  1.66it/s]Extractor Predicting: 502it [05:00,  1.71it/s]Extractor Predicting: 503it [05:01,  1.74it/s]Extractor Predicting: 504it [05:01,  1.75it/s]Extractor Predicting: 505it [05:02,  1.75it/s]Extractor Predicting: 506it [05:03,  1.70it/s]Extractor Predicting: 507it [05:03,  1.62it/s]Extractor Predicting: 508it [05:04,  1.60it/s]Extractor Predicting: 509it [05:04,  1.60it/s]Extractor Predicting: 510it [05:05,  1.66it/s]Extractor Predicting: 511it [05:06,  1.67it/s]Extractor Predicting: 512it [05:06,  1.68it/s]Extractor Predicting: 513it [05:07,  1.65it/s]Extractor Predicting: 514it [05:07,  1.62it/s]Extractor Predicting: 515it [05:08,  1.64it/s]Extractor Predicting: 516it [05:09,  1.63it/s]Extractor Predicting: 517it [05:09,  1.59it/s]Extractor Predicting: 518it [05:10,  1.60it/s]Extractor Predicting: 519it [05:11,  1.60it/s]Extractor Predicting: 520it [05:11,  1.63it/s]Extractor Predicting: 521it [05:12,  1.64it/s]Extractor Predicting: 522it [05:12,  1.63it/s]Extractor Predicting: 523it [05:13,  1.68it/s]Extractor Predicting: 524it [05:14,  1.67it/s]Extractor Predicting: 525it [05:14,  1.64it/s]Extractor Predicting: 526it [05:15,  1.62it/s]Extractor Predicting: 527it [05:15,  1.58it/s]Extractor Predicting: 528it [05:16,  1.60it/s]Extractor Predicting: 529it [05:17,  1.59it/s]Extractor Predicting: 530it [05:17,  1.57it/s]Extractor Predicting: 531it [05:18,  1.57it/s]Extractor Predicting: 532it [05:19,  1.56it/s]Extractor Predicting: 533it [05:19,  1.57it/s]Extractor Predicting: 534it [05:20,  1.59it/s]Extractor Predicting: 535it [05:21,  1.57it/s]Extractor Predicting: 536it [05:21,  1.56it/s]Extractor Predicting: 537it [05:22,  1.56it/s]Extractor Predicting: 538it [05:23,  1.56it/s]Extractor Predicting: 539it [05:23,  1.54it/s]Extractor Predicting: 540it [05:24,  1.56it/s]Extractor Predicting: 541it [05:24,  1.54it/s]Extractor Predicting: 542it [05:25,  1.55it/s]Extractor Predicting: 543it [05:26,  1.57it/s]Extractor Predicting: 544it [05:26,  1.52it/s]Extractor Predicting: 545it [05:27,  1.53it/s]Extractor Predicting: 546it [05:28,  1.53it/s]Extractor Predicting: 547it [05:28,  1.53it/s]Extractor Predicting: 548it [05:29,  1.54it/s]Extractor Predicting: 549it [05:30,  1.54it/s]Extractor Predicting: 550it [05:30,  1.56it/s]Extractor Predicting: 551it [05:31,  1.59it/s]Extractor Predicting: 552it [05:32,  1.59it/s]Extractor Predicting: 553it [05:32,  1.63it/s]Extractor Predicting: 554it [05:33,  1.64it/s]Extractor Predicting: 555it [05:33,  1.61it/s]Extractor Predicting: 556it [05:34,  1.62it/s]Extractor Predicting: 556it [05:34,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:45,571 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:45,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:45,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:45,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:45,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:02:46,169 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:02:46,170 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:02:46,732 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:02:47,799 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:02:47,801 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:50,651 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:50,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:50,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:50,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:02:50,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:02:51,282 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:02:51,285 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:02:51,858 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:02:52,035 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:02:52,035 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2939160239931448,
  "recall": 0.02571021662544037,
  "score": 0.04728425696167631,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'father', 'follows', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'part of the series', 'place of birth', 'production company', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 8650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.78it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.69it/s]Extractor Predicting: 8it [00:04,  1.73it/s]Extractor Predicting: 9it [00:05,  1.75it/s]Extractor Predicting: 10it [00:05,  1.72it/s]Extractor Predicting: 11it [00:06,  1.76it/s]Extractor Predicting: 12it [00:06,  1.75it/s]Extractor Predicting: 13it [00:07,  1.73it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:08,  1.64it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:10,  1.64it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:11,  1.59it/s]Extractor Predicting: 21it [00:12,  1.53it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:14,  1.54it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:15,  1.52it/s]Extractor Predicting: 27it [00:16,  1.54it/s]Extractor Predicting: 28it [00:17,  1.51it/s]Extractor Predicting: 29it [00:17,  1.55it/s]Extractor Predicting: 30it [00:18,  1.57it/s]Extractor Predicting: 31it [00:19,  1.46it/s]Extractor Predicting: 32it [00:20,  1.43it/s]Extractor Predicting: 33it [00:20,  1.50it/s]Extractor Predicting: 34it [00:21,  1.61it/s]Extractor Predicting: 35it [00:21,  1.62it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:24,  1.61it/s]Extractor Predicting: 40it [00:24,  1.61it/s]Extractor Predicting: 41it [00:25,  1.58it/s]Extractor Predicting: 42it [00:26,  1.56it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:27,  1.55it/s]Extractor Predicting: 45it [00:28,  1.56it/s]Extractor Predicting: 46it [00:28,  1.59it/s]Extractor Predicting: 47it [00:29,  1.58it/s]Extractor Predicting: 48it [00:29,  1.63it/s]Extractor Predicting: 49it [00:30,  1.60it/s]Extractor Predicting: 50it [00:31,  1.65it/s]Extractor Predicting: 51it [00:31,  1.64it/s]Extractor Predicting: 52it [00:32,  1.61it/s]Extractor Predicting: 53it [00:32,  1.68it/s]Extractor Predicting: 54it [00:33,  1.75it/s]Extractor Predicting: 55it [00:33,  1.84it/s]Extractor Predicting: 56it [00:34,  1.93it/s]Extractor Predicting: 57it [00:34,  1.98it/s]Extractor Predicting: 58it [00:35,  1.99it/s]Extractor Predicting: 59it [00:35,  2.01it/s]Extractor Predicting: 60it [00:36,  1.99it/s]Extractor Predicting: 61it [00:36,  1.98it/s]Extractor Predicting: 62it [00:37,  1.98it/s]Extractor Predicting: 63it [00:37,  1.97it/s]Extractor Predicting: 64it [00:38,  1.98it/s]Extractor Predicting: 65it [00:38,  2.01it/s]Extractor Predicting: 66it [00:39,  1.97it/s]Extractor Predicting: 67it [00:39,  1.97it/s]Extractor Predicting: 68it [00:40,  2.00it/s]Extractor Predicting: 69it [00:40,  2.02it/s]Extractor Predicting: 70it [00:41,  2.05it/s]Extractor Predicting: 71it [00:41,  2.03it/s]Extractor Predicting: 72it [00:42,  2.01it/s]Extractor Predicting: 73it [00:42,  2.07it/s]Extractor Predicting: 74it [00:43,  1.98it/s]Extractor Predicting: 75it [00:43,  2.00it/s]Extractor Predicting: 76it [00:44,  1.96it/s]Extractor Predicting: 77it [00:44,  1.95it/s]Extractor Predicting: 78it [00:45,  1.98it/s]Extractor Predicting: 79it [00:45,  1.99it/s]Extractor Predicting: 80it [00:46,  1.99it/s]Extractor Predicting: 81it [00:46,  2.01it/s]Extractor Predicting: 82it [00:47,  1.94it/s]Extractor Predicting: 83it [00:48,  1.83it/s]Extractor Predicting: 84it [00:48,  1.72it/s]Extractor Predicting: 85it [00:49,  1.64it/s]Extractor Predicting: 86it [00:50,  1.60it/s]Extractor Predicting: 87it [00:50,  1.58it/s]Extractor Predicting: 88it [00:51,  1.57it/s]Extractor Predicting: 89it [00:51,  1.56it/s]Extractor Predicting: 90it [00:52,  1.54it/s]Extractor Predicting: 91it [00:53,  1.54it/s]Extractor Predicting: 92it [00:53,  1.53it/s]Extractor Predicting: 93it [00:54,  1.53it/s]Extractor Predicting: 94it [00:55,  1.52it/s]Extractor Predicting: 95it [00:55,  1.53it/s]Extractor Predicting: 96it [00:56,  1.51it/s]Extractor Predicting: 97it [00:57,  1.52it/s]Extractor Predicting: 98it [00:57,  1.54it/s]Extractor Predicting: 99it [00:58,  1.54it/s]Extractor Predicting: 100it [00:59,  1.55it/s]Extractor Predicting: 101it [00:59,  1.57it/s]Extractor Predicting: 102it [01:00,  1.57it/s]Extractor Predicting: 103it [01:01,  1.55it/s]Extractor Predicting: 104it [01:01,  1.38it/s]Extractor Predicting: 104it [01:01,  1.68it/s]
[INFO|configuration_utils.py:515] 2023-08-29 06:03:55,406 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:03:55,406 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 06:03:55,414 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:03:55,415 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 06:03:55,417 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 06:03:58,590 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 06:03:58,593 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 06:03:58,614 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:03:58,615 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 06:03:58,622 >> Didn't find file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:03:58,633 >> loading file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:03:58,633 >> loading file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:03:58,633 >> loading file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:03:58,633 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:03:58,633 >> loading file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:03:58,633 >> loading file outputs/wrapper/wiki/unseen_15_seed_4/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7777777777777778,
  "recall": 0.09950156913420713,
  "score": 0.17643207855973814,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 06:03:58,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:03:59,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:03:59,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:00,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:01,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:01,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:02,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:02,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:03,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:03,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:04,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:05,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:05,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:06,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:06,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:07,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:08,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:08,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:09,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:09,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:10,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:11,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:11,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:13<04:12, 13.30s/it][WARNING|generation_utils.py:914] 2023-08-29 06:04:12,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:12,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:13,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:13,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:14,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:14,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:15,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:16,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:16,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:17,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:17,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:18,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:18,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:19,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:20,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:20,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:21,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:22,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:22,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:23,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:23,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:25<03:47, 12.61s/it][WARNING|generation_utils.py:914] 2023-08-29 06:04:24,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:24,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:25,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:25,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:26,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:27,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:27,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:28,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:28,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:29,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:29,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:30,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:30,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:31,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:31,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:32,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:33,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:33,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:34,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:34,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:35,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:36,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:36,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:38<03:37, 12.79s/it][WARNING|generation_utils.py:914] 2023-08-29 06:04:37,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:37,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:38,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:38,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:39,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:40,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:40,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:41,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:41,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:42,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:42,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:43,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:43,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:44,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:45,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:45,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:46,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:46,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:47,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:48,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:48,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:49,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:49,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:51<03:26, 12.88s/it][WARNING|generation_utils.py:914] 2023-08-29 06:04:50,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:50,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:51,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:52,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:52,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:53,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:53,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:54,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:54,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:55,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:56,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:56,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:57,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:57,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:58,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:59,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:04:59,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:00,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:00,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:01,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:01,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:02,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:02,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:04<03:15, 13.01s/it][WARNING|generation_utils.py:914] 2023-08-29 06:05:03,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:04,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:04,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:05,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:05,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:06,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:06,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:07,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:08,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:08,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:09,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:09,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:10,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:10,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:11,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:11,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:12,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:13,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:13,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:14,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:15,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:15,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:16,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:17,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:18<03:07, 13.37s/it][WARNING|generation_utils.py:914] 2023-08-29 06:05:17,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:18,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:18,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:19,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:20,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:20,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:21,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:22,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:22,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:23,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:24,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:24,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:25,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:26,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:26,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:27,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:28,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:28,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:29,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:29,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:30,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:31,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:32<02:56, 13.60s/it][WARNING|generation_utils.py:914] 2023-08-29 06:05:31,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:32,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:33,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:34,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:34,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:35,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:35,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:36,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:37,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:37,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:38,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:38,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:39,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:40,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:40,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:41,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:42,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:42,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:43,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:44,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:44,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:45,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:46<02:45, 13.78s/it][WARNING|generation_utils.py:914] 2023-08-29 06:05:45,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:46,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:46,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:47,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:48,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:48,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:49,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:49,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:50,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:50,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:51,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:51,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:52,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:52,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:53,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:54,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:54,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:55,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:55,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:56,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:56,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:57,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:59<02:26, 13.31s/it][WARNING|generation_utils.py:914] 2023-08-29 06:05:58,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:58,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:59,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:05:59,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:00,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:01,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:01,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:02,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:02,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:03,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:03,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:04,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:04,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:05,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:06,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:06,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:07,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:07,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:08,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:08,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:09,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:09,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:10,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:12<02:11, 13.14s/it][WARNING|generation_utils.py:914] 2023-08-29 06:06:10,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:11,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:12,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:12,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:13,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:14,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:14,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:15,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:15,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:16,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:17,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:17,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:18,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:18,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:19,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:19,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:20,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:20,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:21,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:22,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:22,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:23,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:23,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:25<01:59, 13.31s/it][WARNING|generation_utils.py:914] 2023-08-29 06:06:24,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:25,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:25,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:26,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:27,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:28,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:28,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:29,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:29,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:30,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:31,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:31,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:32,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:33,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:33,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:34,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:35,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:35,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:36,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:37,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:37,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:39<01:47, 13.42s/it][WARNING|generation_utils.py:914] 2023-08-29 06:06:38,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:38,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:39,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:39,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:40,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:40,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:41,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:41,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:42,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:42,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:43,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:44,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:44,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:45,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:45,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:46,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:46,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:47,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:47,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:48,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:48,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:49,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:51<01:30, 12.94s/it][WARNING|generation_utils.py:914] 2023-08-29 06:06:50,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:50,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:51,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:51,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:52,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:52,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:53,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:54,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:54,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:55,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:55,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:56,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:56,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:57,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:57,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:58,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:59,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:06:59,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:00,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:00,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:01,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:03<01:15, 12.62s/it][WARNING|generation_utils.py:914] 2023-08-29 06:07:02,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:02,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:03,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:03,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:04,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:04,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:05,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:05,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:06,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:06,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:07,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:08,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:08,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:09,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:09,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:10,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:10,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:11,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:11,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:12,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:13,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:14<01:01, 12.32s/it][WARNING|generation_utils.py:914] 2023-08-29 06:07:13,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:14,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:14,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:15,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:15,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:16,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:16,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:17,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:18,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:18,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:19,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:19,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:20,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:20,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:21,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:22,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:22,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:23,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:24,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:24,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:25,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:26,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:27<00:50, 12.56s/it][WARNING|generation_utils.py:914] 2023-08-29 06:07:26,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:27,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:27,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:28,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:28,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:29,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:29,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:30,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:31,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:31,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:32,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:32,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:33,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:33,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:34,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:35,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:35,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:36,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:36,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:37,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:37,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:38,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:38,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:39,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:39,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:40,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:41<00:39, 13.02s/it][WARNING|generation_utils.py:914] 2023-08-29 06:07:40,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:41,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:42,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:42,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:43,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:43,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:44,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:45,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:45,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:46,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:46,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:47,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:48,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:48,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:49,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:49,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:50,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:51,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:51,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:52,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:52,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:53,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:53,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:54,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:55,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:57<00:27, 13.65s/it][WARNING|generation_utils.py:914] 2023-08-29 06:07:55,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:56,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:57,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:57,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:58,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:58,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:59,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:07:59,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:00,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:00,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:01,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:01,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:02,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:02,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:03,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:04,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:04,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:05,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:05,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:06,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:06,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:07,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:07,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:09<00:13, 13.31s/it][WARNING|generation_utils.py:914] 2023-08-29 06:08:08,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:09,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:09,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:10,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:10,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:11,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:12,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:12,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:13,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:13,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:14,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:15,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:15,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:16,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:17,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:17,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:18,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:18,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:19,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:19,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:20,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:20,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:08:21,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:23<00:00, 13.37s/it]Generating: 100%|██████████| 20/20 [04:23<00:00, 13.15s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:28,860 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:28,864 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:28,864 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:28,864 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:28,864 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:08:29,582 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:08:29,583 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:08:30,143 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:08:31,250 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:08:31,250 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:34,222 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:34,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:34,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:34,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:34,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:08:34,864 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:08:34,865 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:08:35,437 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:08:35,617 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:08:35,617 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : inception .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9002976190476191, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 575, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : military branch .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8478260869565217, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : applies to jurisdiction .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : family name .', 'success_rate': 0.8607954545454546, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : father .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : follows .', 'success_rate': 0.8636363636363636, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8369565217391305, 'errors': {'', "('MTV', 'genre', '', 'In June 2005 , it was announced that it had signed to MTV in the future , as the series first show to feature an all female cast .')", "('The Legend of Zelda', 'genre', '', 'A member of the underground scene , he has appeared in the video game Metal Gear Solid for Sony Computer Entertainment ( also known as Sony ) and as the voice of Nacho in the video game The Legend of Zelda .')"}}
['Relation : is a list of . Context : The World Wide Web ( HTTP ) is a widely used web protocol invented in 2002 at Google . Head Entity : HTTP , Tail Entity : WW .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('I', 'is a list of', '', 'The first five digits of the alphabet are B , C , D , E , F , G , H , I , J , K , L , M , N , O , P , Q , R , S , and U.')", "('Latin', 'is a list of', '', 'A large number ( probably all ) of letters , or symbols , are found in Latin or Greek , as well as in Aramaic , Syriac , and in many other languages ( particularly in the modern West ) .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8892045454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('first single', 'lyrics by', '', 'Following the death of his father , in 1169 he was brought to Canada and took part in the founding of the Confederation , in which he composed the first single of the second century .')", "('Sing It On', 'lyrics by', '', 'The album reached number four on the Billboard Hot Country Songs chart on August 5 , 1996 , followed by the single Sing It On.')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9226190476190477, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : member of . Context : Later in the year he came to London , where he established himself in other forms at different clubs and had success in various competitions . Head Entity : John , Tail Entity : London .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8536931818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 89, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 272, 'raw': 384}
{'target': 600, 'success': 296, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 347, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 395, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 447, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 534, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 583, 'raw': 800}
{'target': 600, 'success': 610, 'raw': 832}
{'prompt': 'Relation : part of the series .', 'success_rate': 0.7331730769230769, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : place of birth . Context : Later in Life he studied at the Conservatory of Vienna ( now in Vienna ) at the age of eleven , where he also made his first recordings . Head Entity : Wolfgang von Bismarck , Tail Entity : Vienna .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 408, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 478, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 606, 'raw': 800}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.7575, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : production company .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : use .', 'success_rate': 0.8288043478260869, 'errors': {'', "('Asclepius of Apulia', 'use', '', 'According to The Guardian , he said that to the West it was a kind of medievalism , and he had a similar interpretation of the Old Testament , which he named Asclepius of Apulia .')", "('C#', 'use', '', 'is an ini file format adapted from C# , .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 16561
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16661, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.66it/s]Extractor Estimating: 2it [00:01,  1.61it/s]Extractor Estimating: 3it [00:01,  1.66it/s]Extractor Estimating: 4it [00:02,  1.72it/s]Extractor Estimating: 5it [00:02,  1.72it/s]Extractor Estimating: 6it [00:03,  1.71it/s]Extractor Estimating: 7it [00:04,  1.73it/s]Extractor Estimating: 8it [00:04,  1.72it/s]Extractor Estimating: 9it [00:05,  1.72it/s]Extractor Estimating: 10it [00:05,  1.68it/s]Extractor Estimating: 11it [00:06,  1.65it/s]Extractor Estimating: 12it [00:07,  1.67it/s]Extractor Estimating: 13it [00:07,  1.69it/s]Extractor Estimating: 14it [00:08,  1.66it/s]Extractor Estimating: 15it [00:08,  1.66it/s]Extractor Estimating: 16it [00:09,  1.67it/s]Extractor Estimating: 17it [00:10,  1.70it/s]Extractor Estimating: 18it [00:10,  1.70it/s]Extractor Estimating: 19it [00:11,  1.70it/s]Extractor Estimating: 20it [00:11,  1.65it/s]Extractor Estimating: 21it [00:12,  1.65it/s]Extractor Estimating: 22it [00:13,  1.71it/s]Extractor Estimating: 23it [00:13,  1.64it/s]Extractor Estimating: 24it [00:14,  1.68it/s]Extractor Estimating: 25it [00:14,  1.67it/s]Extractor Estimating: 26it [00:15,  1.72it/s]Extractor Estimating: 27it [00:15,  1.75it/s]Extractor Estimating: 28it [00:16,  1.70it/s]Extractor Estimating: 29it [00:17,  1.74it/s]Extractor Estimating: 30it [00:17,  1.77it/s]Extractor Estimating: 31it [00:18,  1.77it/s]Extractor Estimating: 32it [00:18,  1.75it/s]Extractor Estimating: 33it [00:19,  1.75it/s]Extractor Estimating: 34it [00:19,  1.74it/s]Extractor Estimating: 35it [00:20,  1.78it/s]Extractor Estimating: 36it [00:21,  1.78it/s]Extractor Estimating: 37it [00:21,  1.75it/s]Extractor Estimating: 38it [00:22,  1.76it/s]Extractor Estimating: 39it [00:22,  1.73it/s]Extractor Estimating: 40it [00:23,  1.70it/s]Extractor Estimating: 41it [00:24,  1.71it/s]Extractor Estimating: 42it [00:24,  1.73it/s]Extractor Estimating: 43it [00:25,  1.78it/s]Extractor Estimating: 44it [00:25,  1.71it/s]Extractor Estimating: 45it [00:26,  1.68it/s]Extractor Estimating: 46it [00:26,  1.67it/s]Extractor Estimating: 47it [00:27,  1.69it/s]Extractor Estimating: 48it [00:28,  1.71it/s]Extractor Estimating: 49it [00:28,  1.76it/s]Extractor Estimating: 50it [00:29,  1.73it/s]Extractor Estimating: 51it [00:29,  1.69it/s]Extractor Estimating: 52it [00:30,  1.66it/s]Extractor Estimating: 53it [00:31,  1.72it/s]Extractor Estimating: 54it [00:31,  1.74it/s]Extractor Estimating: 55it [00:32,  1.70it/s]Extractor Estimating: 56it [00:32,  1.74it/s]Extractor Estimating: 57it [00:33,  1.75it/s]Extractor Estimating: 58it [00:33,  1.74it/s]Extractor Estimating: 59it [00:34,  1.77it/s]Extractor Estimating: 60it [00:34,  1.81it/s]Extractor Estimating: 61it [00:35,  1.83it/s]Extractor Estimating: 62it [00:36,  1.81it/s]Extractor Estimating: 63it [00:36,  1.72it/s]Extractor Estimating: 64it [00:37,  1.61it/s]Extractor Estimating: 65it [00:37,  1.69it/s]Extractor Estimating: 66it [00:38,  1.68it/s]Extractor Estimating: 67it [00:39,  1.69it/s]Extractor Estimating: 68it [00:39,  1.71it/s]Extractor Estimating: 69it [00:40,  1.71it/s]Extractor Estimating: 70it [00:40,  1.69it/s]Extractor Estimating: 71it [00:41,  1.63it/s]Extractor Estimating: 72it [00:42,  1.67it/s]Extractor Estimating: 73it [00:42,  1.69it/s]Extractor Estimating: 74it [00:43,  1.71it/s]Extractor Estimating: 75it [00:43,  1.66it/s]Extractor Estimating: 76it [00:44,  1.68it/s]Extractor Estimating: 77it [00:45,  1.65it/s]Extractor Estimating: 78it [00:45,  1.64it/s]Extractor Estimating: 79it [00:46,  1.60it/s]Extractor Estimating: 80it [00:46,  1.64it/s]Extractor Estimating: 81it [00:47,  1.66it/s]Extractor Estimating: 82it [00:48,  1.69it/s]Extractor Estimating: 83it [00:48,  1.74it/s]Extractor Estimating: 84it [00:49,  1.69it/s]Extractor Estimating: 85it [00:49,  1.70it/s]Extractor Estimating: 86it [00:50,  1.68it/s]Extractor Estimating: 87it [00:51,  1.71it/s]Extractor Estimating: 88it [00:51,  1.71it/s]Extractor Estimating: 89it [00:52,  1.70it/s]Extractor Estimating: 90it [00:52,  1.67it/s]Extractor Estimating: 91it [00:53,  1.72it/s]Extractor Estimating: 92it [00:53,  1.74it/s]Extractor Estimating: 93it [00:54,  1.73it/s]Extractor Estimating: 94it [00:55,  1.67it/s]Extractor Estimating: 95it [00:55,  1.56it/s]Extractor Estimating: 96it [00:56,  1.60it/s]Extractor Estimating: 97it [00:57,  1.64it/s]Extractor Estimating: 98it [00:57,  1.62it/s]Extractor Estimating: 99it [00:58,  1.62it/s]Extractor Estimating: 100it [00:58,  1.67it/s]Extractor Estimating: 101it [00:59,  1.67it/s]Extractor Estimating: 102it [01:00,  1.65it/s]Extractor Estimating: 103it [01:00,  1.63it/s]Extractor Estimating: 104it [01:01,  1.65it/s]Extractor Estimating: 105it [01:01,  1.69it/s]Extractor Estimating: 106it [01:02,  1.68it/s]Extractor Estimating: 107it [01:03,  1.71it/s]Extractor Estimating: 108it [01:03,  1.71it/s]Extractor Estimating: 109it [01:04,  1.69it/s]Extractor Estimating: 110it [01:04,  1.68it/s]Extractor Estimating: 111it [01:05,  1.68it/s]Extractor Estimating: 112it [01:06,  1.69it/s]Extractor Estimating: 113it [01:06,  1.66it/s]Extractor Estimating: 114it [01:07,  1.66it/s]Extractor Estimating: 115it [01:07,  1.64it/s]Extractor Estimating: 116it [01:08,  1.68it/s]Extractor Estimating: 117it [01:09,  1.64it/s]Extractor Estimating: 118it [01:09,  1.63it/s]Extractor Estimating: 119it [01:10,  1.64it/s]Extractor Estimating: 120it [01:10,  1.62it/s]Extractor Estimating: 121it [01:11,  1.65it/s]Extractor Estimating: 122it [01:12,  1.67it/s]Extractor Estimating: 123it [01:12,  1.65it/s]Extractor Estimating: 124it [01:13,  1.63it/s]Extractor Estimating: 125it [01:13,  1.67it/s]Extractor Estimating: 126it [01:14,  1.63it/s]Extractor Estimating: 127it [01:15,  1.62it/s]Extractor Estimating: 128it [01:15,  1.62it/s]Extractor Estimating: 129it [01:16,  1.63it/s]Extractor Estimating: 130it [01:16,  1.67it/s]Extractor Estimating: 131it [01:17,  1.64it/s]Extractor Estimating: 132it [01:18,  1.63it/s]Extractor Estimating: 133it [01:18,  1.65it/s]Extractor Estimating: 134it [01:19,  1.61it/s]Extractor Estimating: 135it [01:20,  1.58it/s]Extractor Estimating: 136it [01:20,  1.48it/s]Extractor Estimating: 137it [01:21,  1.52it/s]Extractor Estimating: 138it [01:22,  1.54it/s]Extractor Estimating: 139it [01:22,  1.55it/s]Extractor Estimating: 140it [01:23,  1.59it/s]Extractor Estimating: 141it [01:23,  1.63it/s]Extractor Estimating: 142it [01:24,  1.60it/s]Extractor Estimating: 143it [01:25,  1.63it/s]Extractor Estimating: 144it [01:25,  1.62it/s]Extractor Estimating: 145it [01:26,  1.56it/s]Extractor Estimating: 146it [01:27,  1.58it/s]Extractor Estimating: 147it [01:27,  1.57it/s]Extractor Estimating: 148it [01:28,  1.57it/s]Extractor Estimating: 149it [01:29,  1.57it/s]Extractor Estimating: 150it [01:29,  1.60it/s]Extractor Estimating: 151it [01:30,  1.59it/s]Extractor Estimating: 152it [01:30,  1.55it/s]Extractor Estimating: 153it [01:31,  1.54it/s]Extractor Estimating: 154it [01:32,  1.53it/s]Extractor Estimating: 155it [01:32,  1.52it/s]Extractor Estimating: 156it [01:33,  1.54it/s]Extractor Estimating: 157it [01:34,  1.49it/s]Extractor Estimating: 158it [01:34,  1.55it/s]Extractor Estimating: 159it [01:35,  1.55it/s]Extractor Estimating: 160it [01:36,  1.54it/s]Extractor Estimating: 161it [01:36,  1.57it/s]Extractor Estimating: 162it [01:37,  1.57it/s]Extractor Estimating: 163it [01:38,  1.54it/s]Extractor Estimating: 164it [01:38,  1.55it/s]Extractor Estimating: 165it [01:39,  1.58it/s]Extractor Estimating: 166it [01:40,  1.59it/s]Extractor Estimating: 167it [01:40,  1.58it/s]Extractor Estimating: 168it [01:41,  1.59it/s]Extractor Estimating: 169it [01:41,  1.55it/s]Extractor Estimating: 170it [01:42,  1.56it/s]Extractor Estimating: 171it [01:43,  1.57it/s]Extractor Estimating: 172it [01:43,  1.58it/s]Extractor Estimating: 173it [01:44,  1.58it/s]Extractor Estimating: 174it [01:45,  1.59it/s]Extractor Estimating: 175it [01:45,  1.61it/s]Extractor Estimating: 176it [01:46,  1.50it/s]Extractor Estimating: 177it [01:47,  1.49it/s]Extractor Estimating: 178it [01:47,  1.55it/s]Extractor Estimating: 179it [01:48,  1.55it/s]Extractor Estimating: 180it [01:48,  1.60it/s]Extractor Estimating: 181it [01:49,  1.64it/s]Extractor Estimating: 182it [01:50,  1.61it/s]Extractor Estimating: 183it [01:50,  1.64it/s]Extractor Estimating: 184it [01:51,  1.65it/s]Extractor Estimating: 185it [01:51,  1.65it/s]Extractor Estimating: 186it [01:52,  1.64it/s]Extractor Estimating: 187it [01:53,  1.67it/s]Extractor Estimating: 188it [01:53,  1.61it/s]Extractor Estimating: 189it [01:54,  1.55it/s]Extractor Estimating: 190it [01:55,  1.55it/s]Extractor Estimating: 191it [01:55,  1.59it/s]Extractor Estimating: 192it [01:56,  1.60it/s]Extractor Estimating: 193it [01:57,  1.54it/s]Extractor Estimating: 194it [01:57,  1.53it/s]Extractor Estimating: 195it [01:58,  1.52it/s]Extractor Estimating: 196it [01:59,  1.52it/s]Extractor Estimating: 197it [01:59,  1.53it/s]Extractor Estimating: 198it [02:00,  1.53it/s]Extractor Estimating: 199it [02:00,  1.59it/s]Extractor Estimating: 200it [02:01,  1.60it/s]Extractor Estimating: 201it [02:02,  1.65it/s]Extractor Estimating: 202it [02:02,  1.64it/s]Extractor Estimating: 203it [02:03,  1.65it/s]Extractor Estimating: 204it [02:03,  1.66it/s]Extractor Estimating: 205it [02:04,  1.69it/s]Extractor Estimating: 206it [02:05,  1.73it/s]Extractor Estimating: 207it [02:05,  1.72it/s]Extractor Estimating: 208it [02:06,  1.72it/s]Extractor Estimating: 209it [02:06,  1.74it/s]Extractor Estimating: 210it [02:07,  1.77it/s]Extractor Estimating: 211it [02:07,  1.74it/s]Extractor Estimating: 212it [02:08,  1.77it/s]Extractor Estimating: 213it [02:09,  1.77it/s]Extractor Estimating: 214it [02:09,  1.71it/s]Extractor Estimating: 215it [02:10,  1.55it/s]Extractor Estimating: 216it [02:11,  1.56it/s]Extractor Estimating: 217it [02:11,  1.58it/s]Extractor Estimating: 218it [02:12,  1.58it/s]Extractor Estimating: 219it [02:12,  1.60it/s]Extractor Estimating: 220it [02:13,  1.64it/s]Extractor Estimating: 221it [02:14,  1.64it/s]Extractor Estimating: 222it [02:14,  1.61it/s]Extractor Estimating: 223it [02:15,  1.63it/s]Extractor Estimating: 224it [02:15,  1.63it/s]Extractor Estimating: 225it [02:16,  1.67it/s]Extractor Estimating: 226it [02:17,  1.69it/s]Extractor Estimating: 227it [02:17,  1.68it/s]Extractor Estimating: 228it [02:18,  1.71it/s]Extractor Estimating: 229it [02:18,  1.73it/s]Extractor Estimating: 230it [02:19,  1.72it/s]Extractor Estimating: 231it [02:19,  1.73it/s]Extractor Estimating: 232it [02:20,  1.72it/s]Extractor Estimating: 233it [02:21,  1.74it/s]Extractor Estimating: 234it [02:21,  1.76it/s]Extractor Estimating: 235it [02:22,  1.78it/s]Extractor Estimating: 236it [02:22,  1.76it/s]Extractor Estimating: 237it [02:23,  1.78it/s]Extractor Estimating: 238it [02:23,  1.76it/s]Extractor Estimating: 239it [02:24,  1.75it/s]Extractor Estimating: 240it [02:25,  1.80it/s]Extractor Estimating: 241it [02:25,  1.76it/s]Extractor Estimating: 242it [02:26,  1.77it/s]Extractor Estimating: 243it [02:26,  1.72it/s]Extractor Estimating: 244it [02:27,  1.74it/s]Extractor Estimating: 245it [02:27,  1.75it/s]Extractor Estimating: 246it [02:28,  1.80it/s]Extractor Estimating: 247it [02:29,  1.74it/s]Extractor Estimating: 248it [02:29,  1.72it/s]Extractor Estimating: 249it [02:30,  1.76it/s]Extractor Estimating: 250it [02:30,  1.80it/s]Extractor Estimating: 251it [02:31,  1.82it/s]Extractor Estimating: 252it [02:31,  1.82it/s]Extractor Estimating: 253it [02:32,  1.81it/s]Extractor Estimating: 254it [02:32,  1.78it/s]Extractor Estimating: 255it [02:33,  1.72it/s]Extractor Estimating: 256it [02:34,  1.76it/s]Extractor Estimating: 257it [02:34,  1.80it/s]Extractor Estimating: 258it [02:35,  1.71it/s]Extractor Estimating: 259it [02:35,  1.73it/s]Extractor Estimating: 260it [02:36,  1.73it/s]Extractor Estimating: 261it [02:36,  1.77it/s]Extractor Estimating: 262it [02:37,  1.76it/s]Extractor Estimating: 263it [02:38,  1.74it/s]Extractor Estimating: 264it [02:38,  1.76it/s]Extractor Estimating: 265it [02:39,  1.77it/s]Extractor Estimating: 266it [02:39,  1.68it/s]Extractor Estimating: 267it [02:40,  1.67it/s]Extractor Estimating: 268it [02:41,  1.66it/s]Extractor Estimating: 269it [02:41,  1.75it/s]Extractor Estimating: 270it [02:42,  1.79it/s]Extractor Estimating: 271it [02:42,  1.81it/s]Extractor Estimating: 272it [02:43,  1.80it/s]Extractor Estimating: 273it [02:43,  1.74it/s]Extractor Estimating: 274it [02:44,  1.71it/s]Extractor Estimating: 275it [02:44,  1.80it/s]Extractor Estimating: 276it [02:45,  1.79it/s]Extractor Estimating: 277it [02:46,  1.82it/s]Extractor Estimating: 278it [02:46,  1.83it/s]Extractor Estimating: 279it [02:47,  1.83it/s]Extractor Estimating: 280it [02:47,  1.81it/s]Extractor Estimating: 281it [02:48,  1.80it/s]Extractor Estimating: 282it [02:48,  1.84it/s]Extractor Estimating: 283it [02:49,  1.84it/s]Extractor Estimating: 284it [02:49,  1.82it/s]Extractor Estimating: 285it [02:50,  1.85it/s]Extractor Estimating: 286it [02:51,  1.66it/s]Extractor Estimating: 287it [02:51,  1.69it/s]Extractor Estimating: 288it [02:52,  1.73it/s]Extractor Estimating: 289it [02:52,  1.78it/s]Extractor Estimating: 290it [02:53,  1.83it/s]Extractor Estimating: 291it [02:53,  1.84it/s]Extractor Estimating: 292it [02:54,  1.83it/s]Extractor Estimating: 293it [02:54,  1.85it/s]Extractor Estimating: 294it [02:55,  1.84it/s]Extractor Estimating: 295it [02:56,  1.84it/s]Extractor Estimating: 296it [02:56,  1.85it/s]Extractor Estimating: 297it [02:57,  1.87it/s]Extractor Estimating: 298it [02:57,  1.87it/s]Extractor Estimating: 299it [02:58,  1.83it/s]Extractor Estimating: 300it [02:58,  1.86it/s]Extractor Estimating: 301it [02:59,  1.85it/s]Extractor Estimating: 302it [02:59,  1.83it/s]Extractor Estimating: 303it [03:00,  1.89it/s]Extractor Estimating: 304it [03:00,  1.87it/s]Extractor Estimating: 305it [03:01,  1.88it/s]Extractor Estimating: 306it [03:01,  1.92it/s]Extractor Estimating: 307it [03:02,  1.85it/s]Extractor Estimating: 308it [03:03,  1.84it/s]Extractor Estimating: 309it [03:03,  1.82it/s]Extractor Estimating: 310it [03:04,  1.76it/s]Extractor Estimating: 311it [03:04,  1.78it/s]Extractor Estimating: 312it [03:05,  1.81it/s]Extractor Estimating: 313it [03:05,  1.85it/s]Extractor Estimating: 314it [03:06,  1.84it/s]Extractor Estimating: 315it [03:06,  1.87it/s]Extractor Estimating: 316it [03:07,  1.83it/s]Extractor Estimating: 317it [03:07,  1.84it/s]Extractor Estimating: 318it [03:08,  1.80it/s]Extractor Estimating: 319it [03:09,  1.85it/s]Extractor Estimating: 320it [03:09,  1.84it/s]Extractor Estimating: 321it [03:10,  1.83it/s]Extractor Estimating: 322it [03:10,  1.86it/s]Extractor Estimating: 323it [03:11,  1.83it/s]Extractor Estimating: 324it [03:11,  1.85it/s]Extractor Estimating: 325it [03:12,  1.82it/s]Extractor Estimating: 326it [03:12,  1.77it/s]Extractor Estimating: 327it [03:13,  1.74it/s]Extractor Estimating: 328it [03:14,  1.71it/s]Extractor Estimating: 329it [03:14,  1.71it/s]Extractor Estimating: 330it [03:15,  1.62it/s]Extractor Estimating: 331it [03:16,  1.61it/s]Extractor Estimating: 332it [03:16,  1.63it/s]Extractor Estimating: 333it [03:17,  1.63it/s]Extractor Estimating: 334it [03:17,  1.59it/s]Extractor Estimating: 335it [03:18,  1.56it/s]Extractor Estimating: 336it [03:19,  1.60it/s]Extractor Estimating: 337it [03:19,  1.65it/s]Extractor Estimating: 338it [03:20,  1.65it/s]Extractor Estimating: 339it [03:20,  1.70it/s]Extractor Estimating: 340it [03:21,  1.64it/s]Extractor Estimating: 341it [03:22,  1.67it/s]Extractor Estimating: 342it [03:22,  1.70it/s]Extractor Estimating: 343it [03:23,  1.66it/s]Extractor Estimating: 344it [03:23,  1.64it/s]Extractor Estimating: 345it [03:24,  1.61it/s]Extractor Estimating: 346it [03:25,  1.61it/s]Extractor Estimating: 347it [03:25,  1.63it/s]Extractor Estimating: 348it [03:26,  1.68it/s]Extractor Estimating: 349it [03:26,  1.73it/s]Extractor Estimating: 350it [03:27,  1.70it/s]Extractor Estimating: 351it [03:28,  1.72it/s]Extractor Estimating: 352it [03:28,  1.76it/s]Extractor Estimating: 353it [03:29,  1.74it/s]Extractor Estimating: 354it [03:29,  1.78it/s]Extractor Estimating: 355it [03:30,  1.77it/s]Extractor Estimating: 356it [03:30,  1.74it/s]Extractor Estimating: 357it [03:31,  1.76it/s]Extractor Estimating: 358it [03:32,  1.80it/s]Extractor Estimating: 359it [03:32,  1.84it/s]Extractor Estimating: 360it [03:33,  1.91it/s]Extractor Estimating: 361it [03:33,  1.79it/s]Extractor Estimating: 362it [03:34,  1.75it/s]Extractor Estimating: 363it [03:34,  1.78it/s]Extractor Estimating: 364it [03:35,  1.85it/s]Extractor Estimating: 365it [03:35,  1.78it/s]Extractor Estimating: 366it [03:36,  1.76it/s]Extractor Estimating: 367it [03:37,  1.78it/s]Extractor Estimating: 368it [03:37,  1.76it/s]Extractor Estimating: 369it [03:38,  1.72it/s]Extractor Estimating: 370it [03:38,  1.73it/s]Extractor Estimating: 371it [03:39,  1.77it/s]Extractor Estimating: 372it [03:39,  1.67it/s]Extractor Estimating: 373it [03:40,  1.68it/s]Extractor Estimating: 374it [03:41,  1.74it/s]Extractor Estimating: 375it [03:41,  1.78it/s]Extractor Estimating: 376it [03:42,  1.78it/s]Extractor Estimating: 377it [03:42,  1.70it/s]Extractor Estimating: 378it [03:43,  1.67it/s]Extractor Estimating: 379it [03:44,  1.66it/s]Extractor Estimating: 380it [03:44,  1.63it/s]Extractor Estimating: 381it [03:45,  1.64it/s]Extractor Estimating: 382it [03:45,  1.66it/s]Extractor Estimating: 383it [03:46,  1.64it/s]Extractor Estimating: 384it [03:47,  1.68it/s]Extractor Estimating: 385it [03:47,  1.51it/s]Extractor Estimating: 386it [03:48,  1.56it/s]Extractor Estimating: 387it [03:49,  1.62it/s]Extractor Estimating: 388it [03:49,  1.60it/s]Extractor Estimating: 389it [03:50,  1.64it/s]Extractor Estimating: 390it [03:50,  1.68it/s]Extractor Estimating: 391it [03:51,  1.63it/s]Extractor Estimating: 392it [03:52,  1.63it/s]Extractor Estimating: 393it [03:52,  1.64it/s]Extractor Estimating: 394it [03:53,  1.61it/s]Extractor Estimating: 395it [03:53,  1.65it/s]Extractor Estimating: 396it [03:54,  1.66it/s]Extractor Estimating: 397it [03:55,  1.72it/s]Extractor Estimating: 398it [03:55,  1.71it/s]Extractor Estimating: 399it [03:56,  1.70it/s]Extractor Estimating: 400it [03:56,  1.70it/s]Extractor Estimating: 401it [03:57,  1.74it/s]Extractor Estimating: 402it [03:57,  1.75it/s]Extractor Estimating: 403it [03:58,  1.75it/s]Extractor Estimating: 404it [03:59,  1.73it/s]Extractor Estimating: 405it [03:59,  1.72it/s]Extractor Estimating: 406it [04:00,  1.78it/s]Extractor Estimating: 407it [04:00,  1.81it/s]Extractor Estimating: 408it [04:01,  1.79it/s]Extractor Estimating: 409it [04:01,  1.75it/s]Extractor Estimating: 410it [04:02,  1.80it/s]Extractor Estimating: 411it [04:03,  1.77it/s]Extractor Estimating: 412it [04:03,  1.72it/s]Extractor Estimating: 413it [04:04,  1.73it/s]Extractor Estimating: 414it [04:04,  1.70it/s]Extractor Estimating: 415it [04:05,  1.67it/s]Extractor Estimating: 416it [04:06,  1.66it/s]Extractor Estimating: 417it [04:06,  1.69it/s]Extractor Estimating: 418it [04:07,  1.71it/s]Extractor Estimating: 419it [04:07,  1.71it/s]Extractor Estimating: 420it [04:08,  1.70it/s]Extractor Estimating: 421it [04:08,  1.72it/s]Extractor Estimating: 422it [04:09,  1.67it/s]Extractor Estimating: 423it [04:10,  1.73it/s]Extractor Estimating: 424it [04:10,  1.74it/s]Extractor Estimating: 425it [04:11,  1.75it/s]Extractor Estimating: 426it [04:11,  1.69it/s]Extractor Estimating: 427it [04:12,  1.62it/s]Extractor Estimating: 428it [04:13,  1.67it/s]Extractor Estimating: 429it [04:13,  1.73it/s]Extractor Estimating: 430it [04:14,  1.71it/s]Extractor Estimating: 431it [04:14,  1.70it/s]Extractor Estimating: 432it [04:15,  1.67it/s]Extractor Estimating: 433it [04:16,  1.64it/s]Extractor Estimating: 434it [04:16,  1.65it/s]Extractor Estimating: 435it [04:17,  1.69it/s]Extractor Estimating: 436it [04:17,  1.66it/s]Extractor Estimating: 437it [04:18,  1.67it/s]Extractor Estimating: 438it [04:19,  1.73it/s]Extractor Estimating: 439it [04:19,  1.72it/s]Extractor Estimating: 440it [04:20,  1.71it/s]Extractor Estimating: 441it [04:20,  1.63it/s]Extractor Estimating: 442it [04:21,  1.63it/s]Extractor Estimating: 443it [04:22,  1.65it/s]Extractor Estimating: 444it [04:22,  1.65it/s]Extractor Estimating: 445it [04:23,  1.67it/s]Extractor Estimating: 446it [04:23,  1.72it/s]Extractor Estimating: 447it [04:24,  1.69it/s]Extractor Estimating: 448it [04:24,  1.74it/s]Extractor Estimating: 449it [04:25,  1.70it/s]Extractor Estimating: 450it [04:26,  1.67it/s]Extractor Estimating: 451it [04:26,  1.70it/s]Extractor Estimating: 452it [04:27,  1.67it/s]Extractor Estimating: 453it [04:27,  1.69it/s]Extractor Estimating: 454it [04:28,  1.71it/s]Extractor Estimating: 455it [04:29,  1.71it/s]Extractor Estimating: 456it [04:29,  1.74it/s]Extractor Estimating: 457it [04:30,  1.76it/s]Extractor Estimating: 458it [04:30,  1.79it/s]Extractor Estimating: 459it [04:31,  1.75it/s]Extractor Estimating: 460it [04:31,  1.76it/s]Extractor Estimating: 461it [04:32,  1.72it/s]Extractor Estimating: 462it [04:33,  1.71it/s]Extractor Estimating: 463it [04:33,  1.71it/s]Extractor Estimating: 464it [04:34,  1.70it/s]Extractor Estimating: 465it [04:34,  1.70it/s]Extractor Estimating: 466it [04:35,  1.70it/s]Extractor Estimating: 467it [04:36,  1.70it/s]Extractor Estimating: 468it [04:36,  1.75it/s]Extractor Estimating: 469it [04:37,  1.79it/s]Extractor Estimating: 470it [04:37,  1.71it/s]Extractor Estimating: 471it [04:38,  1.74it/s]Extractor Estimating: 472it [04:38,  1.75it/s]Extractor Estimating: 473it [04:39,  1.58it/s]Extractor Estimating: 474it [04:40,  1.61it/s]Extractor Estimating: 475it [04:40,  1.66it/s]Extractor Estimating: 476it [04:41,  1.71it/s]Extractor Estimating: 477it [04:41,  1.75it/s]Extractor Estimating: 478it [04:42,  1.73it/s]Extractor Estimating: 479it [04:43,  1.73it/s]Extractor Estimating: 480it [04:43,  1.71it/s]Extractor Estimating: 481it [04:44,  1.73it/s]Extractor Estimating: 482it [04:44,  1.71it/s]Extractor Estimating: 483it [04:45,  1.70it/s]Extractor Estimating: 484it [04:46,  1.71it/s]Extractor Estimating: 485it [04:46,  1.76it/s]Extractor Estimating: 486it [04:49,  1.14s/it]Extractor Estimating: 487it [04:49,  1.01it/s]Extractor Estimating: 488it [04:50,  1.15it/s]Extractor Estimating: 489it [04:50,  1.28it/s]Extractor Estimating: 490it [04:51,  1.38it/s]Extractor Estimating: 491it [04:51,  1.49it/s]Extractor Estimating: 492it [04:52,  1.56it/s]Extractor Estimating: 493it [04:53,  1.62it/s]Extractor Estimating: 494it [04:53,  1.59it/s]Extractor Estimating: 495it [04:54,  1.62it/s]Extractor Estimating: 496it [04:54,  1.68it/s]Extractor Estimating: 497it [04:55,  1.74it/s]Extractor Estimating: 498it [04:55,  1.79it/s]Extractor Estimating: 499it [04:56,  1.71it/s]Extractor Estimating: 500it [04:57,  1.83it/s]Extractor Estimating: 500it [04:57,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:13:49,523 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:13:49,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:13:49,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:13:49,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:13:49,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:13:50,262 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:13:50,263 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:13:50,942 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:13:52,008 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:13:52,008 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:13:53,296 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:13:53,300 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:13:53,300 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:13:53,300 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:13:53,300 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:13:53,618 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:13:53,619 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:13:54,196 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:13:54,343 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:13:54,343 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 09:11:37,802 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 09:11:37,806 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 9890 mean pseudo reward: 0.933466325809255
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl'}
train vocab size: 29621
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 29721, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=29721, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.937, loss:929.8844
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.930, loss:874.7626
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.925, loss:845.4789
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.918, loss:885.8529
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 87, avg_time 0.926, loss:841.6064
>> valid entity prec:0.4829, rec:0.4551, f1:0.4686
>> valid relation prec:0.3195, rec:0.0302, f1:0.0552
>> valid relation with NER prec:0.3195, rec:0.0302, f1:0.0552
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 187, avg_time 2.777, loss:866.3402
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 287, avg_time 0.928, loss:850.9313
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 387, avg_time 0.933, loss:853.8953
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 74, avg_time 0.926, loss:866.3860
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 174, avg_time 0.930, loss:829.6916
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4716, rec:0.4144, f1:0.4411
>> valid relation prec:0.4070, rec:0.0239, f1:0.0451
>> valid relation with NER prec:0.4070, rec:0.0239, f1:0.0451
g_step 1100, step 274, avg_time 2.776, loss:840.7895
g_step 1200, step 374, avg_time 0.937, loss:869.7127
g_step 1300, step 61, avg_time 0.920, loss:846.9769
g_step 1400, step 161, avg_time 0.942, loss:816.5728
g_step 1500, step 261, avg_time 0.932, loss:849.3607
>> valid entity prec:0.4573, rec:0.5536, f1:0.5009
>> valid relation prec:0.3419, rec:0.0592, f1:0.1009
>> valid relation with NER prec:0.3419, rec:0.0592, f1:0.1009
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 361, avg_time 2.786, loss:831.5315
g_step 1700, step 48, avg_time 0.930, loss:821.0834
g_step 1800, step 148, avg_time 0.929, loss:811.9080
g_step 1900, step 248, avg_time 0.934, loss:776.1690
g_step 2000, step 348, avg_time 0.937, loss:799.1724
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5252, rec:0.3092, f1:0.3892
>> valid relation prec:0.4478, rec:0.0205, f1:0.0391
>> valid relation with NER prec:0.4478, rec:0.0205, f1:0.0391
g_step 2100, step 35, avg_time 2.741, loss:762.3660
g_step 2200, step 135, avg_time 0.938, loss:735.3716
g_step 2300, step 235, avg_time 0.932, loss:775.6714
g_step 2400, step 335, avg_time 0.938, loss:775.1986
g_step 2500, step 22, avg_time 0.924, loss:762.3613
>> valid entity prec:0.4780, rec:0.4497, f1:0.4634
>> valid relation prec:0.3358, rec:0.0314, f1:0.0574
>> valid relation with NER prec:0.3358, rec:0.0314, f1:0.0574
g_step 2600, step 122, avg_time 2.783, loss:720.7986
g_step 2700, step 222, avg_time 0.922, loss:732.1481
g_step 2800, step 322, avg_time 0.927, loss:721.0228
g_step 2900, step 9, avg_time 0.929, loss:767.7635
g_step 3000, step 109, avg_time 0.929, loss:675.7726
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4667, rec:0.4076, f1:0.4352
>> valid relation prec:0.2235, rec:0.0227, f1:0.0412
>> valid relation with NER prec:0.2235, rec:0.0227, f1:0.0412
g_step 3100, step 209, avg_time 2.776, loss:714.8978
g_step 3200, step 309, avg_time 0.930, loss:736.9247
g_step 3300, step 409, avg_time 0.921, loss:717.8651
g_step 3400, step 96, avg_time 0.913, loss:647.4080
g_step 3500, step 196, avg_time 0.936, loss:683.6620
>> valid entity prec:0.5367, rec:0.2923, f1:0.3785
>> valid relation prec:0.2581, rec:0.0189, f1:0.0353
>> valid relation with NER prec:0.2581, rec:0.0189, f1:0.0353
g_step 3600, step 296, avg_time 2.751, loss:689.0289
g_step 3700, step 396, avg_time 0.935, loss:722.9318
g_step 3800, step 83, avg_time 0.929, loss:634.0611
g_step 3900, step 183, avg_time 0.927, loss:682.4988
g_step 4000, step 283, avg_time 0.931, loss:650.9617
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4381, rec:0.4099, f1:0.4235
>> valid relation prec:0.2500, rec:0.0268, f1:0.0484
>> valid relation with NER prec:0.2500, rec:0.0268, f1:0.0484
g_step 4100, step 383, avg_time 2.773, loss:689.0026
g_step 4200, step 70, avg_time 0.931, loss:614.4213
g_step 4300, step 170, avg_time 0.925, loss:625.8483
g_step 4400, step 270, avg_time 0.926, loss:657.4163
g_step 4500, step 370, avg_time 0.934, loss:676.5386
>> valid entity prec:0.4387, rec:0.4905, f1:0.4631
>> valid relation prec:0.2580, rec:0.0483, f1:0.0813
>> valid relation with NER prec:0.2580, rec:0.0483, f1:0.0813
g_step 4600, step 57, avg_time 2.785, loss:615.3692
g_step 4700, step 157, avg_time 0.931, loss:611.7893
g_step 4800, step 257, avg_time 0.940, loss:604.9416
g_step 4900, step 357, avg_time 0.942, loss:648.7974
g_step 5000, step 44, avg_time 0.937, loss:611.4591
learning rate was adjusted to 0.0008
>> valid entity prec:0.4524, rec:0.4910, f1:0.4709
>> valid relation prec:0.2378, rec:0.0457, f1:0.0767
>> valid relation with NER prec:0.2378, rec:0.0457, f1:0.0767
g_step 5100, step 144, avg_time 2.799, loss:586.7650
g_step 5200, step 244, avg_time 0.936, loss:609.7109
g_step 5300, step 344, avg_time 0.940, loss:615.9035
g_step 5400, step 31, avg_time 0.926, loss:591.9772
g_step 5500, step 131, avg_time 0.939, loss:566.9086
>> valid entity prec:0.4996, rec:0.4107, f1:0.4508
>> valid relation prec:0.2424, rec:0.0314, f1:0.0556
>> valid relation with NER prec:0.2424, rec:0.0314, f1:0.0556
g_step 5600, step 231, avg_time 2.791, loss:589.0386
g_step 5700, step 331, avg_time 0.947, loss:599.6767
g_step 5800, step 18, avg_time 0.936, loss:598.7761
g_step 5900, step 118, avg_time 0.947, loss:536.4156
g_step 6000, step 218, avg_time 0.934, loss:562.1008
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4415, rec:0.4690, f1:0.4549
>> valid relation prec:0.2610, rec:0.0565, f1:0.0928
>> valid relation with NER prec:0.2610, rec:0.0565, f1:0.0928
g_step 6100, step 318, avg_time 2.790, loss:588.7815
g_step 6200, step 5, avg_time 0.925, loss:583.3110
g_step 6300, step 105, avg_time 0.947, loss:542.5230
g_step 6400, step 205, avg_time 0.936, loss:543.2469
g_step 6500, step 305, avg_time 0.936, loss:555.8188
>> valid entity prec:0.4453, rec:0.4711, f1:0.4578
>> valid relation prec:0.2000, rec:0.0384, f1:0.0644
>> valid relation with NER prec:0.2000, rec:0.0384, f1:0.0644
g_step 6600, step 405, avg_time 2.783, loss:544.0005
g_step 6700, step 92, avg_time 0.932, loss:510.5314
g_step 6800, step 192, avg_time 0.934, loss:525.1408
g_step 6900, step 292, avg_time 0.935, loss:541.2382
g_step 7000, step 392, avg_time 0.945, loss:541.7618
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4675, rec:0.3985, f1:0.4303
>> valid relation prec:0.1778, rec:0.0276, f1:0.0478
>> valid relation with NER prec:0.1778, rec:0.0276, f1:0.0478
g_step 7100, step 79, avg_time 2.783, loss:495.9213
g_step 7200, step 179, avg_time 0.936, loss:517.7739
g_step 7300, step 279, avg_time 0.939, loss:521.2494
g_step 7400, step 379, avg_time 0.942, loss:531.5659
g_step 7500, step 66, avg_time 0.928, loss:485.8108
>> valid entity prec:0.4858, rec:0.4171, f1:0.4489
>> valid relation prec:0.2710, rec:0.0462, f1:0.0790
>> valid relation with NER prec:0.2710, rec:0.0462, f1:0.0790
g_step 7600, step 166, avg_time 2.774, loss:485.5138
g_step 7700, step 266, avg_time 0.942, loss:507.5631
g_step 7800, step 366, avg_time 0.946, loss:516.7073
g_step 7900, step 53, avg_time 0.928, loss:494.0812
g_step 8000, step 153, avg_time 0.932, loss:476.0384
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4324, rec:0.4997, f1:0.4636
>> valid relation prec:0.1873, rec:0.0401, f1:0.0660
>> valid relation with NER prec:0.1873, rec:0.0401, f1:0.0660
g_step 8100, step 253, avg_time 2.788, loss:475.4710
g_step 8200, step 353, avg_time 0.945, loss:516.8118
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 09:11:37 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 09:11:37 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_09-11-37_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 09:11:38 - WARNING - datasets.builder -   Using custom data configuration default-d1c8499343936588
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-d1c8499343936588/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 09:11:39,140 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:11:39,141 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 09:11:39,141 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:11:39,142 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 09:11:39,152 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:11:39,158 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:11:39,158 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:11:39,158 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:11:39,158 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:11:39,158 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:11:39,158 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 09:11:39,306 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 09:11:42,322 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 09:11:42,324 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-d1c8499343936588/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.63ba/s] 20%|██        | 2/10 [00:00<00:01,  4.28ba/s] 30%|███       | 3/10 [00:00<00:01,  4.50ba/s] 40%|████      | 4/10 [00:00<00:01,  4.59ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.65ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.70ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.71ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.74ba/s] 90%|█████████ | 9/10 [00:01<00:00,  4.75ba/s]100%|██████████| 10/10 [00:02<00:00,  4.73ba/s]100%|██████████| 10/10 [00:02<00:00,  4.63ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  4.27ba/s] 33%|███▎      | 2/6 [00:00<00:00,  4.49ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.58ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.61ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.70ba/s]100%|██████████| 6/6 [00:01<00:00,  4.12ba/s]100%|██████████| 6/6 [00:01<00:00,  4.19ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:00,  9.78ba/s] 30%|███       | 3/10 [00:00<00:00, 11.12ba/s] 50%|█████     | 5/10 [00:00<00:00, 11.33ba/s] 70%|███████   | 7/10 [00:00<00:00, 11.44ba/s] 90%|█████████ | 9/10 [00:00<00:00, 11.46ba/s]100%|██████████| 10/10 [00:00<00:00, 11.36ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  9.99ba/s] 50%|█████     | 3/6 [00:00<00:00, 11.06ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 11.20ba/s]100%|██████████| 6/6 [00:00<00:00, 11.37ba/s]
[INFO|trainer.py:414] 2023-08-29 09:11:47,823 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 09:11:47,843 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 09:11:47,843 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 09:11:47,843 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 09:11:47,843 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 09:11:47,843 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 09:11:47,843 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 09:11:47,843 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<10:22,  1.25it/s]  0%|          | 2/780 [00:01<08:42,  1.49it/s]  0%|          | 3/780 [00:01<07:14,  1.79it/s]  1%|          | 4/780 [00:02<06:01,  2.15it/s]  1%|          | 5/780 [00:02<05:17,  2.44it/s]  1%|          | 6/780 [00:02<04:48,  2.68it/s]  1%|          | 7/780 [00:03<04:30,  2.86it/s]  1%|          | 8/780 [00:03<04:24,  2.92it/s]  1%|          | 9/780 [00:03<04:19,  2.97it/s]  1%|▏         | 10/780 [00:03<04:08,  3.10it/s]  1%|▏         | 11/780 [00:04<04:00,  3.19it/s]  2%|▏         | 12/780 [00:04<03:55,  3.26it/s]  2%|▏         | 13/780 [00:04<03:51,  3.31it/s]  2%|▏         | 14/780 [00:05<03:48,  3.35it/s]  2%|▏         | 15/780 [00:05<03:46,  3.37it/s]  2%|▏         | 16/780 [00:05<03:45,  3.39it/s]  2%|▏         | 17/780 [00:06<03:44,  3.40it/s]  2%|▏         | 18/780 [00:06<03:45,  3.38it/s]  2%|▏         | 19/780 [00:06<03:44,  3.40it/s]  3%|▎         | 20/780 [00:06<03:43,  3.40it/s]  3%|▎         | 21/780 [00:07<03:42,  3.42it/s]  3%|▎         | 22/780 [00:07<03:41,  3.42it/s]  3%|▎         | 23/780 [00:07<03:40,  3.43it/s]  3%|▎         | 24/780 [00:08<03:40,  3.43it/s]  3%|▎         | 25/780 [00:08<03:39,  3.44it/s]  3%|▎         | 26/780 [00:08<03:39,  3.43it/s]  3%|▎         | 27/780 [00:08<03:39,  3.44it/s]  4%|▎         | 28/780 [00:09<03:38,  3.44it/s]  4%|▎         | 29/780 [00:09<03:38,  3.44it/s]  4%|▍         | 30/780 [00:09<03:38,  3.44it/s]  4%|▍         | 31/780 [00:10<03:38,  3.43it/s]  4%|▍         | 32/780 [00:10<03:38,  3.43it/s]  4%|▍         | 33/780 [00:10<03:37,  3.44it/s]  4%|▍         | 34/780 [00:10<03:36,  3.44it/s]  4%|▍         | 35/780 [00:11<03:36,  3.44it/s]  5%|▍         | 36/780 [00:11<03:36,  3.44it/s]  5%|▍         | 37/780 [00:11<03:35,  3.44it/s]  5%|▍         | 38/780 [00:12<03:35,  3.44it/s]  5%|▌         | 39/780 [00:12<03:35,  3.45it/s]  5%|▌         | 40/780 [00:12<03:34,  3.44it/s]  5%|▌         | 41/780 [00:13<03:34,  3.44it/s]  5%|▌         | 42/780 [00:13<03:35,  3.43it/s]  6%|▌         | 43/780 [00:13<03:35,  3.42it/s]  6%|▌         | 44/780 [00:13<03:34,  3.43it/s]  6%|▌         | 45/780 [00:14<03:34,  3.43it/s]  6%|▌         | 46/780 [00:14<03:33,  3.44it/s]  6%|▌         | 47/780 [00:14<03:33,  3.44it/s]  6%|▌         | 48/780 [00:15<03:32,  3.44it/s]  6%|▋         | 49/780 [00:15<03:32,  3.44it/s]  6%|▋         | 50/780 [00:15<03:32,  3.44it/s]  7%|▋         | 51/780 [00:15<03:32,  3.44it/s]  7%|▋         | 52/780 [00:16<03:31,  3.44it/s]  7%|▋         | 53/780 [00:16<03:31,  3.43it/s]  7%|▋         | 54/780 [00:16<03:31,  3.44it/s]  7%|▋         | 55/780 [00:17<03:30,  3.44it/s]  7%|▋         | 56/780 [00:17<03:30,  3.44it/s]  7%|▋         | 57/780 [00:17<03:30,  3.44it/s]  7%|▋         | 58/780 [00:17<03:29,  3.44it/s]  8%|▊         | 59/780 [00:18<03:29,  3.44it/s]  8%|▊         | 60/780 [00:18<03:29,  3.44it/s]  8%|▊         | 61/780 [00:18<03:29,  3.44it/s]  8%|▊         | 62/780 [00:19<03:28,  3.44it/s]  8%|▊         | 63/780 [00:19<03:28,  3.44it/s]  8%|▊         | 64/780 [00:19<03:28,  3.43it/s]  8%|▊         | 65/780 [00:19<03:28,  3.44it/s]  8%|▊         | 66/780 [00:20<03:27,  3.43it/s]  9%|▊         | 67/780 [00:20<03:27,  3.44it/s]  9%|▊         | 68/780 [00:20<03:27,  3.44it/s]  9%|▉         | 69/780 [00:21<03:26,  3.44it/s]  9%|▉         | 70/780 [00:21<03:26,  3.44it/s]  9%|▉         | 71/780 [00:21<03:26,  3.44it/s]  9%|▉         | 72/780 [00:22<03:26,  3.44it/s]  9%|▉         | 73/780 [00:22<03:25,  3.44it/s]  9%|▉         | 74/780 [00:22<03:25,  3.44it/s] 10%|▉         | 75/780 [00:22<03:25,  3.43it/s] 10%|▉         | 76/780 [00:23<03:25,  3.43it/s] 10%|▉         | 77/780 [00:23<03:24,  3.43it/s] 10%|█         | 78/780 [00:23<03:24,  3.44it/s] 10%|█         | 79/780 [00:24<03:24,  3.44it/s] 10%|█         | 80/780 [00:24<03:23,  3.43it/s] 10%|█         | 81/780 [00:24<03:23,  3.43it/s] 11%|█         | 82/780 [00:24<03:23,  3.44it/s] 11%|█         | 83/780 [00:25<03:22,  3.43it/s] 11%|█         | 84/780 [00:25<03:22,  3.43it/s] 11%|█         | 85/780 [00:25<03:22,  3.44it/s] 11%|█         | 86/780 [00:26<03:24,  3.40it/s] 11%|█         | 87/780 [00:26<03:23,  3.41it/s] 11%|█▏        | 88/780 [00:26<03:22,  3.42it/s] 11%|█▏        | 89/780 [00:26<03:22,  3.42it/s] 12%|█▏        | 90/780 [00:27<03:21,  3.43it/s] 12%|█▏        | 91/780 [00:27<03:21,  3.43it/s] 12%|█▏        | 92/780 [00:27<03:20,  3.43it/s] 12%|█▏        | 93/780 [00:28<03:20,  3.43it/s] 12%|█▏        | 94/780 [00:28<03:19,  3.44it/s] 12%|█▏        | 95/780 [00:28<03:19,  3.44it/s] 12%|█▏        | 96/780 [00:29<03:18,  3.44it/s] 12%|█▏        | 97/780 [00:29<03:19,  3.43it/s] 13%|█▎        | 98/780 [00:29<03:18,  3.44it/s] 13%|█▎        | 99/780 [00:29<03:18,  3.43it/s] 13%|█▎        | 100/780 [00:30<03:18,  3.43it/s] 13%|█▎        | 101/780 [00:30<03:17,  3.43it/s] 13%|█▎        | 102/780 [00:30<03:17,  3.44it/s] 13%|█▎        | 103/780 [00:31<03:17,  3.44it/s] 13%|█▎        | 104/780 [00:31<03:16,  3.44it/s] 13%|█▎        | 105/780 [00:31<03:16,  3.44it/s] 14%|█▎        | 106/780 [00:31<03:16,  3.44it/s] 14%|█▎        | 107/780 [00:32<03:15,  3.44it/s] 14%|█▍        | 108/780 [00:32<03:15,  3.43it/s] 14%|█▍        | 109/780 [00:32<03:15,  3.43it/s] 14%|█▍        | 110/780 [00:33<03:15,  3.43it/s] 14%|█▍        | 111/780 [00:33<03:15,  3.43it/s] 14%|█▍        | 112/780 [00:33<03:14,  3.43it/s] 14%|█▍        | 113/780 [00:33<03:14,  3.42it/s] 15%|█▍        | 114/780 [00:34<03:15,  3.41it/s] 15%|█▍        | 115/780 [00:34<03:15,  3.41it/s] 15%|█▍        | 116/780 [00:34<03:14,  3.42it/s] 15%|█▌        | 117/780 [00:35<03:13,  3.42it/s] 15%|█▌        | 118/780 [00:35<03:13,  3.43it/s] 15%|█▌        | 119/780 [00:35<03:12,  3.43it/s] 15%|█▌        | 120/780 [00:36<03:12,  3.43it/s] 16%|█▌        | 121/780 [00:36<03:11,  3.43it/s] 16%|█▌        | 122/780 [00:36<03:11,  3.44it/s] 16%|█▌        | 123/780 [00:36<03:11,  3.44it/s] 16%|█▌        | 124/780 [00:37<03:10,  3.44it/s] 16%|█▌        | 125/780 [00:37<03:10,  3.43it/s] 16%|█▌        | 126/780 [00:37<03:10,  3.44it/s] 16%|█▋        | 127/780 [00:38<03:10,  3.44it/s] 16%|█▋        | 128/780 [00:38<03:09,  3.44it/s] 17%|█▋        | 129/780 [00:38<03:09,  3.44it/s] 17%|█▋        | 130/780 [00:38<03:09,  3.44it/s] 17%|█▋        | 131/780 [00:39<03:08,  3.43it/s] 17%|█▋        | 132/780 [00:39<03:08,  3.44it/s] 17%|█▋        | 133/780 [00:39<03:08,  3.44it/s] 17%|█▋        | 134/780 [00:40<03:08,  3.43it/s] 17%|█▋        | 135/780 [00:40<03:08,  3.43it/s] 17%|█▋        | 136/780 [00:40<03:13,  3.32it/s] 18%|█▊        | 137/780 [00:41<03:12,  3.35it/s] 18%|█▊        | 138/780 [00:41<03:10,  3.38it/s] 18%|█▊        | 139/780 [00:41<03:08,  3.40it/s] 18%|█▊        | 140/780 [00:41<03:07,  3.41it/s] 18%|█▊        | 141/780 [00:42<03:06,  3.42it/s] 18%|█▊        | 142/780 [00:42<03:06,  3.42it/s] 18%|█▊        | 143/780 [00:42<03:05,  3.43it/s] 18%|█▊        | 144/780 [00:43<03:05,  3.43it/s] 19%|█▊        | 145/780 [00:43<03:04,  3.44it/s] 19%|█▊        | 146/780 [00:43<03:04,  3.44it/s] 19%|█▉        | 147/780 [00:43<03:04,  3.42it/s] 19%|█▉        | 148/780 [00:44<03:04,  3.43it/s] 19%|█▉        | 149/780 [00:44<03:03,  3.43it/s] 19%|█▉        | 150/780 [00:44<03:03,  3.43it/s] 19%|█▉        | 151/780 [00:45<03:03,  3.44it/s] 19%|█▉        | 152/780 [00:45<03:02,  3.43it/s] 20%|█▉        | 153/780 [00:45<03:02,  3.43it/s] 20%|█▉        | 154/780 [00:45<03:02,  3.42it/s] 20%|█▉        | 155/780 [00:46<03:02,  3.42it/s] 20%|██        | 156/780 [00:46<03:02,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 09:12:34,429 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:12:34,429 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 09:12:34,429 >>   Batch size = 8

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 55.32it/s][A
  2%|▏         | 12/733 [00:00<00:15, 47.48it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.54it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.89it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.39it/s][A
  4%|▍         | 32/733 [00:00<00:15, 44.01it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.88it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.75it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.81it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.93it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.89it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.84it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.62it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.64it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.68it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.54it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.55it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.55it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.82it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.71it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.69it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.52it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.62it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.57it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.57it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.51it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.69it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.81it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.74it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.73it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.58it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.64it/s][A
 23%|██▎       | 167/733 [00:03<00:12, 43.61it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.48it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.61it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.65it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.74it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.74it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.65it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.63it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.61it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.54it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.59it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.67it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.67it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.72it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.65it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.64it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.68it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.49it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.53it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.63it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.75it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.71it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.63it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.59it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.63it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.56it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.49it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.50it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.56it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.67it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.69it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.70it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.65it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.55it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.60it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.60it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.62it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.59it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.67it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.70it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.70it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.65it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.46it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.56it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.63it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.68it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.72it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.61it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.69it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.68it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.57it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.40it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.42it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.63it/s][A
 60%|█████▉    | 437/733 [00:09<00:06, 43.71it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.67it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.71it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.62it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.65it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.59it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.62it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.60it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.62it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.56it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.64it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.68it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.65it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.63it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.56it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.65it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.56it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.59it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.66it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.63it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.59it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.67it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.68it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.60it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.56it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.56it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.62it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.61it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.61it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.55it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.61it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.66it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.65it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.65it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.61it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.53it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.40it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.49it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.37it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.42it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.49it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.62it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.61it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.57it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.51it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.55it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.46it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.52it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.50it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.64it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.44it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.62it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.72it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.72it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.68it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.55it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.39it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.57it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.56it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.48it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.48it/s][A 20%|██        | 156/780 [01:03<03:02,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:12:51,304 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 09:12:51,327 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:12:53,716 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:12:53,856 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:12:53,920 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:10<1:15:32,  7.27s/it] 20%|██        | 158/780 [01:10<53:43,  5.18s/it]   20%|██        | 159/780 [01:10<38:27,  3.72s/it] 21%|██        | 160/780 [01:11<27:47,  2.69s/it] 21%|██        | 161/780 [01:11<20:20,  1.97s/it] 21%|██        | 162/780 [01:11<15:07,  1.47s/it] 21%|██        | 163/780 [01:11<11:29,  1.12s/it] 21%|██        | 164/780 [01:12<08:56,  1.15it/s] 21%|██        | 165/780 [01:12<07:09,  1.43it/s] 21%|██▏       | 166/780 [01:12<05:54,  1.73it/s] 21%|██▏       | 167/780 [01:13<05:02,  2.03it/s] 22%|██▏       | 168/780 [01:13<04:25,  2.31it/s] 22%|██▏       | 169/780 [01:13<03:59,  2.55it/s] 22%|██▏       | 170/780 [01:13<03:41,  2.75it/s] 22%|██▏       | 171/780 [01:14<03:29,  2.91it/s] 22%|██▏       | 172/780 [01:14<03:19,  3.04it/s] 22%|██▏       | 173/780 [01:14<03:13,  3.14it/s] 22%|██▏       | 174/780 [01:15<03:08,  3.21it/s] 22%|██▏       | 175/780 [01:15<03:05,  3.26it/s] 23%|██▎       | 176/780 [01:15<03:03,  3.30it/s] 23%|██▎       | 177/780 [01:16<03:01,  3.32it/s] 23%|██▎       | 178/780 [01:16<03:00,  3.34it/s] 23%|██▎       | 179/780 [01:16<02:59,  3.35it/s] 23%|██▎       | 180/780 [01:16<02:59,  3.35it/s] 23%|██▎       | 181/780 [01:17<02:58,  3.36it/s] 23%|██▎       | 182/780 [01:17<02:57,  3.37it/s] 23%|██▎       | 183/780 [01:17<02:57,  3.37it/s] 24%|██▎       | 184/780 [01:18<02:56,  3.38it/s] 24%|██▎       | 185/780 [01:18<02:56,  3.38it/s] 24%|██▍       | 186/780 [01:18<02:55,  3.39it/s] 24%|██▍       | 187/780 [01:18<02:55,  3.38it/s] 24%|██▍       | 188/780 [01:19<02:54,  3.39it/s] 24%|██▍       | 189/780 [01:19<02:54,  3.38it/s] 24%|██▍       | 190/780 [01:19<02:54,  3.38it/s] 24%|██▍       | 191/780 [01:20<02:54,  3.37it/s] 25%|██▍       | 192/780 [01:20<02:54,  3.38it/s] 25%|██▍       | 193/780 [01:20<02:53,  3.38it/s] 25%|██▍       | 194/780 [01:21<02:53,  3.38it/s] 25%|██▌       | 195/780 [01:21<02:53,  3.37it/s] 25%|██▌       | 196/780 [01:21<02:53,  3.37it/s] 25%|██▌       | 197/780 [01:21<02:52,  3.37it/s] 25%|██▌       | 198/780 [01:22<02:52,  3.38it/s] 26%|██▌       | 199/780 [01:22<02:51,  3.38it/s] 26%|██▌       | 200/780 [01:22<02:51,  3.39it/s] 26%|██▌       | 201/780 [01:23<02:51,  3.38it/s] 26%|██▌       | 202/780 [01:23<02:51,  3.37it/s] 26%|██▌       | 203/780 [01:23<02:51,  3.37it/s] 26%|██▌       | 204/780 [01:24<02:50,  3.38it/s] 26%|██▋       | 205/780 [01:24<02:50,  3.38it/s] 26%|██▋       | 206/780 [01:24<02:49,  3.39it/s] 27%|██▋       | 207/780 [01:24<02:48,  3.40it/s] 27%|██▋       | 208/780 [01:25<02:47,  3.41it/s] 27%|██▋       | 209/780 [01:25<02:47,  3.42it/s] 27%|██▋       | 210/780 [01:25<02:46,  3.42it/s] 27%|██▋       | 211/780 [01:26<02:46,  3.43it/s] 27%|██▋       | 212/780 [01:26<02:45,  3.43it/s] 27%|██▋       | 213/780 [01:26<02:46,  3.41it/s] 27%|██▋       | 214/780 [01:26<02:45,  3.42it/s] 28%|██▊       | 215/780 [01:27<02:44,  3.43it/s] 28%|██▊       | 216/780 [01:27<02:44,  3.43it/s] 28%|██▊       | 217/780 [01:27<02:44,  3.43it/s] 28%|██▊       | 218/780 [01:28<02:43,  3.43it/s] 28%|██▊       | 219/780 [01:28<02:43,  3.43it/s] 28%|██▊       | 220/780 [01:28<02:43,  3.43it/s] 28%|██▊       | 221/780 [01:28<02:42,  3.43it/s] 28%|██▊       | 222/780 [01:29<02:42,  3.43it/s] 29%|██▊       | 223/780 [01:29<02:42,  3.44it/s] 29%|██▊       | 224/780 [01:29<02:42,  3.43it/s] 29%|██▉       | 225/780 [01:30<02:41,  3.43it/s] 29%|██▉       | 226/780 [01:30<02:41,  3.43it/s] 29%|██▉       | 227/780 [01:30<02:41,  3.43it/s] 29%|██▉       | 228/780 [01:31<02:40,  3.43it/s] 29%|██▉       | 229/780 [01:31<02:40,  3.43it/s] 29%|██▉       | 230/780 [01:31<02:40,  3.43it/s] 30%|██▉       | 231/780 [01:31<02:39,  3.43it/s] 30%|██▉       | 232/780 [01:32<02:39,  3.43it/s] 30%|██▉       | 233/780 [01:32<02:39,  3.43it/s] 30%|███       | 234/780 [01:32<02:39,  3.43it/s] 30%|███       | 235/780 [01:33<02:38,  3.43it/s] 30%|███       | 236/780 [01:33<02:38,  3.44it/s] 30%|███       | 237/780 [01:33<02:38,  3.43it/s] 31%|███       | 238/780 [01:33<02:37,  3.43it/s] 31%|███       | 239/780 [01:34<02:37,  3.43it/s] 31%|███       | 240/780 [01:34<02:37,  3.43it/s] 31%|███       | 241/780 [01:34<02:37,  3.43it/s] 31%|███       | 242/780 [01:35<02:36,  3.43it/s] 31%|███       | 243/780 [01:35<02:36,  3.43it/s] 31%|███▏      | 244/780 [01:35<02:37,  3.41it/s] 31%|███▏      | 245/780 [01:35<02:36,  3.41it/s] 32%|███▏      | 246/780 [01:36<02:36,  3.42it/s] 32%|███▏      | 247/780 [01:36<02:35,  3.43it/s] 32%|███▏      | 248/780 [01:36<02:35,  3.43it/s] 32%|███▏      | 249/780 [01:37<02:34,  3.43it/s] 32%|███▏      | 250/780 [01:37<02:34,  3.43it/s] 32%|███▏      | 251/780 [01:37<02:34,  3.43it/s] 32%|███▏      | 252/780 [01:38<02:33,  3.43it/s] 32%|███▏      | 253/780 [01:38<02:33,  3.43it/s] 33%|███▎      | 254/780 [01:38<02:33,  3.43it/s] 33%|███▎      | 255/780 [01:38<02:33,  3.42it/s] 33%|███▎      | 256/780 [01:39<02:32,  3.43it/s] 33%|███▎      | 257/780 [01:39<02:32,  3.43it/s] 33%|███▎      | 258/780 [01:39<02:32,  3.43it/s] 33%|███▎      | 259/780 [01:40<02:31,  3.43it/s] 33%|███▎      | 260/780 [01:40<02:31,  3.44it/s] 33%|███▎      | 261/780 [01:40<02:31,  3.43it/s] 34%|███▎      | 262/780 [01:40<02:30,  3.44it/s] 34%|███▎      | 263/780 [01:41<02:30,  3.43it/s] 34%|███▍      | 264/780 [01:41<02:30,  3.43it/s] 34%|███▍      | 265/780 [01:41<02:30,  3.43it/s] 34%|███▍      | 266/780 [01:42<02:30,  3.40it/s] 34%|███▍      | 267/780 [01:42<02:30,  3.41it/s] 34%|███▍      | 268/780 [01:42<02:29,  3.42it/s] 34%|███▍      | 269/780 [01:42<02:29,  3.42it/s] 35%|███▍      | 270/780 [01:43<02:28,  3.43it/s] 35%|███▍      | 271/780 [01:43<02:28,  3.43it/s] 35%|███▍      | 272/780 [01:43<02:28,  3.43it/s] 35%|███▌      | 273/780 [01:44<02:27,  3.43it/s] 35%|███▌      | 274/780 [01:44<02:27,  3.43it/s] 35%|███▌      | 275/780 [01:44<02:27,  3.43it/s] 35%|███▌      | 276/780 [01:45<02:26,  3.43it/s] 36%|███▌      | 277/780 [01:45<02:26,  3.43it/s] 36%|███▌      | 278/780 [01:45<02:26,  3.42it/s] 36%|███▌      | 279/780 [01:45<02:26,  3.43it/s] 36%|███▌      | 280/780 [01:46<02:25,  3.43it/s] 36%|███▌      | 281/780 [01:46<02:25,  3.44it/s] 36%|███▌      | 282/780 [01:46<02:25,  3.43it/s] 36%|███▋      | 283/780 [01:47<02:24,  3.44it/s] 36%|███▋      | 284/780 [01:47<02:24,  3.44it/s] 37%|███▋      | 285/780 [01:47<02:23,  3.44it/s] 37%|███▋      | 286/780 [01:47<02:23,  3.44it/s] 37%|███▋      | 287/780 [01:48<02:23,  3.44it/s] 37%|███▋      | 288/780 [01:48<02:23,  3.42it/s] 37%|███▋      | 289/780 [01:48<02:23,  3.43it/s] 37%|███▋      | 290/780 [01:49<02:22,  3.43it/s] 37%|███▋      | 291/780 [01:49<02:22,  3.43it/s] 37%|███▋      | 292/780 [01:49<02:22,  3.43it/s] 38%|███▊      | 293/780 [01:49<02:21,  3.44it/s] 38%|███▊      | 294/780 [01:50<02:21,  3.43it/s] 38%|███▊      | 295/780 [01:50<02:21,  3.43it/s] 38%|███▊      | 296/780 [01:50<02:20,  3.43it/s] 38%|███▊      | 297/780 [01:51<02:20,  3.43it/s] 38%|███▊      | 298/780 [01:51<02:20,  3.43it/s] 38%|███▊      | 299/780 [01:51<02:20,  3.42it/s] 38%|███▊      | 300/780 [01:52<02:20,  3.42it/s] 39%|███▊      | 301/780 [01:52<02:19,  3.42it/s] 39%|███▊      | 302/780 [01:52<02:19,  3.43it/s] 39%|███▉      | 303/780 [01:52<02:19,  3.43it/s] 39%|███▉      | 304/780 [01:53<02:18,  3.43it/s] 39%|███▉      | 305/780 [01:53<02:18,  3.43it/s] 39%|███▉      | 306/780 [01:53<02:17,  3.44it/s] 39%|███▉      | 307/780 [01:54<02:17,  3.43it/s] 39%|███▉      | 308/780 [01:54<02:17,  3.43it/s] 40%|███▉      | 309/780 [01:54<02:17,  3.44it/s] 40%|███▉      | 310/780 [01:54<02:17,  3.43it/s] 40%|███▉      | 311/780 [01:55<02:16,  3.43it/s] 40%|████      | 312/780 [01:55<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 09:13:43,404 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:13:43,404 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 09:13:43,404 >>   Batch size = 8
{'eval_loss': 0.9157085418701172, 'eval_runtime': 16.8482, 'eval_samples_per_second': 348.049, 'eval_steps_per_second': 43.506, 'epoch': 1.0}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 54.39it/s][A
  2%|▏         | 12/733 [00:00<00:15, 46.73it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.23it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.65it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.18it/s][A
  4%|▍         | 32/733 [00:00<00:15, 43.94it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.80it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.82it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.88it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.88it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.81it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.62it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.57it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.51it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.51it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.49it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.61it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.69it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.73it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.71it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.52it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.54it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.54it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.58it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.50it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.64it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.66it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.70it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.63it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.61it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.53it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.41it/s][A
 23%|██▎       | 167/733 [00:03<00:12, 43.54it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.50it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.65it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.70it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.60it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.64it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.57it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.49it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.52it/s][A
 29%|██▉       | 212/733 [00:04<00:12, 43.41it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.61it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.65it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.68it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.72it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.62it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.51it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.53it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.51it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.50it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.50it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.67it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.67it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.66it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.65it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.55it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.49it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.34it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.37it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.46it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.35it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.45it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.57it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.59it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.54it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.55it/s][A
 47%|████▋     | 342/733 [00:07<00:09, 43.17it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.58it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.60it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.45it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.65it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.47it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.71it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.64it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.65it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.59it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.49it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.53it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.64it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.61it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.56it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.45it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.42it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.39it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.39it/s][A
 60%|█████▉    | 437/733 [00:10<00:06, 43.42it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.13it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.14it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.53it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.46it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.53it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.61it/s][A
 64%|██████▍   | 472/733 [00:10<00:06, 43.36it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.42it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.53it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.54it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.55it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.36it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.54it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.53it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.51it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.65it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.63it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.69it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.69it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.53it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.48it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.50it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.14it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.52it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.54it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.52it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.57it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.53it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.46it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.44it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.44it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.36it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.42it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.42it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.61it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.66it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.61it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.58it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.40it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.40it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.38it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.34it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.40it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.36it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.66it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.52it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.52it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.42it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.40it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.61it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.66it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.56it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.74it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.60it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.62it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.61it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.58it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.64it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.50it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.50it/s][A 40%|████      | 312/780 [02:12<02:16,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:14:00,297 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 09:14:00,314 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:14:02,272 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:14:02,286 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:14:02,300 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:17<54:00,  6.94s/it] 40%|████      | 314/780 [02:18<38:24,  4.95s/it] 40%|████      | 315/780 [02:18<27:31,  3.55s/it] 41%|████      | 316/780 [02:18<19:54,  2.57s/it] 41%|████      | 317/780 [02:19<14:35,  1.89s/it] 41%|████      | 318/780 [02:19<10:52,  1.41s/it] 41%|████      | 319/780 [02:19<08:16,  1.08s/it] 41%|████      | 320/780 [02:20<06:27,  1.19it/s] 41%|████      | 321/780 [02:20<05:11,  1.47it/s] 41%|████▏     | 322/780 [02:20<04:17,  1.78it/s] 41%|████▏     | 323/780 [02:20<03:40,  2.07it/s] 42%|████▏     | 324/780 [02:21<03:14,  2.34it/s] 42%|████▏     | 325/780 [02:21<02:57,  2.57it/s] 42%|████▏     | 326/780 [02:21<02:44,  2.76it/s] 42%|████▏     | 327/780 [02:22<02:35,  2.92it/s] 42%|████▏     | 328/780 [02:22<02:28,  3.04it/s] 42%|████▏     | 329/780 [02:22<02:23,  3.13it/s] 42%|████▏     | 330/780 [02:23<02:20,  3.20it/s] 42%|████▏     | 331/780 [02:23<02:18,  3.25it/s] 43%|████▎     | 332/780 [02:23<02:16,  3.28it/s] 43%|████▎     | 333/780 [02:23<02:15,  3.31it/s] 43%|████▎     | 334/780 [02:24<02:13,  3.33it/s] 43%|████▎     | 335/780 [02:24<02:12,  3.35it/s] 43%|████▎     | 336/780 [02:24<02:12,  3.35it/s] 43%|████▎     | 337/780 [02:25<02:11,  3.36it/s] 43%|████▎     | 338/780 [02:25<02:11,  3.37it/s] 43%|████▎     | 339/780 [02:25<02:10,  3.38it/s] 44%|████▎     | 340/780 [02:25<02:10,  3.38it/s] 44%|████▎     | 341/780 [02:26<02:09,  3.38it/s] 44%|████▍     | 342/780 [02:26<02:09,  3.38it/s] 44%|████▍     | 343/780 [02:26<02:08,  3.39it/s] 44%|████▍     | 344/780 [02:27<02:08,  3.39it/s] 44%|████▍     | 345/780 [02:27<02:08,  3.39it/s] 44%|████▍     | 346/780 [02:27<02:08,  3.39it/s] 44%|████▍     | 347/780 [02:28<02:08,  3.38it/s] 45%|████▍     | 348/780 [02:28<02:08,  3.37it/s] 45%|████▍     | 349/780 [02:28<02:07,  3.37it/s] 45%|████▍     | 350/780 [02:28<02:07,  3.38it/s] 45%|████▌     | 351/780 [02:29<02:06,  3.38it/s] 45%|████▌     | 352/780 [02:29<02:06,  3.38it/s] 45%|████▌     | 353/780 [02:29<02:06,  3.39it/s] 45%|████▌     | 354/780 [02:30<02:05,  3.39it/s] 46%|████▌     | 355/780 [02:30<02:05,  3.39it/s] 46%|████▌     | 356/780 [02:30<02:05,  3.39it/s] 46%|████▌     | 357/780 [02:30<02:04,  3.39it/s] 46%|████▌     | 358/780 [02:31<02:04,  3.38it/s] 46%|████▌     | 359/780 [02:31<02:04,  3.38it/s] 46%|████▌     | 360/780 [02:31<02:04,  3.38it/s] 46%|████▋     | 361/780 [02:32<02:03,  3.39it/s] 46%|████▋     | 362/780 [02:32<02:03,  3.39it/s] 47%|████▋     | 363/780 [02:32<02:03,  3.39it/s] 47%|████▋     | 364/780 [02:33<02:02,  3.38it/s] 47%|████▋     | 365/780 [02:33<02:02,  3.38it/s] 47%|████▋     | 366/780 [02:33<02:02,  3.37it/s] 47%|████▋     | 367/780 [02:33<02:03,  3.35it/s] 47%|████▋     | 368/780 [02:34<02:02,  3.35it/s] 47%|████▋     | 369/780 [02:34<02:02,  3.37it/s] 47%|████▋     | 370/780 [02:34<02:01,  3.38it/s] 48%|████▊     | 371/780 [02:35<02:01,  3.37it/s] 48%|████▊     | 372/780 [02:35<02:00,  3.38it/s] 48%|████▊     | 373/780 [02:35<02:00,  3.38it/s] 48%|████▊     | 374/780 [02:36<02:00,  3.38it/s] 48%|████▊     | 375/780 [02:36<01:59,  3.38it/s] 48%|████▊     | 376/780 [02:36<01:59,  3.38it/s] 48%|████▊     | 377/780 [02:36<01:59,  3.38it/s] 48%|████▊     | 378/780 [02:37<01:59,  3.38it/s] 49%|████▊     | 379/780 [02:37<01:58,  3.38it/s] 49%|████▊     | 380/780 [02:37<01:58,  3.38it/s] 49%|████▉     | 381/780 [02:38<01:58,  3.38it/s] 49%|████▉     | 382/780 [02:38<01:57,  3.38it/s] 49%|████▉     | 383/780 [02:38<01:57,  3.38it/s] 49%|████▉     | 384/780 [02:38<01:57,  3.38it/s] 49%|████▉     | 385/780 [02:39<01:56,  3.39it/s] 49%|████▉     | 386/780 [02:39<01:56,  3.39it/s] 50%|████▉     | 387/780 [02:39<01:56,  3.39it/s] 50%|████▉     | 388/780 [02:40<01:56,  3.37it/s] 50%|████▉     | 389/780 [02:40<01:56,  3.36it/s] 50%|█████     | 390/780 [02:40<01:58,  3.30it/s] 50%|█████     | 391/780 [02:41<01:56,  3.33it/s] 50%|█████     | 392/780 [02:41<01:56,  3.34it/s] 50%|█████     | 393/780 [02:41<01:55,  3.35it/s] 51%|█████     | 394/780 [02:41<01:54,  3.36it/s] 51%|█████     | 395/780 [02:42<01:54,  3.37it/s] 51%|█████     | 396/780 [02:42<01:53,  3.38it/s] 51%|█████     | 397/780 [02:42<01:53,  3.38it/s] 51%|█████     | 398/780 [02:43<01:53,  3.38it/s] 51%|█████     | 399/780 [02:43<01:52,  3.38it/s] 51%|█████▏    | 400/780 [02:43<01:52,  3.38it/s] 51%|█████▏    | 401/780 [02:44<01:52,  3.38it/s] 52%|█████▏    | 402/780 [02:44<01:51,  3.38it/s] 52%|█████▏    | 403/780 [02:44<01:51,  3.38it/s] 52%|█████▏    | 404/780 [02:44<01:51,  3.38it/s] 52%|█████▏    | 405/780 [02:45<01:50,  3.38it/s] 52%|█████▏    | 406/780 [02:45<01:50,  3.38it/s] 52%|█████▏    | 407/780 [02:45<01:50,  3.38it/s] 52%|█████▏    | 408/780 [02:46<01:49,  3.38it/s] 52%|█████▏    | 409/780 [02:46<01:49,  3.39it/s] 53%|█████▎    | 410/780 [02:46<01:49,  3.38it/s] 53%|█████▎    | 411/780 [02:46<01:49,  3.37it/s] 53%|█████▎    | 412/780 [02:47<01:48,  3.38it/s] 53%|█████▎    | 413/780 [02:47<01:48,  3.38it/s] 53%|█████▎    | 414/780 [02:47<01:48,  3.38it/s] 53%|█████▎    | 415/780 [02:48<01:47,  3.38it/s] 53%|█████▎    | 416/780 [02:48<01:47,  3.38it/s] 53%|█████▎    | 417/780 [02:48<01:47,  3.38it/s] 54%|█████▎    | 418/780 [02:49<01:47,  3.37it/s] 54%|█████▎    | 419/780 [02:49<01:46,  3.39it/s] 54%|█████▍    | 420/780 [02:49<01:45,  3.40it/s] 54%|█████▍    | 421/780 [02:49<01:45,  3.41it/s] 54%|█████▍    | 422/780 [02:50<01:45,  3.41it/s] 54%|█████▍    | 423/780 [02:50<01:44,  3.41it/s] 54%|█████▍    | 424/780 [02:50<01:44,  3.42it/s] 54%|█████▍    | 425/780 [02:51<01:43,  3.43it/s] 55%|█████▍    | 426/780 [02:51<01:43,  3.43it/s] 55%|█████▍    | 427/780 [02:51<01:42,  3.43it/s] 55%|█████▍    | 428/780 [02:51<01:42,  3.43it/s] 55%|█████▌    | 429/780 [02:52<01:42,  3.43it/s] 55%|█████▌    | 430/780 [02:52<01:41,  3.43it/s] 55%|█████▌    | 431/780 [02:52<01:41,  3.43it/s] 55%|█████▌    | 432/780 [02:53<01:41,  3.43it/s] 56%|█████▌    | 433/780 [02:53<01:41,  3.42it/s] 56%|█████▌    | 434/780 [02:53<01:40,  3.43it/s] 56%|█████▌    | 435/780 [02:54<01:40,  3.43it/s] 56%|█████▌    | 436/780 [02:54<01:40,  3.43it/s] 56%|█████▌    | 437/780 [02:54<01:39,  3.43it/s] 56%|█████▌    | 438/780 [02:54<01:39,  3.44it/s] 56%|█████▋    | 439/780 [02:55<01:39,  3.44it/s] 56%|█████▋    | 440/780 [02:55<01:38,  3.44it/s] 57%|█████▋    | 441/780 [02:55<01:38,  3.44it/s] 57%|█████▋    | 442/780 [02:56<01:38,  3.44it/s] 57%|█████▋    | 443/780 [02:56<01:38,  3.43it/s] 57%|█████▋    | 444/780 [02:56<01:38,  3.42it/s] 57%|█████▋    | 445/780 [02:56<01:37,  3.42it/s] 57%|█████▋    | 446/780 [02:57<01:37,  3.43it/s] 57%|█████▋    | 447/780 [02:57<01:37,  3.43it/s] 57%|█████▋    | 448/780 [02:57<01:36,  3.43it/s] 58%|█████▊    | 449/780 [02:58<01:36,  3.43it/s] 58%|█████▊    | 450/780 [02:58<01:36,  3.43it/s] 58%|█████▊    | 451/780 [02:58<01:35,  3.43it/s] 58%|█████▊    | 452/780 [02:58<01:35,  3.43it/s] 58%|█████▊    | 453/780 [02:59<01:35,  3.43it/s] 58%|█████▊    | 454/780 [02:59<01:34,  3.43it/s] 58%|█████▊    | 455/780 [02:59<01:34,  3.43it/s] 58%|█████▊    | 456/780 [03:00<01:34,  3.43it/s] 59%|█████▊    | 457/780 [03:00<01:34,  3.43it/s] 59%|█████▊    | 458/780 [03:00<01:33,  3.44it/s] 59%|█████▉    | 459/780 [03:01<01:33,  3.44it/s] 59%|█████▉    | 460/780 [03:01<01:33,  3.43it/s] 59%|█████▉    | 461/780 [03:01<01:32,  3.44it/s] 59%|█████▉    | 462/780 [03:01<01:32,  3.44it/s] 59%|█████▉    | 463/780 [03:02<01:32,  3.44it/s] 59%|█████▉    | 464/780 [03:02<01:32,  3.43it/s] 60%|█████▉    | 465/780 [03:02<01:31,  3.43it/s] 60%|█████▉    | 466/780 [03:03<01:32,  3.41it/s] 60%|█████▉    | 467/780 [03:03<01:31,  3.42it/s] 60%|██████    | 468/780 [03:03<01:31,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 09:14:51,531 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:14:51,531 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 09:14:51,532 >>   Batch size = 8
{'eval_loss': 0.9237025380134583, 'eval_runtime': 16.8762, 'eval_samples_per_second': 347.472, 'eval_steps_per_second': 43.434, 'epoch': 2.0}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 55.10it/s][A
  2%|▏         | 12/733 [00:00<00:15, 47.09it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.53it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.90it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.38it/s][A
  4%|▍         | 32/733 [00:00<00:15, 44.06it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.86it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.71it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.89it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.90it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.82it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.75it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.70it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.52it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.51it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.56it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.64it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.69it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.68it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.66it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.63it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.58it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.45it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.57it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.54it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.61it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.66it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.71it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.72it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.53it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.56it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.42it/s][A
 23%|██▎       | 167/733 [00:03<00:13, 43.53it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.56it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.60it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.63it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.63it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.67it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.55it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.56it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.49it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.58it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.67it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.68it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.62it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.62it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.63it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.55it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.50it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.43it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.63it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.62it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.73it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.73it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.64it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.55it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.56it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.51it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.60it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.63it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.65it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.66it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.61it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.51it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.54it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.49it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.57it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.60it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.70it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.71it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.67it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.50it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.62it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.62it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.51it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.58it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.68it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.69it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.66it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.64it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.55it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.59it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.54it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.58it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.66it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.60it/s][A
 60%|█████▉    | 437/733 [00:09<00:06, 43.62it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.63it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.60it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.57it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.58it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.63it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.55it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.56it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.58it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.59it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.62it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.53it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.66it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.59it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.60it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.62it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.61it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.61it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.60it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.63it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.63it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.59it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.58it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.51it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.68it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.62it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.58it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.59it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.54it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.64it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.55it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.64it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.69it/s][A
 82%|████████▏ | 602/733 [00:13<00:02, 43.71it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.61it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.57it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.63it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.62it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.56it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.62it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.61it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.60it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.69it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.63it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.43it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.52it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.57it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.66it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.69it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.58it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.74it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.67it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.64it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.66it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.64it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.63it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.54it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.64it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.69it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.60it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.60it/s][A 60%|██████    | 468/780 [03:20<01:31,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:15:08,401 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 09:15:08,417 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:15:10,181 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:15:10,193 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:15:10,203 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:26<36:17,  7.00s/it] 60%|██████    | 470/780 [03:26<25:46,  4.99s/it] 60%|██████    | 471/780 [03:26<18:26,  3.58s/it] 61%|██████    | 472/780 [03:27<13:19,  2.60s/it] 61%|██████    | 473/780 [03:27<09:44,  1.91s/it] 61%|██████    | 474/780 [03:27<07:15,  1.42s/it] 61%|██████    | 475/780 [03:28<05:30,  1.08s/it] 61%|██████    | 476/780 [03:28<04:17,  1.18it/s] 61%|██████    | 477/780 [03:28<03:26,  1.47it/s] 61%|██████▏   | 478/780 [03:28<02:50,  1.77it/s] 61%|██████▏   | 479/780 [03:29<02:25,  2.06it/s] 62%|██████▏   | 480/780 [03:29<02:08,  2.34it/s] 62%|██████▏   | 481/780 [03:29<01:56,  2.57it/s] 62%|██████▏   | 482/780 [03:30<01:47,  2.77it/s] 62%|██████▏   | 483/780 [03:30<01:41,  2.93it/s] 62%|██████▏   | 484/780 [03:30<01:36,  3.05it/s] 62%|██████▏   | 485/780 [03:31<01:33,  3.15it/s] 62%|██████▏   | 486/780 [03:31<01:31,  3.22it/s] 62%|██████▏   | 487/780 [03:31<01:29,  3.27it/s] 63%|██████▎   | 488/780 [03:31<01:28,  3.30it/s] 63%|██████▎   | 489/780 [03:32<01:27,  3.33it/s] 63%|██████▎   | 490/780 [03:32<01:26,  3.35it/s] 63%|██████▎   | 491/780 [03:32<01:26,  3.36it/s] 63%|██████▎   | 492/780 [03:33<01:25,  3.35it/s] 63%|██████▎   | 493/780 [03:33<01:25,  3.36it/s] 63%|██████▎   | 494/780 [03:33<01:24,  3.37it/s] 63%|██████▎   | 495/780 [03:33<01:24,  3.37it/s] 64%|██████▎   | 496/780 [03:34<01:24,  3.38it/s] 64%|██████▎   | 497/780 [03:34<01:23,  3.38it/s] 64%|██████▍   | 498/780 [03:34<01:23,  3.38it/s] 64%|██████▍   | 499/780 [03:35<01:23,  3.38it/s] 64%|██████▍   | 500/780 [03:35<01:22,  3.38it/s]                                                  64%|██████▍   | 500/780 [03:35<01:22,  3.38it/s] 64%|██████▍   | 501/780 [03:35<01:22,  3.38it/s] 64%|██████▍   | 502/780 [03:36<01:22,  3.38it/s] 64%|██████▍   | 503/780 [03:36<01:21,  3.39it/s] 65%|██████▍   | 504/780 [03:36<01:21,  3.38it/s] 65%|██████▍   | 505/780 [03:36<01:21,  3.38it/s] 65%|██████▍   | 506/780 [03:37<01:20,  3.39it/s] 65%|██████▌   | 507/780 [03:37<01:20,  3.41it/s] 65%|██████▌   | 508/780 [03:37<01:19,  3.41it/s] 65%|██████▌   | 509/780 [03:38<01:19,  3.42it/s] 65%|██████▌   | 510/780 [03:38<01:18,  3.42it/s] 66%|██████▌   | 511/780 [03:38<01:18,  3.43it/s] 66%|██████▌   | 512/780 [03:38<01:18,  3.42it/s] 66%|██████▌   | 513/780 [03:39<01:18,  3.41it/s] 66%|██████▌   | 514/780 [03:39<01:18,  3.40it/s] 66%|██████▌   | 515/780 [03:39<01:17,  3.40it/s] 66%|██████▌   | 516/780 [03:40<01:17,  3.40it/s] 66%|██████▋   | 517/780 [03:40<01:17,  3.39it/s] 66%|██████▋   | 518/780 [03:40<01:17,  3.38it/s] 67%|██████▋   | 519/780 [03:41<01:19,  3.28it/s] 67%|██████▋   | 520/780 [03:41<01:18,  3.31it/s] 67%|██████▋   | 521/780 [03:41<01:18,  3.32it/s] 67%|██████▋   | 522/780 [03:41<01:17,  3.34it/s] 67%|██████▋   | 523/780 [03:42<01:16,  3.35it/s] 67%|██████▋   | 524/780 [03:42<01:16,  3.36it/s] 67%|██████▋   | 525/780 [03:42<01:15,  3.36it/s] 67%|██████▋   | 526/780 [03:43<01:15,  3.37it/s] 68%|██████▊   | 527/780 [03:43<01:14,  3.38it/s] 68%|██████▊   | 528/780 [03:43<01:14,  3.38it/s] 68%|██████▊   | 529/780 [03:44<01:14,  3.38it/s] 68%|██████▊   | 530/780 [03:44<01:13,  3.39it/s] 68%|██████▊   | 531/780 [03:44<01:13,  3.38it/s] 68%|██████▊   | 532/780 [03:44<01:13,  3.38it/s] 68%|██████▊   | 533/780 [03:45<01:13,  3.38it/s] 68%|██████▊   | 534/780 [03:45<01:12,  3.39it/s] 69%|██████▊   | 535/780 [03:45<01:12,  3.38it/s] 69%|██████▊   | 536/780 [03:46<01:12,  3.38it/s] 69%|██████▉   | 537/780 [03:46<01:11,  3.38it/s] 69%|██████▉   | 538/780 [03:46<01:11,  3.39it/s] 69%|██████▉   | 539/780 [03:46<01:11,  3.39it/s] 69%|██████▉   | 540/780 [03:47<01:10,  3.39it/s] 69%|██████▉   | 541/780 [03:47<01:10,  3.39it/s] 69%|██████▉   | 542/780 [03:47<01:09,  3.40it/s] 70%|██████▉   | 543/780 [03:48<01:09,  3.40it/s] 70%|██████▉   | 544/780 [03:48<01:09,  3.41it/s] 70%|██████▉   | 545/780 [03:48<01:08,  3.42it/s] 70%|███████   | 546/780 [03:49<01:08,  3.42it/s] 70%|███████   | 547/780 [03:49<01:08,  3.42it/s] 70%|███████   | 548/780 [03:49<01:07,  3.42it/s] 70%|███████   | 549/780 [03:49<01:07,  3.43it/s] 71%|███████   | 550/780 [03:50<01:07,  3.43it/s] 71%|███████   | 551/780 [03:50<01:06,  3.43it/s] 71%|███████   | 552/780 [03:50<01:06,  3.43it/s] 71%|███████   | 553/780 [03:51<01:06,  3.43it/s] 71%|███████   | 554/780 [03:51<01:06,  3.42it/s] 71%|███████   | 555/780 [03:51<01:05,  3.43it/s] 71%|███████▏  | 556/780 [03:51<01:05,  3.43it/s] 71%|███████▏  | 557/780 [03:52<01:04,  3.43it/s] 72%|███████▏  | 558/780 [03:52<01:04,  3.43it/s] 72%|███████▏  | 559/780 [03:52<01:04,  3.43it/s] 72%|███████▏  | 560/780 [03:53<01:04,  3.44it/s] 72%|███████▏  | 561/780 [03:53<01:03,  3.43it/s] 72%|███████▏  | 562/780 [03:53<01:03,  3.43it/s] 72%|███████▏  | 563/780 [03:54<01:03,  3.43it/s] 72%|███████▏  | 564/780 [03:54<01:02,  3.44it/s] 72%|███████▏  | 565/780 [03:54<01:02,  3.43it/s] 73%|███████▎  | 566/780 [03:54<01:02,  3.43it/s] 73%|███████▎  | 567/780 [03:55<01:02,  3.43it/s] 73%|███████▎  | 568/780 [03:55<01:01,  3.43it/s] 73%|███████▎  | 569/780 [03:55<01:01,  3.43it/s] 73%|███████▎  | 570/780 [03:56<01:01,  3.43it/s] 73%|███████▎  | 571/780 [03:56<01:00,  3.44it/s] 73%|███████▎  | 572/780 [03:56<01:00,  3.43it/s] 73%|███████▎  | 573/780 [03:56<01:00,  3.44it/s] 74%|███████▎  | 574/780 [03:57<00:59,  3.43it/s] 74%|███████▎  | 575/780 [03:57<00:59,  3.44it/s] 74%|███████▍  | 576/780 [03:57<00:59,  3.43it/s] 74%|███████▍  | 577/780 [03:58<00:59,  3.44it/s] 74%|███████▍  | 578/780 [03:58<00:58,  3.43it/s] 74%|███████▍  | 579/780 [03:58<00:58,  3.44it/s] 74%|███████▍  | 580/780 [03:58<00:58,  3.44it/s] 74%|███████▍  | 581/780 [03:59<00:57,  3.44it/s] 75%|███████▍  | 582/780 [03:59<00:57,  3.44it/s] 75%|███████▍  | 583/780 [03:59<00:57,  3.44it/s] 75%|███████▍  | 584/780 [04:00<00:57,  3.44it/s] 75%|███████▌  | 585/780 [04:00<00:56,  3.44it/s] 75%|███████▌  | 586/780 [04:00<00:56,  3.43it/s] 75%|███████▌  | 587/780 [04:00<00:56,  3.42it/s] 75%|███████▌  | 588/780 [04:01<00:56,  3.42it/s] 76%|███████▌  | 589/780 [04:01<00:55,  3.42it/s] 76%|███████▌  | 590/780 [04:01<00:55,  3.42it/s] 76%|███████▌  | 591/780 [04:02<00:55,  3.42it/s] 76%|███████▌  | 592/780 [04:02<00:54,  3.42it/s] 76%|███████▌  | 593/780 [04:02<00:54,  3.42it/s] 76%|███████▌  | 594/780 [04:03<00:54,  3.43it/s] 76%|███████▋  | 595/780 [04:03<00:53,  3.43it/s] 76%|███████▋  | 596/780 [04:03<00:53,  3.43it/s] 77%|███████▋  | 597/780 [04:03<00:53,  3.43it/s] 77%|███████▋  | 598/780 [04:04<00:53,  3.41it/s] 77%|███████▋  | 599/780 [04:04<00:52,  3.42it/s] 77%|███████▋  | 600/780 [04:04<00:52,  3.43it/s] 77%|███████▋  | 601/780 [04:05<00:52,  3.43it/s] 77%|███████▋  | 602/780 [04:05<00:51,  3.43it/s] 77%|███████▋  | 603/780 [04:05<00:51,  3.43it/s] 77%|███████▋  | 604/780 [04:05<00:51,  3.44it/s] 78%|███████▊  | 605/780 [04:06<00:50,  3.44it/s] 78%|███████▊  | 606/780 [04:06<00:50,  3.44it/s] 78%|███████▊  | 607/780 [04:06<00:50,  3.44it/s] 78%|███████▊  | 608/780 [04:07<00:49,  3.44it/s] 78%|███████▊  | 609/780 [04:07<00:49,  3.44it/s] 78%|███████▊  | 610/780 [04:07<00:49,  3.44it/s] 78%|███████▊  | 611/780 [04:07<00:49,  3.44it/s] 78%|███████▊  | 612/780 [04:08<00:48,  3.44it/s] 79%|███████▊  | 613/780 [04:08<00:48,  3.44it/s] 79%|███████▊  | 614/780 [04:08<00:48,  3.44it/s] 79%|███████▉  | 615/780 [04:09<00:48,  3.43it/s] 79%|███████▉  | 616/780 [04:09<00:47,  3.43it/s] 79%|███████▉  | 617/780 [04:09<00:47,  3.43it/s] 79%|███████▉  | 618/780 [04:10<00:47,  3.44it/s] 79%|███████▉  | 619/780 [04:10<00:46,  3.44it/s] 79%|███████▉  | 620/780 [04:10<00:46,  3.44it/s] 80%|███████▉  | 621/780 [04:10<00:46,  3.44it/s] 80%|███████▉  | 622/780 [04:11<00:46,  3.43it/s] 80%|███████▉  | 623/780 [04:11<00:45,  3.43it/s] 80%|████████  | 624/780 [04:11<00:45,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 09:15:59,657 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:15:59,657 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 09:15:59,657 >>   Batch size = 8
{'eval_loss': 0.9373242855072021, 'eval_runtime': 16.8473, 'eval_samples_per_second': 348.067, 'eval_steps_per_second': 43.508, 'epoch': 3.0}
{'loss': 0.6806, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 55.38it/s][A
  2%|▏         | 12/733 [00:00<00:15, 47.56it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.53it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.88it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.33it/s][A
  4%|▍         | 32/733 [00:00<00:15, 44.19it/s][A
  5%|▌         | 37/733 [00:00<00:15, 44.00it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.79it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.90it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.78it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.76it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.68it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.62it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.65it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.61it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.55it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.62it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.58it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.55it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.59it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.60it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.57it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.59it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.59it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.67it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.57it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.60it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.42it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.34it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.31it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.23it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.36it/s][A
 23%|██▎       | 167/733 [00:03<00:13, 43.48it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.55it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.62it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.56it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.59it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.57it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.48it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.33it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.57it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.57it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.66it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.65it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.66it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.70it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.53it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.48it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.60it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.61it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.65it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.56it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.72it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.57it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.48it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.70it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.60it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.64it/s][A
 41%|████      | 297/733 [00:06<00:09, 43.62it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.72it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.57it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.61it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.65it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.69it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.69it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.58it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.57it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.57it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.64it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.67it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.66it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.69it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.13it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.31it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.45it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.52it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.50it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.39it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.60it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.63it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.59it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.58it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.59it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.68it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.68it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.59it/s][A
 60%|█████▉    | 437/733 [00:10<00:06, 43.63it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.66it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.62it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.49it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.66it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.70it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.60it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.63it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.66it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.66it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.55it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.68it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.52it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.65it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.59it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.63it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.61it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.57it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.60it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.66it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.66it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.62it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.47it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.65it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.62it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.62it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.65it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.72it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.58it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.52it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.66it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.68it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.61it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.53it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.66it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.68it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.62it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.50it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.65it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.68it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.65it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.63it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.70it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.68it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.56it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.59it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.63it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.64it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.68it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.55it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.45it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.53it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.52it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.62it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.59it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.55it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.63it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.64it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.60it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.63it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.63it/s][A 80%|████████  | 624/780 [04:28<00:45,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:16:16,542 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 09:16:16,562 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:16:18,285 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:16:18,308 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:16:18,322 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:34<17:45,  6.87s/it] 80%|████████  | 626/780 [04:34<12:34,  4.90s/it] 80%|████████  | 627/780 [04:34<08:58,  3.52s/it] 81%|████████  | 628/780 [04:34<06:27,  2.55s/it] 81%|████████  | 629/780 [04:35<04:43,  1.87s/it] 81%|████████  | 630/780 [04:35<03:30,  1.40s/it] 81%|████████  | 631/780 [04:35<02:39,  1.07s/it] 81%|████████  | 632/780 [04:36<02:03,  1.19it/s] 81%|████████  | 633/780 [04:36<01:39,  1.48it/s] 81%|████████▏ | 634/780 [04:36<01:21,  1.78it/s] 81%|████████▏ | 635/780 [04:36<01:09,  2.08it/s] 82%|████████▏ | 636/780 [04:37<01:01,  2.36it/s] 82%|████████▏ | 637/780 [04:37<00:54,  2.60it/s] 82%|████████▏ | 638/780 [04:37<00:50,  2.81it/s] 82%|████████▏ | 639/780 [04:38<00:47,  2.97it/s] 82%|████████▏ | 640/780 [04:38<00:45,  3.09it/s] 82%|████████▏ | 641/780 [04:38<00:43,  3.19it/s] 82%|████████▏ | 642/780 [04:38<00:42,  3.26it/s] 82%|████████▏ | 643/780 [04:39<00:41,  3.31it/s] 83%|████████▎ | 644/780 [04:39<00:40,  3.35it/s] 83%|████████▎ | 645/780 [04:39<00:40,  3.37it/s] 83%|████████▎ | 646/780 [04:40<00:39,  3.39it/s] 83%|████████▎ | 647/780 [04:40<00:39,  3.40it/s] 83%|████████▎ | 648/780 [04:40<00:38,  3.40it/s] 83%|████████▎ | 649/780 [04:41<00:38,  3.41it/s] 83%|████████▎ | 650/780 [04:41<00:38,  3.34it/s] 83%|████████▎ | 651/780 [04:41<00:38,  3.37it/s] 84%|████████▎ | 652/780 [04:41<00:37,  3.39it/s] 84%|████████▎ | 653/780 [04:42<00:37,  3.41it/s] 84%|████████▍ | 654/780 [04:42<00:36,  3.42it/s] 84%|████████▍ | 655/780 [04:42<00:36,  3.42it/s] 84%|████████▍ | 656/780 [04:43<00:36,  3.42it/s] 84%|████████▍ | 657/780 [04:43<00:35,  3.42it/s] 84%|████████▍ | 658/780 [04:43<00:35,  3.43it/s] 84%|████████▍ | 659/780 [04:43<00:35,  3.43it/s] 85%|████████▍ | 660/780 [04:44<00:34,  3.44it/s] 85%|████████▍ | 661/780 [04:44<00:34,  3.43it/s] 85%|████████▍ | 662/780 [04:44<00:34,  3.44it/s] 85%|████████▌ | 663/780 [04:45<00:34,  3.43it/s] 85%|████████▌ | 664/780 [04:45<00:33,  3.44it/s] 85%|████████▌ | 665/780 [04:45<00:33,  3.43it/s] 85%|████████▌ | 666/780 [04:46<00:33,  3.43it/s] 86%|████████▌ | 667/780 [04:46<00:32,  3.43it/s] 86%|████████▌ | 668/780 [04:46<00:32,  3.42it/s] 86%|████████▌ | 669/780 [04:46<00:32,  3.42it/s] 86%|████████▌ | 670/780 [04:47<00:32,  3.42it/s] 86%|████████▌ | 671/780 [04:47<00:31,  3.42it/s] 86%|████████▌ | 672/780 [04:47<00:31,  3.43it/s] 86%|████████▋ | 673/780 [04:48<00:31,  3.42it/s] 86%|████████▋ | 674/780 [04:48<00:30,  3.43it/s] 87%|████████▋ | 675/780 [04:48<00:30,  3.43it/s] 87%|████████▋ | 676/780 [04:48<00:30,  3.43it/s] 87%|████████▋ | 677/780 [04:49<00:30,  3.43it/s] 87%|████████▋ | 678/780 [04:49<00:29,  3.42it/s] 87%|████████▋ | 679/780 [04:49<00:29,  3.43it/s] 87%|████████▋ | 680/780 [04:50<00:29,  3.43it/s] 87%|████████▋ | 681/780 [04:50<00:28,  3.43it/s] 87%|████████▋ | 682/780 [04:50<00:28,  3.42it/s] 88%|████████▊ | 683/780 [04:50<00:28,  3.41it/s] 88%|████████▊ | 684/780 [04:51<00:28,  3.40it/s] 88%|████████▊ | 685/780 [04:51<00:27,  3.39it/s] 88%|████████▊ | 686/780 [04:51<00:27,  3.39it/s] 88%|████████▊ | 687/780 [04:52<00:27,  3.39it/s] 88%|████████▊ | 688/780 [04:52<00:27,  3.39it/s] 88%|████████▊ | 689/780 [04:52<00:26,  3.37it/s] 88%|████████▊ | 690/780 [04:53<00:26,  3.38it/s] 89%|████████▊ | 691/780 [04:53<00:26,  3.38it/s] 89%|████████▊ | 692/780 [04:53<00:26,  3.38it/s] 89%|████████▉ | 693/780 [04:53<00:25,  3.38it/s] 89%|████████▉ | 694/780 [04:54<00:25,  3.38it/s] 89%|████████▉ | 695/780 [04:54<00:25,  3.37it/s] 89%|████████▉ | 696/780 [04:54<00:24,  3.37it/s] 89%|████████▉ | 697/780 [04:55<00:24,  3.38it/s] 89%|████████▉ | 698/780 [04:55<00:24,  3.38it/s] 90%|████████▉ | 699/780 [04:55<00:23,  3.38it/s] 90%|████████▉ | 700/780 [04:56<00:23,  3.37it/s] 90%|████████▉ | 701/780 [04:56<00:23,  3.38it/s] 90%|█████████ | 702/780 [04:56<00:23,  3.38it/s] 90%|█████████ | 703/780 [04:56<00:22,  3.38it/s] 90%|█████████ | 704/780 [04:57<00:22,  3.38it/s] 90%|█████████ | 705/780 [04:57<00:22,  3.38it/s] 91%|█████████ | 706/780 [04:57<00:21,  3.39it/s] 91%|█████████ | 707/780 [04:58<00:21,  3.39it/s] 91%|█████████ | 708/780 [04:58<00:21,  3.39it/s] 91%|█████████ | 709/780 [04:58<00:20,  3.39it/s] 91%|█████████ | 710/780 [04:58<00:20,  3.38it/s] 91%|█████████ | 711/780 [04:59<00:20,  3.38it/s] 91%|█████████▏| 712/780 [04:59<00:20,  3.37it/s] 91%|█████████▏| 713/780 [04:59<00:19,  3.37it/s] 92%|█████████▏| 714/780 [05:00<00:19,  3.37it/s] 92%|█████████▏| 715/780 [05:00<00:19,  3.37it/s] 92%|█████████▏| 716/780 [05:00<00:18,  3.38it/s] 92%|█████████▏| 717/780 [05:01<00:18,  3.38it/s] 92%|█████████▏| 718/780 [05:01<00:18,  3.38it/s] 92%|█████████▏| 719/780 [05:01<00:18,  3.38it/s] 92%|█████████▏| 720/780 [05:01<00:17,  3.38it/s] 92%|█████████▏| 721/780 [05:02<00:17,  3.38it/s] 93%|█████████▎| 722/780 [05:02<00:17,  3.37it/s] 93%|█████████▎| 723/780 [05:02<00:16,  3.37it/s] 93%|█████████▎| 724/780 [05:03<00:16,  3.37it/s] 93%|█████████▎| 725/780 [05:03<00:16,  3.38it/s] 93%|█████████▎| 726/780 [05:03<00:15,  3.38it/s] 93%|█████████▎| 727/780 [05:04<00:15,  3.38it/s] 93%|█████████▎| 728/780 [05:04<00:15,  3.38it/s] 93%|█████████▎| 729/780 [05:04<00:15,  3.38it/s] 94%|█████████▎| 730/780 [05:04<00:14,  3.38it/s] 94%|█████████▎| 731/780 [05:05<00:14,  3.39it/s] 94%|█████████▍| 732/780 [05:05<00:14,  3.38it/s] 94%|█████████▍| 733/780 [05:05<00:13,  3.38it/s] 94%|█████████▍| 734/780 [05:06<00:13,  3.37it/s] 94%|█████████▍| 735/780 [05:06<00:13,  3.38it/s] 94%|█████████▍| 736/780 [05:06<00:13,  3.38it/s] 94%|█████████▍| 737/780 [05:06<00:12,  3.38it/s] 95%|█████████▍| 738/780 [05:07<00:12,  3.38it/s] 95%|█████████▍| 739/780 [05:07<00:12,  3.37it/s] 95%|█████████▍| 740/780 [05:07<00:11,  3.37it/s] 95%|█████████▌| 741/780 [05:08<00:11,  3.37it/s] 95%|█████████▌| 742/780 [05:08<00:11,  3.38it/s] 95%|█████████▌| 743/780 [05:08<00:10,  3.38it/s] 95%|█████████▌| 744/780 [05:09<00:10,  3.38it/s] 96%|█████████▌| 745/780 [05:09<00:10,  3.38it/s] 96%|█████████▌| 746/780 [05:09<00:10,  3.39it/s] 96%|█████████▌| 747/780 [05:09<00:09,  3.38it/s] 96%|█████████▌| 748/780 [05:10<00:09,  3.39it/s] 96%|█████████▌| 749/780 [05:10<00:09,  3.38it/s] 96%|█████████▌| 750/780 [05:10<00:08,  3.38it/s] 96%|█████████▋| 751/780 [05:11<00:08,  3.38it/s] 96%|█████████▋| 752/780 [05:11<00:08,  3.38it/s] 97%|█████████▋| 753/780 [05:11<00:07,  3.38it/s] 97%|█████████▋| 754/780 [05:11<00:07,  3.39it/s] 97%|█████████▋| 755/780 [05:12<00:07,  3.38it/s] 97%|█████████▋| 756/780 [05:12<00:07,  3.38it/s] 97%|█████████▋| 757/780 [05:12<00:06,  3.38it/s] 97%|█████████▋| 758/780 [05:13<00:06,  3.38it/s] 97%|█████████▋| 759/780 [05:13<00:06,  3.38it/s] 97%|█████████▋| 760/780 [05:13<00:05,  3.38it/s] 98%|█████████▊| 761/780 [05:14<00:05,  3.37it/s] 98%|█████████▊| 762/780 [05:14<00:05,  3.37it/s] 98%|█████████▊| 763/780 [05:14<00:05,  3.37it/s] 98%|█████████▊| 764/780 [05:14<00:04,  3.37it/s] 98%|█████████▊| 765/780 [05:15<00:04,  3.37it/s] 98%|█████████▊| 766/780 [05:15<00:04,  3.37it/s] 98%|█████████▊| 767/780 [05:15<00:03,  3.37it/s] 98%|█████████▊| 768/780 [05:16<00:03,  3.38it/s] 99%|█████████▊| 769/780 [05:16<00:03,  3.38it/s] 99%|█████████▊| 770/780 [05:16<00:02,  3.37it/s] 99%|█████████▉| 771/780 [05:17<00:02,  3.37it/s] 99%|█████████▉| 772/780 [05:17<00:02,  3.37it/s] 99%|█████████▉| 773/780 [05:17<00:02,  3.37it/s] 99%|█████████▉| 774/780 [05:17<00:01,  3.39it/s] 99%|█████████▉| 775/780 [05:18<00:01,  3.40it/s] 99%|█████████▉| 776/780 [05:18<00:01,  3.41it/s]100%|█████████▉| 777/780 [05:18<00:00,  3.42it/s]100%|█████████▉| 778/780 [05:19<00:00,  3.42it/s]100%|█████████▉| 779/780 [05:19<00:00,  3.42it/s]100%|██████████| 780/780 [05:19<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 09:17:07,512 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:17:07,512 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 09:17:07,512 >>   Batch size = 8
{'eval_loss': 0.9420405030250549, 'eval_runtime': 16.8554, 'eval_samples_per_second': 347.9, 'eval_steps_per_second': 43.488, 'epoch': 4.0}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 55.26it/s][A
  2%|▏         | 12/733 [00:00<00:15, 47.34it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.43it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.55it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.25it/s][A
  4%|▍         | 32/733 [00:00<00:15, 43.88it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.97it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.80it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.66it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.91it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.86it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.67it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.62it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.48it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.61it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.58it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.58it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.73it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.73it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.62it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.61it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.55it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.54it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.58it/s][A
 17%|█▋        | 127/733 [00:02<00:14, 43.09it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.33it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.46it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.50it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.50it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.34it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.46it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.41it/s][A
 23%|██▎       | 167/733 [00:03<00:13, 43.50it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.62it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.73it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.73it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.62it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.53it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.64it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.60it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.51it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.61it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.58it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.66it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.74it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.69it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.56it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.61it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.55it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.58it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.58it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.62it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.64it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.59it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.57it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.68it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.58it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.58it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.54it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.64it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.68it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.55it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.63it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.67it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.61it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.49it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.50it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.68it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.71it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.55it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.67it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.66it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.58it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.59it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.54it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.49it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.65it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.62it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.65it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.67it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.55it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.61it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.55it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.57it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.62it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.48it/s][A
 60%|█████▉    | 437/733 [00:10<00:06, 43.62it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.59it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.47it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.60it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.44it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.55it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.56it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.61it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.59it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.56it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.52it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.57it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.64it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.59it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.63it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.53it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.58it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.63it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.67it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.59it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.63it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.56it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.51it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.60it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.60it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.46it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.62it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.62it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.64it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.47it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.45it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.53it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.55it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.64it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.71it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.53it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.63it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.63it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.71it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.56it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.53it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.54it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.65it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.57it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.50it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.53it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.65it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.50it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.42it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.62it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.54it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.69it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.61it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.68it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.71it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.60it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.54it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.47it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.62it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.58it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.58it/s][A100%|██████████| 780/780 [05:36<00:00,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:17:24,363 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 09:17:24,383 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:17:26,169 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:17:26,188 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:17:26,198 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 09:17:29,701 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 09:17:29,705 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-156 (score: 0.9157085418701172).
                                                 100%|██████████| 780/780 [05:43<00:00,  3.42it/s]100%|██████████| 780/780 [05:43<00:00,  2.27it/s]
[INFO|trainer.py:1894] 2023-08-29 09:17:31,488 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 09:17:31,507 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:17:33,441 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:17:33,469 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:17:33,480 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:17:33,677 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:33,678 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:33,678 >>   train_loss               =     0.6687
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:33,678 >>   train_runtime            = 0:05:43.63
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:33,678 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:33,678 >>   train_samples_per_second =    145.501
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:33,678 >>   train_steps_per_second   =       2.27
{'eval_loss': 0.9466191530227661, 'eval_runtime': 16.8204, 'eval_samples_per_second': 348.624, 'eval_steps_per_second': 43.578, 'epoch': 5.0}
{'train_runtime': 343.64, 'train_samples_per_second': 145.501, 'train_steps_per_second': 2.27, 'train_loss': 0.6686783228165064, 'epoch': 5.0}
08/29/2023 09:17:33 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 09:17:33,725 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:17:33,725 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 09:17:33,725 >>   Batch size = 8
  0%|          | 0/733 [00:00<?, ?it/s]  1%|          | 6/733 [00:00<00:13, 54.24it/s]  2%|▏         | 12/733 [00:00<00:15, 47.94it/s]  2%|▏         | 17/733 [00:00<00:15, 46.32it/s]  3%|▎         | 22/733 [00:00<00:15, 45.64it/s]  4%|▎         | 27/733 [00:00<00:15, 45.05it/s]  4%|▍         | 32/733 [00:00<00:15, 44.72it/s]  5%|▌         | 37/733 [00:00<00:15, 44.46it/s]  6%|▌         | 42/733 [00:00<00:15, 43.84it/s]  6%|▋         | 47/733 [00:01<00:15, 43.28it/s]  7%|▋         | 52/733 [00:01<00:15, 43.29it/s]  8%|▊         | 57/733 [00:01<00:15, 43.35it/s]  8%|▊         | 62/733 [00:01<00:15, 43.58it/s]  9%|▉         | 67/733 [00:01<00:15, 43.74it/s] 10%|▉         | 72/733 [00:01<00:15, 43.91it/s] 11%|█         | 77/733 [00:01<00:14, 44.03it/s] 11%|█         | 82/733 [00:01<00:14, 43.81it/s] 12%|█▏        | 87/733 [00:01<00:14, 43.35it/s] 13%|█▎        | 92/733 [00:02<00:14, 43.24it/s] 13%|█▎        | 97/733 [00:02<00:14, 43.25it/s] 14%|█▍        | 102/733 [00:02<00:14, 43.44it/s] 15%|█▍        | 107/733 [00:02<00:14, 43.66it/s] 15%|█▌        | 112/733 [00:02<00:14, 43.71it/s] 16%|█▌        | 117/733 [00:02<00:14, 43.82it/s] 17%|█▋        | 122/733 [00:02<00:13, 43.83it/s] 17%|█▋        | 127/733 [00:02<00:13, 43.48it/s] 18%|█▊        | 132/733 [00:02<00:13, 43.44it/s] 19%|█▊        | 137/733 [00:03<00:13, 43.22it/s] 19%|█▉        | 142/733 [00:03<00:13, 43.21it/s] 20%|██        | 147/733 [00:03<00:13, 43.46it/s] 21%|██        | 152/733 [00:03<00:13, 43.71it/s] 21%|██▏       | 157/733 [00:03<00:13, 43.85it/s] 22%|██▏       | 162/733 [00:03<00:12, 43.94it/s] 23%|██▎       | 167/733 [00:03<00:12, 43.79it/s] 23%|██▎       | 172/733 [00:03<00:12, 43.47it/s] 24%|██▍       | 177/733 [00:04<00:12, 43.34it/s] 25%|██▍       | 182/733 [00:04<00:12, 43.32it/s] 26%|██▌       | 187/733 [00:04<00:12, 43.41it/s] 26%|██▌       | 192/733 [00:04<00:12, 43.40it/s] 27%|██▋       | 197/733 [00:04<00:12, 43.54it/s] 28%|██▊       | 202/733 [00:04<00:12, 43.77it/s] 28%|██▊       | 207/733 [00:04<00:12, 43.82it/s] 29%|██▉       | 212/733 [00:04<00:11, 43.68it/s] 30%|██▉       | 217/733 [00:04<00:11, 43.48it/s] 30%|███       | 222/733 [00:05<00:11, 43.32it/s] 31%|███       | 227/733 [00:05<00:11, 43.35it/s] 32%|███▏      | 232/733 [00:05<00:11, 43.49it/s] 32%|███▏      | 237/733 [00:05<00:11, 43.43it/s] 33%|███▎      | 242/733 [00:05<00:11, 43.67it/s] 34%|███▎      | 247/733 [00:05<00:11, 43.71it/s] 34%|███▍      | 252/733 [00:05<00:10, 43.76it/s] 35%|███▌      | 257/733 [00:05<00:10, 43.62it/s] 36%|███▌      | 262/733 [00:05<00:10, 43.52it/s] 36%|███▋      | 267/733 [00:06<00:10, 43.42it/s] 37%|███▋      | 272/733 [00:06<00:10, 43.37it/s] 38%|███▊      | 277/733 [00:06<00:10, 43.45it/s] 38%|███▊      | 282/733 [00:06<00:10, 43.61it/s] 39%|███▉      | 287/733 [00:06<00:10, 43.74it/s] 40%|███▉      | 292/733 [00:06<00:10, 43.60it/s] 41%|████      | 297/733 [00:06<00:09, 43.63it/s] 41%|████      | 302/733 [00:06<00:09, 43.50it/s] 42%|████▏     | 307/733 [00:07<00:09, 43.50it/s] 43%|████▎     | 312/733 [00:07<00:09, 43.44it/s] 43%|████▎     | 317/733 [00:07<00:09, 43.37it/s] 44%|████▍     | 322/733 [00:07<00:09, 43.51it/s] 45%|████▍     | 327/733 [00:07<00:09, 43.64it/s] 45%|████▌     | 332/733 [00:07<00:09, 43.60it/s] 46%|████▌     | 337/733 [00:07<00:09, 43.64it/s] 47%|████▋     | 342/733 [00:07<00:08, 43.63it/s] 47%|████▋     | 347/733 [00:07<00:08, 43.39it/s] 48%|████▊     | 352/733 [00:08<00:08, 43.39it/s] 49%|████▊     | 357/733 [00:08<00:08, 43.44it/s] 49%|████▉     | 362/733 [00:08<00:08, 43.52it/s] 50%|█████     | 367/733 [00:08<00:08, 43.57it/s] 51%|█████     | 372/733 [00:08<00:08, 43.60it/s] 51%|█████▏    | 377/733 [00:08<00:08, 43.49it/s] 52%|█████▏    | 382/733 [00:08<00:08, 43.61it/s] 53%|█████▎    | 387/733 [00:08<00:07, 43.46it/s] 53%|█████▎    | 392/733 [00:08<00:07, 43.47it/s] 54%|█████▍    | 397/733 [00:09<00:07, 43.49it/s] 55%|█████▍    | 402/733 [00:09<00:07, 43.46it/s] 56%|█████▌    | 407/733 [00:09<00:07, 43.55it/s] 56%|█████▌    | 412/733 [00:09<00:07, 43.60it/s] 57%|█████▋    | 417/733 [00:09<00:07, 43.57it/s] 58%|█████▊    | 422/733 [00:09<00:07, 43.58it/s] 58%|█████▊    | 427/733 [00:09<00:07, 43.47it/s] 59%|█████▉    | 432/733 [00:09<00:06, 43.45it/s] 60%|█████▉    | 437/733 [00:10<00:06, 43.45it/s] 60%|██████    | 442/733 [00:10<00:06, 43.37it/s] 61%|██████    | 447/733 [00:10<00:06, 43.52it/s] 62%|██████▏   | 452/733 [00:10<00:06, 43.56it/s] 62%|██████▏   | 457/733 [00:10<00:06, 43.58it/s] 63%|██████▎   | 462/733 [00:10<00:06, 43.55it/s] 64%|██████▎   | 467/733 [00:10<00:06, 43.55it/s] 64%|██████▍   | 472/733 [00:10<00:05, 43.51it/s] 65%|██████▌   | 477/733 [00:10<00:05, 43.50it/s] 66%|██████▌   | 482/733 [00:11<00:05, 43.45it/s] 66%|██████▋   | 487/733 [00:11<00:05, 43.37it/s] 67%|██████▋   | 492/733 [00:11<00:05, 43.49it/s] 68%|██████▊   | 497/733 [00:11<00:05, 43.53it/s] 68%|██████▊   | 502/733 [00:11<00:05, 43.55it/s] 69%|██████▉   | 507/733 [00:11<00:05, 43.56it/s] 70%|██████▉   | 512/733 [00:11<00:05, 43.52it/s] 71%|███████   | 517/733 [00:11<00:04, 43.56it/s] 71%|███████   | 522/733 [00:11<00:04, 43.43it/s] 72%|███████▏  | 527/733 [00:12<00:04, 43.41it/s] 73%|███████▎  | 532/733 [00:12<00:04, 43.43it/s] 73%|███████▎  | 537/733 [00:12<00:04, 43.47it/s] 74%|███████▍  | 542/733 [00:12<00:04, 43.59it/s] 75%|███████▍  | 547/733 [00:12<00:04, 43.66it/s] 75%|███████▌  | 552/733 [00:12<00:04, 43.57it/s] 76%|███████▌  | 557/733 [00:12<00:04, 43.70it/s] 77%|███████▋  | 562/733 [00:12<00:03, 43.72it/s] 77%|███████▋  | 567/733 [00:12<00:03, 43.71it/s] 78%|███████▊  | 572/733 [00:13<00:03, 43.68it/s] 79%|███████▊  | 577/733 [00:13<00:03, 43.57it/s] 79%|███████▉  | 582/733 [00:13<00:03, 43.60it/s] 80%|████████  | 587/733 [00:13<00:03, 43.73it/s] 81%|████████  | 592/733 [00:13<00:03, 43.75it/s] 81%|████████▏ | 597/733 [00:13<00:03, 43.77it/s] 82%|████████▏ | 602/733 [00:13<00:02, 43.75it/s] 83%|████████▎ | 607/733 [00:13<00:02, 43.56it/s] 83%|████████▎ | 612/733 [00:14<00:02, 43.68it/s] 84%|████████▍ | 617/733 [00:14<00:02, 43.72it/s] 85%|████████▍ | 622/733 [00:14<00:02, 43.70it/s] 86%|████████▌ | 627/733 [00:14<00:02, 43.66it/s] 86%|████████▌ | 632/733 [00:14<00:02, 43.73it/s] 87%|████████▋ | 637/733 [00:14<00:02, 43.60it/s] 88%|████████▊ | 642/733 [00:14<00:02, 43.59it/s] 88%|████████▊ | 647/733 [00:14<00:01, 43.69it/s] 89%|████████▉ | 652/733 [00:14<00:01, 43.72it/s] 90%|████████▉ | 657/733 [00:15<00:01, 43.64it/s] 90%|█████████ | 662/733 [00:15<00:01, 43.56it/s] 91%|█████████ | 667/733 [00:15<00:01, 43.68it/s] 92%|█████████▏| 672/733 [00:15<00:01, 43.74it/s] 92%|█████████▏| 677/733 [00:15<00:01, 43.66it/s] 93%|█████████▎| 682/733 [00:15<00:01, 43.75it/s] 94%|█████████▎| 687/733 [00:15<00:01, 43.58it/s] 94%|█████████▍| 692/733 [00:15<00:00, 43.67it/s] 95%|█████████▌| 697/733 [00:15<00:00, 43.65it/s] 96%|█████████▌| 702/733 [00:16<00:00, 43.69it/s] 96%|█████████▋| 707/733 [00:16<00:00, 43.72it/s] 97%|█████████▋| 712/733 [00:16<00:00, 43.72it/s] 98%|█████████▊| 717/733 [00:16<00:00, 43.68it/s] 98%|█████████▊| 722/733 [00:16<00:00, 43.75it/s] 99%|█████████▉| 727/733 [00:16<00:00, 43.74it/s]100%|█████████▉| 732/733 [00:16<00:00, 43.71it/s]100%|██████████| 733/733 [00:16<00:00, 43.62it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:17:50,545 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:50,545 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:50,545 >>   eval_loss               =     0.9157
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:50,545 >>   eval_runtime            = 0:00:16.82
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:50,545 >>   eval_samples            =       5864
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:50,545 >>   eval_samples_per_second =    348.627
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:50,545 >>   eval_steps_per_second   =     43.578
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:17:50,545 >>   perplexity              =     2.4985
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:57,350 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:57,352 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:57,352 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:57,352 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:57,352 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:17:58,049 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:17:58,050 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:17:58,633 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:17:59,677 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:17:59,683 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:02,490 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:02,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:02,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:02,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:02,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:18:03,116 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:18:03,119 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:18:03,697 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:18:03,871 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:18:03,871 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl', 'labels': ['inception', 'located on terrain feature', 'military branch', 'occupant', 'occupation'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14932
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15032, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.76it/s]Extractor Predicting: 7it [00:04,  1.77it/s]Extractor Predicting: 8it [00:04,  1.78it/s]Extractor Predicting: 9it [00:05,  1.76it/s]Extractor Predicting: 10it [00:05,  1.77it/s]Extractor Predicting: 11it [00:06,  1.76it/s]Extractor Predicting: 12it [00:06,  1.74it/s]Extractor Predicting: 13it [00:07,  1.74it/s]Extractor Predicting: 14it [00:08,  1.73it/s]Extractor Predicting: 15it [00:08,  1.72it/s]Extractor Predicting: 16it [00:09,  1.69it/s]Extractor Predicting: 17it [00:09,  1.68it/s]Extractor Predicting: 18it [00:10,  1.73it/s]Extractor Predicting: 19it [00:11,  1.75it/s]Extractor Predicting: 20it [00:11,  1.74it/s]Extractor Predicting: 21it [00:12,  1.76it/s]Extractor Predicting: 22it [00:12,  1.77it/s]Extractor Predicting: 23it [00:13,  1.80it/s]Extractor Predicting: 24it [00:13,  1.84it/s]Extractor Predicting: 25it [00:14,  1.85it/s]Extractor Predicting: 26it [00:14,  1.79it/s]Extractor Predicting: 27it [00:15,  1.81it/s]Extractor Predicting: 28it [00:16,  1.78it/s]Extractor Predicting: 29it [00:16,  1.78it/s]Extractor Predicting: 30it [00:17,  1.75it/s]Extractor Predicting: 31it [00:17,  1.77it/s]Extractor Predicting: 32it [00:18,  1.78it/s]Extractor Predicting: 33it [00:18,  1.74it/s]Extractor Predicting: 34it [00:19,  1.77it/s]Extractor Predicting: 35it [00:20,  1.71it/s]Extractor Predicting: 36it [00:20,  1.71it/s]Extractor Predicting: 37it [00:21,  1.69it/s]Extractor Predicting: 38it [00:21,  1.73it/s]Extractor Predicting: 39it [00:22,  1.76it/s]Extractor Predicting: 40it [00:22,  1.82it/s]Extractor Predicting: 41it [00:23,  1.76it/s]Extractor Predicting: 42it [00:24,  1.71it/s]Extractor Predicting: 43it [00:24,  1.63it/s]Extractor Predicting: 44it [00:25,  1.58it/s]Extractor Predicting: 45it [00:26,  1.57it/s]Extractor Predicting: 46it [00:26,  1.60it/s]Extractor Predicting: 47it [00:27,  1.55it/s]Extractor Predicting: 48it [00:27,  1.59it/s]Extractor Predicting: 49it [00:28,  1.59it/s]Extractor Predicting: 50it [00:29,  1.54it/s]Extractor Predicting: 51it [00:29,  1.59it/s]Extractor Predicting: 52it [00:30,  1.56it/s]Extractor Predicting: 53it [00:31,  1.59it/s]Extractor Predicting: 54it [00:31,  1.58it/s]Extractor Predicting: 55it [00:32,  1.61it/s]Extractor Predicting: 56it [00:33,  1.60it/s]Extractor Predicting: 57it [00:33,  1.62it/s]Extractor Predicting: 58it [00:34,  1.62it/s]Extractor Predicting: 59it [00:34,  1.61it/s]Extractor Predicting: 60it [00:35,  1.63it/s]Extractor Predicting: 61it [00:36,  1.63it/s]Extractor Predicting: 62it [00:36,  1.63it/s]Extractor Predicting: 63it [00:37,  1.68it/s]Extractor Predicting: 64it [00:37,  1.63it/s]Extractor Predicting: 65it [00:38,  1.64it/s]Extractor Predicting: 66it [00:39,  1.62it/s]Extractor Predicting: 67it [00:39,  1.65it/s]Extractor Predicting: 68it [00:40,  1.66it/s]Extractor Predicting: 69it [00:40,  1.65it/s]Extractor Predicting: 70it [00:41,  1.65it/s]Extractor Predicting: 71it [00:42,  1.65it/s]Extractor Predicting: 72it [00:42,  1.67it/s]Extractor Predicting: 73it [00:43,  1.66it/s]Extractor Predicting: 74it [00:44,  1.52it/s]Extractor Predicting: 75it [00:44,  1.53it/s]Extractor Predicting: 76it [00:45,  1.58it/s]Extractor Predicting: 77it [00:45,  1.62it/s]Extractor Predicting: 78it [00:46,  1.60it/s]Extractor Predicting: 79it [00:47,  1.59it/s]Extractor Predicting: 80it [00:47,  1.61it/s]Extractor Predicting: 81it [00:48,  1.63it/s]Extractor Predicting: 82it [00:49,  1.63it/s]Extractor Predicting: 83it [00:49,  1.63it/s]Extractor Predicting: 84it [00:50,  1.63it/s]Extractor Predicting: 85it [00:50,  1.65it/s]Extractor Predicting: 86it [00:51,  1.65it/s]Extractor Predicting: 87it [00:52,  1.62it/s]Extractor Predicting: 88it [00:52,  1.63it/s]Extractor Predicting: 89it [00:53,  1.64it/s]Extractor Predicting: 90it [00:53,  1.66it/s]Extractor Predicting: 91it [00:54,  1.67it/s]Extractor Predicting: 92it [00:55,  1.66it/s]Extractor Predicting: 93it [00:55,  1.64it/s]Extractor Predicting: 94it [00:56,  1.66it/s]Extractor Predicting: 95it [00:56,  1.64it/s]Extractor Predicting: 96it [00:57,  1.62it/s]Extractor Predicting: 97it [00:58,  1.61it/s]Extractor Predicting: 98it [00:58,  1.59it/s]Extractor Predicting: 99it [00:59,  1.62it/s]Extractor Predicting: 100it [01:00,  1.61it/s]Extractor Predicting: 101it [01:00,  1.62it/s]Extractor Predicting: 102it [01:01,  1.62it/s]Extractor Predicting: 103it [01:01,  1.62it/s]Extractor Predicting: 104it [01:02,  1.62it/s]Extractor Predicting: 105it [01:03,  1.62it/s]Extractor Predicting: 106it [01:03,  1.65it/s]Extractor Predicting: 107it [01:04,  1.65it/s]Extractor Predicting: 108it [01:04,  1.67it/s]Extractor Predicting: 109it [01:05,  1.66it/s]Extractor Predicting: 110it [01:06,  1.66it/s]Extractor Predicting: 111it [01:06,  1.68it/s]Extractor Predicting: 112it [01:07,  1.66it/s]Extractor Predicting: 113it [01:07,  1.67it/s]Extractor Predicting: 114it [01:08,  1.69it/s]Extractor Predicting: 115it [01:09,  1.66it/s]Extractor Predicting: 116it [01:09,  1.68it/s]Extractor Predicting: 117it [01:10,  1.67it/s]Extractor Predicting: 118it [01:10,  1.67it/s]Extractor Predicting: 119it [01:11,  1.64it/s]Extractor Predicting: 120it [01:12,  1.63it/s]Extractor Predicting: 121it [01:12,  1.66it/s]Extractor Predicting: 122it [01:13,  1.67it/s]Extractor Predicting: 123it [01:13,  1.67it/s]Extractor Predicting: 124it [01:14,  1.68it/s]Extractor Predicting: 125it [01:15,  1.63it/s]Extractor Predicting: 126it [01:15,  1.62it/s]Extractor Predicting: 127it [01:16,  1.62it/s]Extractor Predicting: 128it [01:17,  1.63it/s]Extractor Predicting: 129it [01:17,  1.65it/s]Extractor Predicting: 130it [01:18,  1.65it/s]Extractor Predicting: 131it [01:18,  1.66it/s]Extractor Predicting: 132it [01:19,  1.66it/s]Extractor Predicting: 133it [01:19,  1.70it/s]Extractor Predicting: 134it [01:20,  1.64it/s]Extractor Predicting: 135it [01:21,  1.63it/s]Extractor Predicting: 136it [01:21,  1.64it/s]Extractor Predicting: 137it [01:22,  1.65it/s]Extractor Predicting: 138it [01:23,  1.60it/s]Extractor Predicting: 139it [01:23,  1.61it/s]Extractor Predicting: 140it [01:24,  1.59it/s]Extractor Predicting: 141it [01:25,  1.58it/s]Extractor Predicting: 142it [01:25,  1.55it/s]Extractor Predicting: 143it [01:26,  1.57it/s]Extractor Predicting: 144it [01:26,  1.58it/s]Extractor Predicting: 145it [01:27,  1.60it/s]Extractor Predicting: 146it [01:28,  1.59it/s]Extractor Predicting: 147it [01:28,  1.58it/s]Extractor Predicting: 148it [01:29,  1.57it/s]Extractor Predicting: 149it [01:30,  1.55it/s]Extractor Predicting: 150it [01:30,  1.57it/s]Extractor Predicting: 151it [01:31,  1.56it/s]Extractor Predicting: 152it [01:32,  1.57it/s]Extractor Predicting: 153it [01:32,  1.60it/s]Extractor Predicting: 154it [01:33,  1.60it/s]Extractor Predicting: 155it [01:33,  1.57it/s]Extractor Predicting: 156it [01:34,  1.61it/s]Extractor Predicting: 157it [01:35,  1.61it/s]Extractor Predicting: 158it [01:35,  1.62it/s]Extractor Predicting: 159it [01:36,  1.59it/s]Extractor Predicting: 160it [01:36,  1.60it/s]Extractor Predicting: 161it [01:37,  1.62it/s]Extractor Predicting: 162it [01:38,  1.55it/s]Extractor Predicting: 163it [01:38,  1.54it/s]Extractor Predicting: 164it [01:39,  1.54it/s]Extractor Predicting: 165it [01:40,  1.55it/s]Extractor Predicting: 166it [01:40,  1.57it/s]Extractor Predicting: 167it [01:41,  1.57it/s]Extractor Predicting: 168it [01:42,  1.41it/s]Extractor Predicting: 169it [01:43,  1.45it/s]Extractor Predicting: 170it [01:43,  1.43it/s]Extractor Predicting: 171it [01:44,  1.50it/s]Extractor Predicting: 172it [01:44,  1.52it/s]Extractor Predicting: 173it [01:45,  1.51it/s]Extractor Predicting: 174it [01:46,  1.52it/s]Extractor Predicting: 175it [01:46,  1.54it/s]Extractor Predicting: 176it [01:47,  1.55it/s]Extractor Predicting: 177it [01:48,  1.52it/s]Extractor Predicting: 178it [01:48,  1.53it/s]Extractor Predicting: 179it [01:49,  1.53it/s]Extractor Predicting: 180it [01:50,  1.51it/s]Extractor Predicting: 181it [01:50,  1.49it/s]Extractor Predicting: 182it [01:51,  1.50it/s]Extractor Predicting: 183it [01:52,  1.55it/s]Extractor Predicting: 184it [01:52,  1.60it/s]Extractor Predicting: 185it [01:53,  1.60it/s]Extractor Predicting: 186it [01:53,  1.60it/s]Extractor Predicting: 187it [01:54,  1.56it/s]Extractor Predicting: 188it [01:55,  1.58it/s]Extractor Predicting: 189it [01:55,  1.62it/s]Extractor Predicting: 190it [01:56,  1.65it/s]Extractor Predicting: 191it [01:57,  1.65it/s]Extractor Predicting: 192it [01:57,  1.63it/s]Extractor Predicting: 193it [01:58,  1.63it/s]Extractor Predicting: 194it [01:58,  1.61it/s]Extractor Predicting: 195it [01:59,  1.63it/s]Extractor Predicting: 196it [02:00,  1.62it/s]Extractor Predicting: 197it [02:00,  1.64it/s]Extractor Predicting: 198it [02:01,  1.60it/s]Extractor Predicting: 199it [02:02,  1.59it/s]Extractor Predicting: 200it [02:02,  1.61it/s]Extractor Predicting: 201it [02:03,  1.62it/s]Extractor Predicting: 202it [02:03,  1.66it/s]Extractor Predicting: 203it [02:04,  1.68it/s]Extractor Predicting: 204it [02:05,  1.66it/s]Extractor Predicting: 205it [02:05,  1.65it/s]Extractor Predicting: 206it [02:06,  1.64it/s]Extractor Predicting: 207it [02:06,  1.64it/s]Extractor Predicting: 208it [02:07,  1.62it/s]Extractor Predicting: 209it [02:08,  1.64it/s]Extractor Predicting: 210it [02:08,  1.60it/s]Extractor Predicting: 211it [02:09,  1.62it/s]Extractor Predicting: 212it [02:09,  1.61it/s]Extractor Predicting: 213it [02:10,  1.60it/s]Extractor Predicting: 214it [02:11,  1.60it/s]Extractor Predicting: 215it [02:11,  1.58it/s]Extractor Predicting: 216it [02:12,  1.56it/s]Extractor Predicting: 217it [02:13,  1.58it/s]Extractor Predicting: 218it [02:13,  1.59it/s]Extractor Predicting: 219it [02:14,  1.60it/s]Extractor Predicting: 220it [02:14,  1.65it/s]Extractor Predicting: 221it [02:15,  1.67it/s]Extractor Predicting: 222it [02:16,  1.67it/s]Extractor Predicting: 223it [02:16,  1.60it/s]Extractor Predicting: 224it [02:17,  1.81it/s]Extractor Predicting: 224it [02:17,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:29,828 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:29,834 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:29,834 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:29,834 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:29,834 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:20:30,119 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:20:30,120 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:20:30,371 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:20:31,394 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:20:31,394 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:33,080 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:33,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:33,092 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:33,092 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:33,092 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:20:33,407 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:20:33,408 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:20:33,661 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:20:33,797 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:20:33,797 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4365361803084223,
  "recall": 0.06275579809004093,
  "score": 0.10973609661547637,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'father', 'follows', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'part of the series', 'place of birth', 'production company', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 30214
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 30314, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.71it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.67it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.70it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:08,  1.69it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:10,  1.63it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.42it/s]Extractor Predicting: 21it [00:12,  1.49it/s]Extractor Predicting: 22it [00:13,  1.52it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.66it/s]Extractor Predicting: 26it [00:15,  1.68it/s]Extractor Predicting: 27it [00:16,  1.68it/s]Extractor Predicting: 28it [00:17,  1.71it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:18,  1.68it/s]Extractor Predicting: 31it [00:18,  1.71it/s]Extractor Predicting: 32it [00:19,  1.67it/s]Extractor Predicting: 33it [00:20,  1.66it/s]Extractor Predicting: 34it [00:20,  1.63it/s]Extractor Predicting: 35it [00:21,  1.65it/s]Extractor Predicting: 36it [00:22,  1.44it/s]Extractor Predicting: 37it [00:22,  1.55it/s]Extractor Predicting: 38it [00:23,  1.56it/s]Extractor Predicting: 39it [00:23,  1.60it/s]Extractor Predicting: 40it [00:24,  1.59it/s]Extractor Predicting: 41it [00:25,  1.59it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.62it/s]Extractor Predicting: 44it [00:27,  1.61it/s]Extractor Predicting: 45it [00:27,  1.59it/s]Extractor Predicting: 46it [00:28,  1.59it/s]Extractor Predicting: 47it [00:28,  1.58it/s]Extractor Predicting: 48it [00:29,  1.57it/s]Extractor Predicting: 49it [00:30,  1.61it/s]Extractor Predicting: 50it [00:30,  1.68it/s]Extractor Predicting: 51it [00:31,  1.69it/s]Extractor Predicting: 52it [00:31,  1.70it/s]Extractor Predicting: 53it [00:32,  1.66it/s]Extractor Predicting: 54it [00:33,  1.67it/s]Extractor Predicting: 55it [00:33,  1.69it/s]Extractor Predicting: 56it [00:34,  1.70it/s]Extractor Predicting: 57it [00:34,  1.70it/s]Extractor Predicting: 58it [00:35,  1.73it/s]Extractor Predicting: 59it [00:35,  1.79it/s]Extractor Predicting: 60it [00:36,  1.81it/s]Extractor Predicting: 61it [00:37,  1.75it/s]Extractor Predicting: 62it [00:37,  1.78it/s]Extractor Predicting: 63it [00:38,  1.78it/s]Extractor Predicting: 64it [00:38,  1.84it/s]Extractor Predicting: 65it [00:39,  1.83it/s]Extractor Predicting: 66it [00:39,  1.79it/s]Extractor Predicting: 67it [00:40,  1.74it/s]Extractor Predicting: 68it [00:40,  1.79it/s]Extractor Predicting: 69it [00:41,  1.77it/s]Extractor Predicting: 70it [00:42,  1.74it/s]Extractor Predicting: 71it [00:42,  1.78it/s]Extractor Predicting: 72it [00:43,  1.75it/s]Extractor Predicting: 73it [00:43,  1.80it/s]Extractor Predicting: 74it [00:44,  1.79it/s]Extractor Predicting: 75it [00:44,  1.84it/s]Extractor Predicting: 76it [00:45,  1.83it/s]Extractor Predicting: 77it [00:46,  1.74it/s]Extractor Predicting: 78it [00:46,  1.75it/s]Extractor Predicting: 79it [00:47,  1.76it/s]Extractor Predicting: 80it [00:47,  1.71it/s]Extractor Predicting: 81it [00:48,  1.70it/s]Extractor Predicting: 82it [00:48,  1.70it/s]Extractor Predicting: 83it [00:49,  1.68it/s]Extractor Predicting: 84it [00:50,  1.71it/s]Extractor Predicting: 85it [00:50,  1.76it/s]Extractor Predicting: 86it [00:51,  1.79it/s]Extractor Predicting: 87it [00:51,  1.80it/s]Extractor Predicting: 88it [00:52,  1.78it/s]Extractor Predicting: 89it [00:52,  1.77it/s]Extractor Predicting: 90it [00:53,  1.71it/s]Extractor Predicting: 91it [00:54,  1.67it/s]Extractor Predicting: 92it [00:54,  1.71it/s]Extractor Predicting: 93it [00:55,  1.79it/s]Extractor Predicting: 94it [00:55,  1.79it/s]Extractor Predicting: 95it [00:56,  1.79it/s]Extractor Predicting: 96it [00:56,  1.80it/s]Extractor Predicting: 97it [00:57,  1.87it/s]Extractor Predicting: 98it [00:57,  1.87it/s]Extractor Predicting: 99it [00:58,  1.87it/s]Extractor Predicting: 100it [00:58,  1.88it/s]Extractor Predicting: 101it [00:59,  1.88it/s]Extractor Predicting: 102it [01:00,  1.84it/s]Extractor Predicting: 103it [01:00,  1.77it/s]Extractor Predicting: 104it [01:01,  1.81it/s]Extractor Predicting: 105it [01:01,  1.81it/s]Extractor Predicting: 106it [01:02,  1.83it/s]Extractor Predicting: 107it [01:02,  1.85it/s]Extractor Predicting: 108it [01:03,  1.84it/s]Extractor Predicting: 109it [01:03,  1.84it/s]Extractor Predicting: 110it [01:04,  1.80it/s]Extractor Predicting: 111it [01:05,  1.85it/s]Extractor Predicting: 112it [01:05,  1.86it/s]Extractor Predicting: 113it [01:06,  1.87it/s]Extractor Predicting: 114it [01:06,  1.85it/s]Extractor Predicting: 115it [01:07,  1.86it/s]Extractor Predicting: 116it [01:07,  1.83it/s]Extractor Predicting: 117it [01:08,  1.87it/s]Extractor Predicting: 118it [01:08,  1.81it/s]Extractor Predicting: 119it [01:09,  1.79it/s]Extractor Predicting: 120it [01:09,  1.80it/s]Extractor Predicting: 121it [01:10,  1.78it/s]Extractor Predicting: 122it [01:11,  1.77it/s]Extractor Predicting: 123it [01:11,  1.77it/s]Extractor Predicting: 124it [01:12,  1.74it/s]Extractor Predicting: 125it [01:12,  1.76it/s]Extractor Predicting: 126it [01:13,  1.77it/s]Extractor Predicting: 127it [01:13,  1.81it/s]Extractor Predicting: 128it [01:14,  1.76it/s]Extractor Predicting: 129it [01:15,  1.62it/s]Extractor Predicting: 130it [01:15,  1.59it/s]Extractor Predicting: 131it [01:16,  1.61it/s]Extractor Predicting: 132it [01:17,  1.61it/s]Extractor Predicting: 133it [01:17,  1.63it/s]Extractor Predicting: 134it [01:18,  1.67it/s]Extractor Predicting: 135it [01:18,  1.69it/s]Extractor Predicting: 136it [01:19,  1.71it/s]Extractor Predicting: 137it [01:19,  1.73it/s]Extractor Predicting: 138it [01:20,  1.71it/s]Extractor Predicting: 139it [01:21,  1.71it/s]Extractor Predicting: 140it [01:21,  1.72it/s]Extractor Predicting: 141it [01:22,  1.64it/s]Extractor Predicting: 142it [01:22,  1.66it/s]Extractor Predicting: 143it [01:23,  1.55it/s]Extractor Predicting: 144it [01:24,  1.59it/s]Extractor Predicting: 145it [01:24,  1.60it/s]Extractor Predicting: 146it [01:25,  1.57it/s]Extractor Predicting: 147it [01:26,  1.58it/s]Extractor Predicting: 148it [01:26,  1.62it/s]Extractor Predicting: 149it [01:27,  1.62it/s]Extractor Predicting: 150it [01:28,  1.63it/s]Extractor Predicting: 151it [01:28,  1.62it/s]Extractor Predicting: 152it [01:29,  1.63it/s]Extractor Predicting: 153it [01:29,  1.66it/s]Extractor Predicting: 154it [01:30,  1.67it/s]Extractor Predicting: 155it [01:31,  1.66it/s]Extractor Predicting: 156it [01:31,  1.65it/s]Extractor Predicting: 157it [01:32,  1.66it/s]Extractor Predicting: 158it [01:32,  1.69it/s]Extractor Predicting: 159it [01:33,  1.68it/s]Extractor Predicting: 160it [01:34,  1.63it/s]Extractor Predicting: 161it [01:34,  1.63it/s]Extractor Predicting: 162it [01:35,  1.62it/s]Extractor Predicting: 163it [01:35,  1.62it/s]Extractor Predicting: 164it [01:36,  1.64it/s]Extractor Predicting: 165it [01:37,  1.61it/s]Extractor Predicting: 166it [01:37,  1.63it/s]Extractor Predicting: 167it [01:38,  1.62it/s]Extractor Predicting: 168it [01:39,  1.38it/s]Extractor Predicting: 169it [01:40,  1.42it/s]Extractor Predicting: 170it [01:40,  1.49it/s]Extractor Predicting: 171it [01:41,  1.46it/s]Extractor Predicting: 172it [01:41,  1.50it/s]Extractor Predicting: 173it [01:42,  1.53it/s]Extractor Predicting: 174it [01:43,  1.55it/s]Extractor Predicting: 175it [01:43,  1.59it/s]Extractor Predicting: 176it [01:44,  1.62it/s]Extractor Predicting: 177it [01:44,  1.64it/s]Extractor Predicting: 178it [01:45,  1.67it/s]Extractor Predicting: 179it [01:46,  1.68it/s]Extractor Predicting: 180it [01:46,  1.69it/s]Extractor Predicting: 181it [01:47,  1.70it/s]Extractor Predicting: 182it [01:47,  1.75it/s]Extractor Predicting: 183it [01:48,  1.71it/s]Extractor Predicting: 184it [01:49,  1.74it/s]Extractor Predicting: 185it [01:49,  1.69it/s]Extractor Predicting: 186it [01:50,  1.70it/s]Extractor Predicting: 187it [01:50,  1.73it/s]Extractor Predicting: 188it [01:51,  1.73it/s]Extractor Predicting: 189it [01:51,  1.69it/s]Extractor Predicting: 190it [01:52,  1.66it/s]Extractor Predicting: 191it [01:53,  1.68it/s]Extractor Predicting: 192it [01:53,  1.72it/s]Extractor Predicting: 193it [01:54,  1.68it/s]Extractor Predicting: 194it [01:54,  1.68it/s]Extractor Predicting: 195it [01:55,  1.73it/s]Extractor Predicting: 196it [01:56,  1.74it/s]Extractor Predicting: 197it [01:56,  1.72it/s]Extractor Predicting: 198it [01:57,  1.75it/s]Extractor Predicting: 199it [01:57,  1.76it/s]Extractor Predicting: 200it [01:58,  1.72it/s]Extractor Predicting: 201it [01:58,  1.74it/s]Extractor Predicting: 202it [01:59,  1.71it/s]Extractor Predicting: 203it [02:00,  1.71it/s]Extractor Predicting: 204it [02:00,  1.68it/s]Extractor Predicting: 205it [02:01,  1.69it/s]Extractor Predicting: 206it [02:01,  1.68it/s]Extractor Predicting: 207it [02:02,  1.69it/s]Extractor Predicting: 208it [02:03,  1.71it/s]Extractor Predicting: 209it [02:03,  1.69it/s]Extractor Predicting: 210it [02:04,  1.70it/s]Extractor Predicting: 211it [02:04,  1.65it/s]Extractor Predicting: 212it [02:05,  1.67it/s]Extractor Predicting: 213it [02:06,  1.73it/s]Extractor Predicting: 214it [02:06,  1.68it/s]Extractor Predicting: 215it [02:07,  1.67it/s]Extractor Predicting: 216it [02:07,  1.66it/s]Extractor Predicting: 217it [02:08,  1.71it/s]Extractor Predicting: 218it [02:09,  1.70it/s]Extractor Predicting: 219it [02:09,  1.67it/s]Extractor Predicting: 220it [02:10,  1.69it/s]Extractor Predicting: 221it [02:10,  1.72it/s]Extractor Predicting: 222it [02:11,  1.73it/s]Extractor Predicting: 223it [02:11,  1.71it/s]Extractor Predicting: 224it [02:12,  1.74it/s]Extractor Predicting: 225it [02:13,  1.70it/s]Extractor Predicting: 226it [02:13,  1.65it/s]Extractor Predicting: 227it [02:14,  1.71it/s]Extractor Predicting: 228it [02:14,  1.70it/s]Extractor Predicting: 229it [02:15,  1.68it/s]Extractor Predicting: 230it [02:16,  1.71it/s]Extractor Predicting: 231it [02:16,  1.66it/s]Extractor Predicting: 232it [02:17,  1.68it/s]Extractor Predicting: 233it [02:17,  1.69it/s]Extractor Predicting: 234it [02:18,  1.69it/s]Extractor Predicting: 235it [02:19,  1.69it/s]Extractor Predicting: 236it [02:19,  1.68it/s]Extractor Predicting: 237it [02:20,  1.72it/s]Extractor Predicting: 238it [02:20,  1.71it/s]Extractor Predicting: 239it [02:21,  1.74it/s]Extractor Predicting: 240it [02:22,  1.69it/s]Extractor Predicting: 241it [02:22,  1.67it/s]Extractor Predicting: 242it [02:23,  1.72it/s]Extractor Predicting: 243it [02:23,  1.73it/s]Extractor Predicting: 244it [02:24,  1.72it/s]Extractor Predicting: 245it [02:24,  1.69it/s]Extractor Predicting: 246it [02:25,  1.68it/s]Extractor Predicting: 247it [02:26,  1.70it/s]Extractor Predicting: 248it [02:26,  1.70it/s]Extractor Predicting: 249it [02:27,  1.71it/s]Extractor Predicting: 250it [02:27,  1.69it/s]Extractor Predicting: 251it [02:28,  1.72it/s]Extractor Predicting: 252it [02:29,  1.70it/s]Extractor Predicting: 253it [02:29,  1.69it/s]Extractor Predicting: 254it [02:30,  1.66it/s]Extractor Predicting: 255it [02:30,  1.69it/s]Extractor Predicting: 256it [02:31,  1.68it/s]Extractor Predicting: 257it [02:32,  1.70it/s]Extractor Predicting: 258it [02:32,  1.69it/s]Extractor Predicting: 259it [02:33,  1.68it/s]Extractor Predicting: 260it [02:33,  1.67it/s]Extractor Predicting: 261it [02:34,  1.66it/s]Extractor Predicting: 262it [02:35,  1.63it/s]Extractor Predicting: 263it [02:35,  1.68it/s]Extractor Predicting: 264it [02:36,  1.64it/s]Extractor Predicting: 265it [02:36,  1.63it/s]Extractor Predicting: 266it [02:37,  1.61it/s]Extractor Predicting: 267it [02:38,  1.61it/s]Extractor Predicting: 268it [02:38,  1.59it/s]Extractor Predicting: 269it [02:39,  1.61it/s]Extractor Predicting: 270it [02:40,  1.62it/s]Extractor Predicting: 271it [02:40,  1.70it/s]Extractor Predicting: 272it [02:41,  1.64it/s]Extractor Predicting: 273it [02:41,  1.64it/s]Extractor Predicting: 274it [02:42,  1.65it/s]Extractor Predicting: 275it [02:43,  1.65it/s]Extractor Predicting: 276it [02:43,  1.64it/s]Extractor Predicting: 277it [02:44,  1.64it/s]Extractor Predicting: 278it [02:44,  1.63it/s]Extractor Predicting: 279it [02:45,  1.63it/s]Extractor Predicting: 280it [02:46,  1.61it/s]Extractor Predicting: 281it [02:46,  1.63it/s]Extractor Predicting: 282it [02:47,  1.64it/s]Extractor Predicting: 283it [02:47,  1.65it/s]Extractor Predicting: 284it [02:48,  1.66it/s]Extractor Predicting: 285it [02:49,  1.65it/s]Extractor Predicting: 286it [02:49,  1.64it/s]Extractor Predicting: 287it [02:50,  1.62it/s]Extractor Predicting: 288it [02:51,  1.60it/s]Extractor Predicting: 289it [02:51,  1.61it/s]Extractor Predicting: 290it [02:52,  1.60it/s]Extractor Predicting: 291it [02:52,  1.62it/s]Extractor Predicting: 292it [02:53,  1.59it/s]Extractor Predicting: 293it [02:54,  1.61it/s]Extractor Predicting: 294it [02:54,  1.60it/s]Extractor Predicting: 295it [02:55,  1.60it/s]Extractor Predicting: 296it [02:56,  1.60it/s]Extractor Predicting: 297it [02:56,  1.59it/s]Extractor Predicting: 298it [02:57,  1.57it/s]Extractor Predicting: 299it [02:58,  1.53it/s]Extractor Predicting: 300it [02:58,  1.51it/s]Extractor Predicting: 301it [02:59,  1.51it/s]Extractor Predicting: 302it [03:00,  1.50it/s]Extractor Predicting: 303it [03:01,  1.30it/s]Extractor Predicting: 304it [03:01,  1.41it/s]Extractor Predicting: 305it [03:02,  1.47it/s]Extractor Predicting: 306it [03:02,  1.52it/s]Extractor Predicting: 307it [03:03,  1.53it/s]Extractor Predicting: 308it [03:04,  1.56it/s]Extractor Predicting: 309it [03:04,  1.56it/s]Extractor Predicting: 310it [03:05,  1.54it/s]Extractor Predicting: 311it [03:06,  1.54it/s]Extractor Predicting: 312it [03:06,  1.55it/s]Extractor Predicting: 313it [03:07,  1.54it/s]Extractor Predicting: 314it [03:08,  1.51it/s]Extractor Predicting: 315it [03:08,  1.54it/s]Extractor Predicting: 316it [03:09,  1.53it/s]Extractor Predicting: 317it [03:09,  1.54it/s]Extractor Predicting: 318it [03:10,  1.54it/s]Extractor Predicting: 319it [03:11,  1.54it/s]Extractor Predicting: 320it [03:11,  1.55it/s]Extractor Predicting: 321it [03:12,  1.52it/s]Extractor Predicting: 322it [03:13,  1.46it/s]Extractor Predicting: 323it [03:13,  1.49it/s]Extractor Predicting: 324it [03:14,  1.48it/s]Extractor Predicting: 325it [03:15,  1.49it/s]Extractor Predicting: 326it [03:15,  1.48it/s]Extractor Predicting: 327it [03:16,  1.48it/s]Extractor Predicting: 328it [03:17,  1.52it/s]Extractor Predicting: 329it [03:17,  1.52it/s]Extractor Predicting: 330it [03:18,  1.55it/s]Extractor Predicting: 331it [03:19,  1.57it/s]Extractor Predicting: 332it [03:19,  1.52it/s]Extractor Predicting: 333it [03:20,  1.53it/s]Extractor Predicting: 334it [03:21,  1.54it/s]Extractor Predicting: 335it [03:21,  1.56it/s]Extractor Predicting: 336it [03:22,  1.50it/s]Extractor Predicting: 337it [03:23,  1.50it/s]Extractor Predicting: 338it [03:23,  1.53it/s]Extractor Predicting: 339it [03:24,  1.51it/s]Extractor Predicting: 340it [03:25,  1.50it/s]Extractor Predicting: 341it [03:25,  1.49it/s]Extractor Predicting: 342it [03:26,  1.49it/s]Extractor Predicting: 343it [03:27,  1.45it/s]Extractor Predicting: 344it [03:27,  1.46it/s]Extractor Predicting: 345it [03:28,  1.50it/s]Extractor Predicting: 346it [03:29,  1.53it/s]Extractor Predicting: 347it [03:29,  1.56it/s]Extractor Predicting: 348it [03:30,  1.60it/s]Extractor Predicting: 349it [03:31,  1.57it/s]Extractor Predicting: 350it [03:31,  1.57it/s]Extractor Predicting: 351it [03:32,  1.57it/s]Extractor Predicting: 352it [03:32,  1.59it/s]Extractor Predicting: 353it [03:33,  1.64it/s]Extractor Predicting: 354it [03:34,  1.67it/s]Extractor Predicting: 355it [03:34,  1.65it/s]Extractor Predicting: 356it [03:35,  1.67it/s]Extractor Predicting: 357it [03:35,  1.69it/s]Extractor Predicting: 358it [03:36,  1.70it/s]Extractor Predicting: 359it [03:37,  1.69it/s]Extractor Predicting: 360it [03:37,  1.72it/s]Extractor Predicting: 361it [03:38,  1.68it/s]Extractor Predicting: 362it [03:38,  1.67it/s]Extractor Predicting: 363it [03:39,  1.69it/s]Extractor Predicting: 364it [03:39,  1.73it/s]Extractor Predicting: 365it [03:40,  1.71it/s]Extractor Predicting: 366it [03:41,  1.72it/s]Extractor Predicting: 367it [03:41,  1.74it/s]Extractor Predicting: 368it [03:42,  1.71it/s]Extractor Predicting: 369it [03:42,  1.67it/s]Extractor Predicting: 370it [03:43,  1.65it/s]Extractor Predicting: 371it [03:44,  1.60it/s]Extractor Predicting: 372it [03:44,  1.65it/s]Extractor Predicting: 373it [03:45,  1.66it/s]Extractor Predicting: 374it [03:45,  1.64it/s]Extractor Predicting: 375it [03:46,  1.63it/s]Extractor Predicting: 376it [03:47,  1.62it/s]Extractor Predicting: 377it [03:47,  1.62it/s]Extractor Predicting: 378it [03:48,  1.68it/s]Extractor Predicting: 379it [03:48,  1.75it/s]Extractor Predicting: 380it [03:49,  1.79it/s]Extractor Predicting: 381it [03:50,  1.73it/s]Extractor Predicting: 382it [03:50,  1.70it/s]Extractor Predicting: 383it [03:51,  1.69it/s]Extractor Predicting: 384it [03:51,  1.74it/s]Extractor Predicting: 385it [03:52,  1.79it/s]Extractor Predicting: 386it [03:52,  1.76it/s]Extractor Predicting: 387it [03:53,  1.81it/s]Extractor Predicting: 388it [03:53,  1.79it/s]Extractor Predicting: 389it [03:54,  1.74it/s]Extractor Predicting: 390it [03:55,  1.74it/s]Extractor Predicting: 391it [03:55,  1.76it/s]Extractor Predicting: 392it [03:56,  1.71it/s]Extractor Predicting: 393it [03:56,  1.74it/s]Extractor Predicting: 394it [03:57,  1.76it/s]Extractor Predicting: 395it [03:58,  1.72it/s]Extractor Predicting: 396it [03:58,  1.70it/s]Extractor Predicting: 397it [03:59,  1.72it/s]Extractor Predicting: 398it [03:59,  1.72it/s]Extractor Predicting: 399it [04:00,  1.72it/s]Extractor Predicting: 400it [04:00,  1.73it/s]Extractor Predicting: 401it [04:01,  1.76it/s]Extractor Predicting: 402it [04:02,  1.74it/s]Extractor Predicting: 403it [04:02,  1.75it/s]Extractor Predicting: 404it [04:03,  1.70it/s]Extractor Predicting: 405it [04:03,  1.70it/s]Extractor Predicting: 406it [04:04,  1.65it/s]Extractor Predicting: 407it [04:05,  1.59it/s]Extractor Predicting: 408it [04:05,  1.63it/s]Extractor Predicting: 409it [04:06,  1.67it/s]Extractor Predicting: 410it [04:06,  1.66it/s]Extractor Predicting: 411it [04:07,  1.65it/s]Extractor Predicting: 412it [04:08,  1.63it/s]Extractor Predicting: 413it [04:08,  1.67it/s]Extractor Predicting: 414it [04:09,  1.69it/s]Extractor Predicting: 415it [04:09,  1.71it/s]Extractor Predicting: 416it [04:10,  1.66it/s]Extractor Predicting: 417it [04:11,  1.64it/s]Extractor Predicting: 418it [04:11,  1.66it/s]Extractor Predicting: 419it [04:12,  1.70it/s]Extractor Predicting: 420it [04:12,  1.75it/s]Extractor Predicting: 421it [04:13,  1.76it/s]Extractor Predicting: 422it [04:13,  1.82it/s]Extractor Predicting: 423it [04:14,  1.84it/s]Extractor Predicting: 424it [04:15,  1.80it/s]Extractor Predicting: 425it [04:15,  1.81it/s]Extractor Predicting: 426it [04:16,  1.78it/s]Extractor Predicting: 427it [04:16,  1.71it/s]Extractor Predicting: 428it [04:17,  1.65it/s]Extractor Predicting: 429it [04:18,  1.65it/s]Extractor Predicting: 430it [04:18,  1.67it/s]Extractor Predicting: 431it [04:19,  1.47it/s]Extractor Predicting: 432it [04:20,  1.55it/s]Extractor Predicting: 433it [04:20,  1.58it/s]Extractor Predicting: 434it [04:21,  1.58it/s]Extractor Predicting: 435it [04:21,  1.61it/s]Extractor Predicting: 436it [04:22,  1.63it/s]Extractor Predicting: 437it [04:23,  1.61it/s]Extractor Predicting: 438it [04:23,  1.60it/s]Extractor Predicting: 439it [04:24,  1.60it/s]Extractor Predicting: 440it [04:25,  1.58it/s]Extractor Predicting: 441it [04:25,  1.62it/s]Extractor Predicting: 442it [04:26,  1.62it/s]Extractor Predicting: 443it [04:26,  1.66it/s]Extractor Predicting: 444it [04:27,  1.63it/s]Extractor Predicting: 445it [04:28,  1.62it/s]Extractor Predicting: 446it [04:28,  1.60it/s]Extractor Predicting: 447it [04:29,  1.60it/s]Extractor Predicting: 448it [04:29,  1.62it/s]Extractor Predicting: 449it [04:30,  1.61it/s]Extractor Predicting: 450it [04:31,  1.66it/s]Extractor Predicting: 451it [04:31,  1.62it/s]Extractor Predicting: 452it [04:32,  1.62it/s]Extractor Predicting: 453it [04:33,  1.62it/s]Extractor Predicting: 454it [04:33,  1.61it/s]Extractor Predicting: 455it [04:34,  1.62it/s]Extractor Predicting: 456it [04:34,  1.62it/s]Extractor Predicting: 457it [04:35,  1.60it/s]Extractor Predicting: 458it [04:36,  1.60it/s]Extractor Predicting: 459it [04:36,  1.63it/s]Extractor Predicting: 460it [04:37,  1.61it/s]Extractor Predicting: 461it [04:38,  1.62it/s]Extractor Predicting: 462it [04:38,  1.61it/s]Extractor Predicting: 463it [04:39,  1.62it/s]Extractor Predicting: 464it [04:39,  1.64it/s]Extractor Predicting: 465it [04:40,  1.65it/s]Extractor Predicting: 466it [04:41,  1.64it/s]Extractor Predicting: 467it [04:41,  1.64it/s]Extractor Predicting: 468it [04:42,  1.66it/s]Extractor Predicting: 469it [04:42,  1.63it/s]Extractor Predicting: 470it [04:43,  1.62it/s]Extractor Predicting: 471it [04:44,  1.62it/s]Extractor Predicting: 472it [04:44,  1.66it/s]Extractor Predicting: 473it [04:45,  1.64it/s]Extractor Predicting: 474it [04:45,  1.62it/s]Extractor Predicting: 475it [04:46,  1.65it/s]Extractor Predicting: 476it [04:47,  1.67it/s]Extractor Predicting: 477it [04:47,  1.71it/s]Extractor Predicting: 478it [04:48,  1.73it/s]Extractor Predicting: 479it [04:48,  1.73it/s]Extractor Predicting: 480it [04:49,  1.66it/s]Extractor Predicting: 481it [04:50,  1.61it/s]Extractor Predicting: 482it [04:50,  1.58it/s]Extractor Predicting: 483it [04:51,  1.53it/s]Extractor Predicting: 484it [04:52,  1.54it/s]Extractor Predicting: 485it [04:52,  1.54it/s]Extractor Predicting: 486it [04:53,  1.58it/s]Extractor Predicting: 487it [04:53,  1.64it/s]Extractor Predicting: 488it [04:54,  1.68it/s]Extractor Predicting: 489it [04:55,  1.63it/s]Extractor Predicting: 490it [04:55,  1.63it/s]Extractor Predicting: 491it [04:56,  1.63it/s]Extractor Predicting: 492it [04:56,  1.64it/s]Extractor Predicting: 493it [04:57,  1.64it/s]Extractor Predicting: 494it [04:58,  1.63it/s]Extractor Predicting: 495it [04:58,  1.65it/s]Extractor Predicting: 496it [04:59,  1.69it/s]Extractor Predicting: 497it [04:59,  1.70it/s]Extractor Predicting: 498it [05:00,  1.66it/s]Extractor Predicting: 499it [05:01,  1.64it/s]Extractor Predicting: 500it [05:01,  1.66it/s]Extractor Predicting: 501it [05:02,  1.65it/s]Extractor Predicting: 502it [05:02,  1.70it/s]Extractor Predicting: 503it [05:03,  1.73it/s]Extractor Predicting: 504it [05:04,  1.74it/s]Extractor Predicting: 505it [05:04,  1.74it/s]Extractor Predicting: 506it [05:05,  1.69it/s]Extractor Predicting: 507it [05:05,  1.61it/s]Extractor Predicting: 508it [05:06,  1.60it/s]Extractor Predicting: 509it [05:07,  1.60it/s]Extractor Predicting: 510it [05:07,  1.66it/s]Extractor Predicting: 511it [05:08,  1.67it/s]Extractor Predicting: 512it [05:08,  1.68it/s]Extractor Predicting: 513it [05:09,  1.64it/s]Extractor Predicting: 514it [05:10,  1.61it/s]Extractor Predicting: 515it [05:10,  1.63it/s]Extractor Predicting: 516it [05:11,  1.62it/s]Extractor Predicting: 517it [05:12,  1.58it/s]Extractor Predicting: 518it [05:12,  1.60it/s]Extractor Predicting: 519it [05:13,  1.60it/s]Extractor Predicting: 520it [05:13,  1.62it/s]Extractor Predicting: 521it [05:14,  1.64it/s]Extractor Predicting: 522it [05:15,  1.63it/s]Extractor Predicting: 523it [05:15,  1.66it/s]Extractor Predicting: 524it [05:16,  1.65it/s]Extractor Predicting: 525it [05:17,  1.62it/s]Extractor Predicting: 526it [05:17,  1.60it/s]Extractor Predicting: 527it [05:18,  1.57it/s]Extractor Predicting: 528it [05:18,  1.58it/s]Extractor Predicting: 529it [05:19,  1.57it/s]Extractor Predicting: 530it [05:20,  1.55it/s]Extractor Predicting: 531it [05:20,  1.56it/s]Extractor Predicting: 532it [05:21,  1.55it/s]Extractor Predicting: 533it [05:22,  1.57it/s]Extractor Predicting: 534it [05:22,  1.58it/s]Extractor Predicting: 535it [05:23,  1.57it/s]Extractor Predicting: 536it [05:24,  1.56it/s]Extractor Predicting: 537it [05:24,  1.55it/s]Extractor Predicting: 538it [05:25,  1.55it/s]Extractor Predicting: 539it [05:26,  1.34it/s]Extractor Predicting: 540it [05:26,  1.41it/s]Extractor Predicting: 541it [05:27,  1.42it/s]Extractor Predicting: 542it [05:28,  1.46it/s]Extractor Predicting: 543it [05:28,  1.49it/s]Extractor Predicting: 544it [05:29,  1.46it/s]Extractor Predicting: 545it [05:30,  1.48it/s]Extractor Predicting: 546it [05:31,  1.49it/s]Extractor Predicting: 547it [05:31,  1.49it/s]Extractor Predicting: 548it [05:32,  1.51it/s]Extractor Predicting: 549it [05:32,  1.51it/s]Extractor Predicting: 550it [05:33,  1.55it/s]Extractor Predicting: 551it [05:34,  1.57it/s]Extractor Predicting: 552it [05:34,  1.58it/s]Extractor Predicting: 553it [05:35,  1.61it/s]Extractor Predicting: 554it [05:36,  1.62it/s]Extractor Predicting: 555it [05:36,  1.60it/s]Extractor Predicting: 556it [05:37,  1.61it/s]Extractor Predicting: 556it [05:37,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:26:21,714 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:26:21,720 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:26:21,720 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:26:21,720 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:26:21,720 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:26:22,328 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:26:22,329 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:26:22,889 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:26:23,974 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:26:23,974 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:26:26,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:26:26,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:26:26,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:26:26,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:26:26,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:26:27,632 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:26:27,632 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:26:28,207 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:26:28,385 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:26:28,385 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.28286014721345953,
  "recall": 0.04032681208305224,
  "score": 0.07058977891491176,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'father', 'follows', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'part of the series', 'place of birth', 'production company', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 8650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.76it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.70it/s]Extractor Predicting: 10it [00:06,  1.68it/s]Extractor Predicting: 11it [00:06,  1.72it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.70it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:10,  1.62it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:12,  1.52it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.40it/s]Extractor Predicting: 24it [00:15,  1.45it/s]Extractor Predicting: 25it [00:15,  1.49it/s]Extractor Predicting: 26it [00:16,  1.47it/s]Extractor Predicting: 27it [00:16,  1.49it/s]Extractor Predicting: 28it [00:17,  1.47it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:18,  1.54it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.47it/s]Extractor Predicting: 33it [00:20,  1.53it/s]Extractor Predicting: 34it [00:21,  1.63it/s]Extractor Predicting: 35it [00:22,  1.63it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:23,  1.62it/s]Extractor Predicting: 38it [00:23,  1.64it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:25,  1.61it/s]Extractor Predicting: 41it [00:25,  1.58it/s]Extractor Predicting: 42it [00:26,  1.55it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:28,  1.57it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:29,  1.58it/s]Extractor Predicting: 48it [00:30,  1.61it/s]Extractor Predicting: 49it [00:30,  1.60it/s]Extractor Predicting: 50it [00:31,  1.63it/s]Extractor Predicting: 51it [00:32,  1.62it/s]Extractor Predicting: 52it [00:32,  1.60it/s]Extractor Predicting: 53it [00:33,  1.67it/s]Extractor Predicting: 54it [00:33,  1.75it/s]Extractor Predicting: 55it [00:34,  1.84it/s]Extractor Predicting: 56it [00:34,  1.92it/s]Extractor Predicting: 57it [00:35,  1.98it/s]Extractor Predicting: 58it [00:35,  1.98it/s]Extractor Predicting: 59it [00:36,  2.00it/s]Extractor Predicting: 60it [00:36,  1.98it/s]Extractor Predicting: 61it [00:37,  1.97it/s]Extractor Predicting: 62it [00:37,  1.96it/s]Extractor Predicting: 63it [00:38,  1.96it/s]Extractor Predicting: 64it [00:38,  1.97it/s]Extractor Predicting: 65it [00:39,  2.00it/s]Extractor Predicting: 66it [00:39,  1.96it/s]Extractor Predicting: 67it [00:40,  1.96it/s]Extractor Predicting: 68it [00:40,  1.99it/s]Extractor Predicting: 69it [00:41,  2.03it/s]Extractor Predicting: 70it [00:41,  2.04it/s]Extractor Predicting: 71it [00:42,  2.01it/s]Extractor Predicting: 72it [00:42,  2.00it/s]Extractor Predicting: 73it [00:43,  2.07it/s]Extractor Predicting: 74it [00:43,  1.99it/s]Extractor Predicting: 75it [00:44,  2.01it/s]Extractor Predicting: 76it [00:44,  1.97it/s]Extractor Predicting: 77it [00:45,  1.96it/s]Extractor Predicting: 78it [00:45,  1.99it/s]Extractor Predicting: 79it [00:46,  2.00it/s]Extractor Predicting: 80it [00:46,  2.00it/s]Extractor Predicting: 81it [00:47,  2.00it/s]Extractor Predicting: 82it [00:47,  1.94it/s]Extractor Predicting: 83it [00:48,  1.82it/s]Extractor Predicting: 84it [00:49,  1.71it/s]Extractor Predicting: 85it [00:49,  1.63it/s]Extractor Predicting: 86it [00:50,  1.58it/s]Extractor Predicting: 87it [00:51,  1.56it/s]Extractor Predicting: 88it [00:51,  1.56it/s]Extractor Predicting: 89it [00:52,  1.54it/s]Extractor Predicting: 90it [00:53,  1.53it/s]Extractor Predicting: 91it [00:53,  1.53it/s]Extractor Predicting: 92it [00:54,  1.52it/s]Extractor Predicting: 93it [00:55,  1.52it/s]Extractor Predicting: 94it [00:55,  1.52it/s]Extractor Predicting: 95it [00:56,  1.53it/s]Extractor Predicting: 96it [00:57,  1.51it/s]Extractor Predicting: 97it [00:57,  1.51it/s]Extractor Predicting: 98it [00:58,  1.54it/s]Extractor Predicting: 99it [00:58,  1.54it/s]Extractor Predicting: 100it [00:59,  1.55it/s]Extractor Predicting: 101it [01:00,  1.57it/s]Extractor Predicting: 102it [01:00,  1.57it/s]Extractor Predicting: 103it [01:01,  1.54it/s]Extractor Predicting: 104it [01:01,  1.93it/s]Extractor Predicting: 104it [01:01,  1.68it/s]
[INFO|configuration_utils.py:515] 2023-08-29 09:27:31,626 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:27:31,627 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 09:27:31,631 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:27:31,631 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 09:27:31,635 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 09:27:34,731 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 09:27:34,735 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 09:27:34,746 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:27:34,747 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 09:27:34,751 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:27:34,756 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:27:34,756 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:27:34,756 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:27:34,756 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:27:34,756 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:27:34,756 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7447916666666666,
  "recall": 0.10559350193834226,
  "score": 0.18496362166531932,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 09:27:35,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:35,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:36,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:36,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:37,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:37,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:38,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:38,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:39,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:40,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:40,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:41,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:41,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:42,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:42,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:43,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:44,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:44,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:45,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:45,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:46,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:46,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:12<03:56, 12.47s/it][WARNING|generation_utils.py:914] 2023-08-29 09:27:47,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:48,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:48,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:49,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:49,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:50,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:50,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:51,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:52,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:52,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:53,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:53,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:54,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:54,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:55,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:56,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:56,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:57,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:57,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:58,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:27:59,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:24<03:40, 12.23s/it][WARNING|generation_utils.py:914] 2023-08-29 09:27:59,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:00,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:00,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:01,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:01,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:02,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:02,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:03,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:03,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:04,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:04,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:05,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:06,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:06,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:07,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:07,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:08,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:08,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:09,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:09,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:10,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:11,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:36<03:25, 12.11s/it][WARNING|generation_utils.py:914] 2023-08-29 09:28:11,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:12,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:12,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:13,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:13,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:14,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:14,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:15,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:16,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:16,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:17,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:17,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:18,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:18,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:19,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:19,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:20,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:21,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:21,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:22,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:22,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:23,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:48<03:15, 12.22s/it][WARNING|generation_utils.py:914] 2023-08-29 09:28:23,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:24,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:25,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:25,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:26,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:26,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:27,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:27,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:28,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:28,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:29,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:30,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:30,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:31,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:31,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:32,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:32,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:33,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:34,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:34,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:35,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:35,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:01<03:03, 12.26s/it][WARNING|generation_utils.py:914] 2023-08-29 09:28:36,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:36,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:37,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:38,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:38,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:39,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:39,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:40,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:40,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:41,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:41,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:42,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:43,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:43,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:44,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:44,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:45,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:45,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:46,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:47,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:47,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:48,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:13<02:52, 12.33s/it][WARNING|generation_utils.py:914] 2023-08-29 09:28:48,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:49,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:50,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:50,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:51,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:51,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:52,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:53,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:53,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:54,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:54,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:55,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:56,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:56,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:57,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:57,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:58,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:59,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:28:59,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:00,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:01,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:01,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:27<02:45, 12.75s/it][WARNING|generation_utils.py:914] 2023-08-29 09:29:02,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:02,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:03,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:04,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:04,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:05,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:06,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:06,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:07,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:08,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:08,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:09,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:09,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:10,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:11,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:11,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:12,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:12,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:13,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:14,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:14,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:15,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:15,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:41<02:38, 13.20s/it][WARNING|generation_utils.py:914] 2023-08-29 09:29:16,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:17,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:17,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:18,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:18,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:19,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:19,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:20,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:20,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:21,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:21,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:22,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:23,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:23,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:24,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:24,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:25,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:25,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:26,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:27,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:27,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:53<02:19, 12.70s/it][WARNING|generation_utils.py:914] 2023-08-29 09:29:28,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:28,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:29,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:29,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:30,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:30,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:31,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:31,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:32,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:32,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:33,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:34,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:34,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:35,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:35,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:36,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:36,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:37,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:37,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:38,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:39,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:39,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:40,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:40,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:06<02:08, 12.87s/it][WARNING|generation_utils.py:914] 2023-08-29 09:29:41,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:41,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:42,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:43,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:44,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:44,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:45,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:45,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:46,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:47,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:47,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:48,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:48,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:49,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:49,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:50,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:51,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:51,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:52,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:52,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:53,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:53,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:54,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:19<01:58, 13.12s/it][WARNING|generation_utils.py:914] 2023-08-29 09:29:55,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:55,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:56,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:57,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:57,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:58,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:58,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:29:59,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:00,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:01,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:01,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:02,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:02,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:03,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:04,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:04,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:05,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:05,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:06,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:07,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:07,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:33<01:45, 13.20s/it][WARNING|generation_utils.py:914] 2023-08-29 09:30:08,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:08,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:09,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:09,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:10,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:10,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:11,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:11,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:12,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:13,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:13,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:14,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:14,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:15,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:15,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:16,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:16,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:17,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:17,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:18,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:18,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:44<01:27, 12.46s/it][WARNING|generation_utils.py:914] 2023-08-29 09:30:19,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:19,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:20,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:20,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:21,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:22,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:22,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:23,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:23,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:24,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:25,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:25,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:26,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:26,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:27,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:27,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:28,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:28,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:29,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:29,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:30,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [02:55<01:13, 12.24s/it][WARNING|generation_utils.py:914] 2023-08-29 09:30:30,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:31,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:32,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:32,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:33,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:33,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:34,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:34,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:35,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:35,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:36,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:36,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:37,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:37,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:38,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:38,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:39,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:40,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:40,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:41,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:06<00:59, 11.82s/it][WARNING|generation_utils.py:914] 2023-08-29 09:30:41,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:42,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:42,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:43,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:43,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:44,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:44,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:45,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:45,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:46,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:47,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:47,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:48,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:48,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:49,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:49,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:50,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:51,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:51,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:52,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:52,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:53,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:18<00:47, 11.92s/it][WARNING|generation_utils.py:914] 2023-08-29 09:30:53,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:54,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:54,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:55,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:56,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:56,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:57,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:57,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:58,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:58,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:59,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:30:59,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:00,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:00,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:01,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:01,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:02,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:02,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:03,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:03,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:04,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:04,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:05,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:05,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:31<00:36, 12.09s/it][WARNING|generation_utils.py:914] 2023-08-29 09:31:06,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:06,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:07,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:08,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:08,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:09,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:09,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:10,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:11,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:11,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:12,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:12,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:13,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:14,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:14,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:15,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:15,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:16,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:16,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:17,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:18,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:18,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:19,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:44<00:25, 12.51s/it][WARNING|generation_utils.py:914] 2023-08-29 09:31:19,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:20,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:20,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:21,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:22,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:22,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:23,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:23,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:24,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:24,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:25,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:25,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:26,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:26,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:27,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:27,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:28,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:29,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:29,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:30,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:30,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:31,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [03:56<00:12, 12.37s/it][WARNING|generation_utils.py:914] 2023-08-29 09:31:31,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:32,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:32,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:33,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:33,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:34,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:35,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:35,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:36,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:36,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:37,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:37,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:38,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:38,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:39,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:39,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:40,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:40,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:41,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:41,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:42,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:31:42,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:08<00:00, 12.10s/it]Generating: 100%|██████████| 20/20 [04:08<00:00, 12.42s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:48,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:48,803 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:48,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:48,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:48,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:31:49,098 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:31:49,099 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:31:49,353 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:31:50,439 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:31:50,440 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:52,106 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:52,112 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:52,112 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:52,112 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:52,112 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:31:52,431 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:31:52,432 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:31:52,687 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:31:52,864 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:31:52,864 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : inception .', 'success_rate': 0.8934659090909091, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9241071428571429, 'errors': {'', "('Red Sea', 'located on terrain feature', '', 'On the northern slopes of the peninsula , the Red Sea lies on bedrock , with the Atlantic Ocean and Greenland around the border .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : military branch .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8877840909090909, 'errors': {'', "('Chief Clerk', 'occupation', '', 'He was first elected to the Oregon House of Representatives by a Republican margin in the 1910 election after serving as Chief Clerk of the Oregon House of Representatives .')"}}
['Relation : applies to jurisdiction . Context : Following his initial dismissal , the court of appeals has ruled that his trial attorneys should not have tried to obtain a decision on whether a motion to suppress has been granted . Head Entity : dismissed , Tail Entity : appellate .\n']
['Relation : applies to jurisdiction . Context : Following his initial dismissal , the court of appeals has ruled that his trial attorneys should not have tried to obtain a decision on whether a motion to suppress has been granted . Head Entity : dismissed , Tail Entity : appellate .\n', 'Relation : applies to jurisdiction . Context : After he was deposed in 1986 , his nephew , Henry James James Johnson , ran the South Carolina House of Representatives as a Republican . Head Entity : Henry James Johnson , Tail Entity : South Carolina .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : applies to jurisdiction .', 'success_rate': 0.8551136363636364, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : family name .', 'success_rate': 0.8849431818181818, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : father .', 'success_rate': 0.8301630434782609, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : is a list of . Context : The list of languages most commonly spoken at the time of writing is of French ( 9,769 or 94 . 5 % ) , German ( 1,280 or 88 . 5 % ) , and Italian ( 2,098 or 86 . 2 % ) . Head Entity : 8 % , Tail Entity : French .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8410326086956522, 'errors': {'', "('birth date', 'is a list of', '', 'In 2012 , the list includes names , birth dates , and place of birth .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8792613636363636, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 444, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 601, 'raw': 768}
{'prompt': 'Relation : part of the series .', 'success_rate': 0.7825520833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('DC Comics', 'part of the series', '', 'It was produced by David W. Curtis of DC Comics and produced by Mark Millar .')"}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.8288043478260869, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : production company .', 'success_rate': 0.8778409090909091, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : use .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 15090
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15190, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.40it/s]Extractor Estimating: 2it [00:01,  1.52it/s]Extractor Estimating: 3it [00:01,  1.60it/s]Extractor Estimating: 4it [00:02,  1.60it/s]Extractor Estimating: 5it [00:03,  1.61it/s]Extractor Estimating: 6it [00:03,  1.68it/s]Extractor Estimating: 7it [00:04,  1.68it/s]Extractor Estimating: 8it [00:04,  1.65it/s]Extractor Estimating: 9it [00:05,  1.67it/s]Extractor Estimating: 10it [00:06,  1.67it/s]Extractor Estimating: 11it [00:06,  1.61it/s]Extractor Estimating: 12it [00:07,  1.65it/s]Extractor Estimating: 13it [00:07,  1.64it/s]Extractor Estimating: 14it [00:08,  1.68it/s]Extractor Estimating: 15it [00:09,  1.68it/s]Extractor Estimating: 16it [00:09,  1.71it/s]Extractor Estimating: 17it [00:10,  1.71it/s]Extractor Estimating: 18it [00:10,  1.67it/s]Extractor Estimating: 19it [00:11,  1.66it/s]Extractor Estimating: 20it [00:12,  1.68it/s]Extractor Estimating: 21it [00:12,  1.67it/s]Extractor Estimating: 22it [00:13,  1.67it/s]Extractor Estimating: 23it [00:13,  1.66it/s]Extractor Estimating: 24it [00:14,  1.66it/s]Extractor Estimating: 25it [00:15,  1.69it/s]Extractor Estimating: 26it [00:15,  1.73it/s]Extractor Estimating: 27it [00:16,  1.74it/s]Extractor Estimating: 28it [00:16,  1.63it/s]Extractor Estimating: 29it [00:17,  1.67it/s]Extractor Estimating: 30it [00:18,  1.69it/s]Extractor Estimating: 31it [00:18,  1.77it/s]Extractor Estimating: 32it [00:19,  1.75it/s]Extractor Estimating: 33it [00:19,  1.69it/s]Extractor Estimating: 34it [00:20,  1.72it/s]Extractor Estimating: 35it [00:20,  1.75it/s]Extractor Estimating: 36it [00:21,  1.77it/s]Extractor Estimating: 37it [00:21,  1.79it/s]Extractor Estimating: 38it [00:22,  1.73it/s]Extractor Estimating: 39it [00:23,  1.71it/s]Extractor Estimating: 40it [00:23,  1.73it/s]Extractor Estimating: 41it [00:24,  1.77it/s]Extractor Estimating: 42it [00:24,  1.71it/s]Extractor Estimating: 43it [00:25,  1.69it/s]Extractor Estimating: 44it [00:26,  1.71it/s]Extractor Estimating: 45it [00:26,  1.76it/s]Extractor Estimating: 46it [00:27,  1.73it/s]Extractor Estimating: 47it [00:27,  1.72it/s]Extractor Estimating: 48it [00:28,  1.70it/s]Extractor Estimating: 49it [00:29,  1.68it/s]Extractor Estimating: 50it [00:29,  1.71it/s]Extractor Estimating: 51it [00:30,  1.69it/s]Extractor Estimating: 52it [00:30,  1.68it/s]Extractor Estimating: 53it [00:31,  1.69it/s]Extractor Estimating: 54it [00:31,  1.67it/s]Extractor Estimating: 55it [00:32,  1.68it/s]Extractor Estimating: 56it [00:33,  1.66it/s]Extractor Estimating: 57it [00:33,  1.71it/s]Extractor Estimating: 58it [00:34,  1.76it/s]Extractor Estimating: 59it [00:34,  1.79it/s]Extractor Estimating: 60it [00:35,  1.80it/s]Extractor Estimating: 61it [00:35,  1.83it/s]Extractor Estimating: 62it [00:36,  1.83it/s]Extractor Estimating: 63it [00:36,  1.83it/s]Extractor Estimating: 64it [00:37,  1.79it/s]Extractor Estimating: 65it [00:38,  1.76it/s]Extractor Estimating: 66it [00:38,  1.75it/s]Extractor Estimating: 67it [00:39,  1.80it/s]Extractor Estimating: 68it [00:39,  1.75it/s]Extractor Estimating: 69it [00:40,  1.76it/s]Extractor Estimating: 70it [00:41,  1.70it/s]Extractor Estimating: 71it [00:41,  1.66it/s]Extractor Estimating: 72it [00:42,  1.67it/s]Extractor Estimating: 73it [00:42,  1.69it/s]Extractor Estimating: 74it [00:43,  1.71it/s]Extractor Estimating: 75it [00:43,  1.74it/s]Extractor Estimating: 76it [00:44,  1.74it/s]Extractor Estimating: 77it [00:45,  1.73it/s]Extractor Estimating: 78it [00:45,  1.74it/s]Extractor Estimating: 79it [00:46,  1.72it/s]Extractor Estimating: 80it [00:46,  1.73it/s]Extractor Estimating: 81it [00:47,  1.75it/s]Extractor Estimating: 82it [00:48,  1.72it/s]Extractor Estimating: 83it [00:48,  1.73it/s]Extractor Estimating: 84it [00:49,  1.68it/s]Extractor Estimating: 85it [00:49,  1.64it/s]Extractor Estimating: 86it [00:50,  1.63it/s]Extractor Estimating: 87it [00:51,  1.62it/s]Extractor Estimating: 88it [00:51,  1.66it/s]Extractor Estimating: 89it [00:52,  1.64it/s]Extractor Estimating: 90it [00:53,  1.53it/s]Extractor Estimating: 91it [00:53,  1.61it/s]Extractor Estimating: 92it [00:54,  1.57it/s]Extractor Estimating: 93it [00:54,  1.60it/s]Extractor Estimating: 94it [00:55,  1.59it/s]Extractor Estimating: 95it [00:56,  1.63it/s]Extractor Estimating: 96it [00:56,  1.63it/s]Extractor Estimating: 97it [00:57,  1.59it/s]Extractor Estimating: 98it [00:57,  1.62it/s]Extractor Estimating: 99it [00:58,  1.69it/s]Extractor Estimating: 100it [00:59,  1.71it/s]Extractor Estimating: 101it [00:59,  1.71it/s]Extractor Estimating: 102it [01:00,  1.72it/s]Extractor Estimating: 103it [01:00,  1.66it/s]Extractor Estimating: 104it [01:01,  1.67it/s]Extractor Estimating: 105it [01:02,  1.67it/s]Extractor Estimating: 106it [01:02,  1.71it/s]Extractor Estimating: 107it [01:03,  1.69it/s]Extractor Estimating: 108it [01:03,  1.67it/s]Extractor Estimating: 109it [01:04,  1.71it/s]Extractor Estimating: 110it [01:05,  1.66it/s]Extractor Estimating: 111it [01:05,  1.66it/s]Extractor Estimating: 112it [01:06,  1.74it/s]Extractor Estimating: 113it [01:06,  1.74it/s]Extractor Estimating: 114it [01:07,  1.71it/s]Extractor Estimating: 115it [01:07,  1.68it/s]Extractor Estimating: 116it [01:08,  1.66it/s]Extractor Estimating: 117it [01:09,  1.67it/s]Extractor Estimating: 118it [01:09,  1.70it/s]Extractor Estimating: 119it [01:10,  1.69it/s]Extractor Estimating: 120it [01:10,  1.69it/s]Extractor Estimating: 121it [01:11,  1.71it/s]Extractor Estimating: 122it [01:12,  1.65it/s]Extractor Estimating: 123it [01:12,  1.68it/s]Extractor Estimating: 124it [01:13,  1.65it/s]Extractor Estimating: 125it [01:13,  1.69it/s]Extractor Estimating: 126it [01:14,  1.64it/s]Extractor Estimating: 127it [01:15,  1.62it/s]Extractor Estimating: 128it [01:15,  1.67it/s]Extractor Estimating: 129it [01:16,  1.66it/s]Extractor Estimating: 130it [01:16,  1.68it/s]Extractor Estimating: 131it [01:17,  1.70it/s]Extractor Estimating: 132it [01:18,  1.67it/s]Extractor Estimating: 133it [01:18,  1.68it/s]Extractor Estimating: 134it [01:19,  1.69it/s]Extractor Estimating: 135it [01:19,  1.62it/s]Extractor Estimating: 136it [01:20,  1.60it/s]Extractor Estimating: 137it [01:21,  1.63it/s]Extractor Estimating: 138it [01:21,  1.58it/s]Extractor Estimating: 139it [01:22,  1.59it/s]Extractor Estimating: 140it [01:23,  1.56it/s]Extractor Estimating: 141it [01:23,  1.58it/s]Extractor Estimating: 142it [01:24,  1.59it/s]Extractor Estimating: 143it [01:25,  1.58it/s]Extractor Estimating: 144it [01:25,  1.60it/s]Extractor Estimating: 145it [01:26,  1.64it/s]Extractor Estimating: 146it [01:26,  1.64it/s]Extractor Estimating: 147it [01:27,  1.66it/s]Extractor Estimating: 148it [01:28,  1.68it/s]Extractor Estimating: 149it [01:28,  1.67it/s]Extractor Estimating: 150it [01:29,  1.68it/s]Extractor Estimating: 151it [01:29,  1.65it/s]Extractor Estimating: 152it [01:30,  1.62it/s]Extractor Estimating: 153it [01:31,  1.62it/s]Extractor Estimating: 154it [01:31,  1.60it/s]Extractor Estimating: 155it [01:32,  1.64it/s]Extractor Estimating: 156it [01:32,  1.63it/s]Extractor Estimating: 157it [01:33,  1.62it/s]Extractor Estimating: 158it [01:34,  1.64it/s]Extractor Estimating: 159it [01:34,  1.57it/s]Extractor Estimating: 160it [01:35,  1.46it/s]Extractor Estimating: 161it [01:36,  1.52it/s]Extractor Estimating: 162it [01:36,  1.57it/s]Extractor Estimating: 163it [01:37,  1.58it/s]Extractor Estimating: 164it [01:38,  1.59it/s]Extractor Estimating: 165it [01:38,  1.60it/s]Extractor Estimating: 166it [01:39,  1.61it/s]Extractor Estimating: 167it [01:39,  1.59it/s]Extractor Estimating: 168it [01:40,  1.58it/s]Extractor Estimating: 169it [01:41,  1.59it/s]Extractor Estimating: 170it [01:41,  1.59it/s]Extractor Estimating: 171it [01:42,  1.63it/s]Extractor Estimating: 172it [01:43,  1.61it/s]Extractor Estimating: 173it [01:43,  1.64it/s]Extractor Estimating: 174it [01:44,  1.63it/s]Extractor Estimating: 175it [01:44,  1.60it/s]Extractor Estimating: 176it [01:45,  1.63it/s]Extractor Estimating: 177it [01:46,  1.58it/s]Extractor Estimating: 178it [01:46,  1.63it/s]Extractor Estimating: 179it [01:47,  1.63it/s]Extractor Estimating: 180it [01:47,  1.65it/s]Extractor Estimating: 181it [01:48,  1.66it/s]Extractor Estimating: 182it [01:49,  1.61it/s]Extractor Estimating: 183it [01:49,  1.61it/s]Extractor Estimating: 184it [01:50,  1.62it/s]Extractor Estimating: 185it [01:51,  1.62it/s]Extractor Estimating: 186it [01:51,  1.58it/s]Extractor Estimating: 187it [01:52,  1.57it/s]Extractor Estimating: 188it [01:53,  1.57it/s]Extractor Estimating: 189it [01:53,  1.57it/s]Extractor Estimating: 190it [01:54,  1.59it/s]Extractor Estimating: 191it [01:54,  1.56it/s]Extractor Estimating: 192it [01:55,  1.61it/s]Extractor Estimating: 193it [01:56,  1.63it/s]Extractor Estimating: 194it [01:56,  1.59it/s]Extractor Estimating: 195it [01:57,  1.63it/s]Extractor Estimating: 196it [01:57,  1.63it/s]Extractor Estimating: 197it [01:58,  1.64it/s]Extractor Estimating: 198it [01:59,  1.67it/s]Extractor Estimating: 199it [01:59,  1.66it/s]Extractor Estimating: 200it [02:00,  1.66it/s]Extractor Estimating: 201it [02:00,  1.69it/s]Extractor Estimating: 202it [02:01,  1.73it/s]Extractor Estimating: 203it [02:02,  1.68it/s]Extractor Estimating: 204it [02:02,  1.69it/s]Extractor Estimating: 205it [02:03,  1.65it/s]Extractor Estimating: 206it [02:03,  1.66it/s]Extractor Estimating: 207it [02:04,  1.65it/s]Extractor Estimating: 208it [02:05,  1.62it/s]Extractor Estimating: 209it [02:05,  1.68it/s]Extractor Estimating: 210it [02:06,  1.73it/s]Extractor Estimating: 211it [02:06,  1.74it/s]Extractor Estimating: 212it [02:07,  1.66it/s]Extractor Estimating: 213it [02:08,  1.66it/s]Extractor Estimating: 214it [02:08,  1.68it/s]Extractor Estimating: 215it [02:09,  1.68it/s]Extractor Estimating: 216it [02:09,  1.65it/s]Extractor Estimating: 217it [02:10,  1.64it/s]Extractor Estimating: 218it [02:11,  1.62it/s]Extractor Estimating: 219it [02:11,  1.65it/s]Extractor Estimating: 220it [02:12,  1.66it/s]Extractor Estimating: 221it [02:12,  1.71it/s]Extractor Estimating: 222it [02:13,  1.73it/s]Extractor Estimating: 223it [02:14,  1.70it/s]Extractor Estimating: 224it [02:14,  1.67it/s]Extractor Estimating: 225it [02:15,  1.65it/s]Extractor Estimating: 226it [02:15,  1.67it/s]Extractor Estimating: 227it [02:16,  1.67it/s]Extractor Estimating: 228it [02:17,  1.68it/s]Extractor Estimating: 229it [02:17,  1.67it/s]Extractor Estimating: 230it [02:18,  1.75it/s]Extractor Estimating: 231it [02:18,  1.79it/s]Extractor Estimating: 232it [02:19,  1.76it/s]Extractor Estimating: 233it [02:19,  1.74it/s]Extractor Estimating: 234it [02:20,  1.73it/s]Extractor Estimating: 235it [02:21,  1.73it/s]Extractor Estimating: 236it [02:21,  1.78it/s]Extractor Estimating: 237it [02:22,  1.74it/s]Extractor Estimating: 238it [02:22,  1.79it/s]Extractor Estimating: 239it [02:23,  1.77it/s]Extractor Estimating: 240it [02:23,  1.78it/s]Extractor Estimating: 241it [02:24,  1.76it/s]Extractor Estimating: 242it [02:25,  1.72it/s]Extractor Estimating: 243it [02:25,  1.79it/s]Extractor Estimating: 244it [02:26,  1.74it/s]Extractor Estimating: 245it [02:26,  1.69it/s]Extractor Estimating: 246it [02:27,  1.51it/s]Extractor Estimating: 247it [02:28,  1.60it/s]Extractor Estimating: 248it [02:28,  1.67it/s]Extractor Estimating: 249it [02:29,  1.72it/s]Extractor Estimating: 250it [02:29,  1.75it/s]Extractor Estimating: 251it [02:30,  1.80it/s]Extractor Estimating: 252it [02:30,  1.69it/s]Extractor Estimating: 253it [02:31,  1.77it/s]Extractor Estimating: 254it [02:31,  1.78it/s]Extractor Estimating: 255it [02:32,  1.76it/s]Extractor Estimating: 256it [02:33,  1.76it/s]Extractor Estimating: 257it [02:33,  1.81it/s]Extractor Estimating: 258it [02:34,  1.85it/s]Extractor Estimating: 259it [02:34,  1.74it/s]Extractor Estimating: 260it [02:35,  1.82it/s]Extractor Estimating: 261it [02:35,  1.79it/s]Extractor Estimating: 262it [02:36,  1.76it/s]Extractor Estimating: 263it [02:37,  1.76it/s]Extractor Estimating: 264it [02:37,  1.71it/s]Extractor Estimating: 265it [02:38,  1.74it/s]Extractor Estimating: 266it [02:38,  1.81it/s]Extractor Estimating: 267it [02:39,  1.71it/s]Extractor Estimating: 268it [02:39,  1.76it/s]Extractor Estimating: 269it [02:40,  1.78it/s]Extractor Estimating: 270it [02:40,  1.86it/s]Extractor Estimating: 271it [02:41,  1.94it/s]Extractor Estimating: 272it [02:41,  1.91it/s]Extractor Estimating: 273it [02:42,  1.86it/s]Extractor Estimating: 274it [02:43,  1.87it/s]Extractor Estimating: 275it [02:43,  1.87it/s]Extractor Estimating: 276it [02:44,  1.87it/s]Extractor Estimating: 277it [02:44,  1.87it/s]Extractor Estimating: 278it [02:45,  1.86it/s]Extractor Estimating: 279it [02:45,  1.87it/s]Extractor Estimating: 280it [02:46,  1.89it/s]Extractor Estimating: 281it [02:46,  1.88it/s]Extractor Estimating: 282it [02:47,  1.83it/s]Extractor Estimating: 283it [02:47,  1.82it/s]Extractor Estimating: 284it [02:48,  1.81it/s]Extractor Estimating: 285it [02:48,  1.86it/s]Extractor Estimating: 286it [02:49,  1.86it/s]Extractor Estimating: 287it [02:50,  1.82it/s]Extractor Estimating: 288it [02:50,  1.82it/s]Extractor Estimating: 289it [02:51,  1.80it/s]Extractor Estimating: 290it [02:51,  1.83it/s]Extractor Estimating: 291it [02:52,  1.89it/s]Extractor Estimating: 292it [02:52,  1.88it/s]Extractor Estimating: 293it [02:53,  1.88it/s]Extractor Estimating: 294it [02:53,  1.91it/s]Extractor Estimating: 295it [02:54,  1.89it/s]Extractor Estimating: 296it [02:54,  1.88it/s]Extractor Estimating: 297it [02:55,  1.89it/s]Extractor Estimating: 298it [02:55,  1.89it/s]Extractor Estimating: 299it [02:56,  1.68it/s]Extractor Estimating: 300it [02:57,  1.68it/s]Extractor Estimating: 301it [02:57,  1.75it/s]Extractor Estimating: 302it [02:58,  1.80it/s]Extractor Estimating: 303it [02:58,  1.80it/s]Extractor Estimating: 304it [02:59,  1.86it/s]Extractor Estimating: 305it [02:59,  1.87it/s]Extractor Estimating: 306it [03:00,  1.87it/s]Extractor Estimating: 307it [03:00,  1.84it/s]Extractor Estimating: 308it [03:01,  1.88it/s]Extractor Estimating: 309it [03:02,  1.89it/s]Extractor Estimating: 310it [03:02,  1.85it/s]Extractor Estimating: 311it [03:03,  1.89it/s]Extractor Estimating: 312it [03:03,  1.83it/s]Extractor Estimating: 313it [03:04,  1.86it/s]Extractor Estimating: 314it [03:04,  1.90it/s]Extractor Estimating: 315it [03:05,  1.94it/s]Extractor Estimating: 316it [03:05,  1.93it/s]Extractor Estimating: 317it [03:06,  1.92it/s]Extractor Estimating: 318it [03:06,  1.90it/s]Extractor Estimating: 319it [03:07,  1.90it/s]Extractor Estimating: 320it [03:07,  1.92it/s]Extractor Estimating: 321it [03:08,  1.95it/s]Extractor Estimating: 322it [03:08,  2.01it/s]Extractor Estimating: 323it [03:09,  1.94it/s]Extractor Estimating: 324it [03:09,  1.92it/s]Extractor Estimating: 325it [03:10,  1.91it/s]Extractor Estimating: 326it [03:11,  1.80it/s]Extractor Estimating: 327it [03:11,  1.73it/s]Extractor Estimating: 328it [03:12,  1.59it/s]Extractor Estimating: 329it [03:12,  1.61it/s]Extractor Estimating: 330it [03:13,  1.65it/s]Extractor Estimating: 331it [03:14,  1.64it/s]Extractor Estimating: 332it [03:14,  1.66it/s]Extractor Estimating: 333it [03:15,  1.66it/s]Extractor Estimating: 334it [03:15,  1.65it/s]Extractor Estimating: 335it [03:16,  1.66it/s]Extractor Estimating: 336it [03:17,  1.56it/s]Extractor Estimating: 337it [03:17,  1.61it/s]Extractor Estimating: 338it [03:18,  1.62it/s]Extractor Estimating: 339it [03:19,  1.59it/s]Extractor Estimating: 340it [03:19,  1.64it/s]Extractor Estimating: 341it [03:20,  1.67it/s]Extractor Estimating: 342it [03:20,  1.66it/s]Extractor Estimating: 343it [03:21,  1.50it/s]Extractor Estimating: 344it [03:22,  1.57it/s]Extractor Estimating: 345it [03:22,  1.57it/s]Extractor Estimating: 346it [03:23,  1.61it/s]Extractor Estimating: 347it [03:24,  1.65it/s]Extractor Estimating: 348it [03:24,  1.67it/s]Extractor Estimating: 349it [03:25,  1.65it/s]Extractor Estimating: 350it [03:25,  1.67it/s]Extractor Estimating: 351it [03:26,  1.68it/s]Extractor Estimating: 352it [03:26,  1.74it/s]Extractor Estimating: 353it [03:27,  1.61it/s]Extractor Estimating: 354it [03:28,  1.69it/s]Extractor Estimating: 355it [03:28,  1.74it/s]Extractor Estimating: 356it [03:29,  1.74it/s]Extractor Estimating: 357it [03:29,  1.78it/s]Extractor Estimating: 358it [03:30,  1.72it/s]Extractor Estimating: 359it [03:30,  1.80it/s]Extractor Estimating: 360it [03:31,  1.80it/s]Extractor Estimating: 361it [03:32,  1.84it/s]Extractor Estimating: 362it [03:32,  1.81it/s]Extractor Estimating: 363it [03:33,  1.85it/s]Extractor Estimating: 364it [03:33,  1.85it/s]Extractor Estimating: 365it [03:34,  1.84it/s]Extractor Estimating: 366it [03:34,  1.83it/s]Extractor Estimating: 367it [03:35,  1.82it/s]Extractor Estimating: 368it [03:35,  1.88it/s]Extractor Estimating: 369it [03:36,  1.86it/s]Extractor Estimating: 370it [03:36,  1.87it/s]Extractor Estimating: 371it [03:37,  1.85it/s]Extractor Estimating: 372it [03:38,  1.81it/s]Extractor Estimating: 373it [03:38,  1.84it/s]Extractor Estimating: 374it [03:39,  1.84it/s]Extractor Estimating: 375it [03:39,  1.81it/s]Extractor Estimating: 376it [03:40,  1.82it/s]Extractor Estimating: 377it [03:40,  1.75it/s]Extractor Estimating: 378it [03:41,  1.79it/s]Extractor Estimating: 379it [03:41,  1.75it/s]Extractor Estimating: 380it [03:42,  1.74it/s]Extractor Estimating: 381it [03:43,  1.70it/s]Extractor Estimating: 382it [03:43,  1.72it/s]Extractor Estimating: 383it [03:44,  1.74it/s]Extractor Estimating: 384it [03:44,  1.74it/s]Extractor Estimating: 385it [03:45,  1.71it/s]Extractor Estimating: 386it [03:46,  1.70it/s]Extractor Estimating: 387it [03:46,  1.72it/s]Extractor Estimating: 388it [03:47,  1.78it/s]Extractor Estimating: 389it [03:47,  1.73it/s]Extractor Estimating: 390it [03:48,  1.75it/s]Extractor Estimating: 391it [03:48,  1.67it/s]Extractor Estimating: 392it [03:49,  1.65it/s]Extractor Estimating: 393it [03:50,  1.65it/s]Extractor Estimating: 394it [03:50,  1.67it/s]Extractor Estimating: 395it [03:51,  1.69it/s]Extractor Estimating: 396it [03:51,  1.69it/s]Extractor Estimating: 397it [03:52,  1.65it/s]Extractor Estimating: 398it [03:53,  1.63it/s]Extractor Estimating: 399it [03:53,  1.61it/s]Extractor Estimating: 400it [03:54,  1.65it/s]Extractor Estimating: 401it [03:54,  1.70it/s]Extractor Estimating: 402it [03:55,  1.73it/s]Extractor Estimating: 403it [03:56,  1.72it/s]Extractor Estimating: 404it [03:56,  1.74it/s]Extractor Estimating: 405it [03:57,  1.71it/s]Extractor Estimating: 406it [03:57,  1.80it/s]Extractor Estimating: 407it [03:58,  1.87it/s]Extractor Estimating: 408it [03:58,  1.81it/s]Extractor Estimating: 409it [03:59,  1.85it/s]Extractor Estimating: 410it [03:59,  1.90it/s]Extractor Estimating: 411it [04:00,  1.87it/s]Extractor Estimating: 412it [04:00,  1.86it/s]Extractor Estimating: 413it [04:01,  1.84it/s]Extractor Estimating: 414it [04:02,  1.84it/s]Extractor Estimating: 415it [04:02,  1.77it/s]Extractor Estimating: 416it [04:03,  1.78it/s]Extractor Estimating: 417it [04:03,  1.74it/s]Extractor Estimating: 418it [04:04,  1.74it/s]Extractor Estimating: 419it [04:04,  1.77it/s]Extractor Estimating: 420it [04:05,  1.77it/s]Extractor Estimating: 421it [04:06,  1.75it/s]Extractor Estimating: 422it [04:06,  1.79it/s]Extractor Estimating: 423it [04:07,  1.78it/s]Extractor Estimating: 424it [04:07,  1.64it/s]Extractor Estimating: 425it [04:08,  1.71it/s]Extractor Estimating: 426it [04:09,  1.68it/s]Extractor Estimating: 427it [04:09,  1.69it/s]Extractor Estimating: 428it [04:10,  1.69it/s]Extractor Estimating: 429it [04:10,  1.67it/s]Extractor Estimating: 430it [04:11,  1.71it/s]Extractor Estimating: 431it [04:12,  1.66it/s]Extractor Estimating: 432it [04:12,  1.64it/s]Extractor Estimating: 433it [04:13,  1.66it/s]Extractor Estimating: 434it [04:13,  1.68it/s]Extractor Estimating: 435it [04:14,  1.71it/s]Extractor Estimating: 436it [04:15,  1.67it/s]Extractor Estimating: 437it [04:15,  1.65it/s]Extractor Estimating: 438it [04:16,  1.66it/s]Extractor Estimating: 439it [04:16,  1.66it/s]Extractor Estimating: 440it [04:17,  1.66it/s]Extractor Estimating: 441it [04:18,  1.68it/s]Extractor Estimating: 442it [04:18,  1.65it/s]Extractor Estimating: 443it [04:19,  1.65it/s]Extractor Estimating: 444it [04:19,  1.68it/s]Extractor Estimating: 445it [04:20,  1.66it/s]Extractor Estimating: 446it [04:21,  1.63it/s]Extractor Estimating: 447it [04:21,  1.65it/s]Extractor Estimating: 448it [04:22,  1.65it/s]Extractor Estimating: 449it [04:22,  1.68it/s]Extractor Estimating: 450it [04:23,  1.74it/s]Extractor Estimating: 451it [04:23,  1.77it/s]Extractor Estimating: 452it [04:24,  1.74it/s]Extractor Estimating: 453it [04:25,  1.73it/s]Extractor Estimating: 454it [04:25,  1.74it/s]Extractor Estimating: 455it [04:26,  1.79it/s]Extractor Estimating: 456it [04:26,  1.78it/s]Extractor Estimating: 457it [04:27,  1.81it/s]Extractor Estimating: 458it [04:27,  1.83it/s]Extractor Estimating: 459it [04:28,  1.82it/s]Extractor Estimating: 460it [04:28,  1.80it/s]Extractor Estimating: 461it [04:29,  1.82it/s]Extractor Estimating: 462it [04:30,  1.79it/s]Extractor Estimating: 463it [04:30,  1.79it/s]Extractor Estimating: 464it [04:31,  1.79it/s]Extractor Estimating: 465it [04:31,  1.73it/s]Extractor Estimating: 466it [04:32,  1.77it/s]Extractor Estimating: 467it [04:32,  1.75it/s]Extractor Estimating: 468it [04:33,  1.67it/s]Extractor Estimating: 469it [04:34,  1.67it/s]Extractor Estimating: 470it [04:34,  1.70it/s]Extractor Estimating: 471it [04:35,  1.70it/s]Extractor Estimating: 472it [04:35,  1.67it/s]Extractor Estimating: 473it [04:36,  1.67it/s]Extractor Estimating: 474it [04:37,  1.72it/s]Extractor Estimating: 475it [04:37,  1.73it/s]Extractor Estimating: 476it [04:38,  1.75it/s]Extractor Estimating: 477it [04:38,  1.70it/s]Extractor Estimating: 478it [04:39,  1.81it/s]Extractor Estimating: 479it [04:39,  1.85it/s]Extractor Estimating: 480it [04:40,  1.82it/s]Extractor Estimating: 481it [04:40,  1.84it/s]Extractor Estimating: 482it [04:41,  1.71it/s]Extractor Estimating: 483it [04:42,  1.75it/s]Extractor Estimating: 484it [04:42,  1.80it/s]Extractor Estimating: 485it [04:43,  1.82it/s]Extractor Estimating: 486it [04:43,  1.85it/s]Extractor Estimating: 487it [04:44,  1.75it/s]Extractor Estimating: 488it [04:44,  1.77it/s]Extractor Estimating: 489it [04:45,  1.79it/s]Extractor Estimating: 490it [04:46,  1.78it/s]Extractor Estimating: 491it [04:46,  1.77it/s]Extractor Estimating: 492it [04:47,  1.80it/s]Extractor Estimating: 493it [04:47,  1.78it/s]Extractor Estimating: 494it [04:48,  1.76it/s]Extractor Estimating: 495it [04:48,  1.80it/s]Extractor Estimating: 496it [04:49,  1.79it/s]Extractor Estimating: 497it [04:50,  1.58it/s]Extractor Estimating: 498it [04:50,  1.60it/s]Extractor Estimating: 499it [04:51,  1.68it/s]Extractor Estimating: 500it [04:51,  1.90it/s]Extractor Estimating: 500it [04:51,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:01,743 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:01,748 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:01,748 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:01,748 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:01,748 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:37:02,391 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:37:02,392 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:37:03,069 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:37:04,141 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:37:04,141 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:06,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:06,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:06,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:06,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:06,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:37:07,631 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:37:07,632 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:37:08,195 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:37:08,369 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:37:08,369 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 12:32:48,978 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 12:32:49,011 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9945 mean pseudo reward: 0.9461738194168651
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl'}
train vocab size: 27095
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27195, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27195, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.934, loss:788.0326
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.923, loss:766.0523
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.925, loss:728.7670
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.936, loss:728.1194
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 85, avg_time 0.926, loss:704.6405
>> valid entity prec:0.4868, rec:0.4439, f1:0.4644
>> valid relation prec:0.3711, rec:0.0240, f1:0.0452
>> valid relation with NER prec:0.3711, rec:0.0240, f1:0.0452
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 185, avg_time 2.789, loss:746.8743
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 285, avg_time 0.929, loss:730.5199
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 385, avg_time 0.930, loss:742.0883
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 70, avg_time 0.926, loss:741.0513
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 170, avg_time 0.931, loss:707.4277
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4641, rec:0.4202, f1:0.4411
>> valid relation prec:0.4360, rec:0.0401, f1:0.0734
>> valid relation with NER prec:0.4360, rec:0.0401, f1:0.0734
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 270, avg_time 2.773, loss:713.1072
g_step 1200, step 370, avg_time 0.932, loss:775.4886
g_step 1300, step 55, avg_time 0.925, loss:738.6538
g_step 1400, step 155, avg_time 0.926, loss:720.4973
g_step 1500, step 255, avg_time 0.936, loss:694.2241
>> valid entity prec:0.5005, rec:0.3924, f1:0.4399
>> valid relation prec:0.3467, rec:0.0206, f1:0.0390
>> valid relation with NER prec:0.3467, rec:0.0206, f1:0.0390
g_step 1600, step 355, avg_time 2.777, loss:710.1240
g_step 1700, step 40, avg_time 0.925, loss:686.5985
g_step 1800, step 140, avg_time 0.936, loss:667.4683
g_step 1900, step 240, avg_time 0.932, loss:685.3351
g_step 2000, step 340, avg_time 0.924, loss:674.1656
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4991, rec:0.4319, f1:0.4631
>> valid relation prec:0.4072, rec:0.0348, f1:0.0641
>> valid relation with NER prec:0.4072, rec:0.0348, f1:0.0641
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 25, avg_time 2.775, loss:693.5568
g_step 2200, step 125, avg_time 0.931, loss:633.2099
g_step 2300, step 225, avg_time 0.931, loss:651.9407
g_step 2400, step 325, avg_time 0.925, loss:664.3179
g_step 2500, step 10, avg_time 0.921, loss:677.0276
>> valid entity prec:0.4648, rec:0.5099, f1:0.4863
>> valid relation prec:0.2594, rec:0.0200, f1:0.0371
>> valid relation with NER prec:0.2594, rec:0.0200, f1:0.0371
new max entity f1 on valid!
g_step 2600, step 110, avg_time 2.776, loss:631.9214
g_step 2700, step 210, avg_time 0.934, loss:664.9611
g_step 2800, step 310, avg_time 0.926, loss:589.5783
g_step 2900, step 410, avg_time 0.931, loss:638.0611
g_step 3000, step 95, avg_time 0.940, loss:601.2042
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5193, rec:0.4415, f1:0.4772
>> valid relation prec:0.3529, rec:0.0524, f1:0.0912
>> valid relation with NER prec:0.3529, rec:0.0524, f1:0.0912
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 195, avg_time 2.769, loss:601.8736
g_step 3200, step 295, avg_time 0.929, loss:608.1315
g_step 3300, step 395, avg_time 0.928, loss:634.2637
g_step 3400, step 80, avg_time 0.926, loss:586.0410
g_step 3500, step 180, avg_time 0.926, loss:580.5055
>> valid entity prec:0.5022, rec:0.5230, f1:0.5124
>> valid relation prec:0.2640, rec:0.0522, f1:0.0872
>> valid relation with NER prec:0.2640, rec:0.0522, f1:0.0872
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 280, avg_time 2.785, loss:604.1051
g_step 3700, step 380, avg_time 0.937, loss:605.6877
g_step 3800, step 65, avg_time 0.927, loss:543.2058
g_step 3900, step 165, avg_time 0.939, loss:538.2716
g_step 4000, step 265, avg_time 0.923, loss:599.5763
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4929, rec:0.4778, f1:0.4852
>> valid relation prec:0.2576, rec:0.0406, f1:0.0701
>> valid relation with NER prec:0.2576, rec:0.0406, f1:0.0701
g_step 4100, step 365, avg_time 2.779, loss:588.8798
g_step 4200, step 50, avg_time 0.924, loss:544.8208
g_step 4300, step 150, avg_time 0.940, loss:540.9739
g_step 4400, step 250, avg_time 0.926, loss:559.7857
g_step 4500, step 350, avg_time 0.932, loss:563.4689
>> valid entity prec:0.4729, rec:0.4806, f1:0.4767
>> valid relation prec:0.2522, rec:0.0433, f1:0.0739
>> valid relation with NER prec:0.2522, rec:0.0433, f1:0.0739
g_step 4600, step 35, avg_time 2.780, loss:540.8167
g_step 4700, step 135, avg_time 0.926, loss:529.1191
g_step 4800, step 235, avg_time 0.925, loss:543.1437
g_step 4900, step 335, avg_time 0.932, loss:540.9216
g_step 5000, step 20, avg_time 0.931, loss:537.6599
learning rate was adjusted to 0.0008
>> valid entity prec:0.4976, rec:0.4047, f1:0.4463
>> valid relation prec:0.3319, rec:0.0520, f1:0.0899
>> valid relation with NER prec:0.3319, rec:0.0520, f1:0.0899
g_step 5100, step 120, avg_time 2.750, loss:510.8463
g_step 5200, step 220, avg_time 0.901, loss:501.0382
g_step 5300, step 320, avg_time 0.899, loss:542.9929
g_step 5400, step 5, avg_time 0.897, loss:520.3198
g_step 5500, step 105, avg_time 0.901, loss:505.6101
>> valid entity prec:0.5477, rec:0.3910, f1:0.4563
>> valid relation prec:0.3120, rec:0.0457, f1:0.0797
>> valid relation with NER prec:0.3120, rec:0.0457, f1:0.0797
g_step 5600, step 205, avg_time 2.687, loss:493.1463
g_step 5700, step 305, avg_time 0.910, loss:510.6885
g_step 5800, step 405, avg_time 0.893, loss:533.0832
g_step 5900, step 90, avg_time 0.896, loss:464.2703
g_step 6000, step 190, avg_time 0.905, loss:482.7075
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4775, rec:0.4165, f1:0.4449
>> valid relation prec:0.2457, rec:0.0314, f1:0.0557
>> valid relation with NER prec:0.2457, rec:0.0314, f1:0.0557
g_step 6100, step 290, avg_time 2.677, loss:499.1830
g_step 6200, step 390, avg_time 0.908, loss:507.6300
g_step 6300, step 75, avg_time 0.891, loss:481.5695
g_step 6400, step 175, avg_time 0.900, loss:463.8321
g_step 6500, step 275, avg_time 0.893, loss:469.1189
>> valid entity prec:0.4676, rec:0.5113, f1:0.4885
>> valid relation prec:0.2232, rec:0.0391, f1:0.0665
>> valid relation with NER prec:0.2232, rec:0.0391, f1:0.0665
g_step 6600, step 375, avg_time 2.692, loss:497.0875
g_step 6700, step 60, avg_time 0.893, loss:454.0252
g_step 6800, step 160, avg_time 0.890, loss:459.3172
g_step 6900, step 260, avg_time 0.900, loss:456.0441
g_step 7000, step 360, avg_time 0.906, loss:469.7624
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4517, rec:0.4480, f1:0.4498
>> valid relation prec:0.2121, rec:0.0353, f1:0.0605
>> valid relation with NER prec:0.2121, rec:0.0353, f1:0.0605
g_step 7100, step 45, avg_time 2.701, loss:450.6878
g_step 7200, step 145, avg_time 0.904, loss:426.1444
g_step 7300, step 245, avg_time 0.892, loss:453.2269
g_step 7400, step 345, avg_time 0.897, loss:455.0999
g_step 7500, step 30, avg_time 0.896, loss:438.0925
>> valid entity prec:0.4849, rec:0.4447, f1:0.4639
>> valid relation prec:0.2440, rec:0.0466, f1:0.0782
>> valid relation with NER prec:0.2440, rec:0.0466, f1:0.0782
g_step 7600, step 130, avg_time 2.674, loss:418.9454
g_step 7700, step 230, avg_time 0.910, loss:432.0081
g_step 7800, step 330, avg_time 0.900, loss:440.1768
g_step 7900, step 15, avg_time 0.894, loss:427.7459
g_step 8000, step 115, avg_time 0.911, loss:385.6512
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4588, rec:0.4523, f1:0.4555
>> valid relation prec:0.2021, rec:0.0334, f1:0.0574
>> valid relation with NER prec:0.2021, rec:0.0334, f1:0.0574
g_step 8100, step 215, avg_time 2.690, loss:415.1631
g_step 8200, step 315, avg_time 0.898, loss:428.6658
g_step 8300, step 415, avg_time 0.898, loss:448.8056
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 12:32:49 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 12:32:49 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_12-32-48_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 12:32:49 - WARNING - datasets.builder -   Using custom data configuration default-aa2252efd803f1aa
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-aa2252efd803f1aa/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 12:32:50,338 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:32:50,339 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 12:32:50,340 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:32:50,341 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 12:32:50,352 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:32:50,357 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:32:50,357 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:32:50,357 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:32:50,357 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:32:50,357 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:32:50,357 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 12:32:50,495 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 12:32:53,447 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 12:32:53,451 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-aa2252efd803f1aa/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.44ba/s] 20%|██        | 2/10 [00:00<00:01,  4.26ba/s] 30%|███       | 3/10 [00:00<00:01,  3.56ba/s] 40%|████      | 4/10 [00:01<00:01,  4.05ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.35ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.57ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.74ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.83ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.88ba/s]100%|██████████| 10/10 [00:02<00:00,  4.94ba/s]100%|██████████| 10/10 [00:02<00:00,  4.53ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  4.42ba/s] 33%|███▎      | 2/6 [00:00<00:00,  4.68ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.82ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.84ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.83ba/s]100%|██████████| 6/6 [00:01<00:00,  5.07ba/s]100%|██████████| 6/6 [00:01<00:00,  4.91ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:00,  9.77ba/s] 30%|███       | 3/10 [00:00<00:00, 11.54ba/s] 50%|█████     | 5/10 [00:00<00:00, 11.94ba/s] 70%|███████   | 7/10 [00:00<00:00, 12.09ba/s] 90%|█████████ | 9/10 [00:00<00:00,  9.03ba/s]100%|██████████| 10/10 [00:00<00:00, 10.14ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 33%|███▎      | 2/6 [00:00<00:00, 11.33ba/s] 67%|██████▋   | 4/6 [00:00<00:00, 11.81ba/s]100%|██████████| 6/6 [00:00<00:00, 12.28ba/s]100%|██████████| 6/6 [00:00<00:00, 12.09ba/s]
[INFO|trainer.py:414] 2023-08-29 12:32:58,843 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 12:32:58,858 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 12:32:58,858 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 12:32:58,858 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 12:32:58,858 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 12:32:58,858 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 12:32:58,858 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 12:32:58,858 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:52,  3.35it/s]  0%|          | 2/780 [00:00<03:47,  3.42it/s]  0%|          | 3/780 [00:00<03:45,  3.44it/s]  1%|          | 4/780 [00:01<03:46,  3.43it/s]  1%|          | 5/780 [00:01<03:45,  3.43it/s]  1%|          | 6/780 [00:01<03:46,  3.42it/s]  1%|          | 7/780 [00:02<03:46,  3.42it/s]  1%|          | 8/780 [00:02<03:46,  3.41it/s]  1%|          | 9/780 [00:02<03:46,  3.41it/s]  1%|▏         | 10/780 [00:02<03:46,  3.41it/s]  1%|▏         | 11/780 [00:03<03:45,  3.41it/s]  2%|▏         | 12/780 [00:03<03:45,  3.40it/s]  2%|▏         | 13/780 [00:03<03:45,  3.40it/s]  2%|▏         | 14/780 [00:04<03:44,  3.41it/s]  2%|▏         | 15/780 [00:04<03:45,  3.39it/s]  2%|▏         | 16/780 [00:04<03:45,  3.39it/s]  2%|▏         | 17/780 [00:04<03:44,  3.40it/s]  2%|▏         | 18/780 [00:05<03:44,  3.40it/s]  2%|▏         | 19/780 [00:05<03:43,  3.40it/s]  3%|▎         | 20/780 [00:05<03:43,  3.40it/s]  3%|▎         | 21/780 [00:06<03:42,  3.40it/s]  3%|▎         | 22/780 [00:06<03:43,  3.40it/s]  3%|▎         | 23/780 [00:06<03:42,  3.40it/s]  3%|▎         | 24/780 [00:07<03:42,  3.40it/s]  3%|▎         | 25/780 [00:07<03:42,  3.40it/s]  3%|▎         | 26/780 [00:07<03:42,  3.38it/s]  3%|▎         | 27/780 [00:07<03:42,  3.39it/s]  4%|▎         | 28/780 [00:08<03:41,  3.39it/s]  4%|▎         | 29/780 [00:08<03:41,  3.39it/s]  4%|▍         | 30/780 [00:08<03:40,  3.39it/s]  4%|▍         | 31/780 [00:09<03:40,  3.39it/s]  4%|▍         | 32/780 [00:09<03:40,  3.40it/s]  4%|▍         | 33/780 [00:09<03:39,  3.40it/s]  4%|▍         | 34/780 [00:09<03:39,  3.40it/s]  4%|▍         | 35/780 [00:10<03:39,  3.40it/s]  5%|▍         | 36/780 [00:10<03:38,  3.40it/s]  5%|▍         | 37/780 [00:10<03:39,  3.39it/s]  5%|▍         | 38/780 [00:11<03:38,  3.39it/s]  5%|▌         | 39/780 [00:11<03:38,  3.40it/s]  5%|▌         | 40/780 [00:11<03:37,  3.40it/s]  5%|▌         | 41/780 [00:12<03:37,  3.40it/s]  5%|▌         | 42/780 [00:12<03:37,  3.40it/s]  6%|▌         | 43/780 [00:12<03:36,  3.40it/s]  6%|▌         | 44/780 [00:12<03:36,  3.40it/s]  6%|▌         | 45/780 [00:13<03:36,  3.40it/s]  6%|▌         | 46/780 [00:13<03:35,  3.40it/s]  6%|▌         | 47/780 [00:13<03:35,  3.40it/s]  6%|▌         | 48/780 [00:14<03:36,  3.39it/s]  6%|▋         | 49/780 [00:14<03:35,  3.39it/s]  6%|▋         | 50/780 [00:14<03:34,  3.40it/s]  7%|▋         | 51/780 [00:14<03:34,  3.40it/s]  7%|▋         | 52/780 [00:15<03:34,  3.39it/s]  7%|▋         | 53/780 [00:15<03:33,  3.40it/s]  7%|▋         | 54/780 [00:15<03:33,  3.40it/s]  7%|▋         | 55/780 [00:16<03:33,  3.40it/s]  7%|▋         | 56/780 [00:16<03:33,  3.40it/s]  7%|▋         | 57/780 [00:16<03:32,  3.40it/s]  7%|▋         | 58/780 [00:17<03:32,  3.40it/s]  8%|▊         | 59/780 [00:17<03:33,  3.38it/s]  8%|▊         | 60/780 [00:17<03:32,  3.39it/s]  8%|▊         | 61/780 [00:17<03:32,  3.38it/s]  8%|▊         | 62/780 [00:18<03:31,  3.39it/s]  8%|▊         | 63/780 [00:18<03:31,  3.39it/s]  8%|▊         | 64/780 [00:18<03:31,  3.39it/s]  8%|▊         | 65/780 [00:19<03:29,  3.41it/s]  8%|▊         | 66/780 [00:19<03:28,  3.42it/s]  9%|▊         | 67/780 [00:19<03:27,  3.44it/s]  9%|▊         | 68/780 [00:19<03:27,  3.44it/s]  9%|▉         | 69/780 [00:20<03:26,  3.44it/s]  9%|▉         | 70/780 [00:20<03:27,  3.43it/s]  9%|▉         | 71/780 [00:20<03:26,  3.43it/s]  9%|▉         | 72/780 [00:21<03:25,  3.44it/s]  9%|▉         | 73/780 [00:21<03:25,  3.44it/s]  9%|▉         | 74/780 [00:21<03:24,  3.45it/s] 10%|▉         | 75/780 [00:22<03:24,  3.45it/s] 10%|▉         | 76/780 [00:22<03:23,  3.45it/s] 10%|▉         | 77/780 [00:22<03:23,  3.45it/s] 10%|█         | 78/780 [00:22<03:23,  3.45it/s] 10%|█         | 79/780 [00:23<03:23,  3.44it/s] 10%|█         | 80/780 [00:23<03:23,  3.45it/s] 10%|█         | 81/780 [00:23<03:23,  3.43it/s] 11%|█         | 82/780 [00:24<03:22,  3.44it/s] 11%|█         | 83/780 [00:24<03:22,  3.44it/s] 11%|█         | 84/780 [00:24<03:21,  3.45it/s] 11%|█         | 85/780 [00:24<03:21,  3.45it/s] 11%|█         | 86/780 [00:25<03:21,  3.45it/s] 11%|█         | 87/780 [00:25<03:20,  3.45it/s] 11%|█▏        | 88/780 [00:25<03:20,  3.45it/s] 11%|█▏        | 89/780 [00:26<03:19,  3.46it/s] 12%|█▏        | 90/780 [00:26<03:19,  3.45it/s] 12%|█▏        | 91/780 [00:26<03:19,  3.45it/s] 12%|█▏        | 92/780 [00:26<03:20,  3.43it/s] 12%|█▏        | 93/780 [00:27<03:19,  3.44it/s] 12%|█▏        | 94/780 [00:27<03:19,  3.44it/s] 12%|█▏        | 95/780 [00:27<03:18,  3.44it/s] 12%|█▏        | 96/780 [00:28<03:18,  3.45it/s] 12%|█▏        | 97/780 [00:28<03:18,  3.45it/s] 13%|█▎        | 98/780 [00:28<03:17,  3.45it/s] 13%|█▎        | 99/780 [00:28<03:17,  3.45it/s] 13%|█▎        | 100/780 [00:29<03:17,  3.45it/s] 13%|█▎        | 101/780 [00:29<03:16,  3.45it/s] 13%|█▎        | 102/780 [00:29<03:16,  3.45it/s] 13%|█▎        | 103/780 [00:30<03:16,  3.44it/s] 13%|█▎        | 104/780 [00:30<03:16,  3.44it/s] 13%|█▎        | 105/780 [00:30<03:15,  3.45it/s] 14%|█▎        | 106/780 [00:31<03:15,  3.45it/s] 14%|█▎        | 107/780 [00:31<03:15,  3.45it/s] 14%|█▍        | 108/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 109/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 110/780 [00:32<03:13,  3.45it/s] 14%|█▍        | 111/780 [00:32<03:13,  3.45it/s] 14%|█▍        | 112/780 [00:32<03:13,  3.45it/s] 14%|█▍        | 113/780 [00:33<03:13,  3.46it/s] 15%|█▍        | 114/780 [00:33<03:13,  3.44it/s] 15%|█▍        | 115/780 [00:33<03:13,  3.44it/s] 15%|█▍        | 116/780 [00:33<03:12,  3.45it/s] 15%|█▌        | 117/780 [00:34<03:12,  3.45it/s] 15%|█▌        | 118/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 119/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 120/780 [00:35<03:11,  3.45it/s] 16%|█▌        | 121/780 [00:35<03:11,  3.45it/s] 16%|█▌        | 122/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 123/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 124/780 [00:36<03:10,  3.45it/s] 16%|█▌        | 125/780 [00:36<03:10,  3.44it/s] 16%|█▌        | 126/780 [00:36<03:09,  3.45it/s] 16%|█▋        | 127/780 [00:37<03:09,  3.45it/s] 16%|█▋        | 128/780 [00:37<03:09,  3.45it/s] 17%|█▋        | 129/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 130/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 131/780 [00:38<03:08,  3.44it/s] 17%|█▋        | 132/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 133/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 134/780 [00:39<03:07,  3.45it/s] 17%|█▋        | 135/780 [00:39<03:07,  3.45it/s] 17%|█▋        | 136/780 [00:39<03:07,  3.43it/s] 18%|█▊        | 137/780 [00:40<03:07,  3.44it/s] 18%|█▊        | 138/780 [00:40<03:06,  3.44it/s] 18%|█▊        | 139/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 140/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 141/780 [00:41<03:05,  3.45it/s] 18%|█▊        | 142/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 143/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 144/780 [00:42<03:04,  3.45it/s] 19%|█▊        | 145/780 [00:42<03:04,  3.45it/s] 19%|█▊        | 146/780 [00:42<03:03,  3.45it/s] 19%|█▉        | 147/780 [00:42<03:03,  3.44it/s] 19%|█▉        | 148/780 [00:43<03:03,  3.45it/s] 19%|█▉        | 149/780 [00:43<03:02,  3.45it/s] 19%|█▉        | 150/780 [00:43<03:02,  3.45it/s] 19%|█▉        | 151/780 [00:44<03:02,  3.44it/s] 19%|█▉        | 152/780 [00:44<03:02,  3.44it/s] 20%|█▉        | 153/780 [00:44<03:01,  3.45it/s] 20%|█▉        | 154/780 [00:44<03:01,  3.45it/s] 20%|█▉        | 155/780 [00:45<03:01,  3.45it/s] 20%|██        | 156/780 [00:45<03:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 12:33:44,411 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:33:44,411 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 12:33:44,411 >>   Batch size = 8

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 54.04it/s][A
  2%|▏         | 12/733 [00:00<00:15, 46.77it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.17it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.61it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.17it/s][A
  4%|▍         | 32/733 [00:00<00:15, 43.93it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.98it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.87it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.99it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.92it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.73it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.45it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.51it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.59it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.62it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.70it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.63it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.87it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.79it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.63it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.51it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.50it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.57it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.60it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.64it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.82it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.84it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.79it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.37it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.66it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.55it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.59it/s][A
 23%|██▎       | 167/733 [00:03<00:12, 43.64it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.73it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.78it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.82it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.76it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.62it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.57it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 42.17it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 42.80it/s][A
 29%|██▉       | 212/733 [00:04<00:12, 43.16it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.36it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.51it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.56it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.57it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.50it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.35it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.38it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.64it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.47it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.85it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.83it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.79it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.69it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.48it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.51it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.54it/s][A
 41%|████      | 297/733 [00:06<00:09, 43.65it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.76it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.74it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.62it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.66it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.55it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.53it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.57it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.66it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.76it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.73it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.69it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.71it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.66it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.65it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.51it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.64it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.69it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.69it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.69it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.64it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.66it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.66it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.58it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.58it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.69it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.67it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.69it/s][A
 60%|█████▉    | 437/733 [00:09<00:06, 43.70it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.65it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.67it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.66it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.58it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.64it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.72it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.69it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.65it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.59it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.69it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.65it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.60it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.57it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.62it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.71it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.70it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.62it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.68it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.70it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.65it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.48it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.66it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.71it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.69it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.73it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.74it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.72it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.68it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.59it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.57it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.58it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.70it/s][A
 82%|████████▏ | 602/733 [00:13<00:02, 43.69it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.72it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.70it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.73it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.69it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.56it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.58it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.70it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.68it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.54it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.72it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.78it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.75it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.62it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.52it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.68it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.69it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.56it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.63it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.78it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.65it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.75it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.63it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.63it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.66it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.68it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.69it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.69it/s][A 20%|██        | 156/780 [01:02<03:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:34:01,292 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 12:34:01,313 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:34:03,100 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:34:03,120 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:34:03,148 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:08<1:14:09,  7.14s/it] 20%|██        | 158/780 [01:08<52:45,  5.09s/it]   20%|██        | 159/780 [01:09<37:47,  3.65s/it] 21%|██        | 160/780 [01:09<27:19,  2.64s/it] 21%|██        | 161/780 [01:09<20:00,  1.94s/it] 21%|██        | 162/780 [01:10<14:53,  1.45s/it] 21%|██        | 163/780 [01:10<11:18,  1.10s/it] 21%|██        | 164/780 [01:10<08:48,  1.16it/s] 21%|██        | 165/780 [01:11<07:03,  1.45it/s] 21%|██▏       | 166/780 [01:11<05:50,  1.75it/s] 21%|██▏       | 167/780 [01:11<04:59,  2.05it/s] 22%|██▏       | 168/780 [01:11<04:23,  2.33it/s] 22%|██▏       | 169/780 [01:12<03:58,  2.56it/s] 22%|██▏       | 170/780 [01:12<03:40,  2.77it/s] 22%|██▏       | 171/780 [01:12<03:27,  2.93it/s] 22%|██▏       | 172/780 [01:13<03:18,  3.06it/s] 22%|██▏       | 173/780 [01:13<03:12,  3.15it/s] 22%|██▏       | 174/780 [01:13<03:07,  3.22it/s] 22%|██▏       | 175/780 [01:13<03:04,  3.27it/s] 23%|██▎       | 176/780 [01:14<03:02,  3.31it/s] 23%|██▎       | 177/780 [01:14<03:00,  3.34it/s] 23%|██▎       | 178/780 [01:14<02:59,  3.36it/s] 23%|██▎       | 179/780 [01:15<02:57,  3.39it/s] 23%|██▎       | 180/780 [01:15<02:56,  3.40it/s] 23%|██▎       | 181/780 [01:15<02:55,  3.41it/s] 23%|██▎       | 182/780 [01:15<02:54,  3.43it/s] 23%|██▎       | 183/780 [01:16<02:53,  3.44it/s] 24%|██▎       | 184/780 [01:16<02:53,  3.44it/s] 24%|██▎       | 185/780 [01:16<02:52,  3.45it/s] 24%|██▍       | 186/780 [01:17<02:52,  3.45it/s] 24%|██▍       | 187/780 [01:17<02:51,  3.45it/s] 24%|██▍       | 188/780 [01:17<02:51,  3.45it/s] 24%|██▍       | 189/780 [01:18<02:51,  3.46it/s] 24%|██▍       | 190/780 [01:18<02:50,  3.46it/s] 24%|██▍       | 191/780 [01:18<02:51,  3.44it/s] 25%|██▍       | 192/780 [01:18<02:50,  3.45it/s] 25%|██▍       | 193/780 [01:19<02:50,  3.45it/s] 25%|██▍       | 194/780 [01:19<02:50,  3.45it/s] 25%|██▌       | 195/780 [01:19<02:49,  3.45it/s] 25%|██▌       | 196/780 [01:20<02:49,  3.45it/s] 25%|██▌       | 197/780 [01:20<02:49,  3.45it/s] 25%|██▌       | 198/780 [01:20<02:48,  3.45it/s] 26%|██▌       | 199/780 [01:20<02:48,  3.45it/s] 26%|██▌       | 200/780 [01:21<02:48,  3.45it/s] 26%|██▌       | 201/780 [01:21<02:47,  3.45it/s] 26%|██▌       | 202/780 [01:21<02:48,  3.43it/s] 26%|██▌       | 203/780 [01:22<02:47,  3.44it/s] 26%|██▌       | 204/780 [01:22<02:47,  3.44it/s] 26%|██▋       | 205/780 [01:22<02:46,  3.44it/s] 26%|██▋       | 206/780 [01:22<02:46,  3.44it/s] 27%|██▋       | 207/780 [01:23<02:46,  3.44it/s] 27%|██▋       | 208/780 [01:23<02:46,  3.44it/s] 27%|██▋       | 209/780 [01:23<02:45,  3.44it/s] 27%|██▋       | 210/780 [01:24<02:45,  3.45it/s] 27%|██▋       | 211/780 [01:24<02:45,  3.45it/s] 27%|██▋       | 212/780 [01:24<02:44,  3.45it/s] 27%|██▋       | 213/780 [01:24<02:44,  3.44it/s] 27%|██▋       | 214/780 [01:25<02:44,  3.44it/s] 28%|██▊       | 215/780 [01:25<02:44,  3.44it/s] 28%|██▊       | 216/780 [01:25<02:43,  3.44it/s] 28%|██▊       | 217/780 [01:26<02:43,  3.44it/s] 28%|██▊       | 218/780 [01:26<02:43,  3.44it/s] 28%|██▊       | 219/780 [01:26<02:42,  3.45it/s] 28%|██▊       | 220/780 [01:27<02:42,  3.45it/s] 28%|██▊       | 221/780 [01:27<02:42,  3.45it/s] 28%|██▊       | 222/780 [01:27<02:41,  3.45it/s] 29%|██▊       | 223/780 [01:27<02:41,  3.45it/s] 29%|██▊       | 224/780 [01:28<02:41,  3.45it/s] 29%|██▉       | 225/780 [01:28<02:41,  3.43it/s] 29%|██▉       | 226/780 [01:28<02:41,  3.44it/s] 29%|██▉       | 227/780 [01:29<02:40,  3.44it/s] 29%|██▉       | 228/780 [01:29<02:40,  3.45it/s] 29%|██▉       | 229/780 [01:29<02:39,  3.45it/s] 29%|██▉       | 230/780 [01:29<02:39,  3.45it/s] 30%|██▉       | 231/780 [01:30<02:39,  3.45it/s] 30%|██▉       | 232/780 [01:30<02:38,  3.45it/s] 30%|██▉       | 233/780 [01:30<02:38,  3.45it/s] 30%|███       | 234/780 [01:31<02:38,  3.45it/s] 30%|███       | 235/780 [01:31<02:38,  3.45it/s] 30%|███       | 236/780 [01:31<02:38,  3.42it/s] 30%|███       | 237/780 [01:31<02:38,  3.43it/s] 31%|███       | 238/780 [01:32<02:37,  3.44it/s] 31%|███       | 239/780 [01:32<02:37,  3.44it/s] 31%|███       | 240/780 [01:32<02:36,  3.44it/s] 31%|███       | 241/780 [01:33<02:36,  3.44it/s] 31%|███       | 242/780 [01:33<02:36,  3.44it/s] 31%|███       | 243/780 [01:33<02:35,  3.45it/s] 31%|███▏      | 244/780 [01:33<02:35,  3.45it/s] 31%|███▏      | 245/780 [01:34<02:35,  3.45it/s] 32%|███▏      | 246/780 [01:34<02:34,  3.45it/s] 32%|███▏      | 247/780 [01:34<02:35,  3.43it/s] 32%|███▏      | 248/780 [01:35<02:34,  3.44it/s] 32%|███▏      | 249/780 [01:35<02:34,  3.44it/s] 32%|███▏      | 250/780 [01:35<02:33,  3.44it/s] 32%|███▏      | 251/780 [01:36<02:33,  3.44it/s] 32%|███▏      | 252/780 [01:36<02:33,  3.45it/s] 32%|███▏      | 253/780 [01:36<02:33,  3.44it/s] 33%|███▎      | 254/780 [01:36<02:32,  3.45it/s] 33%|███▎      | 255/780 [01:37<02:32,  3.44it/s] 33%|███▎      | 256/780 [01:37<02:32,  3.45it/s] 33%|███▎      | 257/780 [01:37<02:31,  3.44it/s] 33%|███▎      | 258/780 [01:38<02:32,  3.42it/s] 33%|███▎      | 259/780 [01:38<02:31,  3.43it/s] 33%|███▎      | 260/780 [01:38<02:31,  3.44it/s] 33%|███▎      | 261/780 [01:38<02:30,  3.44it/s] 34%|███▎      | 262/780 [01:39<02:30,  3.44it/s] 34%|███▎      | 263/780 [01:39<02:30,  3.45it/s] 34%|███▍      | 264/780 [01:39<02:29,  3.44it/s] 34%|███▍      | 265/780 [01:40<02:29,  3.45it/s] 34%|███▍      | 266/780 [01:40<02:29,  3.44it/s] 34%|███▍      | 267/780 [01:40<02:28,  3.44it/s] 34%|███▍      | 268/780 [01:40<02:28,  3.44it/s] 34%|███▍      | 269/780 [01:41<02:28,  3.44it/s] 35%|███▍      | 270/780 [01:41<02:28,  3.44it/s] 35%|███▍      | 271/780 [01:41<02:27,  3.44it/s] 35%|███▍      | 272/780 [01:42<02:27,  3.44it/s] 35%|███▌      | 273/780 [01:42<02:27,  3.44it/s] 35%|███▌      | 274/780 [01:42<02:26,  3.44it/s] 35%|███▌      | 275/780 [01:43<02:26,  3.44it/s] 35%|███▌      | 276/780 [01:43<02:26,  3.44it/s] 36%|███▌      | 277/780 [01:43<02:26,  3.44it/s] 36%|███▌      | 278/780 [01:43<02:25,  3.44it/s] 36%|███▌      | 279/780 [01:44<02:25,  3.44it/s] 36%|███▌      | 280/780 [01:44<02:25,  3.43it/s] 36%|███▌      | 281/780 [01:44<02:25,  3.43it/s] 36%|███▌      | 282/780 [01:45<02:24,  3.44it/s] 36%|███▋      | 283/780 [01:45<02:24,  3.44it/s] 36%|███▋      | 284/780 [01:45<02:24,  3.44it/s] 37%|███▋      | 285/780 [01:45<02:23,  3.44it/s] 37%|███▋      | 286/780 [01:46<02:23,  3.44it/s] 37%|███▋      | 287/780 [01:46<02:23,  3.44it/s] 37%|███▋      | 288/780 [01:46<02:22,  3.44it/s] 37%|███▋      | 289/780 [01:47<02:22,  3.44it/s] 37%|███▋      | 290/780 [01:47<02:22,  3.44it/s] 37%|███▋      | 291/780 [01:47<02:23,  3.42it/s] 37%|███▋      | 292/780 [01:47<02:22,  3.43it/s] 38%|███▊      | 293/780 [01:48<02:21,  3.43it/s] 38%|███▊      | 294/780 [01:48<02:21,  3.43it/s] 38%|███▊      | 295/780 [01:48<02:21,  3.43it/s] 38%|███▊      | 296/780 [01:49<02:20,  3.44it/s] 38%|███▊      | 297/780 [01:49<02:20,  3.44it/s] 38%|███▊      | 298/780 [01:49<02:20,  3.44it/s] 38%|███▊      | 299/780 [01:49<02:19,  3.44it/s] 38%|███▊      | 300/780 [01:50<02:23,  3.34it/s] 39%|███▊      | 301/780 [01:50<02:22,  3.37it/s] 39%|███▊      | 302/780 [01:50<02:21,  3.38it/s] 39%|███▉      | 303/780 [01:51<02:20,  3.40it/s] 39%|███▉      | 304/780 [01:51<02:19,  3.41it/s] 39%|███▉      | 305/780 [01:51<02:18,  3.42it/s] 39%|███▉      | 306/780 [01:52<02:18,  3.43it/s] 39%|███▉      | 307/780 [01:52<02:17,  3.43it/s] 39%|███▉      | 308/780 [01:52<02:17,  3.44it/s] 40%|███▉      | 309/780 [01:52<02:17,  3.44it/s] 40%|███▉      | 310/780 [01:53<02:16,  3.44it/s] 40%|███▉      | 311/780 [01:53<02:16,  3.44it/s] 40%|████      | 312/780 [01:53<02:16,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 12:34:52,690 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:34:52,690 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 12:34:52,690 >>   Batch size = 8
{'eval_loss': 0.9377340078353882, 'eval_runtime': 16.8436, 'eval_samples_per_second': 348.144, 'eval_steps_per_second': 43.518, 'epoch': 1.0}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 54.00it/s][A
  2%|▏         | 12/733 [00:00<00:15, 46.85it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.35it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.73it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.29it/s][A
  4%|▍         | 32/733 [00:00<00:15, 44.03it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.94it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.82it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.84it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.82it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.72it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.65it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.60it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.68it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.62it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.56it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.64it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.71it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.40it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.48it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.46it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.61it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.62it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.58it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.56it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.64it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.70it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.61it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.58it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.63it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.65it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.59it/s][A
 23%|██▎       | 167/733 [00:03<00:12, 43.57it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.67it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.72it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.66it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.66it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.66it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.63it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.62it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.65it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.54it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.59it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.62it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.67it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.64it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.63it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.64it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.59it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.54it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.55it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.65it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.64it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.64it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.69it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.66it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.67it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.56it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.52it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.64it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.63it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.59it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.57it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.63it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.71it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.62it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.58it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.59it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.58it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.61it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.59it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.61it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.67it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.68it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.62it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.59it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.54it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.69it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.59it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.56it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.52it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.70it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.67it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.65it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.62it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.70it/s][A
 60%|█████▉    | 437/733 [00:09<00:06, 43.71it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.60it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.56it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.61it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.62it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.61it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.61it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.63it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.62it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.66it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.63it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.65it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.57it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.63it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.59it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.55it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.48it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.63it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.65it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.61it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.55it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.63it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.64it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.66it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.58it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.66it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.63it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.58it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.59it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.64it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.62it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.60it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.62it/s][A
 82%|████████▏ | 602/733 [00:13<00:02, 43.69it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.47it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.49it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.55it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.65it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.64it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.57it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.59it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.61it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.60it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.53it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.60it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.59it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.65it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.67it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.59it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.62it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.66it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.64it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.67it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.63it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.59it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.73it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.65it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.64it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.64it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.64it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.64it/s][A 40%|████      | 312/780 [02:10<02:16,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:35:09,564 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 12:35:09,594 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:35:12,012 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:35:12,031 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:35:12,044 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:17<57:51,  7.43s/it] 40%|████      | 314/780 [02:18<41:06,  5.29s/it] 40%|████      | 315/780 [02:18<29:24,  3.79s/it] 41%|████      | 316/780 [02:18<21:13,  2.74s/it] 41%|████      | 317/780 [02:19<15:30,  2.01s/it] 41%|████      | 318/780 [02:19<11:30,  1.49s/it] 41%|████      | 319/780 [02:19<08:43,  1.13s/it] 41%|████      | 320/780 [02:19<06:46,  1.13it/s] 41%|████      | 321/780 [02:20<05:24,  1.42it/s] 41%|████▏     | 322/780 [02:20<04:26,  1.72it/s] 41%|████▏     | 323/780 [02:20<03:46,  2.01it/s] 42%|████▏     | 324/780 [02:21<03:18,  2.29it/s] 42%|████▏     | 325/780 [02:21<02:59,  2.54it/s] 42%|████▏     | 326/780 [02:21<02:45,  2.74it/s] 42%|████▏     | 327/780 [02:22<02:35,  2.91it/s] 42%|████▏     | 328/780 [02:22<02:28,  3.04it/s] 42%|████▏     | 329/780 [02:22<02:23,  3.14it/s] 42%|████▏     | 330/780 [02:22<02:20,  3.21it/s] 42%|████▏     | 331/780 [02:23<02:17,  3.26it/s] 43%|████▎     | 332/780 [02:23<02:15,  3.30it/s] 43%|████▎     | 333/780 [02:23<02:14,  3.33it/s] 43%|████▎     | 334/780 [02:24<02:13,  3.35it/s] 43%|████▎     | 335/780 [02:24<02:12,  3.36it/s] 43%|████▎     | 336/780 [02:24<02:11,  3.37it/s] 43%|████▎     | 337/780 [02:24<02:11,  3.38it/s] 43%|████▎     | 338/780 [02:25<02:10,  3.38it/s] 43%|████▎     | 339/780 [02:25<02:10,  3.38it/s] 44%|████▎     | 340/780 [02:25<02:10,  3.38it/s] 44%|████▎     | 341/780 [02:26<02:09,  3.39it/s] 44%|████▍     | 342/780 [02:26<02:09,  3.39it/s] 44%|████▍     | 343/780 [02:26<02:09,  3.38it/s] 44%|████▍     | 344/780 [02:27<02:08,  3.38it/s] 44%|████▍     | 345/780 [02:27<02:08,  3.39it/s] 44%|████▍     | 346/780 [02:27<02:08,  3.38it/s] 44%|████▍     | 347/780 [02:27<02:07,  3.38it/s] 45%|████▍     | 348/780 [02:28<02:07,  3.38it/s] 45%|████▍     | 349/780 [02:28<02:07,  3.39it/s] 45%|████▍     | 350/780 [02:28<02:06,  3.39it/s] 45%|████▌     | 351/780 [02:29<02:06,  3.39it/s] 45%|████▌     | 352/780 [02:29<02:06,  3.39it/s] 45%|████▌     | 353/780 [02:29<02:06,  3.39it/s] 45%|████▌     | 354/780 [02:30<02:06,  3.37it/s] 46%|████▌     | 355/780 [02:30<02:05,  3.38it/s] 46%|████▌     | 356/780 [02:30<02:05,  3.38it/s] 46%|████▌     | 357/780 [02:30<02:05,  3.38it/s] 46%|████▌     | 358/780 [02:31<02:04,  3.38it/s] 46%|████▌     | 359/780 [02:31<02:04,  3.39it/s] 46%|████▌     | 360/780 [02:31<02:04,  3.38it/s] 46%|████▋     | 361/780 [02:32<02:03,  3.39it/s] 46%|████▋     | 362/780 [02:32<02:03,  3.39it/s] 47%|████▋     | 363/780 [02:32<02:03,  3.39it/s] 47%|████▋     | 364/780 [02:32<02:02,  3.39it/s] 47%|████▋     | 365/780 [02:33<02:03,  3.37it/s] 47%|████▋     | 366/780 [02:33<02:02,  3.38it/s] 47%|████▋     | 367/780 [02:33<02:02,  3.38it/s] 47%|████▋     | 368/780 [02:34<02:01,  3.38it/s] 47%|████▋     | 369/780 [02:34<02:01,  3.38it/s] 47%|████▋     | 370/780 [02:34<02:01,  3.38it/s] 48%|████▊     | 371/780 [02:35<02:00,  3.39it/s] 48%|████▊     | 372/780 [02:35<02:00,  3.39it/s] 48%|████▊     | 373/780 [02:35<02:00,  3.39it/s] 48%|████▊     | 374/780 [02:35<01:59,  3.39it/s] 48%|████▊     | 375/780 [02:36<01:59,  3.39it/s] 48%|████▊     | 376/780 [02:36<02:00,  3.35it/s] 48%|████▊     | 377/780 [02:36<01:59,  3.36it/s] 48%|████▊     | 378/780 [02:37<01:59,  3.37it/s] 49%|████▊     | 379/780 [02:37<01:58,  3.38it/s] 49%|████▊     | 380/780 [02:37<01:58,  3.38it/s] 49%|████▉     | 381/780 [02:37<01:58,  3.38it/s] 49%|████▉     | 382/780 [02:38<01:57,  3.38it/s] 49%|████▉     | 383/780 [02:38<01:57,  3.38it/s] 49%|████▉     | 384/780 [02:38<01:57,  3.38it/s] 49%|████▉     | 385/780 [02:39<01:56,  3.38it/s] 49%|████▉     | 386/780 [02:39<01:56,  3.38it/s] 50%|████▉     | 387/780 [02:39<01:56,  3.37it/s] 50%|████▉     | 388/780 [02:40<01:56,  3.38it/s] 50%|████▉     | 389/780 [02:40<01:55,  3.38it/s] 50%|█████     | 390/780 [02:40<01:55,  3.38it/s] 50%|█████     | 391/780 [02:40<01:55,  3.38it/s] 50%|█████     | 392/780 [02:41<01:54,  3.38it/s] 50%|█████     | 393/780 [02:41<01:54,  3.39it/s] 51%|█████     | 394/780 [02:41<01:53,  3.39it/s] 51%|█████     | 395/780 [02:42<01:53,  3.39it/s] 51%|█████     | 396/780 [02:42<01:53,  3.39it/s] 51%|█████     | 397/780 [02:42<01:53,  3.39it/s] 51%|█████     | 398/780 [02:43<01:53,  3.37it/s] 51%|█████     | 399/780 [02:43<01:52,  3.38it/s] 51%|█████▏    | 400/780 [02:43<01:52,  3.38it/s] 51%|█████▏    | 401/780 [02:43<01:52,  3.38it/s] 52%|█████▏    | 402/780 [02:44<01:51,  3.39it/s] 52%|█████▏    | 403/780 [02:44<01:51,  3.39it/s] 52%|█████▏    | 404/780 [02:44<01:51,  3.38it/s] 52%|█████▏    | 405/780 [02:45<01:50,  3.39it/s] 52%|█████▏    | 406/780 [02:45<01:50,  3.39it/s] 52%|█████▏    | 407/780 [02:45<01:50,  3.39it/s] 52%|█████▏    | 408/780 [02:45<01:49,  3.39it/s] 52%|█████▏    | 409/780 [02:46<01:50,  3.36it/s] 53%|█████▎    | 410/780 [02:46<01:49,  3.37it/s] 53%|█████▎    | 411/780 [02:46<01:49,  3.37it/s] 53%|█████▎    | 412/780 [02:47<01:48,  3.38it/s] 53%|█████▎    | 413/780 [02:47<01:48,  3.38it/s] 53%|█████▎    | 414/780 [02:47<01:48,  3.38it/s] 53%|█████▎    | 415/780 [02:48<01:47,  3.39it/s] 53%|█████▎    | 416/780 [02:48<01:47,  3.39it/s] 53%|█████▎    | 417/780 [02:48<01:47,  3.37it/s] 54%|█████▎    | 418/780 [02:48<01:47,  3.38it/s] 54%|█████▎    | 419/780 [02:49<01:46,  3.38it/s] 54%|█████▍    | 420/780 [02:49<01:46,  3.37it/s] 54%|█████▍    | 421/780 [02:49<01:46,  3.37it/s] 54%|█████▍    | 422/780 [02:50<01:45,  3.38it/s] 54%|█████▍    | 423/780 [02:50<01:47,  3.31it/s] 54%|█████▍    | 424/780 [02:50<01:46,  3.33it/s] 54%|█████▍    | 425/780 [02:51<01:46,  3.35it/s] 55%|█████▍    | 426/780 [02:51<01:45,  3.36it/s] 55%|█████▍    | 427/780 [02:51<01:44,  3.37it/s] 55%|█████▍    | 428/780 [02:51<01:44,  3.38it/s] 55%|█████▌    | 429/780 [02:52<01:43,  3.38it/s] 55%|█████▌    | 430/780 [02:52<01:43,  3.37it/s] 55%|█████▌    | 431/780 [02:52<01:43,  3.37it/s] 55%|█████▌    | 432/780 [02:53<01:43,  3.37it/s] 56%|█████▌    | 433/780 [02:53<01:42,  3.38it/s] 56%|█████▌    | 434/780 [02:53<01:42,  3.38it/s] 56%|█████▌    | 435/780 [02:53<01:41,  3.38it/s] 56%|█████▌    | 436/780 [02:54<01:41,  3.38it/s] 56%|█████▌    | 437/780 [02:54<01:41,  3.39it/s] 56%|█████▌    | 438/780 [02:54<01:41,  3.38it/s] 56%|█████▋    | 439/780 [02:55<01:40,  3.39it/s] 56%|█████▋    | 440/780 [02:55<01:40,  3.39it/s] 57%|█████▋    | 441/780 [02:55<01:40,  3.39it/s] 57%|█████▋    | 442/780 [02:56<01:40,  3.38it/s] 57%|█████▋    | 443/780 [02:56<01:39,  3.38it/s] 57%|█████▋    | 444/780 [02:56<01:39,  3.38it/s] 57%|█████▋    | 445/780 [02:56<01:38,  3.38it/s] 57%|█████▋    | 446/780 [02:57<01:38,  3.39it/s] 57%|█████▋    | 447/780 [02:57<01:38,  3.37it/s] 57%|█████▋    | 448/780 [02:57<01:38,  3.38it/s] 58%|█████▊    | 449/780 [02:58<01:37,  3.38it/s] 58%|█████▊    | 450/780 [02:58<01:37,  3.38it/s] 58%|█████▊    | 451/780 [02:58<01:37,  3.39it/s] 58%|█████▊    | 452/780 [02:59<01:36,  3.39it/s] 58%|█████▊    | 453/780 [02:59<01:36,  3.39it/s] 58%|█████▊    | 454/780 [02:59<01:36,  3.39it/s] 58%|█████▊    | 455/780 [02:59<01:35,  3.39it/s] 58%|█████▊    | 456/780 [03:00<01:35,  3.39it/s] 59%|█████▊    | 457/780 [03:00<01:35,  3.39it/s] 59%|█████▊    | 458/780 [03:00<01:35,  3.37it/s] 59%|█████▉    | 459/780 [03:01<01:35,  3.38it/s] 59%|█████▉    | 460/780 [03:01<01:34,  3.38it/s] 59%|█████▉    | 461/780 [03:01<01:34,  3.38it/s] 59%|█████▉    | 462/780 [03:01<01:33,  3.38it/s] 59%|█████▉    | 463/780 [03:02<01:33,  3.39it/s] 59%|█████▉    | 464/780 [03:02<01:33,  3.39it/s] 60%|█████▉    | 465/780 [03:02<01:32,  3.39it/s] 60%|█████▉    | 466/780 [03:03<01:32,  3.39it/s] 60%|█████▉    | 467/780 [03:03<01:32,  3.39it/s] 60%|██████    | 468/780 [03:03<01:31,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 12:36:02,635 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:36:02,635 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 12:36:02,635 >>   Batch size = 8
{'eval_loss': 0.9439476728439331, 'eval_runtime': 16.847, 'eval_samples_per_second': 348.075, 'eval_steps_per_second': 43.509, 'epoch': 2.0}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 53.93it/s][A
  2%|▏         | 12/733 [00:00<00:15, 46.78it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.47it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.80it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.35it/s][A
  4%|▍         | 32/733 [00:00<00:15, 44.09it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.93it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.66it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.70it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.78it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.78it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.68it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.64it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.68it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.67it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.54it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.47it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.61it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.68it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.64it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.59it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.56it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.67it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.63it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.49it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.62it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.72it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.68it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.70it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.60it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.60it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.62it/s][A
 23%|██▎       | 167/733 [00:03<00:12, 43.58it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.57it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.57it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.48it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.65it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.75it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.66it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.62it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.67it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.51it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.65it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.65it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.66it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.70it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.65it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.58it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.64it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.63it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.56it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.62it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.68it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.68it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.62it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.56it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.65it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.65it/s][A
 41%|████      | 297/733 [00:06<00:09, 43.61it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.62it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.63it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.62it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.65it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.60it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.54it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.61it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.74it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.62it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.66it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.67it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.67it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.55it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.53it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.63it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.71it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.59it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.62it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.74it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.72it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.61it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.54it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.59it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.65it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.66it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.69it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.71it/s][A
 60%|█████▉    | 437/733 [00:09<00:06, 43.70it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.65it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.63it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.61it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.62it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.58it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.63it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.65it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.73it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.67it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.66it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.57it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.54it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.64it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.69it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.60it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.54it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.61it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.63it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.57it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.57it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.60it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.66it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.67it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.60it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.64it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.62it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.60it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.59it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.58it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.64it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.62it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.57it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.65it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.63it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.59it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.62it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.60it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.55it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.55it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.69it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.70it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.63it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.69it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.62it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.61it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.48it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.48it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.57it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.68it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.69it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.71it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.64it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.57it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.56it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.58it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.57it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.62it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.63it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.68it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.68it/s][A 60%|██████    | 468/780 [03:20<01:31,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:36:19,503 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 12:36:19,533 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:36:21,589 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:36:21,614 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:36:21,631 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:26<37:01,  7.14s/it] 60%|██████    | 470/780 [03:27<26:17,  5.09s/it] 60%|██████    | 471/780 [03:27<18:48,  3.65s/it] 61%|██████    | 472/780 [03:27<13:34,  2.64s/it] 61%|██████    | 473/780 [03:28<09:55,  1.94s/it] 61%|██████    | 474/780 [03:28<07:22,  1.45s/it] 61%|██████    | 475/780 [03:28<05:35,  1.10s/it] 61%|██████    | 476/780 [03:28<04:21,  1.16it/s] 61%|██████    | 477/780 [03:29<03:29,  1.45it/s] 61%|██████▏   | 478/780 [03:29<02:52,  1.75it/s] 61%|██████▏   | 479/780 [03:29<02:26,  2.05it/s] 62%|██████▏   | 480/780 [03:30<02:09,  2.33it/s] 62%|██████▏   | 481/780 [03:30<01:56,  2.57it/s] 62%|██████▏   | 482/780 [03:30<01:47,  2.77it/s] 62%|██████▏   | 483/780 [03:30<01:41,  2.93it/s] 62%|██████▏   | 484/780 [03:31<01:36,  3.06it/s] 62%|██████▏   | 485/780 [03:31<01:33,  3.14it/s] 62%|██████▏   | 486/780 [03:31<01:31,  3.22it/s] 62%|██████▏   | 487/780 [03:32<01:29,  3.27it/s] 63%|██████▎   | 488/780 [03:32<01:28,  3.31it/s] 63%|██████▎   | 489/780 [03:32<01:27,  3.33it/s] 63%|██████▎   | 490/780 [03:33<01:26,  3.35it/s] 63%|██████▎   | 491/780 [03:33<01:25,  3.37it/s] 63%|██████▎   | 492/780 [03:33<01:25,  3.37it/s] 63%|██████▎   | 493/780 [03:33<01:24,  3.38it/s] 63%|██████▎   | 494/780 [03:34<01:24,  3.39it/s] 63%|██████▎   | 495/780 [03:34<01:24,  3.39it/s] 64%|██████▎   | 496/780 [03:34<01:24,  3.38it/s] 64%|██████▎   | 497/780 [03:35<01:23,  3.39it/s] 64%|██████▍   | 498/780 [03:35<01:23,  3.39it/s] 64%|██████▍   | 499/780 [03:35<01:22,  3.39it/s] 64%|██████▍   | 500/780 [03:35<01:22,  3.39it/s]                                                  64%|██████▍   | 500/780 [03:35<01:22,  3.39it/s] 64%|██████▍   | 501/780 [03:36<01:22,  3.37it/s] 64%|██████▍   | 502/780 [03:36<01:22,  3.38it/s] 64%|██████▍   | 503/780 [03:36<01:21,  3.39it/s] 65%|██████▍   | 504/780 [03:37<01:21,  3.39it/s] 65%|██████▍   | 505/780 [03:37<01:21,  3.39it/s] 65%|██████▍   | 506/780 [03:37<01:20,  3.39it/s] 65%|██████▌   | 507/780 [03:38<01:20,  3.38it/s] 65%|██████▌   | 508/780 [03:38<01:20,  3.38it/s] 65%|██████▌   | 509/780 [03:38<01:20,  3.39it/s] 65%|██████▌   | 510/780 [03:38<01:19,  3.39it/s] 66%|██████▌   | 511/780 [03:39<01:19,  3.39it/s] 66%|██████▌   | 512/780 [03:39<01:18,  3.39it/s] 66%|██████▌   | 513/780 [03:39<01:18,  3.39it/s] 66%|██████▌   | 514/780 [03:40<01:18,  3.40it/s] 66%|██████▌   | 515/780 [03:40<01:18,  3.40it/s] 66%|██████▌   | 516/780 [03:40<01:17,  3.39it/s] 66%|██████▋   | 517/780 [03:41<01:17,  3.39it/s] 66%|██████▋   | 518/780 [03:41<01:17,  3.39it/s] 67%|██████▋   | 519/780 [03:41<01:16,  3.39it/s] 67%|██████▋   | 520/780 [03:41<01:16,  3.39it/s] 67%|██████▋   | 521/780 [03:42<01:16,  3.39it/s] 67%|██████▋   | 522/780 [03:42<01:16,  3.39it/s] 67%|██████▋   | 523/780 [03:42<01:15,  3.39it/s] 67%|██████▋   | 524/780 [03:43<01:15,  3.39it/s] 67%|██████▋   | 525/780 [03:43<01:15,  3.39it/s] 67%|██████▋   | 526/780 [03:43<01:14,  3.40it/s] 68%|██████▊   | 527/780 [03:43<01:14,  3.40it/s] 68%|██████▊   | 528/780 [03:44<01:14,  3.40it/s] 68%|██████▊   | 529/780 [03:44<01:14,  3.38it/s] 68%|██████▊   | 530/780 [03:44<01:13,  3.39it/s] 68%|██████▊   | 531/780 [03:45<01:13,  3.39it/s] 68%|██████▊   | 532/780 [03:45<01:13,  3.39it/s] 68%|██████▊   | 533/780 [03:45<01:12,  3.39it/s] 68%|██████▊   | 534/780 [03:46<01:12,  3.40it/s] 69%|██████▊   | 535/780 [03:46<01:12,  3.40it/s] 69%|██████▊   | 536/780 [03:46<01:11,  3.39it/s] 69%|██████▉   | 537/780 [03:46<01:11,  3.39it/s] 69%|██████▉   | 538/780 [03:47<01:11,  3.40it/s] 69%|██████▉   | 539/780 [03:47<01:10,  3.39it/s] 69%|██████▉   | 540/780 [03:47<01:10,  3.39it/s] 69%|██████▉   | 541/780 [03:48<01:10,  3.39it/s] 69%|██████▉   | 542/780 [03:48<01:09,  3.41it/s] 70%|██████▉   | 543/780 [03:48<01:09,  3.40it/s] 70%|██████▉   | 544/780 [03:48<01:09,  3.42it/s] 70%|██████▉   | 545/780 [03:49<01:08,  3.43it/s] 70%|███████   | 546/780 [03:49<01:08,  3.42it/s] 70%|███████   | 547/780 [03:49<01:08,  3.41it/s] 70%|███████   | 548/780 [03:50<01:08,  3.41it/s] 70%|███████   | 549/780 [03:50<01:09,  3.32it/s] 71%|███████   | 550/780 [03:50<01:08,  3.34it/s] 71%|███████   | 551/780 [03:51<01:08,  3.35it/s] 71%|███████   | 552/780 [03:51<01:07,  3.36it/s] 71%|███████   | 553/780 [03:51<01:07,  3.37it/s] 71%|███████   | 554/780 [03:51<01:06,  3.38it/s] 71%|███████   | 555/780 [03:52<01:06,  3.39it/s] 71%|███████▏  | 556/780 [03:52<01:06,  3.39it/s] 71%|███████▏  | 557/780 [03:52<01:05,  3.39it/s] 72%|███████▏  | 558/780 [03:53<01:05,  3.39it/s] 72%|███████▏  | 559/780 [03:53<01:05,  3.39it/s] 72%|███████▏  | 560/780 [03:53<01:04,  3.39it/s] 72%|███████▏  | 561/780 [03:53<01:04,  3.39it/s] 72%|███████▏  | 562/780 [03:54<01:04,  3.38it/s] 72%|███████▏  | 563/780 [03:54<01:04,  3.38it/s] 72%|███████▏  | 564/780 [03:54<01:03,  3.39it/s] 72%|███████▏  | 565/780 [03:55<01:03,  3.39it/s] 73%|███████▎  | 566/780 [03:55<01:03,  3.39it/s] 73%|███████▎  | 567/780 [03:55<01:02,  3.39it/s] 73%|███████▎  | 568/780 [03:56<01:02,  3.39it/s] 73%|███████▎  | 569/780 [03:56<01:02,  3.39it/s] 73%|███████▎  | 570/780 [03:56<01:01,  3.39it/s] 73%|███████▎  | 571/780 [03:56<01:01,  3.40it/s] 73%|███████▎  | 572/780 [03:57<01:01,  3.39it/s] 73%|███████▎  | 573/780 [03:57<01:00,  3.39it/s] 74%|███████▎  | 574/780 [03:57<01:00,  3.40it/s] 74%|███████▎  | 575/780 [03:58<00:59,  3.42it/s] 74%|███████▍  | 576/780 [03:58<00:59,  3.43it/s] 74%|███████▍  | 577/780 [03:58<00:59,  3.44it/s] 74%|███████▍  | 578/780 [03:58<00:58,  3.43it/s] 74%|███████▍  | 579/780 [03:59<00:58,  3.43it/s] 74%|███████▍  | 580/780 [03:59<00:58,  3.44it/s] 74%|███████▍  | 581/780 [03:59<00:57,  3.44it/s] 75%|███████▍  | 582/780 [04:00<00:57,  3.45it/s] 75%|███████▍  | 583/780 [04:00<00:57,  3.45it/s] 75%|███████▍  | 584/780 [04:00<00:56,  3.45it/s] 75%|███████▌  | 585/780 [04:01<00:56,  3.45it/s] 75%|███████▌  | 586/780 [04:01<00:56,  3.45it/s] 75%|███████▌  | 587/780 [04:01<00:55,  3.45it/s] 75%|███████▌  | 588/780 [04:01<00:55,  3.45it/s] 76%|███████▌  | 589/780 [04:02<00:55,  3.44it/s] 76%|███████▌  | 590/780 [04:02<00:55,  3.44it/s] 76%|███████▌  | 591/780 [04:02<00:54,  3.44it/s] 76%|███████▌  | 592/780 [04:03<00:54,  3.45it/s] 76%|███████▌  | 593/780 [04:03<00:54,  3.45it/s] 76%|███████▌  | 594/780 [04:03<00:53,  3.45it/s] 76%|███████▋  | 595/780 [04:03<00:53,  3.45it/s] 76%|███████▋  | 596/780 [04:04<00:53,  3.45it/s] 77%|███████▋  | 597/780 [04:04<00:53,  3.45it/s] 77%|███████▋  | 598/780 [04:04<00:52,  3.45it/s] 77%|███████▋  | 599/780 [04:05<00:52,  3.46it/s] 77%|███████▋  | 600/780 [04:05<00:52,  3.43it/s] 77%|███████▋  | 601/780 [04:05<00:52,  3.44it/s] 77%|███████▋  | 602/780 [04:05<00:51,  3.44it/s] 77%|███████▋  | 603/780 [04:06<00:51,  3.44it/s] 77%|███████▋  | 604/780 [04:06<00:51,  3.44it/s] 78%|███████▊  | 605/780 [04:06<00:50,  3.45it/s] 78%|███████▊  | 606/780 [04:07<00:50,  3.45it/s] 78%|███████▊  | 607/780 [04:07<00:50,  3.45it/s] 78%|███████▊  | 608/780 [04:07<00:49,  3.45it/s] 78%|███████▊  | 609/780 [04:07<00:49,  3.45it/s] 78%|███████▊  | 610/780 [04:08<00:49,  3.44it/s] 78%|███████▊  | 611/780 [04:08<00:49,  3.44it/s] 78%|███████▊  | 612/780 [04:08<00:48,  3.44it/s] 79%|███████▊  | 613/780 [04:09<00:48,  3.44it/s] 79%|███████▊  | 614/780 [04:09<00:48,  3.45it/s] 79%|███████▉  | 615/780 [04:09<00:47,  3.45it/s] 79%|███████▉  | 616/780 [04:10<00:47,  3.45it/s] 79%|███████▉  | 617/780 [04:10<00:47,  3.45it/s] 79%|███████▉  | 618/780 [04:10<00:46,  3.45it/s] 79%|███████▉  | 619/780 [04:10<00:46,  3.45it/s] 79%|███████▉  | 620/780 [04:11<00:46,  3.44it/s] 80%|███████▉  | 621/780 [04:11<00:46,  3.45it/s] 80%|███████▉  | 622/780 [04:11<00:46,  3.43it/s] 80%|███████▉  | 623/780 [04:12<00:45,  3.43it/s] 80%|████████  | 624/780 [04:12<00:45,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 12:37:11,224 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:37:11,224 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 12:37:11,224 >>   Batch size = 8
{'eval_loss': 0.9566866755485535, 'eval_runtime': 16.8394, 'eval_samples_per_second': 348.23, 'eval_steps_per_second': 43.529, 'epoch': 3.0}
{'loss': 0.6081, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 54.66it/s][A
  2%|▏         | 12/733 [00:00<00:15, 47.29it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.75it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.92it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.27it/s][A
  4%|▍         | 32/733 [00:00<00:15, 44.04it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.85it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.67it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.75it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.88it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.93it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.77it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.78it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.71it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.58it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.49it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.51it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.62it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.71it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.83it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.74it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.71it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.64it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.52it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.39it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.51it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.62it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.74it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.79it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.75it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.69it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.65it/s][A
 23%|██▎       | 167/733 [00:03<00:13, 43.52it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.45it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.47it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.70it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.79it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.75it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.50it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.58it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.54it/s][A
 29%|██▉       | 212/733 [00:04<00:12, 43.42it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.41it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.50it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.65it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.72it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.74it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.72it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.63it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.55it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.48it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.53it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.58it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.68it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.76it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.74it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.65it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.63it/s][A
 41%|████      | 297/733 [00:06<00:09, 43.61it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.54it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.56it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.63it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.71it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.64it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.65it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.69it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.62it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.52it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.58it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.63it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.67it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.68it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.74it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.68it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.63it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.60it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.59it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.62it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.65it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.67it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.72it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.70it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.66it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.63it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.60it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.61it/s][A
 60%|█████▉    | 437/733 [00:09<00:06, 43.58it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.66it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.70it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.63it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.51it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.60it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.63it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.54it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.65it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.73it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.72it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.66it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.75it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.75it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.62it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.65it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.61it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.66it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.67it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.69it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.64it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.67it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.70it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.66it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.65it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.70it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.67it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.62it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.68it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.64it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.65it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.69it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.67it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.57it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.65it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.57it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.64it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.67it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.66it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.66it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.69it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.71it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.71it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.61it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.56it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.66it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.67it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.61it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.53it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.66it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.63it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.65it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.56it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.63it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.67it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.64it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.64it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.60it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.70it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.67it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.67it/s][A 80%|████████  | 624/780 [04:29<00:45,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:37:28,088 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 12:37:28,107 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:37:29,820 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:37:29,832 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:37:29,845 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:34<17:53,  6.92s/it] 80%|████████  | 626/780 [04:35<12:40,  4.94s/it] 80%|████████  | 627/780 [04:35<09:02,  3.54s/it] 81%|████████  | 628/780 [04:35<06:30,  2.57s/it] 81%|████████  | 629/780 [04:35<04:44,  1.89s/it] 81%|████████  | 630/780 [04:36<03:31,  1.41s/it] 81%|████████  | 631/780 [04:36<02:40,  1.07s/it] 81%|████████  | 632/780 [04:36<02:04,  1.19it/s] 81%|████████  | 633/780 [04:37<01:39,  1.48it/s] 81%|████████▏ | 634/780 [04:37<01:22,  1.78it/s] 81%|████████▏ | 635/780 [04:37<01:09,  2.08it/s] 82%|████████▏ | 636/780 [04:37<01:01,  2.35it/s] 82%|████████▏ | 637/780 [04:38<00:55,  2.58it/s] 82%|████████▏ | 638/780 [04:38<00:51,  2.78it/s] 82%|████████▏ | 639/780 [04:38<00:47,  2.94it/s] 82%|████████▏ | 640/780 [04:39<00:45,  3.07it/s] 82%|████████▏ | 641/780 [04:39<00:44,  3.16it/s] 82%|████████▏ | 642/780 [04:39<00:42,  3.23it/s] 82%|████████▏ | 643/780 [04:40<00:41,  3.28it/s] 83%|████████▎ | 644/780 [04:40<00:41,  3.31it/s] 83%|████████▎ | 645/780 [04:40<00:40,  3.33it/s] 83%|████████▎ | 646/780 [04:40<00:39,  3.35it/s] 83%|████████▎ | 647/780 [04:41<00:39,  3.37it/s] 83%|████████▎ | 648/780 [04:41<00:39,  3.36it/s] 83%|████████▎ | 649/780 [04:41<00:38,  3.37it/s] 83%|████████▎ | 650/780 [04:42<00:38,  3.38it/s] 83%|████████▎ | 651/780 [04:42<00:38,  3.38it/s] 84%|████████▎ | 652/780 [04:42<00:37,  3.39it/s] 84%|████████▎ | 653/780 [04:42<00:37,  3.39it/s] 84%|████████▍ | 654/780 [04:43<00:37,  3.39it/s] 84%|████████▍ | 655/780 [04:43<00:36,  3.39it/s] 84%|████████▍ | 656/780 [04:43<00:36,  3.39it/s] 84%|████████▍ | 657/780 [04:44<00:36,  3.39it/s] 84%|████████▍ | 658/780 [04:44<00:35,  3.40it/s] 84%|████████▍ | 659/780 [04:44<00:35,  3.37it/s] 85%|████████▍ | 660/780 [04:45<00:35,  3.38it/s] 85%|████████▍ | 661/780 [04:45<00:35,  3.38it/s] 85%|████████▍ | 662/780 [04:45<00:34,  3.39it/s] 85%|████████▌ | 663/780 [04:45<00:34,  3.39it/s] 85%|████████▌ | 664/780 [04:46<00:34,  3.39it/s] 85%|████████▌ | 665/780 [04:46<00:33,  3.39it/s] 85%|████████▌ | 666/780 [04:46<00:33,  3.39it/s] 86%|████████▌ | 667/780 [04:47<00:33,  3.40it/s] 86%|████████▌ | 668/780 [04:47<00:32,  3.39it/s] 86%|████████▌ | 669/780 [04:47<00:32,  3.40it/s] 86%|████████▌ | 670/780 [04:47<00:32,  3.40it/s] 86%|████████▌ | 671/780 [04:48<00:31,  3.41it/s] 86%|████████▌ | 672/780 [04:48<00:31,  3.43it/s] 86%|████████▋ | 673/780 [04:48<00:31,  3.43it/s] 86%|████████▋ | 674/780 [04:49<00:30,  3.44it/s] 87%|████████▋ | 675/780 [04:49<00:30,  3.44it/s] 87%|████████▋ | 676/780 [04:49<00:30,  3.44it/s] 87%|████████▋ | 677/780 [04:50<00:29,  3.44it/s] 87%|████████▋ | 678/780 [04:50<00:29,  3.44it/s] 87%|████████▋ | 679/780 [04:50<00:29,  3.37it/s] 87%|████████▋ | 680/780 [04:50<00:29,  3.39it/s] 87%|████████▋ | 681/780 [04:51<00:29,  3.40it/s] 87%|████████▋ | 682/780 [04:51<00:28,  3.42it/s] 88%|████████▊ | 683/780 [04:51<00:28,  3.42it/s] 88%|████████▊ | 684/780 [04:52<00:27,  3.43it/s] 88%|████████▊ | 685/780 [04:52<00:27,  3.44it/s] 88%|████████▊ | 686/780 [04:52<00:27,  3.44it/s] 88%|████████▊ | 687/780 [04:52<00:27,  3.44it/s] 88%|████████▊ | 688/780 [04:53<00:26,  3.44it/s] 88%|████████▊ | 689/780 [04:53<00:26,  3.45it/s] 88%|████████▊ | 690/780 [04:53<00:26,  3.45it/s] 89%|████████▊ | 691/780 [04:54<00:25,  3.45it/s] 89%|████████▊ | 692/780 [04:54<00:25,  3.44it/s] 89%|████████▉ | 693/780 [04:54<00:25,  3.45it/s] 89%|████████▉ | 694/780 [04:54<00:24,  3.45it/s] 89%|████████▉ | 695/780 [04:55<00:24,  3.45it/s] 89%|████████▉ | 696/780 [04:55<00:24,  3.45it/s] 89%|████████▉ | 697/780 [04:55<00:24,  3.45it/s] 89%|████████▉ | 698/780 [04:56<00:23,  3.45it/s] 90%|████████▉ | 699/780 [04:56<00:23,  3.45it/s] 90%|████████▉ | 700/780 [04:56<00:23,  3.45it/s] 90%|████████▉ | 701/780 [04:57<00:22,  3.45it/s] 90%|█████████ | 702/780 [04:57<00:22,  3.46it/s] 90%|█████████ | 703/780 [04:57<00:22,  3.45it/s] 90%|█████████ | 704/780 [04:57<00:22,  3.45it/s] 90%|█████████ | 705/780 [04:58<00:21,  3.46it/s] 91%|█████████ | 706/780 [04:58<00:21,  3.45it/s] 91%|█████████ | 707/780 [04:58<00:21,  3.44it/s] 91%|█████████ | 708/780 [04:59<00:20,  3.45it/s] 91%|█████████ | 709/780 [04:59<00:20,  3.45it/s] 91%|█████████ | 710/780 [04:59<00:20,  3.45it/s] 91%|█████████ | 711/780 [04:59<00:19,  3.45it/s] 91%|█████████▏| 712/780 [05:00<00:19,  3.45it/s] 91%|█████████▏| 713/780 [05:00<00:19,  3.44it/s] 92%|█████████▏| 714/780 [05:00<00:19,  3.45it/s] 92%|█████████▏| 715/780 [05:01<00:18,  3.45it/s] 92%|█████████▏| 716/780 [05:01<00:18,  3.45it/s] 92%|█████████▏| 717/780 [05:01<00:18,  3.45it/s] 92%|█████████▏| 718/780 [05:01<00:17,  3.45it/s] 92%|█████████▏| 719/780 [05:02<00:17,  3.45it/s] 92%|█████████▏| 720/780 [05:02<00:17,  3.46it/s] 92%|█████████▏| 721/780 [05:02<00:17,  3.45it/s] 93%|█████████▎| 722/780 [05:03<00:16,  3.45it/s] 93%|█████████▎| 723/780 [05:03<00:16,  3.45it/s] 93%|█████████▎| 724/780 [05:03<00:16,  3.44it/s] 93%|█████████▎| 725/780 [05:03<00:15,  3.45it/s] 93%|█████████▎| 726/780 [05:04<00:15,  3.45it/s] 93%|█████████▎| 727/780 [05:04<00:15,  3.45it/s] 93%|█████████▎| 728/780 [05:04<00:15,  3.45it/s] 93%|█████████▎| 729/780 [05:05<00:14,  3.45it/s] 94%|█████████▎| 730/780 [05:05<00:14,  3.45it/s] 94%|█████████▎| 731/780 [05:05<00:14,  3.45it/s] 94%|█████████▍| 732/780 [05:05<00:13,  3.45it/s] 94%|█████████▍| 733/780 [05:06<00:13,  3.45it/s] 94%|█████████▍| 734/780 [05:06<00:13,  3.45it/s] 94%|█████████▍| 735/780 [05:06<00:13,  3.44it/s] 94%|█████████▍| 736/780 [05:07<00:12,  3.44it/s] 94%|█████████▍| 737/780 [05:07<00:12,  3.45it/s] 95%|█████████▍| 738/780 [05:07<00:12,  3.44it/s] 95%|█████████▍| 739/780 [05:08<00:11,  3.44it/s] 95%|█████████▍| 740/780 [05:08<00:11,  3.44it/s] 95%|█████████▌| 741/780 [05:08<00:11,  3.44it/s] 95%|█████████▌| 742/780 [05:08<00:11,  3.45it/s] 95%|█████████▌| 743/780 [05:09<00:10,  3.45it/s] 95%|█████████▌| 744/780 [05:09<00:10,  3.45it/s] 96%|█████████▌| 745/780 [05:09<00:10,  3.45it/s] 96%|█████████▌| 746/780 [05:10<00:09,  3.44it/s] 96%|█████████▌| 747/780 [05:10<00:09,  3.44it/s] 96%|█████████▌| 748/780 [05:10<00:09,  3.44it/s] 96%|█████████▌| 749/780 [05:10<00:08,  3.45it/s] 96%|█████████▌| 750/780 [05:11<00:08,  3.45it/s] 96%|█████████▋| 751/780 [05:11<00:08,  3.45it/s] 96%|█████████▋| 752/780 [05:11<00:08,  3.45it/s] 97%|█████████▋| 753/780 [05:12<00:07,  3.45it/s] 97%|█████████▋| 754/780 [05:12<00:07,  3.45it/s] 97%|█████████▋| 755/780 [05:12<00:07,  3.45it/s] 97%|█████████▋| 756/780 [05:12<00:06,  3.45it/s] 97%|█████████▋| 757/780 [05:13<00:06,  3.44it/s] 97%|█████████▋| 758/780 [05:13<00:06,  3.44it/s] 97%|█████████▋| 759/780 [05:13<00:06,  3.44it/s] 97%|█████████▋| 760/780 [05:14<00:05,  3.44it/s] 98%|█████████▊| 761/780 [05:14<00:05,  3.45it/s] 98%|█████████▊| 762/780 [05:14<00:05,  3.45it/s] 98%|█████████▊| 763/780 [05:14<00:04,  3.45it/s] 98%|█████████▊| 764/780 [05:15<00:04,  3.45it/s] 98%|█████████▊| 765/780 [05:15<00:04,  3.45it/s] 98%|█████████▊| 766/780 [05:15<00:04,  3.45it/s] 98%|█████████▊| 767/780 [05:16<00:03,  3.45it/s] 98%|█████████▊| 768/780 [05:16<00:03,  3.44it/s] 99%|█████████▊| 769/780 [05:16<00:03,  3.44it/s] 99%|█████████▊| 770/780 [05:17<00:02,  3.44it/s] 99%|█████████▉| 771/780 [05:17<00:02,  3.44it/s] 99%|█████████▉| 772/780 [05:17<00:02,  3.45it/s] 99%|█████████▉| 773/780 [05:17<00:02,  3.45it/s] 99%|█████████▉| 774/780 [05:18<00:01,  3.45it/s] 99%|█████████▉| 775/780 [05:18<00:01,  3.45it/s] 99%|█████████▉| 776/780 [05:18<00:01,  3.45it/s]100%|█████████▉| 777/780 [05:19<00:00,  3.45it/s]100%|█████████▉| 778/780 [05:19<00:00,  3.45it/s]100%|█████████▉| 779/780 [05:19<00:00,  3.43it/s]100%|██████████| 780/780 [05:19<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 12:38:18,790 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:38:18,790 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 12:38:18,790 >>   Batch size = 8
{'eval_loss': 0.9673099517822266, 'eval_runtime': 16.8371, 'eval_samples_per_second': 348.279, 'eval_steps_per_second': 43.535, 'epoch': 4.0}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 54.55it/s][A
  2%|▏         | 12/733 [00:00<00:15, 46.89it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.21it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.62it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.35it/s][A
  4%|▍         | 32/733 [00:00<00:15, 44.12it/s][A
  5%|▌         | 37/733 [00:00<00:15, 44.01it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.93it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.91it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.87it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.68it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.56it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.59it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.66it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.72it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.74it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.71it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.69it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.74it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.60it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.55it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.54it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.64it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.62it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.68it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.72it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.68it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.58it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.56it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.62it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.62it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.68it/s][A
 23%|██▎       | 167/733 [00:03<00:12, 43.70it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.74it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.67it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.66it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.59it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.58it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.64it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.61it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.59it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.68it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.66it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.62it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.66it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.64it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.56it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.59it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.65it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.63it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.65it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.69it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.63it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.60it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.60it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.61it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.65it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.64it/s][A
 41%|████      | 297/733 [00:06<00:09, 43.70it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.69it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.71it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.62it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.56it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.48it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.63it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.56it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.67it/s][A
 47%|████▋     | 342/733 [00:07<00:09, 43.41it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.69it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.63it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.67it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.62it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.63it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.63it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.68it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.70it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.62it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.69it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.69it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.56it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.52it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.60it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.63it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.65it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.62it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.66it/s][A
 60%|█████▉    | 437/733 [00:09<00:06, 43.74it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.65it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.57it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.53it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.64it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.65it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.64it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.68it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.68it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.70it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.64it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.62it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.44it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.56it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.70it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.69it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.63it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.72it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.57it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.59it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.57it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.57it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.68it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.69it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.66it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.74it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.68it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.65it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.61it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.57it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.58it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.67it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.72it/s][A
 82%|████████▏ | 602/733 [00:13<00:02, 43.68it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.71it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.65it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.58it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.58it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.56it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.63it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.70it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.72it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.70it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.73it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.69it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.60it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.53it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.51it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.66it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.67it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.71it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.74it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.71it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.52it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.51it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.55it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.55it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.63it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.71it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.71it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.71it/s][A100%|██████████| 780/780 [05:36<00:00,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:38:35,610 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 12:38:35,634 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:38:37,448 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:38:37,465 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:38:37,476 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 12:38:41,236 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 12:38:41,242 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156 (score: 0.9377340078353882).
                                                 100%|██████████| 780/780 [05:44<00:00,  3.44it/s]100%|██████████| 780/780 [05:44<00:00,  2.27it/s]
[INFO|trainer.py:1894] 2023-08-29 12:38:43,056 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 12:38:43,076 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:38:44,944 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:38:44,962 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:38:44,972 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 12:38:45,191 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:38:45,191 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:38:45,191 >>   train_loss               =      0.597
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:38:45,191 >>   train_runtime            = 0:05:44.19
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:38:45,192 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:38:45,192 >>   train_samples_per_second =    145.267
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:38:45,192 >>   train_steps_per_second   =      2.266
{'eval_loss': 0.9724118709564209, 'eval_runtime': 16.7999, 'eval_samples_per_second': 349.049, 'eval_steps_per_second': 43.631, 'epoch': 5.0}
{'train_runtime': 344.1934, 'train_samples_per_second': 145.267, 'train_steps_per_second': 2.266, 'train_loss': 0.5970259446364182, 'epoch': 5.0}
08/29/2023 12:38:45 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 12:38:45,233 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:38:45,233 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 12:38:45,233 >>   Batch size = 8
  0%|          | 0/733 [00:00<?, ?it/s]  1%|          | 6/733 [00:00<00:13, 55.50it/s]  2%|▏         | 12/733 [00:00<00:14, 48.14it/s]  2%|▏         | 17/733 [00:00<00:15, 46.32it/s]  3%|▎         | 22/733 [00:00<00:15, 45.70it/s]  4%|▎         | 27/733 [00:00<00:15, 45.20it/s]  4%|▍         | 32/733 [00:00<00:15, 44.89it/s]  5%|▌         | 37/733 [00:00<00:15, 44.61it/s]  6%|▌         | 42/733 [00:00<00:15, 44.02it/s]  6%|▋         | 47/733 [00:01<00:15, 43.31it/s]  7%|▋         | 52/733 [00:01<00:15, 43.33it/s]  8%|▊         | 57/733 [00:01<00:15, 43.43it/s]  8%|▊         | 62/733 [00:01<00:15, 43.58it/s]  9%|▉         | 67/733 [00:01<00:15, 43.82it/s] 10%|▉         | 72/733 [00:01<00:15, 43.91it/s] 11%|█         | 77/733 [00:01<00:14, 43.95it/s] 11%|█         | 82/733 [00:01<00:14, 43.81it/s] 12%|█▏        | 87/733 [00:01<00:14, 43.49it/s] 13%|█▎        | 92/733 [00:02<00:14, 43.19it/s] 13%|█▎        | 97/733 [00:02<00:14, 43.15it/s] 14%|█▍        | 102/733 [00:02<00:14, 43.31it/s] 15%|█▍        | 107/733 [00:02<00:14, 43.51it/s] 15%|█▌        | 112/733 [00:02<00:14, 43.69it/s] 16%|█▌        | 117/733 [00:02<00:14, 43.68it/s] 17%|█▋        | 122/733 [00:02<00:13, 43.91it/s] 17%|█▋        | 127/733 [00:02<00:13, 43.77it/s] 18%|█▊        | 132/733 [00:02<00:13, 43.26it/s] 19%|█▊        | 137/733 [00:03<00:13, 43.13it/s] 19%|█▉        | 142/733 [00:03<00:13, 43.22it/s] 20%|██        | 147/733 [00:03<00:13, 43.43it/s] 21%|██        | 152/733 [00:03<00:13, 43.66it/s] 21%|██▏       | 157/733 [00:03<00:13, 43.74it/s] 22%|██▏       | 162/733 [00:03<00:13, 43.83it/s] 23%|██▎       | 167/733 [00:03<00:12, 43.81it/s] 23%|██▎       | 172/733 [00:03<00:12, 43.64it/s] 24%|██▍       | 177/733 [00:04<00:12, 43.31it/s] 25%|██▍       | 182/733 [00:04<00:12, 43.39it/s] 26%|██▌       | 187/733 [00:04<00:13, 40.38it/s] 26%|██▌       | 192/733 [00:04<00:12, 41.72it/s] 27%|██▋       | 197/733 [00:04<00:12, 42.43it/s] 28%|██▊       | 202/733 [00:04<00:12, 42.93it/s] 28%|██▊       | 207/733 [00:04<00:12, 43.31it/s] 29%|██▉       | 212/733 [00:04<00:11, 43.47it/s] 30%|██▉       | 217/733 [00:04<00:11, 43.43it/s] 30%|███       | 222/733 [00:05<00:11, 43.38it/s] 31%|███       | 227/733 [00:05<00:11, 43.27it/s] 32%|███▏      | 232/733 [00:05<00:11, 43.29it/s] 32%|███▏      | 237/733 [00:05<00:11, 43.50it/s] 33%|███▎      | 242/733 [00:05<00:11, 43.75it/s] 34%|███▎      | 247/733 [00:05<00:11, 43.83it/s] 34%|███▍      | 252/733 [00:05<00:10, 44.01it/s] 35%|███▌      | 257/733 [00:05<00:10, 44.05it/s] 36%|███▌      | 262/733 [00:05<00:10, 43.83it/s] 36%|███▋      | 267/733 [00:06<00:10, 43.59it/s] 37%|███▋      | 272/733 [00:06<00:10, 43.47it/s] 38%|███▊      | 277/733 [00:06<00:10, 43.37it/s] 38%|███▊      | 282/733 [00:06<00:10, 43.49it/s] 39%|███▉      | 287/733 [00:06<00:10, 43.51it/s] 40%|███▉      | 292/733 [00:06<00:10, 43.80it/s] 41%|████      | 297/733 [00:06<00:09, 44.03it/s] 41%|████      | 302/733 [00:06<00:09, 44.04it/s] 42%|████▏     | 307/733 [00:07<00:09, 43.87it/s] 43%|████▎     | 312/733 [00:07<00:09, 43.74it/s] 43%|████▎     | 317/733 [00:07<00:09, 43.65it/s] 44%|████▍     | 322/733 [00:07<00:09, 43.58it/s] 45%|████▍     | 327/733 [00:07<00:09, 43.59it/s] 45%|████▌     | 332/733 [00:07<00:09, 43.77it/s] 46%|████▌     | 337/733 [00:07<00:09, 43.92it/s] 47%|████▋     | 342/733 [00:07<00:08, 43.89it/s] 47%|████▋     | 347/733 [00:07<00:08, 43.96it/s] 48%|████▊     | 352/733 [00:08<00:08, 43.84it/s] 49%|████▊     | 357/733 [00:08<00:08, 43.69it/s] 49%|████▉     | 362/733 [00:08<00:08, 43.51it/s] 50%|█████     | 367/733 [00:08<00:08, 43.61it/s] 51%|█████     | 372/733 [00:08<00:08, 43.68it/s] 51%|█████▏    | 377/733 [00:08<00:08, 43.73it/s] 52%|█████▏    | 382/733 [00:08<00:08, 43.87it/s] 53%|█████▎    | 387/733 [00:08<00:07, 43.97it/s] 53%|█████▎    | 392/733 [00:08<00:07, 43.89it/s] 54%|█████▍    | 397/733 [00:09<00:07, 43.72it/s] 55%|█████▍    | 402/733 [00:09<00:07, 43.69it/s] 56%|█████▌    | 407/733 [00:09<00:07, 43.59it/s] 56%|█████▌    | 412/733 [00:09<00:07, 43.63it/s] 57%|█████▋    | 417/733 [00:09<00:07, 43.73it/s] 58%|█████▊    | 422/733 [00:09<00:07, 43.78it/s] 58%|█████▊    | 427/733 [00:09<00:06, 43.89it/s] 59%|█████▉    | 432/733 [00:09<00:06, 43.94it/s] 60%|█████▉    | 437/733 [00:09<00:06, 43.86it/s] 60%|██████    | 442/733 [00:10<00:06, 43.76it/s] 61%|██████    | 447/733 [00:10<00:06, 43.67it/s] 62%|██████▏   | 452/733 [00:10<00:06, 43.65it/s] 62%|██████▏   | 457/733 [00:10<00:06, 43.71it/s] 63%|██████▎   | 462/733 [00:10<00:06, 43.75it/s] 64%|██████▎   | 467/733 [00:10<00:06, 43.77it/s] 64%|██████▍   | 472/733 [00:10<00:05, 43.93it/s] 65%|██████▌   | 477/733 [00:10<00:05, 43.95it/s] 66%|██████▌   | 482/733 [00:11<00:05, 43.83it/s] 66%|██████▋   | 487/733 [00:11<00:05, 43.69it/s] 67%|██████▋   | 492/733 [00:11<00:05, 43.68it/s] 68%|██████▊   | 497/733 [00:11<00:05, 43.68it/s] 68%|██████▊   | 502/733 [00:11<00:05, 43.66it/s] 69%|██████▉   | 507/733 [00:11<00:05, 43.71it/s] 70%|██████▉   | 512/733 [00:11<00:05, 43.84it/s] 71%|███████   | 517/733 [00:11<00:04, 43.83it/s] 71%|███████   | 522/733 [00:11<00:04, 43.92it/s] 72%|███████▏  | 527/733 [00:12<00:04, 43.77it/s] 73%|███████▎  | 532/733 [00:12<00:04, 43.69it/s] 73%|███████▎  | 537/733 [00:12<00:04, 43.62it/s] 74%|███████▍  | 542/733 [00:12<00:04, 43.63it/s] 75%|███████▍  | 547/733 [00:12<00:04, 43.70it/s] 75%|███████▌  | 552/733 [00:12<00:04, 43.71it/s] 76%|███████▌  | 557/733 [00:12<00:04, 43.83it/s] 77%|███████▋  | 562/733 [00:12<00:03, 43.84it/s] 77%|███████▋  | 567/733 [00:12<00:03, 43.83it/s] 78%|███████▊  | 572/733 [00:13<00:03, 43.74it/s] 79%|███████▊  | 577/733 [00:13<00:03, 43.70it/s] 79%|███████▉  | 582/733 [00:13<00:03, 43.72it/s] 80%|████████  | 587/733 [00:13<00:03, 43.69it/s] 81%|████████  | 592/733 [00:13<00:03, 43.70it/s] 81%|████████▏ | 597/733 [00:13<00:03, 43.76it/s] 82%|████████▏ | 602/733 [00:13<00:02, 43.78it/s] 83%|████████▎ | 607/733 [00:13<00:02, 43.80it/s] 83%|████████▎ | 612/733 [00:13<00:02, 43.80it/s] 84%|████████▍ | 617/733 [00:14<00:02, 43.74it/s] 85%|████████▍ | 622/733 [00:14<00:02, 43.73it/s] 86%|████████▌ | 627/733 [00:14<00:02, 43.70it/s] 86%|████████▌ | 632/733 [00:14<00:02, 43.68it/s] 87%|████████▋ | 637/733 [00:14<00:02, 43.76it/s] 88%|████████▊ | 642/733 [00:14<00:02, 43.67it/s] 88%|████████▊ | 647/733 [00:14<00:01, 43.77it/s] 89%|████████▉ | 652/733 [00:14<00:01, 43.67it/s] 90%|████████▉ | 657/733 [00:15<00:01, 43.74it/s] 90%|█████████ | 662/733 [00:15<00:01, 43.71it/s] 91%|█████████ | 667/733 [00:15<00:01, 43.83it/s] 92%|█████████▏| 672/733 [00:15<00:01, 43.82it/s] 92%|█████████▏| 677/733 [00:15<00:01, 43.76it/s] 93%|█████████▎| 682/733 [00:15<00:01, 43.79it/s] 94%|█████████▎| 687/733 [00:15<00:01, 43.82it/s] 94%|█████████▍| 692/733 [00:15<00:00, 43.77it/s] 95%|█████████▌| 697/733 [00:15<00:00, 43.80it/s] 96%|█████████▌| 702/733 [00:16<00:00, 43.77it/s] 96%|█████████▋| 707/733 [00:16<00:00, 43.67it/s] 97%|█████████▋| 712/733 [00:16<00:00, 43.64it/s] 98%|█████████▊| 717/733 [00:16<00:00, 43.73it/s] 98%|█████████▊| 722/733 [00:16<00:00, 43.47it/s] 99%|█████████▉| 727/733 [00:16<00:00, 43.68it/s]100%|█████████▉| 732/733 [00:16<00:00, 43.78it/s]100%|██████████| 733/733 [00:16<00:00, 43.68it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 12:39:02,031 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:39:02,031 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:39:02,031 >>   eval_loss               =     0.9377
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:39:02,031 >>   eval_runtime            = 0:00:16.79
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:39:02,031 >>   eval_samples            =       5864
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:39:02,031 >>   eval_samples_per_second =    349.108
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:39:02,031 >>   eval_steps_per_second   =     43.638
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:39:02,031 >>   perplexity              =     2.5542
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:08,868 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:08,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:08,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:08,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:08,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:39:09,478 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:39:09,478 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:39:10,051 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:39:11,068 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:39:11,068 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:13,906 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:13,913 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:13,913 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:13,913 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:13,913 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:39:14,541 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:39:14,543 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:39:15,108 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:39:15,274 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:39:15,274 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl', 'labels': ['inception', 'located on terrain feature', 'military branch', 'occupant', 'occupation'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14932
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15032, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.76it/s]Extractor Predicting: 6it [00:03,  1.81it/s]Extractor Predicting: 7it [00:04,  1.83it/s]Extractor Predicting: 8it [00:04,  1.85it/s]Extractor Predicting: 9it [00:05,  1.81it/s]Extractor Predicting: 10it [00:05,  1.82it/s]Extractor Predicting: 11it [00:06,  1.82it/s]Extractor Predicting: 12it [00:06,  1.80it/s]Extractor Predicting: 13it [00:07,  1.80it/s]Extractor Predicting: 14it [00:07,  1.79it/s]Extractor Predicting: 15it [00:08,  1.79it/s]Extractor Predicting: 16it [00:09,  1.76it/s]Extractor Predicting: 17it [00:09,  1.75it/s]Extractor Predicting: 18it [00:10,  1.80it/s]Extractor Predicting: 19it [00:10,  1.81it/s]Extractor Predicting: 20it [00:11,  1.81it/s]Extractor Predicting: 21it [00:11,  1.83it/s]Extractor Predicting: 22it [00:12,  1.84it/s]Extractor Predicting: 23it [00:12,  1.86it/s]Extractor Predicting: 24it [00:13,  1.91it/s]Extractor Predicting: 25it [00:13,  1.92it/s]Extractor Predicting: 26it [00:14,  1.85it/s]Extractor Predicting: 27it [00:14,  1.87it/s]Extractor Predicting: 28it [00:15,  1.84it/s]Extractor Predicting: 29it [00:16,  1.84it/s]Extractor Predicting: 30it [00:16,  1.82it/s]Extractor Predicting: 31it [00:17,  1.83it/s]Extractor Predicting: 32it [00:17,  1.84it/s]Extractor Predicting: 33it [00:18,  1.80it/s]Extractor Predicting: 34it [00:18,  1.83it/s]Extractor Predicting: 35it [00:19,  1.78it/s]Extractor Predicting: 36it [00:19,  1.77it/s]Extractor Predicting: 37it [00:20,  1.75it/s]Extractor Predicting: 38it [00:21,  1.79it/s]Extractor Predicting: 39it [00:21,  1.81it/s]Extractor Predicting: 40it [00:22,  1.89it/s]Extractor Predicting: 41it [00:22,  1.82it/s]Extractor Predicting: 42it [00:23,  1.77it/s]Extractor Predicting: 43it [00:23,  1.69it/s]Extractor Predicting: 44it [00:24,  1.64it/s]Extractor Predicting: 45it [00:25,  1.63it/s]Extractor Predicting: 46it [00:25,  1.67it/s]Extractor Predicting: 47it [00:26,  1.62it/s]Extractor Predicting: 48it [00:27,  1.66it/s]Extractor Predicting: 49it [00:27,  1.65it/s]Extractor Predicting: 50it [00:28,  1.59it/s]Extractor Predicting: 51it [00:28,  1.66it/s]Extractor Predicting: 52it [00:29,  1.62it/s]Extractor Predicting: 53it [00:30,  1.65it/s]Extractor Predicting: 54it [00:30,  1.65it/s]Extractor Predicting: 55it [00:31,  1.67it/s]Extractor Predicting: 56it [00:31,  1.67it/s]Extractor Predicting: 57it [00:32,  1.68it/s]Extractor Predicting: 58it [00:33,  1.68it/s]Extractor Predicting: 59it [00:33,  1.67it/s]Extractor Predicting: 60it [00:34,  1.68it/s]Extractor Predicting: 61it [00:34,  1.68it/s]Extractor Predicting: 62it [00:35,  1.67it/s]Extractor Predicting: 63it [00:35,  1.73it/s]Extractor Predicting: 64it [00:36,  1.69it/s]Extractor Predicting: 65it [00:37,  1.70it/s]Extractor Predicting: 66it [00:37,  1.69it/s]Extractor Predicting: 67it [00:38,  1.71it/s]Extractor Predicting: 68it [00:38,  1.71it/s]Extractor Predicting: 69it [00:39,  1.70it/s]Extractor Predicting: 70it [00:40,  1.72it/s]Extractor Predicting: 71it [00:40,  1.72it/s]Extractor Predicting: 72it [00:41,  1.74it/s]Extractor Predicting: 73it [00:41,  1.72it/s]Extractor Predicting: 74it [00:42,  1.73it/s]Extractor Predicting: 75it [00:43,  1.70it/s]Extractor Predicting: 76it [00:43,  1.74it/s]Extractor Predicting: 77it [00:44,  1.75it/s]Extractor Predicting: 78it [00:44,  1.72it/s]Extractor Predicting: 79it [00:45,  1.70it/s]Extractor Predicting: 80it [00:45,  1.71it/s]Extractor Predicting: 81it [00:46,  1.72it/s]Extractor Predicting: 82it [00:47,  1.72it/s]Extractor Predicting: 83it [00:47,  1.72it/s]Extractor Predicting: 84it [00:48,  1.71it/s]Extractor Predicting: 85it [00:48,  1.74it/s]Extractor Predicting: 86it [00:49,  1.57it/s]Extractor Predicting: 87it [00:50,  1.57it/s]Extractor Predicting: 88it [00:50,  1.60it/s]Extractor Predicting: 89it [00:51,  1.65it/s]Extractor Predicting: 90it [00:51,  1.68it/s]Extractor Predicting: 91it [00:52,  1.70it/s]Extractor Predicting: 92it [00:53,  1.70it/s]Extractor Predicting: 93it [00:53,  1.69it/s]Extractor Predicting: 94it [00:54,  1.71it/s]Extractor Predicting: 95it [00:54,  1.70it/s]Extractor Predicting: 96it [00:55,  1.68it/s]Extractor Predicting: 97it [00:56,  1.67it/s]Extractor Predicting: 98it [00:56,  1.64it/s]Extractor Predicting: 99it [00:57,  1.67it/s]Extractor Predicting: 100it [00:57,  1.66it/s]Extractor Predicting: 101it [00:58,  1.67it/s]Extractor Predicting: 102it [00:59,  1.67it/s]Extractor Predicting: 103it [00:59,  1.67it/s]Extractor Predicting: 104it [01:00,  1.67it/s]Extractor Predicting: 105it [01:00,  1.67it/s]Extractor Predicting: 106it [01:01,  1.70it/s]Extractor Predicting: 107it [01:02,  1.70it/s]Extractor Predicting: 108it [01:02,  1.72it/s]Extractor Predicting: 109it [01:03,  1.71it/s]Extractor Predicting: 110it [01:03,  1.72it/s]Extractor Predicting: 111it [01:04,  1.74it/s]Extractor Predicting: 112it [01:04,  1.72it/s]Extractor Predicting: 113it [01:05,  1.72it/s]Extractor Predicting: 114it [01:06,  1.74it/s]Extractor Predicting: 115it [01:06,  1.71it/s]Extractor Predicting: 116it [01:07,  1.73it/s]Extractor Predicting: 117it [01:07,  1.73it/s]Extractor Predicting: 118it [01:08,  1.73it/s]Extractor Predicting: 119it [01:09,  1.71it/s]Extractor Predicting: 120it [01:09,  1.69it/s]Extractor Predicting: 121it [01:10,  1.72it/s]Extractor Predicting: 122it [01:10,  1.74it/s]Extractor Predicting: 123it [01:11,  1.73it/s]Extractor Predicting: 124it [01:11,  1.75it/s]Extractor Predicting: 125it [01:12,  1.70it/s]Extractor Predicting: 126it [01:13,  1.69it/s]Extractor Predicting: 127it [01:13,  1.69it/s]Extractor Predicting: 128it [01:14,  1.71it/s]Extractor Predicting: 129it [01:14,  1.72it/s]Extractor Predicting: 130it [01:15,  1.72it/s]Extractor Predicting: 131it [01:15,  1.73it/s]Extractor Predicting: 132it [01:16,  1.72it/s]Extractor Predicting: 133it [01:17,  1.75it/s]Extractor Predicting: 134it [01:17,  1.70it/s]Extractor Predicting: 135it [01:18,  1.69it/s]Extractor Predicting: 136it [01:18,  1.70it/s]Extractor Predicting: 137it [01:19,  1.70it/s]Extractor Predicting: 138it [01:20,  1.65it/s]Extractor Predicting: 139it [01:20,  1.66it/s]Extractor Predicting: 140it [01:21,  1.65it/s]Extractor Predicting: 141it [01:22,  1.64it/s]Extractor Predicting: 142it [01:22,  1.60it/s]Extractor Predicting: 143it [01:23,  1.62it/s]Extractor Predicting: 144it [01:23,  1.64it/s]Extractor Predicting: 145it [01:24,  1.66it/s]Extractor Predicting: 146it [01:25,  1.64it/s]Extractor Predicting: 147it [01:25,  1.64it/s]Extractor Predicting: 148it [01:26,  1.64it/s]Extractor Predicting: 149it [01:26,  1.62it/s]Extractor Predicting: 150it [01:27,  1.63it/s]Extractor Predicting: 151it [01:28,  1.63it/s]Extractor Predicting: 152it [01:28,  1.64it/s]Extractor Predicting: 153it [01:29,  1.66it/s]Extractor Predicting: 154it [01:29,  1.66it/s]Extractor Predicting: 155it [01:30,  1.63it/s]Extractor Predicting: 156it [01:31,  1.67it/s]Extractor Predicting: 157it [01:31,  1.67it/s]Extractor Predicting: 158it [01:32,  1.68it/s]Extractor Predicting: 159it [01:32,  1.64it/s]Extractor Predicting: 160it [01:33,  1.66it/s]Extractor Predicting: 161it [01:34,  1.68it/s]Extractor Predicting: 162it [01:35,  1.45it/s]Extractor Predicting: 163it [01:35,  1.48it/s]Extractor Predicting: 164it [01:36,  1.50it/s]Extractor Predicting: 165it [01:36,  1.54it/s]Extractor Predicting: 166it [01:37,  1.58it/s]Extractor Predicting: 167it [01:38,  1.59it/s]Extractor Predicting: 168it [01:38,  1.59it/s]Extractor Predicting: 169it [01:39,  1.60it/s]Extractor Predicting: 170it [01:40,  1.56it/s]Extractor Predicting: 171it [01:40,  1.61it/s]Extractor Predicting: 172it [01:41,  1.61it/s]Extractor Predicting: 173it [01:41,  1.59it/s]Extractor Predicting: 174it [01:42,  1.59it/s]Extractor Predicting: 175it [01:43,  1.61it/s]Extractor Predicting: 176it [01:43,  1.61it/s]Extractor Predicting: 177it [01:44,  1.58it/s]Extractor Predicting: 178it [01:45,  1.59it/s]Extractor Predicting: 179it [01:45,  1.58it/s]Extractor Predicting: 180it [01:46,  1.57it/s]Extractor Predicting: 181it [01:46,  1.55it/s]Extractor Predicting: 182it [01:47,  1.55it/s]Extractor Predicting: 183it [01:48,  1.61it/s]Extractor Predicting: 184it [01:48,  1.67it/s]Extractor Predicting: 185it [01:49,  1.67it/s]Extractor Predicting: 186it [01:49,  1.67it/s]Extractor Predicting: 187it [01:50,  1.61it/s]Extractor Predicting: 188it [01:51,  1.63it/s]Extractor Predicting: 189it [01:51,  1.67it/s]Extractor Predicting: 190it [01:52,  1.70it/s]Extractor Predicting: 191it [01:52,  1.70it/s]Extractor Predicting: 192it [01:53,  1.68it/s]Extractor Predicting: 193it [01:54,  1.69it/s]Extractor Predicting: 194it [01:54,  1.67it/s]Extractor Predicting: 195it [01:55,  1.69it/s]Extractor Predicting: 196it [01:55,  1.70it/s]Extractor Predicting: 197it [01:56,  1.72it/s]Extractor Predicting: 198it [01:57,  1.68it/s]Extractor Predicting: 199it [01:57,  1.66it/s]Extractor Predicting: 200it [01:58,  1.68it/s]Extractor Predicting: 201it [01:58,  1.69it/s]Extractor Predicting: 202it [01:59,  1.73it/s]Extractor Predicting: 203it [01:59,  1.74it/s]Extractor Predicting: 204it [02:00,  1.72it/s]Extractor Predicting: 205it [02:01,  1.71it/s]Extractor Predicting: 206it [02:01,  1.71it/s]Extractor Predicting: 207it [02:02,  1.72it/s]Extractor Predicting: 208it [02:02,  1.71it/s]Extractor Predicting: 209it [02:03,  1.74it/s]Extractor Predicting: 210it [02:04,  1.68it/s]Extractor Predicting: 211it [02:04,  1.70it/s]Extractor Predicting: 212it [02:05,  1.67it/s]Extractor Predicting: 213it [02:05,  1.66it/s]Extractor Predicting: 214it [02:06,  1.66it/s]Extractor Predicting: 215it [02:07,  1.64it/s]Extractor Predicting: 216it [02:07,  1.62it/s]Extractor Predicting: 217it [02:08,  1.64it/s]Extractor Predicting: 218it [02:08,  1.65it/s]Extractor Predicting: 219it [02:09,  1.67it/s]Extractor Predicting: 220it [02:10,  1.71it/s]Extractor Predicting: 221it [02:10,  1.73it/s]Extractor Predicting: 222it [02:11,  1.73it/s]Extractor Predicting: 223it [02:11,  1.67it/s]Extractor Predicting: 224it [02:12,  1.88it/s]Extractor Predicting: 224it [02:12,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:41:37,671 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:41:37,675 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:41:37,675 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:41:37,675 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:41:37,675 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:41:38,309 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:41:38,310 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:41:38,874 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:41:39,865 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:41:39,865 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:41:42,794 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:41:42,801 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:41:42,801 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:41:42,801 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:41:42,801 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:41:43,430 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:41:43,431 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:41:44,002 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:41:44,156 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:41:44,156 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3837638376383764,
  "recall": 0.05320600272851296,
  "score": 0.09345514452598473,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'father', 'follows', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'part of the series', 'place of birth', 'production company', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 30214
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 30314, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.81it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.74it/s]Extractor Predicting: 6it [00:03,  1.72it/s]Extractor Predicting: 7it [00:04,  1.71it/s]Extractor Predicting: 8it [00:04,  1.71it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:05,  1.72it/s]Extractor Predicting: 11it [00:06,  1.72it/s]Extractor Predicting: 12it [00:06,  1.72it/s]Extractor Predicting: 13it [00:07,  1.76it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:08,  1.64it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:10,  1.65it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:11,  1.67it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:13,  1.71it/s]Extractor Predicting: 24it [00:14,  1.73it/s]Extractor Predicting: 25it [00:14,  1.75it/s]Extractor Predicting: 26it [00:15,  1.76it/s]Extractor Predicting: 27it [00:15,  1.75it/s]Extractor Predicting: 28it [00:16,  1.77it/s]Extractor Predicting: 29it [00:16,  1.75it/s]Extractor Predicting: 30it [00:17,  1.74it/s]Extractor Predicting: 31it [00:18,  1.77it/s]Extractor Predicting: 32it [00:18,  1.72it/s]Extractor Predicting: 33it [00:19,  1.70it/s]Extractor Predicting: 34it [00:19,  1.68it/s]Extractor Predicting: 35it [00:20,  1.68it/s]Extractor Predicting: 36it [00:21,  1.62it/s]Extractor Predicting: 37it [00:21,  1.70it/s]Extractor Predicting: 38it [00:22,  1.69it/s]Extractor Predicting: 39it [00:22,  1.71it/s]Extractor Predicting: 40it [00:23,  1.69it/s]Extractor Predicting: 41it [00:24,  1.68it/s]Extractor Predicting: 42it [00:24,  1.70it/s]Extractor Predicting: 43it [00:25,  1.69it/s]Extractor Predicting: 44it [00:25,  1.69it/s]Extractor Predicting: 45it [00:26,  1.65it/s]Extractor Predicting: 46it [00:27,  1.65it/s]Extractor Predicting: 47it [00:27,  1.64it/s]Extractor Predicting: 48it [00:28,  1.64it/s]Extractor Predicting: 49it [00:28,  1.68it/s]Extractor Predicting: 50it [00:29,  1.76it/s]Extractor Predicting: 51it [00:29,  1.77it/s]Extractor Predicting: 52it [00:30,  1.78it/s]Extractor Predicting: 53it [00:31,  1.73it/s]Extractor Predicting: 54it [00:31,  1.74it/s]Extractor Predicting: 55it [00:32,  1.76it/s]Extractor Predicting: 56it [00:32,  1.76it/s]Extractor Predicting: 57it [00:33,  1.75it/s]Extractor Predicting: 58it [00:33,  1.79it/s]Extractor Predicting: 59it [00:34,  1.85it/s]Extractor Predicting: 60it [00:34,  1.87it/s]Extractor Predicting: 61it [00:35,  1.79it/s]Extractor Predicting: 62it [00:36,  1.83it/s]Extractor Predicting: 63it [00:36,  1.82it/s]Extractor Predicting: 64it [00:37,  1.89it/s]Extractor Predicting: 65it [00:37,  1.88it/s]Extractor Predicting: 66it [00:38,  1.85it/s]Extractor Predicting: 67it [00:38,  1.80it/s]Extractor Predicting: 68it [00:39,  1.85it/s]Extractor Predicting: 69it [00:39,  1.83it/s]Extractor Predicting: 70it [00:40,  1.80it/s]Extractor Predicting: 71it [00:40,  1.84it/s]Extractor Predicting: 72it [00:41,  1.82it/s]Extractor Predicting: 73it [00:42,  1.86it/s]Extractor Predicting: 74it [00:42,  1.85it/s]Extractor Predicting: 75it [00:43,  1.91it/s]Extractor Predicting: 76it [00:43,  1.90it/s]Extractor Predicting: 77it [00:44,  1.81it/s]Extractor Predicting: 78it [00:44,  1.83it/s]Extractor Predicting: 79it [00:45,  1.83it/s]Extractor Predicting: 80it [00:45,  1.77it/s]Extractor Predicting: 81it [00:46,  1.76it/s]Extractor Predicting: 82it [00:47,  1.76it/s]Extractor Predicting: 83it [00:47,  1.74it/s]Extractor Predicting: 84it [00:48,  1.77it/s]Extractor Predicting: 85it [00:48,  1.82it/s]Extractor Predicting: 86it [00:49,  1.85it/s]Extractor Predicting: 87it [00:49,  1.87it/s]Extractor Predicting: 88it [00:50,  1.85it/s]Extractor Predicting: 89it [00:50,  1.82it/s]Extractor Predicting: 90it [00:51,  1.76it/s]Extractor Predicting: 91it [00:52,  1.73it/s]Extractor Predicting: 92it [00:52,  1.77it/s]Extractor Predicting: 93it [00:53,  1.85it/s]Extractor Predicting: 94it [00:53,  1.85it/s]Extractor Predicting: 95it [00:54,  1.86it/s]Extractor Predicting: 96it [00:54,  1.87it/s]Extractor Predicting: 97it [00:55,  1.95it/s]Extractor Predicting: 98it [00:55,  1.95it/s]Extractor Predicting: 99it [00:56,  1.94it/s]Extractor Predicting: 100it [00:56,  1.95it/s]Extractor Predicting: 101it [00:57,  1.96it/s]Extractor Predicting: 102it [00:57,  1.91it/s]Extractor Predicting: 103it [00:58,  1.85it/s]Extractor Predicting: 104it [00:58,  1.89it/s]Extractor Predicting: 105it [00:59,  1.89it/s]Extractor Predicting: 106it [00:59,  1.91it/s]Extractor Predicting: 107it [01:00,  1.93it/s]Extractor Predicting: 108it [01:00,  1.92it/s]Extractor Predicting: 109it [01:01,  1.92it/s]Extractor Predicting: 110it [01:01,  1.89it/s]Extractor Predicting: 111it [01:02,  1.93it/s]Extractor Predicting: 112it [01:02,  1.94it/s]Extractor Predicting: 113it [01:03,  1.95it/s]Extractor Predicting: 114it [01:03,  1.93it/s]Extractor Predicting: 115it [01:04,  1.95it/s]Extractor Predicting: 116it [01:05,  1.91it/s]Extractor Predicting: 117it [01:05,  1.94it/s]Extractor Predicting: 118it [01:06,  1.87it/s]Extractor Predicting: 119it [01:06,  1.85it/s]Extractor Predicting: 120it [01:07,  1.87it/s]Extractor Predicting: 121it [01:07,  1.85it/s]Extractor Predicting: 122it [01:08,  1.84it/s]Extractor Predicting: 123it [01:08,  1.83it/s]Extractor Predicting: 124it [01:09,  1.81it/s]Extractor Predicting: 125it [01:09,  1.82it/s]Extractor Predicting: 126it [01:10,  1.83it/s]Extractor Predicting: 127it [01:10,  1.88it/s]Extractor Predicting: 128it [01:11,  1.83it/s]Extractor Predicting: 129it [01:12,  1.67it/s]Extractor Predicting: 130it [01:12,  1.65it/s]Extractor Predicting: 131it [01:13,  1.67it/s]Extractor Predicting: 132it [01:14,  1.68it/s]Extractor Predicting: 133it [01:14,  1.69it/s]Extractor Predicting: 134it [01:15,  1.73it/s]Extractor Predicting: 135it [01:15,  1.75it/s]Extractor Predicting: 136it [01:16,  1.77it/s]Extractor Predicting: 137it [01:17,  1.54it/s]Extractor Predicting: 138it [01:17,  1.59it/s]Extractor Predicting: 139it [01:18,  1.63it/s]Extractor Predicting: 140it [01:18,  1.68it/s]Extractor Predicting: 141it [01:19,  1.63it/s]Extractor Predicting: 142it [01:20,  1.67it/s]Extractor Predicting: 143it [01:20,  1.55it/s]Extractor Predicting: 144it [01:21,  1.60it/s]Extractor Predicting: 145it [01:22,  1.63it/s]Extractor Predicting: 146it [01:22,  1.60it/s]Extractor Predicting: 147it [01:23,  1.61it/s]Extractor Predicting: 148it [01:23,  1.65it/s]Extractor Predicting: 149it [01:24,  1.66it/s]Extractor Predicting: 150it [01:25,  1.67it/s]Extractor Predicting: 151it [01:25,  1.66it/s]Extractor Predicting: 152it [01:26,  1.68it/s]Extractor Predicting: 153it [01:26,  1.70it/s]Extractor Predicting: 154it [01:27,  1.71it/s]Extractor Predicting: 155it [01:27,  1.71it/s]Extractor Predicting: 156it [01:28,  1.70it/s]Extractor Predicting: 157it [01:29,  1.71it/s]Extractor Predicting: 158it [01:29,  1.74it/s]Extractor Predicting: 159it [01:30,  1.74it/s]Extractor Predicting: 160it [01:30,  1.68it/s]Extractor Predicting: 161it [01:31,  1.68it/s]Extractor Predicting: 162it [01:32,  1.68it/s]Extractor Predicting: 163it [01:32,  1.66it/s]Extractor Predicting: 164it [01:33,  1.68it/s]Extractor Predicting: 165it [01:33,  1.66it/s]Extractor Predicting: 166it [01:34,  1.68it/s]Extractor Predicting: 167it [01:35,  1.67it/s]Extractor Predicting: 168it [01:35,  1.63it/s]Extractor Predicting: 169it [01:36,  1.62it/s]Extractor Predicting: 170it [01:36,  1.66it/s]Extractor Predicting: 171it [01:37,  1.59it/s]Extractor Predicting: 172it [01:38,  1.61it/s]Extractor Predicting: 173it [01:38,  1.63it/s]Extractor Predicting: 174it [01:39,  1.64it/s]Extractor Predicting: 175it [01:39,  1.67it/s]Extractor Predicting: 176it [01:40,  1.70it/s]Extractor Predicting: 177it [01:41,  1.72it/s]Extractor Predicting: 178it [01:41,  1.74it/s]Extractor Predicting: 179it [01:42,  1.76it/s]Extractor Predicting: 180it [01:42,  1.77it/s]Extractor Predicting: 181it [01:43,  1.77it/s]Extractor Predicting: 182it [01:43,  1.81it/s]Extractor Predicting: 183it [01:44,  1.76it/s]Extractor Predicting: 184it [01:45,  1.80it/s]Extractor Predicting: 185it [01:45,  1.76it/s]Extractor Predicting: 186it [01:46,  1.77it/s]Extractor Predicting: 187it [01:46,  1.79it/s]Extractor Predicting: 188it [01:47,  1.79it/s]Extractor Predicting: 189it [01:47,  1.74it/s]Extractor Predicting: 190it [01:48,  1.72it/s]Extractor Predicting: 191it [01:49,  1.73it/s]Extractor Predicting: 192it [01:49,  1.77it/s]Extractor Predicting: 193it [01:50,  1.73it/s]Extractor Predicting: 194it [01:50,  1.73it/s]Extractor Predicting: 195it [01:51,  1.78it/s]Extractor Predicting: 196it [01:51,  1.80it/s]Extractor Predicting: 197it [01:52,  1.78it/s]Extractor Predicting: 198it [01:52,  1.80it/s]Extractor Predicting: 199it [01:53,  1.82it/s]Extractor Predicting: 200it [01:54,  1.78it/s]Extractor Predicting: 201it [01:54,  1.80it/s]Extractor Predicting: 202it [01:55,  1.77it/s]Extractor Predicting: 203it [01:55,  1.77it/s]Extractor Predicting: 204it [01:56,  1.75it/s]Extractor Predicting: 205it [01:56,  1.77it/s]Extractor Predicting: 206it [01:57,  1.75it/s]Extractor Predicting: 207it [01:58,  1.76it/s]Extractor Predicting: 208it [01:58,  1.78it/s]Extractor Predicting: 209it [01:59,  1.75it/s]Extractor Predicting: 210it [01:59,  1.76it/s]Extractor Predicting: 211it [02:00,  1.71it/s]Extractor Predicting: 212it [02:00,  1.74it/s]Extractor Predicting: 213it [02:01,  1.80it/s]Extractor Predicting: 214it [02:02,  1.74it/s]Extractor Predicting: 215it [02:02,  1.73it/s]Extractor Predicting: 216it [02:03,  1.71it/s]Extractor Predicting: 217it [02:03,  1.77it/s]Extractor Predicting: 218it [02:04,  1.76it/s]Extractor Predicting: 219it [02:04,  1.72it/s]Extractor Predicting: 220it [02:05,  1.74it/s]Extractor Predicting: 221it [02:06,  1.77it/s]Extractor Predicting: 222it [02:06,  1.78it/s]Extractor Predicting: 223it [02:07,  1.76it/s]Extractor Predicting: 224it [02:07,  1.80it/s]Extractor Predicting: 225it [02:08,  1.75it/s]Extractor Predicting: 226it [02:08,  1.70it/s]Extractor Predicting: 227it [02:09,  1.76it/s]Extractor Predicting: 228it [02:10,  1.75it/s]Extractor Predicting: 229it [02:10,  1.74it/s]Extractor Predicting: 230it [02:11,  1.77it/s]Extractor Predicting: 231it [02:11,  1.72it/s]Extractor Predicting: 232it [02:12,  1.74it/s]Extractor Predicting: 233it [02:12,  1.74it/s]Extractor Predicting: 234it [02:13,  1.74it/s]Extractor Predicting: 235it [02:14,  1.75it/s]Extractor Predicting: 236it [02:14,  1.74it/s]Extractor Predicting: 237it [02:15,  1.77it/s]Extractor Predicting: 238it [02:15,  1.76it/s]Extractor Predicting: 239it [02:16,  1.79it/s]Extractor Predicting: 240it [02:16,  1.74it/s]Extractor Predicting: 241it [02:17,  1.71it/s]Extractor Predicting: 242it [02:18,  1.76it/s]Extractor Predicting: 243it [02:18,  1.77it/s]Extractor Predicting: 244it [02:19,  1.77it/s]Extractor Predicting: 245it [02:19,  1.73it/s]Extractor Predicting: 246it [02:20,  1.72it/s]Extractor Predicting: 247it [02:20,  1.74it/s]Extractor Predicting: 248it [02:21,  1.74it/s]Extractor Predicting: 249it [02:22,  1.76it/s]Extractor Predicting: 250it [02:22,  1.74it/s]Extractor Predicting: 251it [02:23,  1.77it/s]Extractor Predicting: 252it [02:23,  1.75it/s]Extractor Predicting: 253it [02:24,  1.73it/s]Extractor Predicting: 254it [02:24,  1.72it/s]Extractor Predicting: 255it [02:25,  1.74it/s]Extractor Predicting: 256it [02:26,  1.73it/s]Extractor Predicting: 257it [02:26,  1.76it/s]Extractor Predicting: 258it [02:27,  1.75it/s]Extractor Predicting: 259it [02:27,  1.74it/s]Extractor Predicting: 260it [02:28,  1.73it/s]Extractor Predicting: 261it [02:29,  1.71it/s]Extractor Predicting: 262it [02:29,  1.68it/s]Extractor Predicting: 263it [02:30,  1.73it/s]Extractor Predicting: 264it [02:30,  1.70it/s]Extractor Predicting: 265it [02:31,  1.69it/s]Extractor Predicting: 266it [02:31,  1.67it/s]Extractor Predicting: 267it [02:32,  1.66it/s]Extractor Predicting: 268it [02:33,  1.43it/s]Extractor Predicting: 269it [02:34,  1.51it/s]Extractor Predicting: 270it [02:34,  1.57it/s]Extractor Predicting: 271it [02:35,  1.66it/s]Extractor Predicting: 272it [02:35,  1.62it/s]Extractor Predicting: 273it [02:36,  1.63it/s]Extractor Predicting: 274it [02:37,  1.67it/s]Extractor Predicting: 275it [02:37,  1.67it/s]Extractor Predicting: 276it [02:38,  1.67it/s]Extractor Predicting: 277it [02:38,  1.66it/s]Extractor Predicting: 278it [02:39,  1.66it/s]Extractor Predicting: 279it [02:40,  1.67it/s]Extractor Predicting: 280it [02:40,  1.66it/s]Extractor Predicting: 281it [02:41,  1.68it/s]Extractor Predicting: 282it [02:41,  1.69it/s]Extractor Predicting: 283it [02:42,  1.70it/s]Extractor Predicting: 284it [02:42,  1.70it/s]Extractor Predicting: 285it [02:43,  1.69it/s]Extractor Predicting: 286it [02:44,  1.68it/s]Extractor Predicting: 287it [02:44,  1.66it/s]Extractor Predicting: 288it [02:45,  1.64it/s]Extractor Predicting: 289it [02:46,  1.65it/s]Extractor Predicting: 290it [02:46,  1.64it/s]Extractor Predicting: 291it [02:47,  1.67it/s]Extractor Predicting: 292it [02:47,  1.64it/s]Extractor Predicting: 293it [02:48,  1.66it/s]Extractor Predicting: 294it [02:49,  1.64it/s]Extractor Predicting: 295it [02:49,  1.65it/s]Extractor Predicting: 296it [02:50,  1.64it/s]Extractor Predicting: 297it [02:50,  1.63it/s]Extractor Predicting: 298it [02:51,  1.60it/s]Extractor Predicting: 299it [02:52,  1.57it/s]Extractor Predicting: 300it [02:52,  1.54it/s]Extractor Predicting: 301it [02:53,  1.55it/s]Extractor Predicting: 302it [02:54,  1.54it/s]Extractor Predicting: 303it [02:54,  1.53it/s]Extractor Predicting: 304it [02:55,  1.60it/s]Extractor Predicting: 305it [02:55,  1.63it/s]Extractor Predicting: 306it [02:56,  1.66it/s]Extractor Predicting: 307it [02:57,  1.65it/s]Extractor Predicting: 308it [02:57,  1.66it/s]Extractor Predicting: 309it [02:58,  1.64it/s]Extractor Predicting: 310it [02:59,  1.62it/s]Extractor Predicting: 311it [02:59,  1.61it/s]Extractor Predicting: 312it [03:00,  1.62it/s]Extractor Predicting: 313it [03:00,  1.59it/s]Extractor Predicting: 314it [03:01,  1.56it/s]Extractor Predicting: 315it [03:02,  1.60it/s]Extractor Predicting: 316it [03:02,  1.59it/s]Extractor Predicting: 317it [03:03,  1.59it/s]Extractor Predicting: 318it [03:04,  1.60it/s]Extractor Predicting: 319it [03:04,  1.60it/s]Extractor Predicting: 320it [03:05,  1.60it/s]Extractor Predicting: 321it [03:05,  1.56it/s]Extractor Predicting: 322it [03:06,  1.50it/s]Extractor Predicting: 323it [03:07,  1.53it/s]Extractor Predicting: 324it [03:07,  1.53it/s]Extractor Predicting: 325it [03:08,  1.54it/s]Extractor Predicting: 326it [03:09,  1.54it/s]Extractor Predicting: 327it [03:09,  1.54it/s]Extractor Predicting: 328it [03:10,  1.57it/s]Extractor Predicting: 329it [03:11,  1.57it/s]Extractor Predicting: 330it [03:11,  1.59it/s]Extractor Predicting: 331it [03:12,  1.62it/s]Extractor Predicting: 332it [03:13,  1.57it/s]Extractor Predicting: 333it [03:13,  1.58it/s]Extractor Predicting: 334it [03:14,  1.59it/s]Extractor Predicting: 335it [03:14,  1.61it/s]Extractor Predicting: 336it [03:15,  1.55it/s]Extractor Predicting: 337it [03:16,  1.55it/s]Extractor Predicting: 338it [03:16,  1.58it/s]Extractor Predicting: 339it [03:17,  1.57it/s]Extractor Predicting: 340it [03:18,  1.56it/s]Extractor Predicting: 341it [03:18,  1.55it/s]Extractor Predicting: 342it [03:19,  1.54it/s]Extractor Predicting: 343it [03:20,  1.50it/s]Extractor Predicting: 344it [03:20,  1.50it/s]Extractor Predicting: 345it [03:21,  1.55it/s]Extractor Predicting: 346it [03:22,  1.58it/s]Extractor Predicting: 347it [03:22,  1.61it/s]Extractor Predicting: 348it [03:23,  1.66it/s]Extractor Predicting: 349it [03:23,  1.63it/s]Extractor Predicting: 350it [03:24,  1.63it/s]Extractor Predicting: 351it [03:25,  1.63it/s]Extractor Predicting: 352it [03:25,  1.66it/s]Extractor Predicting: 353it [03:26,  1.71it/s]Extractor Predicting: 354it [03:26,  1.75it/s]Extractor Predicting: 355it [03:27,  1.73it/s]Extractor Predicting: 356it [03:27,  1.75it/s]Extractor Predicting: 357it [03:28,  1.76it/s]Extractor Predicting: 358it [03:29,  1.75it/s]Extractor Predicting: 359it [03:29,  1.72it/s]Extractor Predicting: 360it [03:30,  1.77it/s]Extractor Predicting: 361it [03:30,  1.73it/s]Extractor Predicting: 362it [03:31,  1.71it/s]Extractor Predicting: 363it [03:31,  1.73it/s]Extractor Predicting: 364it [03:32,  1.76it/s]Extractor Predicting: 365it [03:33,  1.74it/s]Extractor Predicting: 366it [03:33,  1.76it/s]Extractor Predicting: 367it [03:34,  1.76it/s]Extractor Predicting: 368it [03:34,  1.73it/s]Extractor Predicting: 369it [03:35,  1.70it/s]Extractor Predicting: 370it [03:36,  1.67it/s]Extractor Predicting: 371it [03:36,  1.61it/s]Extractor Predicting: 372it [03:37,  1.67it/s]Extractor Predicting: 373it [03:37,  1.69it/s]Extractor Predicting: 374it [03:38,  1.69it/s]Extractor Predicting: 375it [03:39,  1.67it/s]Extractor Predicting: 376it [03:39,  1.66it/s]Extractor Predicting: 377it [03:40,  1.67it/s]Extractor Predicting: 378it [03:40,  1.74it/s]Extractor Predicting: 379it [03:41,  1.81it/s]Extractor Predicting: 380it [03:41,  1.85it/s]Extractor Predicting: 381it [03:42,  1.79it/s]Extractor Predicting: 382it [03:42,  1.77it/s]Extractor Predicting: 383it [03:43,  1.75it/s]Extractor Predicting: 384it [03:44,  1.80it/s]Extractor Predicting: 385it [03:44,  1.86it/s]Extractor Predicting: 386it [03:45,  1.82it/s]Extractor Predicting: 387it [03:45,  1.88it/s]Extractor Predicting: 388it [03:46,  1.84it/s]Extractor Predicting: 389it [03:46,  1.79it/s]Extractor Predicting: 390it [03:47,  1.80it/s]Extractor Predicting: 391it [03:47,  1.83it/s]Extractor Predicting: 392it [03:48,  1.78it/s]Extractor Predicting: 393it [03:48,  1.80it/s]Extractor Predicting: 394it [03:49,  1.82it/s]Extractor Predicting: 395it [03:50,  1.79it/s]Extractor Predicting: 396it [03:50,  1.76it/s]Extractor Predicting: 397it [03:51,  1.77it/s]Extractor Predicting: 398it [03:51,  1.76it/s]Extractor Predicting: 399it [03:52,  1.77it/s]Extractor Predicting: 400it [03:53,  1.54it/s]Extractor Predicting: 401it [03:53,  1.63it/s]Extractor Predicting: 402it [03:54,  1.66it/s]Extractor Predicting: 403it [03:54,  1.71it/s]Extractor Predicting: 404it [03:55,  1.69it/s]Extractor Predicting: 405it [03:56,  1.70it/s]Extractor Predicting: 406it [03:56,  1.67it/s]Extractor Predicting: 407it [03:57,  1.62it/s]Extractor Predicting: 408it [03:57,  1.67it/s]Extractor Predicting: 409it [03:58,  1.71it/s]Extractor Predicting: 410it [03:59,  1.70it/s]Extractor Predicting: 411it [03:59,  1.69it/s]Extractor Predicting: 412it [04:00,  1.68it/s]Extractor Predicting: 413it [04:00,  1.71it/s]Extractor Predicting: 414it [04:01,  1.73it/s]Extractor Predicting: 415it [04:01,  1.75it/s]Extractor Predicting: 416it [04:02,  1.71it/s]Extractor Predicting: 417it [04:03,  1.69it/s]Extractor Predicting: 418it [04:03,  1.71it/s]Extractor Predicting: 419it [04:04,  1.74it/s]Extractor Predicting: 420it [04:04,  1.80it/s]Extractor Predicting: 421it [04:05,  1.80it/s]Extractor Predicting: 422it [04:05,  1.86it/s]Extractor Predicting: 423it [04:06,  1.87it/s]Extractor Predicting: 424it [04:06,  1.85it/s]Extractor Predicting: 425it [04:07,  1.86it/s]Extractor Predicting: 426it [04:08,  1.83it/s]Extractor Predicting: 427it [04:08,  1.76it/s]Extractor Predicting: 428it [04:09,  1.71it/s]Extractor Predicting: 429it [04:09,  1.71it/s]Extractor Predicting: 430it [04:10,  1.73it/s]Extractor Predicting: 431it [04:10,  1.77it/s]Extractor Predicting: 432it [04:11,  1.80it/s]Extractor Predicting: 433it [04:12,  1.78it/s]Extractor Predicting: 434it [04:12,  1.74it/s]Extractor Predicting: 435it [04:13,  1.75it/s]Extractor Predicting: 436it [04:13,  1.74it/s]Extractor Predicting: 437it [04:14,  1.71it/s]Extractor Predicting: 438it [04:15,  1.69it/s]Extractor Predicting: 439it [04:15,  1.68it/s]Extractor Predicting: 440it [04:16,  1.66it/s]Extractor Predicting: 441it [04:16,  1.69it/s]Extractor Predicting: 442it [04:17,  1.70it/s]Extractor Predicting: 443it [04:17,  1.73it/s]Extractor Predicting: 444it [04:18,  1.71it/s]Extractor Predicting: 445it [04:19,  1.69it/s]Extractor Predicting: 446it [04:19,  1.67it/s]Extractor Predicting: 447it [04:20,  1.66it/s]Extractor Predicting: 448it [04:20,  1.67it/s]Extractor Predicting: 449it [04:21,  1.69it/s]Extractor Predicting: 450it [04:22,  1.74it/s]Extractor Predicting: 451it [04:22,  1.70it/s]Extractor Predicting: 452it [04:23,  1.70it/s]Extractor Predicting: 453it [04:23,  1.70it/s]Extractor Predicting: 454it [04:24,  1.68it/s]Extractor Predicting: 455it [04:25,  1.70it/s]Extractor Predicting: 456it [04:25,  1.70it/s]Extractor Predicting: 457it [04:26,  1.68it/s]Extractor Predicting: 458it [04:26,  1.69it/s]Extractor Predicting: 459it [04:27,  1.73it/s]Extractor Predicting: 460it [04:27,  1.71it/s]Extractor Predicting: 461it [04:28,  1.71it/s]Extractor Predicting: 462it [04:29,  1.70it/s]Extractor Predicting: 463it [04:29,  1.71it/s]Extractor Predicting: 464it [04:30,  1.72it/s]Extractor Predicting: 465it [04:30,  1.73it/s]Extractor Predicting: 466it [04:31,  1.71it/s]Extractor Predicting: 467it [04:32,  1.72it/s]Extractor Predicting: 468it [04:32,  1.73it/s]Extractor Predicting: 469it [04:33,  1.70it/s]Extractor Predicting: 470it [04:33,  1.68it/s]Extractor Predicting: 471it [04:34,  1.69it/s]Extractor Predicting: 472it [04:34,  1.73it/s]Extractor Predicting: 473it [04:35,  1.70it/s]Extractor Predicting: 474it [04:36,  1.69it/s]Extractor Predicting: 475it [04:36,  1.71it/s]Extractor Predicting: 476it [04:37,  1.73it/s]Extractor Predicting: 477it [04:37,  1.78it/s]Extractor Predicting: 478it [04:38,  1.81it/s]Extractor Predicting: 479it [04:38,  1.80it/s]Extractor Predicting: 480it [04:39,  1.73it/s]Extractor Predicting: 481it [04:40,  1.68it/s]Extractor Predicting: 482it [04:40,  1.65it/s]Extractor Predicting: 483it [04:41,  1.59it/s]Extractor Predicting: 484it [04:42,  1.60it/s]Extractor Predicting: 485it [04:42,  1.60it/s]Extractor Predicting: 486it [04:43,  1.64it/s]Extractor Predicting: 487it [04:43,  1.70it/s]Extractor Predicting: 488it [04:44,  1.75it/s]Extractor Predicting: 489it [04:45,  1.69it/s]Extractor Predicting: 490it [04:45,  1.69it/s]Extractor Predicting: 491it [04:46,  1.70it/s]Extractor Predicting: 492it [04:46,  1.70it/s]Extractor Predicting: 493it [04:47,  1.70it/s]Extractor Predicting: 494it [04:48,  1.68it/s]Extractor Predicting: 495it [04:48,  1.71it/s]Extractor Predicting: 496it [04:49,  1.75it/s]Extractor Predicting: 497it [04:49,  1.76it/s]Extractor Predicting: 498it [04:50,  1.72it/s]Extractor Predicting: 499it [04:50,  1.70it/s]Extractor Predicting: 500it [04:51,  1.72it/s]Extractor Predicting: 501it [04:52,  1.71it/s]Extractor Predicting: 502it [04:52,  1.76it/s]Extractor Predicting: 503it [04:53,  1.55it/s]Extractor Predicting: 504it [04:53,  1.62it/s]Extractor Predicting: 505it [04:54,  1.67it/s]Extractor Predicting: 506it [04:55,  1.65it/s]Extractor Predicting: 507it [04:55,  1.60it/s]Extractor Predicting: 508it [04:56,  1.61it/s]Extractor Predicting: 509it [04:57,  1.62it/s]Extractor Predicting: 510it [04:57,  1.68it/s]Extractor Predicting: 511it [04:58,  1.70it/s]Extractor Predicting: 512it [04:58,  1.72it/s]Extractor Predicting: 513it [04:59,  1.69it/s]Extractor Predicting: 514it [04:59,  1.66it/s]Extractor Predicting: 515it [05:00,  1.68it/s]Extractor Predicting: 516it [05:01,  1.67it/s]Extractor Predicting: 517it [05:01,  1.62it/s]Extractor Predicting: 518it [05:02,  1.63it/s]Extractor Predicting: 519it [05:03,  1.64it/s]Extractor Predicting: 520it [05:03,  1.66it/s]Extractor Predicting: 521it [05:04,  1.68it/s]Extractor Predicting: 522it [05:04,  1.66it/s]Extractor Predicting: 523it [05:05,  1.70it/s]Extractor Predicting: 524it [05:05,  1.69it/s]Extractor Predicting: 525it [05:06,  1.67it/s]Extractor Predicting: 526it [05:07,  1.65it/s]Extractor Predicting: 527it [05:07,  1.61it/s]Extractor Predicting: 528it [05:08,  1.64it/s]Extractor Predicting: 529it [05:09,  1.63it/s]Extractor Predicting: 530it [05:09,  1.61it/s]Extractor Predicting: 531it [05:10,  1.61it/s]Extractor Predicting: 532it [05:10,  1.61it/s]Extractor Predicting: 533it [05:11,  1.61it/s]Extractor Predicting: 534it [05:12,  1.63it/s]Extractor Predicting: 535it [05:12,  1.61it/s]Extractor Predicting: 536it [05:13,  1.60it/s]Extractor Predicting: 537it [05:14,  1.60it/s]Extractor Predicting: 538it [05:14,  1.60it/s]Extractor Predicting: 539it [05:15,  1.59it/s]Extractor Predicting: 540it [05:15,  1.61it/s]Extractor Predicting: 541it [05:16,  1.58it/s]Extractor Predicting: 542it [05:17,  1.60it/s]Extractor Predicting: 543it [05:17,  1.62it/s]Extractor Predicting: 544it [05:18,  1.56it/s]Extractor Predicting: 545it [05:19,  1.58it/s]Extractor Predicting: 546it [05:19,  1.58it/s]Extractor Predicting: 547it [05:20,  1.57it/s]Extractor Predicting: 548it [05:20,  1.58it/s]Extractor Predicting: 549it [05:21,  1.58it/s]Extractor Predicting: 550it [05:22,  1.62it/s]Extractor Predicting: 551it [05:22,  1.64it/s]Extractor Predicting: 552it [05:23,  1.65it/s]Extractor Predicting: 553it [05:23,  1.68it/s]Extractor Predicting: 554it [05:24,  1.69it/s]Extractor Predicting: 555it [05:25,  1.66it/s]Extractor Predicting: 556it [05:25,  1.67it/s]Extractor Predicting: 556it [05:25,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:47:18,914 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:47:18,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:47:18,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:47:18,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:47:18,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:47:19,654 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:47:19,655 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:47:19,918 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:47:20,966 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:47:20,967 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:47:22,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:47:22,651 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:47:22,651 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:47:22,651 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:47:22,651 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:47:23,085 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:47:23,086 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:47:23,363 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:47:23,528 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:47:23,529 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.25701202060675443,
  "recall": 0.03365564800239862,
  "score": 0.059517497348886526,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'father', 'follows', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'part of the series', 'place of birth', 'production company', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 8650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.82it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.72it/s]Extractor Predicting: 8it [00:04,  1.76it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.75it/s]Extractor Predicting: 11it [00:06,  1.78it/s]Extractor Predicting: 12it [00:06,  1.77it/s]Extractor Predicting: 13it [00:07,  1.77it/s]Extractor Predicting: 14it [00:08,  1.71it/s]Extractor Predicting: 15it [00:08,  1.68it/s]Extractor Predicting: 16it [00:09,  1.70it/s]Extractor Predicting: 17it [00:09,  1.68it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:11,  1.63it/s]Extractor Predicting: 21it [00:12,  1.56it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:13,  1.53it/s]Extractor Predicting: 24it [00:14,  1.57it/s]Extractor Predicting: 25it [00:14,  1.59it/s]Extractor Predicting: 26it [00:15,  1.56it/s]Extractor Predicting: 27it [00:16,  1.57it/s]Extractor Predicting: 28it [00:16,  1.55it/s]Extractor Predicting: 29it [00:17,  1.59it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:18,  1.59it/s]Extractor Predicting: 32it [00:19,  1.53it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:20,  1.69it/s]Extractor Predicting: 35it [00:21,  1.69it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:22,  1.68it/s]Extractor Predicting: 38it [00:22,  1.70it/s]Extractor Predicting: 39it [00:23,  1.66it/s]Extractor Predicting: 40it [00:24,  1.67it/s]Extractor Predicting: 41it [00:24,  1.64it/s]Extractor Predicting: 42it [00:25,  1.61it/s]Extractor Predicting: 43it [00:26,  1.63it/s]Extractor Predicting: 44it [00:26,  1.62it/s]Extractor Predicting: 45it [00:27,  1.63it/s]Extractor Predicting: 46it [00:27,  1.63it/s]Extractor Predicting: 47it [00:28,  1.64it/s]Extractor Predicting: 48it [00:29,  1.68it/s]Extractor Predicting: 49it [00:29,  1.66it/s]Extractor Predicting: 50it [00:30,  1.70it/s]Extractor Predicting: 51it [00:30,  1.69it/s]Extractor Predicting: 52it [00:31,  1.66it/s]Extractor Predicting: 53it [00:31,  1.73it/s]Extractor Predicting: 54it [00:32,  1.81it/s]Extractor Predicting: 55it [00:32,  1.90it/s]Extractor Predicting: 56it [00:33,  1.98it/s]Extractor Predicting: 57it [00:34,  1.83it/s]Extractor Predicting: 58it [00:34,  1.89it/s]Extractor Predicting: 59it [00:34,  1.94it/s]Extractor Predicting: 60it [00:35,  1.96it/s]Extractor Predicting: 61it [00:35,  1.96it/s]Extractor Predicting: 62it [00:36,  1.98it/s]Extractor Predicting: 63it [00:36,  1.98it/s]Extractor Predicting: 64it [00:37,  1.99it/s]Extractor Predicting: 65it [00:37,  2.04it/s]Extractor Predicting: 66it [00:38,  2.01it/s]Extractor Predicting: 67it [00:38,  2.00it/s]Extractor Predicting: 68it [00:39,  2.03it/s]Extractor Predicting: 69it [00:39,  2.07it/s]Extractor Predicting: 70it [00:40,  2.10it/s]Extractor Predicting: 71it [00:40,  2.07it/s]Extractor Predicting: 72it [00:41,  2.06it/s]Extractor Predicting: 73it [00:41,  2.13it/s]Extractor Predicting: 74it [00:42,  2.03it/s]Extractor Predicting: 75it [00:42,  2.04it/s]Extractor Predicting: 76it [00:43,  2.01it/s]Extractor Predicting: 77it [00:43,  2.00it/s]Extractor Predicting: 78it [00:44,  2.03it/s]Extractor Predicting: 79it [00:44,  2.04it/s]Extractor Predicting: 80it [00:45,  2.04it/s]Extractor Predicting: 81it [00:45,  2.05it/s]Extractor Predicting: 82it [00:46,  1.99it/s]Extractor Predicting: 83it [00:46,  1.87it/s]Extractor Predicting: 84it [00:47,  1.77it/s]Extractor Predicting: 85it [00:48,  1.70it/s]Extractor Predicting: 86it [00:48,  1.65it/s]Extractor Predicting: 87it [00:49,  1.63it/s]Extractor Predicting: 88it [00:50,  1.62it/s]Extractor Predicting: 89it [00:50,  1.61it/s]Extractor Predicting: 90it [00:51,  1.59it/s]Extractor Predicting: 91it [00:52,  1.60it/s]Extractor Predicting: 92it [00:52,  1.58it/s]Extractor Predicting: 93it [00:53,  1.58it/s]Extractor Predicting: 94it [00:53,  1.57it/s]Extractor Predicting: 95it [00:54,  1.59it/s]Extractor Predicting: 96it [00:55,  1.57it/s]Extractor Predicting: 97it [00:55,  1.57it/s]Extractor Predicting: 98it [00:56,  1.60it/s]Extractor Predicting: 99it [00:57,  1.60it/s]Extractor Predicting: 100it [00:57,  1.58it/s]Extractor Predicting: 101it [00:58,  1.60it/s]Extractor Predicting: 102it [00:58,  1.61it/s]Extractor Predicting: 103it [00:59,  1.59it/s]Extractor Predicting: 104it [00:59,  1.99it/s]Extractor Predicting: 104it [00:59,  1.74it/s]
[INFO|configuration_utils.py:515] 2023-08-29 12:48:24,718 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:48:24,718 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 12:48:24,724 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:48:24,725 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 12:48:24,728 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 12:48:27,768 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 12:48:27,771 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 12:48:27,788 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:48:27,789 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 12:48:27,803 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:48:27,807 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:48:27,807 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:48:27,807 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:48:27,807 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:48:27,807 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:48:27,807 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.757020757020757,
  "recall": 0.11445449510799335,
  "score": 0.19884541372674794,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 12:48:28,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:28,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:29,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:29,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:30,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:30,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:31,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:31,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:32,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:32,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:33,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:33,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:34,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:34,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:35,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:35,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:36,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:37,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:37,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:38,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:10<03:19, 10.48s/it][WARNING|generation_utils.py:914] 2023-08-29 12:48:38,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:39,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:39,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:40,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:40,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:41,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:41,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:42,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:42,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:43,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:44,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:44,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:45,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:45,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:46,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:46,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:47,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:47,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:48,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:48,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:21<03:14, 10.79s/it][WARNING|generation_utils.py:914] 2023-08-29 12:48:49,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:50,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:50,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:51,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:51,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:52,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:52,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:53,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:53,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:54,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:54,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:55,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:55,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:56,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:56,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:57,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:57,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:58,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:58,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:59,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:48:59,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:00,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:32<03:06, 10.97s/it][WARNING|generation_utils.py:914] 2023-08-29 12:49:00,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:01,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:01,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:02,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:03,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:03,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:04,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:04,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:05,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:05,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:06,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:06,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:07,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:07,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:08,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:08,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:09,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:09,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:10,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:10,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:11,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:44<02:58, 11.15s/it][WARNING|generation_utils.py:914] 2023-08-29 12:49:12,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:12,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:13,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:13,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:14,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:14,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:15,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:15,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:16,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:16,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:17,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:17,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:18,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:18,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:19,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:19,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:20,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:21,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:21,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:22,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [00:54<02:44, 10.94s/it][WARNING|generation_utils.py:914] 2023-08-29 12:49:22,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:23,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:23,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:24,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:24,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:25,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:25,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:26,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:27,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:27,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:28,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:28,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:29,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:29,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:30,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:30,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:31,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:32,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:32,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:33,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:33,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:34,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:06<02:38, 11.32s/it][WARNING|generation_utils.py:914] 2023-08-29 12:49:34,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:35,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:35,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:36,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:37,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:37,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:38,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:38,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:39,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:40,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:40,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:41,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:41,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:42,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:42,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:43,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:43,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:44,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:45,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:45,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:46,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:18<02:29, 11.51s/it][WARNING|generation_utils.py:914] 2023-08-29 12:49:46,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:47,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:47,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:48,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:48,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:49,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:49,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:50,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:51,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:51,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:52,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:52,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:53,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:54,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:54,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:55,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:55,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:56,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:56,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:57,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:57,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:58,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:31<02:21, 11.80s/it][WARNING|generation_utils.py:914] 2023-08-29 12:49:59,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:49:59,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:00,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:00,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:01,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:01,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:02,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:02,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:03,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:03,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:04,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:05,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:05,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:06,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:06,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:07,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:07,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:08,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:08,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:09,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:09,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:42<02:08, 11.65s/it][WARNING|generation_utils.py:914] 2023-08-29 12:50:10,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:10,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:11,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:11,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:12,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:12,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:13,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:14,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:14,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:15,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:15,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:15,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:16,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:16,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:17,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:17,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:18,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:19,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:19,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:20,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:20,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:21,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:21,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [01:54<01:56, 11.68s/it][WARNING|generation_utils.py:914] 2023-08-29 12:50:22,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:22,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:23,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:23,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:23,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:24,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:24,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:25,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:25,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:26,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:26,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:27,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:27,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:28,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:28,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:29,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:29,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:30,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:30,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:31,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:31,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:32,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:04<01:41, 11.32s/it][WARNING|generation_utils.py:914] 2023-08-29 12:50:32,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:33,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:33,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:34,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:35,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:35,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:36,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:36,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:37,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:38,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:38,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:39,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:39,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:40,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:41,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:41,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:42,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:42,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:43,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:44,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:16<01:32, 11.51s/it][WARNING|generation_utils.py:914] 2023-08-29 12:50:44,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:45,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:45,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:46,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:46,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:46,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:47,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:48,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:48,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:49,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:49,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:49,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:50,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:51,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:51,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:52,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:52,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:53,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:53,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:53,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:26<01:16, 11.00s/it][WARNING|generation_utils.py:914] 2023-08-29 12:50:54,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:54,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:55,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:56,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:56,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:57,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:57,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:58,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:58,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:59,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:50:59,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:00,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:00,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:01,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:01,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:02,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:02,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:03,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:04,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:04,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:05,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [02:37<01:06, 11.09s/it][WARNING|generation_utils.py:914] 2023-08-29 12:51:05,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:06,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:06,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:07,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:07,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:08,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:08,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:09,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:09,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:10,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:10,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:10,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:11,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:11,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:12,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:12,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:13,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:14,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:14,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:15,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [02:47<00:53, 10.70s/it][WARNING|generation_utils.py:914] 2023-08-29 12:51:15,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:16,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:16,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:17,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:17,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:18,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:18,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:19,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:19,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:20,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:20,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:21,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:21,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:22,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:22,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:23,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:23,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:24,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:25,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:25,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:26,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [02:58<00:43, 10.80s/it][WARNING|generation_utils.py:914] 2023-08-29 12:51:26,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:27,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:27,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:28,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:28,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:29,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:29,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:30,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:30,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:31,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:31,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:32,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:32,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:33,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:34,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:34,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:35,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:35,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:36,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:36,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:37,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:37,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:09<00:33, 11.00s/it][WARNING|generation_utils.py:914] 2023-08-29 12:51:38,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:38,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:39,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:39,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:40,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:40,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:41,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:41,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:42,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:42,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:43,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:44,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:44,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:45,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:45,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:46,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:46,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:47,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:48,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:48,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:49,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:49,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:22<00:22, 11.35s/it][WARNING|generation_utils.py:914] 2023-08-29 12:51:50,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:50,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:51,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:51,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:52,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:52,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:53,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:53,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:54,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:54,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:55,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:55,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:56,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:56,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:57,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:57,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:58,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:58,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:59,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:51:59,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:00,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [03:32<00:11, 11.05s/it][WARNING|generation_utils.py:914] 2023-08-29 12:52:00,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:01,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:01,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:02,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:02,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:02,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:03,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:04,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:04,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:05,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:05,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:06,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:06,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:06,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:07,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:07,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:08,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:09,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:09,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:10,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:52:10,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [03:43<00:00, 10.91s/it]Generating: 100%|██████████| 20/20 [03:43<00:00, 11.15s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:52:18,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:52:18,149 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:52:18,150 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:52:18,150 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:52:18,150 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:52:18,778 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:52:18,779 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:52:19,342 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:52:20,382 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:52:20,382 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:52:23,462 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:52:23,466 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:52:23,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:52:23,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:52:23,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:52:24,122 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:52:24,122 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:52:24,692 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:52:24,872 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:52:24,872 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : inception .', 'success_rate': 0.9375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : military branch .', 'success_rate': 0.8892045454545454, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9453125, 'errors': {''}}
['Relation : applies to jurisdiction . Context : Following his initial dismissal , the court of appeals has ruled that his trial attorneys should not have tried to obtain a decision on whether a motion to suppress has been granted . Head Entity : dismissed , Tail Entity : district .\n']
['Relation : applies to jurisdiction . Context : Following his initial dismissal , the court of appeals has ruled that his trial attorneys should not have tried to obtain a decision on whether a motion to suppress has been granted . Head Entity : dismissed , Tail Entity : district .\n', 'Relation : applies to jurisdiction . Context : After he was dismissed , the court ruled his trial was unconstitutional , holding that his client , James D. McGowan , was innocent only because the trial judge had erred in concluding that it was illegal to videotape in private . Head Entity : James D. McGowan , Tail Entity : federal court .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : applies to jurisdiction .', 'success_rate': 0.8565340909090909, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : family name .', 'success_rate': 0.9017857142857143, 'errors': {''}}
['Relation : father . Context : Later in Life he married the daughter of the king Ganesha of Tyana at the end of the third century BC , succeeding his father , King Mughal of Mughal India . Head Entity : Ganesha of Tyana , Tail Entity : Agnes Ganesha .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : father .', 'success_rate': 0.8678977272727273, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8260869565217391, 'errors': {'', "('Blue Sky', 'genre', '', 'In the early 1980s , he worked on such projects as Blue Sky with Bill Murray .')", "('The Legend of Zelda', 'genre', '', 'The Legend of Zelda is a fictional franchise in TES , developed and published by Nintendo .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('directories', 'is a list of', '', 'It is an index of all known directories that have at least one member named .')", "('list', 'is a list of', '', 'It is a list of websites or other online services for which every name or date on the list can be used .')", "('directories', 'is a list of', '', 'It is a database of directories and files owned by .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.94375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9196428571428571, 'errors': {'', "('Hot 100', 'lyrics by', '', 'In 1968 , she sang an instrumental recording of the song , which peaked at number 10 on the US Billboard Hot 100 .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9671875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : part of the series .', 'success_rate': 0.8664772727272727, 'errors': {'', "('', 'part of the series', 'anime', 'In the anime , she is portrayed by Masato Miyano and is voiced by Tedd Nettles .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.8565340909090909, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : production company .', 'success_rate': 0.9092261904761905, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : use .', 'success_rate': 0.8988095238095238, 'errors': {'', "('data', 'use', '', 'The following sections introduce the use of the variable variable variable s , usually named after a character , to create , store , and change data .')", 'too many values to unpack (expected 2)', "('Windows XP', 'use', '', 'In Windows XP Service Pack 1 , the application does not support the .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 13683
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13783, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.50it/s]Extractor Estimating: 2it [00:01,  1.59it/s]Extractor Estimating: 3it [00:01,  1.69it/s]Extractor Estimating: 4it [00:02,  1.76it/s]Extractor Estimating: 5it [00:02,  1.77it/s]Extractor Estimating: 6it [00:03,  1.74it/s]Extractor Estimating: 7it [00:04,  1.72it/s]Extractor Estimating: 8it [00:04,  1.72it/s]Extractor Estimating: 9it [00:05,  1.71it/s]Extractor Estimating: 10it [00:05,  1.76it/s]Extractor Estimating: 11it [00:06,  1.75it/s]Extractor Estimating: 12it [00:06,  1.75it/s]Extractor Estimating: 13it [00:07,  1.73it/s]Extractor Estimating: 14it [00:08,  1.73it/s]Extractor Estimating: 15it [00:08,  1.72it/s]Extractor Estimating: 16it [00:09,  1.72it/s]Extractor Estimating: 17it [00:09,  1.74it/s]Extractor Estimating: 18it [00:10,  1.65it/s]Extractor Estimating: 19it [00:11,  1.69it/s]Extractor Estimating: 20it [00:11,  1.73it/s]Extractor Estimating: 21it [00:12,  1.68it/s]Extractor Estimating: 22it [00:12,  1.69it/s]Extractor Estimating: 23it [00:13,  1.74it/s]Extractor Estimating: 24it [00:13,  1.75it/s]Extractor Estimating: 25it [00:14,  1.77it/s]Extractor Estimating: 26it [00:15,  1.76it/s]Extractor Estimating: 27it [00:15,  1.75it/s]Extractor Estimating: 28it [00:16,  1.75it/s]Extractor Estimating: 29it [00:16,  1.73it/s]Extractor Estimating: 30it [00:17,  1.76it/s]Extractor Estimating: 31it [00:17,  1.78it/s]Extractor Estimating: 32it [00:18,  1.77it/s]Extractor Estimating: 33it [00:19,  1.76it/s]Extractor Estimating: 34it [00:19,  1.78it/s]Extractor Estimating: 35it [00:20,  1.75it/s]Extractor Estimating: 36it [00:20,  1.74it/s]Extractor Estimating: 37it [00:21,  1.78it/s]Extractor Estimating: 38it [00:21,  1.82it/s]Extractor Estimating: 39it [00:22,  1.81it/s]Extractor Estimating: 40it [00:22,  1.80it/s]Extractor Estimating: 41it [00:23,  1.79it/s]Extractor Estimating: 42it [00:24,  1.76it/s]Extractor Estimating: 43it [00:24,  1.79it/s]Extractor Estimating: 44it [00:25,  1.78it/s]Extractor Estimating: 45it [00:25,  1.78it/s]Extractor Estimating: 46it [00:26,  1.79it/s]Extractor Estimating: 47it [00:26,  1.78it/s]Extractor Estimating: 48it [00:27,  1.78it/s]Extractor Estimating: 49it [00:28,  1.76it/s]Extractor Estimating: 50it [00:28,  1.69it/s]Extractor Estimating: 51it [00:29,  1.73it/s]Extractor Estimating: 52it [00:29,  1.74it/s]Extractor Estimating: 53it [00:30,  1.76it/s]Extractor Estimating: 54it [00:30,  1.77it/s]Extractor Estimating: 55it [00:31,  1.72it/s]Extractor Estimating: 56it [00:32,  1.75it/s]Extractor Estimating: 57it [00:32,  1.78it/s]Extractor Estimating: 58it [00:33,  1.79it/s]Extractor Estimating: 59it [00:33,  1.78it/s]Extractor Estimating: 60it [00:34,  1.81it/s]Extractor Estimating: 61it [00:34,  1.88it/s]Extractor Estimating: 62it [00:35,  1.83it/s]Extractor Estimating: 63it [00:35,  1.85it/s]Extractor Estimating: 64it [00:36,  1.81it/s]Extractor Estimating: 65it [00:37,  1.78it/s]Extractor Estimating: 66it [00:37,  1.82it/s]Extractor Estimating: 67it [00:38,  1.84it/s]Extractor Estimating: 68it [00:38,  1.84it/s]Extractor Estimating: 69it [00:39,  1.86it/s]Extractor Estimating: 70it [00:39,  1.84it/s]Extractor Estimating: 71it [00:40,  1.84it/s]Extractor Estimating: 72it [00:40,  1.85it/s]Extractor Estimating: 73it [00:41,  1.82it/s]Extractor Estimating: 74it [00:41,  1.86it/s]Extractor Estimating: 75it [00:42,  1.85it/s]Extractor Estimating: 76it [00:43,  1.79it/s]Extractor Estimating: 77it [00:43,  1.79it/s]Extractor Estimating: 78it [00:44,  1.72it/s]Extractor Estimating: 79it [00:44,  1.73it/s]Extractor Estimating: 80it [00:45,  1.75it/s]Extractor Estimating: 81it [00:45,  1.76it/s]Extractor Estimating: 82it [00:46,  1.76it/s]Extractor Estimating: 83it [00:47,  1.78it/s]Extractor Estimating: 84it [00:47,  1.77it/s]Extractor Estimating: 85it [00:48,  1.82it/s]Extractor Estimating: 86it [00:48,  1.80it/s]Extractor Estimating: 87it [00:49,  1.81it/s]Extractor Estimating: 88it [00:49,  1.80it/s]Extractor Estimating: 89it [00:50,  1.76it/s]Extractor Estimating: 90it [00:50,  1.77it/s]Extractor Estimating: 91it [00:51,  1.80it/s]Extractor Estimating: 92it [00:52,  1.81it/s]Extractor Estimating: 93it [00:52,  1.84it/s]Extractor Estimating: 94it [00:53,  1.83it/s]Extractor Estimating: 95it [00:53,  1.83it/s]Extractor Estimating: 96it [00:54,  1.82it/s]Extractor Estimating: 97it [00:54,  1.79it/s]Extractor Estimating: 98it [00:55,  1.75it/s]Extractor Estimating: 99it [00:56,  1.56it/s]Extractor Estimating: 100it [00:56,  1.63it/s]Extractor Estimating: 101it [00:57,  1.66it/s]Extractor Estimating: 102it [00:57,  1.67it/s]Extractor Estimating: 103it [00:58,  1.68it/s]Extractor Estimating: 104it [00:59,  1.71it/s]Extractor Estimating: 105it [00:59,  1.73it/s]Extractor Estimating: 106it [01:00,  1.73it/s]Extractor Estimating: 107it [01:00,  1.77it/s]Extractor Estimating: 108it [01:01,  1.77it/s]Extractor Estimating: 109it [01:01,  1.78it/s]Extractor Estimating: 110it [01:02,  1.75it/s]Extractor Estimating: 111it [01:03,  1.73it/s]Extractor Estimating: 112it [01:03,  1.76it/s]Extractor Estimating: 113it [01:04,  1.75it/s]Extractor Estimating: 114it [01:04,  1.73it/s]Extractor Estimating: 115it [01:05,  1.74it/s]Extractor Estimating: 116it [01:05,  1.74it/s]Extractor Estimating: 117it [01:06,  1.69it/s]Extractor Estimating: 118it [01:07,  1.73it/s]Extractor Estimating: 119it [01:07,  1.75it/s]Extractor Estimating: 120it [01:08,  1.72it/s]Extractor Estimating: 121it [01:08,  1.77it/s]Extractor Estimating: 122it [01:09,  1.72it/s]Extractor Estimating: 123it [01:09,  1.72it/s]Extractor Estimating: 124it [01:10,  1.68it/s]Extractor Estimating: 125it [01:11,  1.72it/s]Extractor Estimating: 126it [01:11,  1.68it/s]Extractor Estimating: 127it [01:12,  1.67it/s]Extractor Estimating: 128it [01:13,  1.64it/s]Extractor Estimating: 129it [01:13,  1.66it/s]Extractor Estimating: 130it [01:14,  1.60it/s]Extractor Estimating: 131it [01:14,  1.61it/s]Extractor Estimating: 132it [01:15,  1.62it/s]Extractor Estimating: 133it [01:16,  1.63it/s]Extractor Estimating: 134it [01:16,  1.56it/s]Extractor Estimating: 135it [01:17,  1.61it/s]Extractor Estimating: 136it [01:17,  1.64it/s]Extractor Estimating: 137it [01:18,  1.67it/s]Extractor Estimating: 138it [01:19,  1.63it/s]Extractor Estimating: 139it [01:19,  1.61it/s]Extractor Estimating: 140it [01:20,  1.63it/s]Extractor Estimating: 141it [01:21,  1.63it/s]Extractor Estimating: 142it [01:21,  1.64it/s]Extractor Estimating: 143it [01:22,  1.66it/s]Extractor Estimating: 144it [01:22,  1.61it/s]Extractor Estimating: 145it [01:23,  1.60it/s]Extractor Estimating: 146it [01:24,  1.62it/s]Extractor Estimating: 147it [01:24,  1.66it/s]Extractor Estimating: 148it [01:25,  1.68it/s]Extractor Estimating: 149it [01:25,  1.73it/s]Extractor Estimating: 150it [01:26,  1.73it/s]Extractor Estimating: 151it [01:26,  1.74it/s]Extractor Estimating: 152it [01:27,  1.72it/s]Extractor Estimating: 153it [01:28,  1.70it/s]Extractor Estimating: 154it [01:28,  1.68it/s]Extractor Estimating: 155it [01:29,  1.74it/s]Extractor Estimating: 156it [01:29,  1.71it/s]Extractor Estimating: 157it [01:30,  1.72it/s]Extractor Estimating: 158it [01:31,  1.71it/s]Extractor Estimating: 159it [01:31,  1.72it/s]Extractor Estimating: 160it [01:32,  1.75it/s]Extractor Estimating: 161it [01:32,  1.74it/s]Extractor Estimating: 162it [01:33,  1.70it/s]Extractor Estimating: 163it [01:33,  1.69it/s]Extractor Estimating: 164it [01:34,  1.70it/s]Extractor Estimating: 165it [01:35,  1.70it/s]Extractor Estimating: 166it [01:35,  1.72it/s]Extractor Estimating: 167it [01:36,  1.70it/s]Extractor Estimating: 168it [01:37,  1.51it/s]Extractor Estimating: 169it [01:37,  1.61it/s]Extractor Estimating: 170it [01:38,  1.66it/s]Extractor Estimating: 171it [01:38,  1.70it/s]Extractor Estimating: 172it [01:39,  1.74it/s]Extractor Estimating: 173it [01:39,  1.76it/s]Extractor Estimating: 174it [01:40,  1.73it/s]Extractor Estimating: 175it [01:41,  1.72it/s]Extractor Estimating: 176it [01:41,  1.71it/s]Extractor Estimating: 177it [01:42,  1.68it/s]Extractor Estimating: 178it [01:42,  1.68it/s]Extractor Estimating: 179it [01:43,  1.70it/s]Extractor Estimating: 180it [01:44,  1.70it/s]Extractor Estimating: 181it [01:44,  1.67it/s]Extractor Estimating: 182it [01:45,  1.68it/s]Extractor Estimating: 183it [01:45,  1.69it/s]Extractor Estimating: 184it [01:46,  1.71it/s]Extractor Estimating: 185it [01:46,  1.73it/s]Extractor Estimating: 186it [01:47,  1.75it/s]Extractor Estimating: 187it [01:48,  1.74it/s]Extractor Estimating: 188it [01:48,  1.76it/s]Extractor Estimating: 189it [01:49,  1.67it/s]Extractor Estimating: 190it [01:49,  1.65it/s]Extractor Estimating: 191it [01:50,  1.65it/s]Extractor Estimating: 192it [01:51,  1.71it/s]Extractor Estimating: 193it [01:51,  1.72it/s]Extractor Estimating: 194it [01:52,  1.75it/s]Extractor Estimating: 195it [01:52,  1.73it/s]Extractor Estimating: 196it [01:53,  1.73it/s]Extractor Estimating: 197it [01:54,  1.67it/s]Extractor Estimating: 198it [01:54,  1.68it/s]Extractor Estimating: 199it [01:55,  1.67it/s]Extractor Estimating: 200it [01:55,  1.69it/s]Extractor Estimating: 201it [01:56,  1.69it/s]Extractor Estimating: 202it [01:56,  1.71it/s]Extractor Estimating: 203it [01:57,  1.74it/s]Extractor Estimating: 204it [01:58,  1.71it/s]Extractor Estimating: 205it [01:58,  1.78it/s]Extractor Estimating: 206it [01:59,  1.76it/s]Extractor Estimating: 207it [01:59,  1.75it/s]Extractor Estimating: 208it [02:00,  1.81it/s]Extractor Estimating: 209it [02:00,  1.79it/s]Extractor Estimating: 210it [02:01,  1.75it/s]Extractor Estimating: 211it [02:02,  1.78it/s]Extractor Estimating: 212it [02:02,  1.70it/s]Extractor Estimating: 213it [02:03,  1.77it/s]Extractor Estimating: 214it [02:03,  1.73it/s]Extractor Estimating: 215it [02:04,  1.73it/s]Extractor Estimating: 216it [02:04,  1.76it/s]Extractor Estimating: 217it [02:05,  1.77it/s]Extractor Estimating: 218it [02:05,  1.81it/s]Extractor Estimating: 219it [02:06,  1.80it/s]Extractor Estimating: 220it [02:07,  1.82it/s]Extractor Estimating: 221it [02:07,  1.82it/s]Extractor Estimating: 222it [02:08,  1.83it/s]Extractor Estimating: 223it [02:08,  1.82it/s]Extractor Estimating: 224it [02:09,  1.74it/s]Extractor Estimating: 225it [02:09,  1.72it/s]Extractor Estimating: 226it [02:10,  1.75it/s]Extractor Estimating: 227it [02:11,  1.78it/s]Extractor Estimating: 228it [02:11,  1.82it/s]Extractor Estimating: 229it [02:12,  1.80it/s]Extractor Estimating: 230it [02:12,  1.81it/s]Extractor Estimating: 231it [02:13,  1.84it/s]Extractor Estimating: 232it [02:13,  1.83it/s]Extractor Estimating: 233it [02:14,  1.87it/s]Extractor Estimating: 234it [02:14,  1.80it/s]Extractor Estimating: 235it [02:15,  1.90it/s]Extractor Estimating: 236it [02:15,  1.96it/s]Extractor Estimating: 237it [02:16,  1.99it/s]Extractor Estimating: 238it [02:16,  1.97it/s]Extractor Estimating: 239it [02:17,  1.98it/s]Extractor Estimating: 240it [02:17,  1.99it/s]Extractor Estimating: 241it [02:18,  1.88it/s]Extractor Estimating: 242it [02:19,  1.80it/s]Extractor Estimating: 243it [02:19,  1.85it/s]Extractor Estimating: 244it [02:20,  1.82it/s]Extractor Estimating: 245it [02:20,  1.82it/s]Extractor Estimating: 246it [02:21,  1.81it/s]Extractor Estimating: 247it [02:21,  1.86it/s]Extractor Estimating: 248it [02:22,  1.87it/s]Extractor Estimating: 249it [02:23,  1.64it/s]Extractor Estimating: 250it [02:23,  1.66it/s]Extractor Estimating: 251it [02:24,  1.83it/s]Extractor Estimating: 252it [02:24,  1.95it/s]Extractor Estimating: 253it [02:24,  1.94it/s]Extractor Estimating: 254it [02:25,  2.01it/s]Extractor Estimating: 255it [02:25,  2.09it/s]Extractor Estimating: 256it [02:26,  2.14it/s]Extractor Estimating: 257it [02:26,  2.21it/s]Extractor Estimating: 258it [02:27,  2.15it/s]Extractor Estimating: 259it [02:27,  2.08it/s]Extractor Estimating: 260it [02:28,  2.12it/s]Extractor Estimating: 261it [02:28,  2.11it/s]Extractor Estimating: 262it [02:29,  2.07it/s]Extractor Estimating: 263it [02:29,  2.10it/s]Extractor Estimating: 264it [02:30,  1.92it/s]Extractor Estimating: 265it [02:30,  1.94it/s]Extractor Estimating: 266it [02:31,  2.01it/s]Extractor Estimating: 267it [02:31,  2.06it/s]Extractor Estimating: 268it [02:32,  2.03it/s]Extractor Estimating: 269it [02:32,  2.03it/s]Extractor Estimating: 270it [02:33,  2.07it/s]Extractor Estimating: 271it [02:33,  2.09it/s]Extractor Estimating: 272it [02:34,  2.14it/s]Extractor Estimating: 273it [02:34,  2.09it/s]Extractor Estimating: 274it [02:35,  2.09it/s]Extractor Estimating: 275it [02:35,  2.08it/s]Extractor Estimating: 276it [02:35,  2.07it/s]Extractor Estimating: 277it [02:36,  2.04it/s]Extractor Estimating: 278it [02:36,  2.06it/s]Extractor Estimating: 279it [02:37,  1.98it/s]Extractor Estimating: 280it [02:37,  2.02it/s]Extractor Estimating: 281it [02:38,  2.00it/s]Extractor Estimating: 282it [02:38,  2.03it/s]Extractor Estimating: 283it [02:39,  2.05it/s]Extractor Estimating: 284it [02:39,  2.08it/s]Extractor Estimating: 285it [02:40,  2.12it/s]Extractor Estimating: 286it [02:40,  2.18it/s]Extractor Estimating: 287it [02:41,  2.16it/s]Extractor Estimating: 288it [02:41,  2.02it/s]Extractor Estimating: 289it [02:42,  2.03it/s]Extractor Estimating: 290it [02:42,  1.93it/s]Extractor Estimating: 291it [02:43,  1.94it/s]Extractor Estimating: 292it [02:43,  1.97it/s]Extractor Estimating: 293it [02:44,  1.96it/s]Extractor Estimating: 294it [02:44,  1.99it/s]Extractor Estimating: 295it [02:45,  1.99it/s]Extractor Estimating: 296it [02:45,  2.04it/s]Extractor Estimating: 297it [02:46,  2.10it/s]Extractor Estimating: 298it [02:46,  2.05it/s]Extractor Estimating: 299it [02:47,  2.03it/s]Extractor Estimating: 300it [02:47,  1.96it/s]Extractor Estimating: 301it [02:48,  2.01it/s]Extractor Estimating: 302it [02:48,  1.98it/s]Extractor Estimating: 303it [02:49,  2.02it/s]Extractor Estimating: 304it [02:49,  2.02it/s]Extractor Estimating: 305it [02:50,  1.99it/s]Extractor Estimating: 306it [02:50,  2.00it/s]Extractor Estimating: 307it [02:51,  1.96it/s]Extractor Estimating: 308it [02:51,  1.90it/s]Extractor Estimating: 309it [02:52,  1.95it/s]Extractor Estimating: 310it [02:52,  2.00it/s]Extractor Estimating: 311it [02:53,  2.02it/s]Extractor Estimating: 312it [02:53,  2.03it/s]Extractor Estimating: 313it [02:54,  1.97it/s]Extractor Estimating: 314it [02:54,  2.04it/s]Extractor Estimating: 315it [02:55,  2.00it/s]Extractor Estimating: 316it [02:55,  2.03it/s]Extractor Estimating: 317it [02:56,  2.01it/s]Extractor Estimating: 318it [02:56,  1.99it/s]Extractor Estimating: 319it [02:57,  1.99it/s]Extractor Estimating: 320it [02:57,  1.93it/s]Extractor Estimating: 321it [02:58,  1.96it/s]Extractor Estimating: 322it [02:58,  1.96it/s]Extractor Estimating: 323it [02:59,  1.96it/s]Extractor Estimating: 324it [02:59,  1.95it/s]Extractor Estimating: 325it [03:00,  1.89it/s]Extractor Estimating: 326it [03:01,  1.85it/s]Extractor Estimating: 327it [03:01,  1.73it/s]Extractor Estimating: 328it [03:02,  1.73it/s]Extractor Estimating: 329it [03:02,  1.76it/s]Extractor Estimating: 330it [03:03,  1.76it/s]Extractor Estimating: 331it [03:04,  1.75it/s]Extractor Estimating: 332it [03:04,  1.72it/s]Extractor Estimating: 333it [03:05,  1.74it/s]Extractor Estimating: 334it [03:05,  1.77it/s]Extractor Estimating: 335it [03:06,  1.81it/s]Extractor Estimating: 336it [03:06,  1.79it/s]Extractor Estimating: 337it [03:07,  1.78it/s]Extractor Estimating: 338it [03:08,  1.74it/s]Extractor Estimating: 339it [03:08,  1.72it/s]Extractor Estimating: 340it [03:09,  1.75it/s]Extractor Estimating: 341it [03:09,  1.72it/s]Extractor Estimating: 342it [03:10,  1.71it/s]Extractor Estimating: 343it [03:10,  1.72it/s]Extractor Estimating: 344it [03:11,  1.70it/s]Extractor Estimating: 345it [03:12,  1.70it/s]Extractor Estimating: 346it [03:12,  1.70it/s]Extractor Estimating: 347it [03:13,  1.46it/s]Extractor Estimating: 348it [03:14,  1.48it/s]Extractor Estimating: 349it [03:14,  1.50it/s]Extractor Estimating: 350it [03:15,  1.54it/s]Extractor Estimating: 351it [03:16,  1.65it/s]Extractor Estimating: 352it [03:16,  1.70it/s]Extractor Estimating: 353it [03:17,  1.79it/s]Extractor Estimating: 354it [03:17,  1.87it/s]Extractor Estimating: 355it [03:18,  1.90it/s]Extractor Estimating: 356it [03:18,  1.97it/s]Extractor Estimating: 357it [03:19,  1.96it/s]Extractor Estimating: 358it [03:19,  1.94it/s]Extractor Estimating: 359it [03:20,  1.95it/s]Extractor Estimating: 360it [03:20,  2.01it/s]Extractor Estimating: 361it [03:21,  1.96it/s]Extractor Estimating: 362it [03:21,  1.99it/s]Extractor Estimating: 363it [03:22,  1.95it/s]Extractor Estimating: 364it [03:22,  1.96it/s]Extractor Estimating: 365it [03:23,  1.95it/s]Extractor Estimating: 366it [03:23,  1.97it/s]Extractor Estimating: 367it [03:24,  1.98it/s]Extractor Estimating: 368it [03:24,  1.94it/s]Extractor Estimating: 369it [03:25,  1.90it/s]Extractor Estimating: 370it [03:25,  1.89it/s]Extractor Estimating: 371it [03:26,  1.90it/s]Extractor Estimating: 372it [03:27,  1.70it/s]Extractor Estimating: 373it [03:27,  1.75it/s]Extractor Estimating: 374it [03:28,  1.80it/s]Extractor Estimating: 375it [03:28,  1.85it/s]Extractor Estimating: 376it [03:29,  1.83it/s]Extractor Estimating: 377it [03:29,  1.82it/s]Extractor Estimating: 378it [03:30,  1.80it/s]Extractor Estimating: 379it [03:30,  1.77it/s]Extractor Estimating: 380it [03:31,  1.81it/s]Extractor Estimating: 381it [03:31,  1.84it/s]Extractor Estimating: 382it [03:32,  1.87it/s]Extractor Estimating: 383it [03:32,  1.86it/s]Extractor Estimating: 384it [03:33,  1.86it/s]Extractor Estimating: 385it [03:34,  1.87it/s]Extractor Estimating: 386it [03:34,  1.82it/s]Extractor Estimating: 387it [03:35,  1.82it/s]Extractor Estimating: 388it [03:35,  1.83it/s]Extractor Estimating: 389it [03:36,  1.81it/s]Extractor Estimating: 390it [03:36,  1.82it/s]Extractor Estimating: 391it [03:37,  1.78it/s]Extractor Estimating: 392it [03:37,  1.76it/s]Extractor Estimating: 393it [03:38,  1.75it/s]Extractor Estimating: 394it [03:39,  1.78it/s]Extractor Estimating: 395it [03:39,  1.80it/s]Extractor Estimating: 396it [03:40,  1.76it/s]Extractor Estimating: 397it [03:40,  1.74it/s]Extractor Estimating: 398it [03:41,  1.79it/s]Extractor Estimating: 399it [03:41,  1.83it/s]Extractor Estimating: 400it [03:42,  1.85it/s]Extractor Estimating: 401it [03:42,  1.89it/s]Extractor Estimating: 402it [03:43,  1.78it/s]Extractor Estimating: 403it [03:44,  1.80it/s]Extractor Estimating: 404it [03:44,  1.79it/s]Extractor Estimating: 405it [03:45,  1.82it/s]Extractor Estimating: 406it [03:45,  1.83it/s]Extractor Estimating: 407it [03:46,  1.89it/s]Extractor Estimating: 408it [03:46,  1.85it/s]Extractor Estimating: 409it [03:47,  1.82it/s]Extractor Estimating: 410it [03:47,  1.84it/s]Extractor Estimating: 411it [03:48,  1.89it/s]Extractor Estimating: 412it [03:48,  1.94it/s]Extractor Estimating: 413it [03:49,  1.92it/s]Extractor Estimating: 414it [03:49,  1.86it/s]Extractor Estimating: 415it [03:50,  1.83it/s]Extractor Estimating: 416it [03:51,  1.85it/s]Extractor Estimating: 417it [03:51,  1.89it/s]Extractor Estimating: 418it [03:52,  1.86it/s]Extractor Estimating: 419it [03:52,  1.87it/s]Extractor Estimating: 420it [03:53,  1.84it/s]Extractor Estimating: 421it [03:53,  1.86it/s]Extractor Estimating: 422it [03:54,  1.84it/s]Extractor Estimating: 423it [03:54,  1.87it/s]Extractor Estimating: 424it [03:55,  1.87it/s]Extractor Estimating: 425it [03:55,  1.85it/s]Extractor Estimating: 426it [03:56,  1.82it/s]Extractor Estimating: 427it [03:57,  1.78it/s]Extractor Estimating: 428it [03:57,  1.81it/s]Extractor Estimating: 429it [03:58,  1.84it/s]Extractor Estimating: 430it [03:58,  1.81it/s]Extractor Estimating: 431it [03:59,  1.81it/s]Extractor Estimating: 432it [03:59,  1.82it/s]Extractor Estimating: 433it [04:00,  1.87it/s]Extractor Estimating: 434it [04:00,  1.84it/s]Extractor Estimating: 435it [04:01,  1.78it/s]Extractor Estimating: 436it [04:01,  1.77it/s]Extractor Estimating: 437it [04:02,  1.80it/s]Extractor Estimating: 438it [04:03,  1.61it/s]Extractor Estimating: 439it [04:03,  1.60it/s]Extractor Estimating: 440it [04:04,  1.66it/s]Extractor Estimating: 441it [04:04,  1.76it/s]Extractor Estimating: 442it [04:05,  1.78it/s]Extractor Estimating: 443it [04:06,  1.80it/s]Extractor Estimating: 444it [04:06,  1.77it/s]Extractor Estimating: 445it [04:07,  1.68it/s]Extractor Estimating: 446it [04:07,  1.70it/s]Extractor Estimating: 447it [04:08,  1.75it/s]Extractor Estimating: 448it [04:08,  1.75it/s]Extractor Estimating: 449it [04:09,  1.78it/s]Extractor Estimating: 450it [04:10,  1.79it/s]Extractor Estimating: 451it [04:10,  1.82it/s]Extractor Estimating: 452it [04:11,  1.83it/s]Extractor Estimating: 453it [04:11,  1.89it/s]Extractor Estimating: 454it [04:12,  1.79it/s]Extractor Estimating: 455it [04:12,  1.80it/s]Extractor Estimating: 456it [04:13,  1.83it/s]Extractor Estimating: 457it [04:13,  1.85it/s]Extractor Estimating: 458it [04:14,  1.84it/s]Extractor Estimating: 459it [04:14,  1.86it/s]Extractor Estimating: 460it [04:15,  1.90it/s]Extractor Estimating: 461it [04:15,  1.90it/s]Extractor Estimating: 462it [04:16,  1.87it/s]Extractor Estimating: 463it [04:17,  1.93it/s]Extractor Estimating: 464it [04:17,  1.87it/s]Extractor Estimating: 465it [04:18,  1.87it/s]Extractor Estimating: 466it [04:18,  1.89it/s]Extractor Estimating: 467it [04:19,  1.90it/s]Extractor Estimating: 468it [04:19,  1.89it/s]Extractor Estimating: 469it [04:20,  1.88it/s]Extractor Estimating: 470it [04:20,  1.89it/s]Extractor Estimating: 471it [04:21,  1.88it/s]Extractor Estimating: 472it [04:21,  1.80it/s]Extractor Estimating: 473it [04:22,  1.78it/s]Extractor Estimating: 474it [04:22,  1.82it/s]Extractor Estimating: 475it [04:23,  1.82it/s]Extractor Estimating: 476it [04:24,  1.89it/s]Extractor Estimating: 477it [04:24,  1.83it/s]Extractor Estimating: 478it [04:25,  1.83it/s]Extractor Estimating: 479it [04:25,  1.82it/s]Extractor Estimating: 480it [04:26,  1.86it/s]Extractor Estimating: 481it [04:26,  1.92it/s]Extractor Estimating: 482it [04:27,  1.85it/s]Extractor Estimating: 483it [04:27,  1.88it/s]Extractor Estimating: 484it [04:28,  1.91it/s]Extractor Estimating: 485it [04:28,  1.81it/s]Extractor Estimating: 486it [04:29,  1.84it/s]Extractor Estimating: 487it [04:29,  1.88it/s]Extractor Estimating: 488it [04:30,  1.79it/s]Extractor Estimating: 489it [04:31,  1.83it/s]Extractor Estimating: 490it [04:31,  1.86it/s]Extractor Estimating: 491it [04:32,  1.88it/s]Extractor Estimating: 492it [04:32,  1.90it/s]Extractor Estimating: 493it [04:33,  1.83it/s]Extractor Estimating: 494it [04:33,  1.89it/s]Extractor Estimating: 495it [04:34,  1.77it/s]Extractor Estimating: 496it [04:34,  1.85it/s]Extractor Estimating: 497it [04:35,  1.83it/s]Extractor Estimating: 498it [04:35,  1.82it/s]Extractor Estimating: 499it [04:36,  1.84it/s]Extractor Estimating: 500it [04:36,  2.19it/s]Extractor Estimating: 500it [04:36,  1.81it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:57:18,467 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:57:18,473 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:57:18,473 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:57:18,473 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:57:18,473 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:57:19,089 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:57:19,090 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:57:19,676 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:57:20,731 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:57:20,732 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:57:23,684 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:57:23,689 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:57:23,689 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:57:23,689 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:57:23,689 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:57:24,317 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:57:24,318 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:57:24,872 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:57:25,045 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:57:25,046 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 15:47:29,444 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 15:47:29,478 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9980 mean pseudo reward: 0.9358189852533241
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl'}
train vocab size: 24225
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24325, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24325, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.896, loss:674.7603
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.899, loss:619.3783
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.892, loss:595.6531
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.946, loss:610.0077
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.902, loss:599.0633
>> valid entity prec:0.5242, rec:0.4526, f1:0.4858
>> valid relation prec:0.3262, rec:0.0391, f1:0.0698
>> valid relation with NER prec:0.3262, rec:0.0391, f1:0.0698
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.682, loss:576.8378
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 0.897, loss:597.4468
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 0.903, loss:605.8073
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.906, loss:552.4204
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.890, loss:589.5414
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4922, rec:0.4504, f1:0.4704
>> valid relation prec:0.3343, rec:0.0401, f1:0.0716
>> valid relation with NER prec:0.3343, rec:0.0401, f1:0.0716
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 268, avg_time 2.673, loss:606.7043
g_step 1200, step 368, avg_time 0.906, loss:605.6779
g_step 1300, step 52, avg_time 0.894, loss:594.2187
g_step 1400, step 152, avg_time 0.899, loss:574.5039
g_step 1500, step 252, avg_time 0.884, loss:585.4748
>> valid entity prec:0.5047, rec:0.4499, f1:0.4757
>> valid relation prec:0.1987, rec:0.0217, f1:0.0391
>> valid relation with NER prec:0.1987, rec:0.0217, f1:0.0391
g_step 1600, step 352, avg_time 2.684, loss:571.0817
g_step 1700, step 36, avg_time 0.898, loss:555.1835
g_step 1800, step 136, avg_time 0.901, loss:547.7334
g_step 1900, step 236, avg_time 0.886, loss:546.5064
g_step 2000, step 336, avg_time 0.900, loss:565.9384
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4842, rec:0.4933, f1:0.4887
>> valid relation prec:0.2543, rec:0.0329, f1:0.0583
>> valid relation with NER prec:0.2543, rec:0.0329, f1:0.0583
new max entity f1 on valid!
g_step 2100, step 20, avg_time 2.630, loss:534.6909
g_step 2200, step 120, avg_time 0.891, loss:528.5848
g_step 2300, step 220, avg_time 0.896, loss:522.8614
g_step 2400, step 320, avg_time 0.878, loss:525.4324
g_step 2500, step 4, avg_time 0.883, loss:541.5217
>> valid entity prec:0.5053, rec:0.4430, f1:0.4721
>> valid relation prec:0.3362, rec:0.0404, f1:0.0722
>> valid relation with NER prec:0.3362, rec:0.0404, f1:0.0722
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 104, avg_time 2.634, loss:484.6124
g_step 2700, step 204, avg_time 0.879, loss:508.3049
g_step 2800, step 304, avg_time 0.878, loss:529.3099
g_step 2900, step 404, avg_time 0.888, loss:505.6418
g_step 3000, step 88, avg_time 0.884, loss:484.3327
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5224, rec:0.3572, f1:0.4243
>> valid relation prec:0.2181, rec:0.0259, f1:0.0463
>> valid relation with NER prec:0.2181, rec:0.0259, f1:0.0463
g_step 3100, step 188, avg_time 2.624, loss:487.2433
g_step 3200, step 288, avg_time 0.884, loss:488.3082
g_step 3300, step 388, avg_time 0.891, loss:505.0988
g_step 3400, step 72, avg_time 0.881, loss:474.1314
g_step 3500, step 172, avg_time 0.887, loss:457.4476
>> valid entity prec:0.4463, rec:0.4125, f1:0.4287
>> valid relation prec:0.2167, rec:0.0283, f1:0.0501
>> valid relation with NER prec:0.2167, rec:0.0283, f1:0.0501
g_step 3600, step 272, avg_time 2.632, loss:478.4476
g_step 3700, step 372, avg_time 0.890, loss:489.3845
g_step 3800, step 56, avg_time 0.875, loss:460.7507
g_step 3900, step 156, avg_time 0.893, loss:446.3729
g_step 4000, step 256, avg_time 0.877, loss:466.6435
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4695, rec:0.4088, f1:0.4371
>> valid relation prec:0.2514, rec:0.0305, f1:0.0544
>> valid relation with NER prec:0.2514, rec:0.0305, f1:0.0544
g_step 4100, step 356, avg_time 2.628, loss:474.7018
g_step 4200, step 40, avg_time 0.896, loss:448.2827
g_step 4300, step 140, avg_time 0.891, loss:434.8590
g_step 4400, step 240, avg_time 0.883, loss:448.4748
g_step 4500, step 340, avg_time 0.872, loss:444.3035
>> valid entity prec:0.4757, rec:0.4286, f1:0.4509
>> valid relation prec:0.2085, rec:0.0242, f1:0.0434
>> valid relation with NER prec:0.2085, rec:0.0242, f1:0.0434
g_step 4600, step 24, avg_time 2.632, loss:447.0217
g_step 4700, step 124, avg_time 0.900, loss:422.2621
g_step 4800, step 224, avg_time 0.883, loss:419.5477
g_step 4900, step 324, avg_time 0.882, loss:428.2908
g_step 5000, step 8, avg_time 0.888, loss:449.0652
learning rate was adjusted to 0.0008
>> valid entity prec:0.4878, rec:0.4085, f1:0.4446
>> valid relation prec:0.2126, rec:0.0259, f1:0.0462
>> valid relation with NER prec:0.2126, rec:0.0259, f1:0.0462
g_step 5100, step 108, avg_time 2.639, loss:415.5312
g_step 5200, step 208, avg_time 0.878, loss:405.2091
g_step 5300, step 308, avg_time 0.884, loss:414.8134
g_step 5400, step 408, avg_time 0.879, loss:436.7012
g_step 5500, step 92, avg_time 0.875, loss:384.3372
>> valid entity prec:0.4523, rec:0.4240, f1:0.4377
>> valid relation prec:0.1909, rec:0.0215, f1:0.0386
>> valid relation with NER prec:0.1909, rec:0.0215, f1:0.0386
g_step 5600, step 192, avg_time 2.647, loss:404.5114
g_step 5700, step 292, avg_time 0.887, loss:410.5717
g_step 5800, step 392, avg_time 0.873, loss:403.7267
g_step 5900, step 76, avg_time 0.883, loss:384.7560
g_step 6000, step 176, avg_time 0.882, loss:383.0252
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4424, rec:0.5004, f1:0.4697
>> valid relation prec:0.1828, rec:0.0239, f1:0.0422
>> valid relation with NER prec:0.1828, rec:0.0239, f1:0.0422
g_step 6100, step 276, avg_time 2.651, loss:394.3677
g_step 6200, step 376, avg_time 0.885, loss:406.2609
g_step 6300, step 60, avg_time 0.887, loss:374.5403
g_step 6400, step 160, avg_time 0.888, loss:381.1101
g_step 6500, step 260, avg_time 0.897, loss:390.5231
>> valid entity prec:0.4765, rec:0.4534, f1:0.4646
>> valid relation prec:0.2257, rec:0.0333, f1:0.0580
>> valid relation with NER prec:0.2257, rec:0.0333, f1:0.0580
g_step 6600, step 360, avg_time 2.631, loss:376.5864
g_step 6700, step 44, avg_time 0.882, loss:387.6288
g_step 6800, step 144, avg_time 0.891, loss:360.0509
g_step 6900, step 244, avg_time 0.874, loss:368.6809
g_step 7000, step 344, avg_time 0.887, loss:383.1803
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4710, rec:0.4099, f1:0.4383
>> valid relation prec:0.1974, rec:0.0256, f1:0.0453
>> valid relation with NER prec:0.1974, rec:0.0256, f1:0.0453
g_step 7100, step 28, avg_time 2.638, loss:371.8288
g_step 7200, step 128, avg_time 0.898, loss:349.5796
g_step 7300, step 228, avg_time 0.881, loss:358.4591
g_step 7400, step 328, avg_time 0.891, loss:367.9391
g_step 7500, step 12, avg_time 0.877, loss:362.2114
>> valid entity prec:0.4793, rec:0.4431, f1:0.4605
>> valid relation prec:0.2254, rec:0.0269, f1:0.0481
>> valid relation with NER prec:0.2254, rec:0.0269, f1:0.0481
g_step 7600, step 112, avg_time 2.627, loss:335.5471
g_step 7700, step 212, avg_time 0.887, loss:329.1047
g_step 7800, step 312, avg_time 0.874, loss:369.1289
g_step 7900, step 412, avg_time 0.897, loss:372.5163
g_step 8000, step 96, avg_time 0.886, loss:319.0273
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4138, rec:0.4587, f1:0.4351
>> valid relation prec:0.1934, rec:0.0300, f1:0.0520
>> valid relation with NER prec:0.1934, rec:0.0300, f1:0.0520
g_step 8100, step 196, avg_time 2.638, loss:341.0015
g_step 8200, step 296, avg_time 0.884, loss:337.1945
g_step 8300, step 396, avg_time 0.884, loss:340.7791
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 15:47:29 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 15:47:29 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_15-47-29_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 15:47:30 - WARNING - datasets.builder -   Using custom data configuration default-fcc54177c0a43fff
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-fcc54177c0a43fff/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  2.05 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 15:47:31,494 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 15:47:31,495 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 15:47:31,496 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 15:47:31,497 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 15:47:31,511 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:47:31,517 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:47:31,517 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:47:31,517 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:47:31,518 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:47:31,518 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:47:31,518 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 15:47:31,677 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 15:47:34,572 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 15:47:34,577 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-fcc54177c0a43fff/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.70ba/s] 20%|██        | 2/10 [00:00<00:01,  4.53ba/s] 30%|███       | 3/10 [00:00<00:01,  4.91ba/s] 40%|████      | 4/10 [00:00<00:01,  5.06ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.15ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.48ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.72ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.91ba/s] 90%|█████████ | 9/10 [00:01<00:00,  5.04ba/s]100%|██████████| 10/10 [00:02<00:00,  5.04ba/s]100%|██████████| 10/10 [00:02<00:00,  4.77ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  4.44ba/s] 33%|███▎      | 2/6 [00:00<00:00,  4.75ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.89ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.94ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.92ba/s]100%|██████████| 6/6 [00:01<00:00,  5.21ba/s]100%|██████████| 6/6 [00:01<00:00,  5.01ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  8.61ba/s] 30%|███       | 3/10 [00:00<00:00, 11.15ba/s] 50%|█████     | 5/10 [00:00<00:00, 11.77ba/s] 70%|███████   | 7/10 [00:00<00:00, 12.06ba/s] 90%|█████████ | 9/10 [00:00<00:00, 12.31ba/s]100%|██████████| 10/10 [00:00<00:00, 11.99ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  9.69ba/s] 50%|█████     | 3/6 [00:00<00:00, 11.57ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 12.11ba/s]100%|██████████| 6/6 [00:00<00:00, 12.22ba/s]
[INFO|trainer.py:414] 2023-08-29 15:47:39,813 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 15:47:39,832 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 15:47:39,832 >>   Num examples = 9999
[INFO|trainer.py:1149] 2023-08-29 15:47:39,832 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 15:47:39,832 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 15:47:39,832 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 15:47:39,832 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 15:47:39,832 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<09:26,  1.38it/s]  0%|          | 2/780 [00:01<06:21,  2.04it/s]  0%|          | 3/780 [00:01<05:17,  2.45it/s]  1%|          | 4/780 [00:01<04:49,  2.68it/s]  1%|          | 5/780 [00:01<04:27,  2.90it/s]  1%|          | 6/780 [00:02<04:16,  3.02it/s]  1%|          | 7/780 [00:02<04:07,  3.12it/s]  1%|          | 8/780 [00:02<04:04,  3.16it/s]  1%|          | 9/780 [00:03<03:58,  3.23it/s]  1%|▏         | 10/780 [00:03<03:55,  3.28it/s]  1%|▏         | 11/780 [00:03<03:53,  3.30it/s]  2%|▏         | 12/780 [00:04<03:50,  3.33it/s]  2%|▏         | 13/780 [00:04<03:50,  3.33it/s]  2%|▏         | 14/780 [00:04<03:47,  3.36it/s]  2%|▏         | 15/780 [00:04<03:46,  3.38it/s]  2%|▏         | 16/780 [00:05<03:45,  3.38it/s]  2%|▏         | 17/780 [00:05<03:44,  3.39it/s]  2%|▏         | 18/780 [00:05<03:44,  3.40it/s]  2%|▏         | 19/780 [00:06<03:43,  3.40it/s]  3%|▎         | 20/780 [00:06<03:43,  3.41it/s]  3%|▎         | 21/780 [00:06<03:42,  3.41it/s]  3%|▎         | 22/780 [00:07<03:42,  3.41it/s]  3%|▎         | 23/780 [00:07<03:41,  3.41it/s]  3%|▎         | 24/780 [00:07<03:42,  3.40it/s]  3%|▎         | 25/780 [00:07<03:42,  3.40it/s]  3%|▎         | 26/780 [00:08<03:41,  3.40it/s]  3%|▎         | 27/780 [00:08<03:40,  3.41it/s]  4%|▎         | 28/780 [00:08<03:40,  3.41it/s]  4%|▎         | 29/780 [00:09<03:40,  3.41it/s]  4%|▍         | 30/780 [00:09<03:39,  3.41it/s]  4%|▍         | 31/780 [00:09<03:39,  3.41it/s]  4%|▍         | 32/780 [00:09<03:39,  3.41it/s]  4%|▍         | 33/780 [00:10<03:38,  3.41it/s]  4%|▍         | 34/780 [00:10<03:38,  3.41it/s]  4%|▍         | 35/780 [00:10<03:39,  3.40it/s]  5%|▍         | 36/780 [00:11<03:38,  3.41it/s]  5%|▍         | 37/780 [00:11<03:38,  3.41it/s]  5%|▍         | 38/780 [00:11<03:37,  3.41it/s]  5%|▌         | 39/780 [00:11<03:37,  3.41it/s]  5%|▌         | 40/780 [00:12<03:37,  3.41it/s]  5%|▌         | 41/780 [00:12<03:36,  3.41it/s]  5%|▌         | 42/780 [00:12<03:36,  3.41it/s]  6%|▌         | 43/780 [00:13<03:36,  3.41it/s]  6%|▌         | 44/780 [00:13<03:35,  3.41it/s]  6%|▌         | 45/780 [00:13<03:35,  3.41it/s]  6%|▌         | 46/780 [00:14<03:35,  3.40it/s]  6%|▌         | 47/780 [00:14<03:35,  3.40it/s]  6%|▌         | 48/780 [00:14<03:35,  3.40it/s]  6%|▋         | 49/780 [00:14<03:34,  3.41it/s]  6%|▋         | 50/780 [00:15<03:34,  3.41it/s]  7%|▋         | 51/780 [00:15<03:33,  3.41it/s]  7%|▋         | 52/780 [00:15<03:33,  3.41it/s]  7%|▋         | 53/780 [00:16<03:33,  3.41it/s]  7%|▋         | 54/780 [00:16<03:32,  3.41it/s]  7%|▋         | 55/780 [00:16<03:32,  3.41it/s]  7%|▋         | 56/780 [00:16<03:32,  3.41it/s]  7%|▋         | 57/780 [00:17<03:32,  3.40it/s]  7%|▋         | 58/780 [00:17<03:32,  3.40it/s]  8%|▊         | 59/780 [00:17<03:32,  3.40it/s]  8%|▊         | 60/780 [00:18<03:31,  3.40it/s]  8%|▊         | 61/780 [00:18<03:31,  3.41it/s]  8%|▊         | 62/780 [00:18<03:30,  3.41it/s]  8%|▊         | 63/780 [00:19<03:30,  3.41it/s]  8%|▊         | 64/780 [00:19<03:30,  3.41it/s]  8%|▊         | 65/780 [00:19<03:29,  3.41it/s]  8%|▊         | 66/780 [00:19<03:29,  3.41it/s]  9%|▊         | 67/780 [00:20<03:29,  3.41it/s]  9%|▊         | 68/780 [00:20<03:29,  3.39it/s]  9%|▉         | 69/780 [00:20<03:29,  3.40it/s]  9%|▉         | 70/780 [00:21<03:28,  3.40it/s]  9%|▉         | 71/780 [00:21<03:28,  3.40it/s]  9%|▉         | 72/780 [00:21<03:27,  3.40it/s]  9%|▉         | 73/780 [00:21<03:27,  3.40it/s]  9%|▉         | 74/780 [00:22<03:27,  3.41it/s] 10%|▉         | 75/780 [00:22<03:27,  3.40it/s] 10%|▉         | 76/780 [00:22<03:26,  3.41it/s] 10%|▉         | 77/780 [00:23<03:26,  3.41it/s] 10%|█         | 78/780 [00:23<03:25,  3.41it/s] 10%|█         | 79/780 [00:23<03:27,  3.39it/s] 10%|█         | 80/780 [00:24<03:26,  3.39it/s] 10%|█         | 81/780 [00:24<03:25,  3.40it/s] 11%|█         | 82/780 [00:24<03:25,  3.40it/s] 11%|█         | 83/780 [00:24<03:24,  3.40it/s] 11%|█         | 84/780 [00:25<03:23,  3.42it/s] 11%|█         | 85/780 [00:25<03:22,  3.44it/s] 11%|█         | 86/780 [00:25<03:21,  3.45it/s] 11%|█         | 87/780 [00:26<03:20,  3.46it/s] 11%|█▏        | 88/780 [00:26<03:19,  3.47it/s] 11%|█▏        | 89/780 [00:26<03:19,  3.47it/s] 12%|█▏        | 90/780 [00:26<03:19,  3.45it/s] 12%|█▏        | 91/780 [00:27<03:19,  3.46it/s] 12%|█▏        | 92/780 [00:27<03:18,  3.46it/s] 12%|█▏        | 93/780 [00:27<03:18,  3.47it/s] 12%|█▏        | 94/780 [00:28<03:17,  3.47it/s] 12%|█▏        | 95/780 [00:28<03:17,  3.47it/s] 12%|█▏        | 96/780 [00:28<03:16,  3.47it/s] 12%|█▏        | 97/780 [00:28<03:16,  3.48it/s] 13%|█▎        | 98/780 [00:29<03:16,  3.48it/s] 13%|█▎        | 99/780 [00:29<03:15,  3.48it/s] 13%|█▎        | 100/780 [00:29<03:15,  3.48it/s] 13%|█▎        | 101/780 [00:30<03:15,  3.47it/s] 13%|█▎        | 102/780 [00:30<03:15,  3.47it/s] 13%|█▎        | 103/780 [00:30<03:15,  3.47it/s] 13%|█▎        | 104/780 [00:30<03:14,  3.47it/s] 13%|█▎        | 105/780 [00:31<03:14,  3.48it/s] 14%|█▎        | 106/780 [00:31<03:13,  3.48it/s] 14%|█▎        | 107/780 [00:31<03:13,  3.48it/s] 14%|█▍        | 108/780 [00:32<03:13,  3.48it/s] 14%|█▍        | 109/780 [00:32<03:12,  3.48it/s] 14%|█▍        | 110/780 [00:32<03:12,  3.48it/s] 14%|█▍        | 111/780 [00:32<03:12,  3.48it/s] 14%|█▍        | 112/780 [00:33<03:12,  3.46it/s] 14%|█▍        | 113/780 [00:33<03:12,  3.47it/s] 15%|█▍        | 114/780 [00:33<03:11,  3.47it/s] 15%|█▍        | 115/780 [00:34<03:11,  3.47it/s] 15%|█▍        | 116/780 [00:34<03:11,  3.48it/s] 15%|█▌        | 117/780 [00:34<03:10,  3.48it/s] 15%|█▌        | 118/780 [00:34<03:10,  3.47it/s] 15%|█▌        | 119/780 [00:35<03:10,  3.48it/s] 15%|█▌        | 120/780 [00:35<03:09,  3.48it/s] 16%|█▌        | 121/780 [00:35<03:09,  3.48it/s] 16%|█▌        | 122/780 [00:36<03:09,  3.48it/s] 16%|█▌        | 123/780 [00:36<03:10,  3.44it/s] 16%|█▌        | 124/780 [00:36<03:09,  3.46it/s] 16%|█▌        | 125/780 [00:37<03:09,  3.46it/s] 16%|█▌        | 126/780 [00:37<03:08,  3.47it/s] 16%|█▋        | 127/780 [00:37<03:08,  3.47it/s] 16%|█▋        | 128/780 [00:37<03:07,  3.47it/s] 17%|█▋        | 129/780 [00:38<03:07,  3.47it/s] 17%|█▋        | 130/780 [00:38<03:07,  3.47it/s] 17%|█▋        | 131/780 [00:38<03:06,  3.47it/s] 17%|█▋        | 132/780 [00:39<03:06,  3.47it/s] 17%|█▋        | 133/780 [00:39<03:06,  3.48it/s] 17%|█▋        | 134/780 [00:39<03:06,  3.46it/s] 17%|█▋        | 135/780 [00:39<03:05,  3.47it/s] 17%|█▋        | 136/780 [00:40<03:05,  3.47it/s] 18%|█▊        | 137/780 [00:40<03:05,  3.47it/s] 18%|█▊        | 138/780 [00:40<03:04,  3.47it/s] 18%|█▊        | 139/780 [00:41<03:04,  3.48it/s] 18%|█▊        | 140/780 [00:41<03:04,  3.47it/s] 18%|█▊        | 141/780 [00:41<03:03,  3.47it/s] 18%|█▊        | 142/780 [00:41<03:03,  3.47it/s] 18%|█▊        | 143/780 [00:42<03:03,  3.47it/s] 18%|█▊        | 144/780 [00:42<03:03,  3.47it/s] 19%|█▊        | 145/780 [00:42<03:04,  3.45it/s] 19%|█▊        | 146/780 [00:43<03:03,  3.46it/s] 19%|█▉        | 147/780 [00:43<03:02,  3.46it/s] 19%|█▉        | 148/780 [00:43<03:03,  3.44it/s] 19%|█▉        | 149/780 [00:43<03:03,  3.43it/s] 19%|█▉        | 150/780 [00:44<03:03,  3.42it/s] 19%|█▉        | 151/780 [00:44<03:04,  3.42it/s] 19%|█▉        | 152/780 [00:44<03:03,  3.41it/s] 20%|█▉        | 153/780 [00:45<03:03,  3.41it/s] 20%|█▉        | 154/780 [00:45<03:03,  3.41it/s] 20%|█▉        | 155/780 [00:45<03:03,  3.41it/s] 20%|██        | 156/780 [00:46<03:03,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 15:48:25,882 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:48:25,882 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 15:48:25,882 >>   Batch size = 8

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 54.35it/s][A
  2%|▏         | 12/733 [00:00<00:15, 46.93it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.84it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.81it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.23it/s][A
  4%|▍         | 32/733 [00:00<00:15, 43.97it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.83it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.67it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.75it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.89it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.90it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.79it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.59it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.56it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.54it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.42it/s][A
 12%|█▏        | 87/733 [00:01<00:15, 42.47it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.91it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.95it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.89it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.72it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.63it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.57it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.54it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.55it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.63it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.67it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.77it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.71it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.60it/s][A
 21%|██▏       | 157/733 [00:03<00:14, 40.97it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 42.04it/s][A
 23%|██▎       | 167/733 [00:03<00:13, 42.63it/s][A
 23%|██▎       | 172/733 [00:03<00:13, 43.00it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.19it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.39it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.49it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.51it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.27it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.19it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.37it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.52it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.55it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.65it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.68it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.73it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.55it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.40it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.32it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.41it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.52it/s][A
 36%|███▌      | 262/733 [00:06<00:10, 43.74it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.75it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.70it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.70it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.56it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.42it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.39it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.48it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.68it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.77it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.75it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.71it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.58it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.45it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.40it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.48it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.62it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.66it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.67it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.78it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.69it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.50it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.44it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.48it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.48it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.53it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.62it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.73it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.76it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.63it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.59it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.53it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.52it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.46it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.54it/s][A
 60%|█████▉    | 437/733 [00:10<00:06, 43.69it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.77it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.70it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.63it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.56it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.51it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.45it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.54it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.57it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.69it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.69it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.65it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.58it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.53it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.53it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.36it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.50it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.60it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.64it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.61it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.61it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.59it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.58it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.53it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.59it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.64it/s][A
 77%|███████▋  | 567/733 [00:13<00:03, 43.67it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.69it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.62it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.64it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.60it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.56it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.49it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.61it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.65it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.67it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.64it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.61it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.63it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.56it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.47it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.60it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.63it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.63it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.66it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.66it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.61it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.50it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.51it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.53it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.61it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.62it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.66it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.72it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.72it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.62it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.57it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.53it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.49it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.51it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.51it/s][A 20%|██        | 156/780 [01:02<03:03,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 15:48:42,780 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 15:48:42,801 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:48:44,987 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:48:45,009 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:48:45,018 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:10<1:18:15,  7.54s/it] 20%|██        | 158/780 [01:10<55:37,  5.37s/it]   20%|██        | 159/780 [01:11<39:46,  3.84s/it] 21%|██        | 160/780 [01:11<28:42,  2.78s/it] 21%|██        | 161/780 [01:11<20:58,  2.03s/it] 21%|██        | 162/780 [01:11<15:33,  1.51s/it] 21%|██        | 163/780 [01:12<11:47,  1.15s/it] 21%|██        | 164/780 [01:12<09:08,  1.12it/s] 21%|██        | 165/780 [01:12<07:17,  1.41it/s] 21%|██▏       | 166/780 [01:13<05:59,  1.71it/s] 21%|██▏       | 167/780 [01:13<05:05,  2.01it/s] 22%|██▏       | 168/780 [01:13<04:27,  2.29it/s] 22%|██▏       | 169/780 [01:13<04:01,  2.53it/s] 22%|██▏       | 170/780 [01:14<03:42,  2.75it/s] 22%|██▏       | 171/780 [01:14<03:28,  2.92it/s] 22%|██▏       | 172/780 [01:14<03:19,  3.05it/s] 22%|██▏       | 173/780 [01:15<03:12,  3.15it/s] 22%|██▏       | 174/780 [01:15<03:08,  3.22it/s] 22%|██▏       | 175/780 [01:15<03:04,  3.28it/s] 23%|██▎       | 176/780 [01:16<03:02,  3.32it/s] 23%|██▎       | 177/780 [01:16<03:00,  3.34it/s] 23%|██▎       | 178/780 [01:16<02:59,  3.36it/s] 23%|██▎       | 179/780 [01:16<02:58,  3.37it/s] 23%|██▎       | 180/780 [01:17<02:58,  3.37it/s] 23%|██▎       | 181/780 [01:17<02:57,  3.38it/s] 23%|██▎       | 182/780 [01:17<02:56,  3.39it/s] 23%|██▎       | 183/780 [01:18<02:55,  3.40it/s] 24%|██▎       | 184/780 [01:18<02:55,  3.40it/s] 24%|██▎       | 185/780 [01:18<02:54,  3.40it/s] 24%|██▍       | 186/780 [01:18<02:54,  3.40it/s] 24%|██▍       | 187/780 [01:19<02:54,  3.41it/s] 24%|██▍       | 188/780 [01:19<02:53,  3.41it/s] 24%|██▍       | 189/780 [01:19<02:53,  3.41it/s] 24%|██▍       | 190/780 [01:20<02:53,  3.41it/s] 24%|██▍       | 191/780 [01:20<02:53,  3.40it/s] 25%|██▍       | 192/780 [01:20<02:52,  3.40it/s] 25%|██▍       | 193/780 [01:21<02:52,  3.40it/s] 25%|██▍       | 194/780 [01:21<02:52,  3.40it/s] 25%|██▌       | 195/780 [01:21<02:52,  3.40it/s] 25%|██▌       | 196/780 [01:21<02:51,  3.40it/s] 25%|██▌       | 197/780 [01:22<02:51,  3.40it/s] 25%|██▌       | 198/780 [01:22<02:50,  3.41it/s] 26%|██▌       | 199/780 [01:22<02:50,  3.41it/s] 26%|██▌       | 200/780 [01:23<02:50,  3.41it/s] 26%|██▌       | 201/780 [01:23<02:49,  3.41it/s] 26%|██▌       | 202/780 [01:23<02:49,  3.41it/s] 26%|██▌       | 203/780 [01:23<02:49,  3.41it/s] 26%|██▌       | 204/780 [01:24<02:48,  3.41it/s] 26%|██▋       | 205/780 [01:24<02:48,  3.41it/s] 26%|██▋       | 206/780 [01:24<02:48,  3.41it/s] 27%|██▋       | 207/780 [01:25<02:48,  3.39it/s] 27%|██▋       | 208/780 [01:25<02:48,  3.40it/s] 27%|██▋       | 209/780 [01:25<02:48,  3.40it/s] 27%|██▋       | 210/780 [01:26<02:47,  3.40it/s] 27%|██▋       | 211/780 [01:26<02:47,  3.40it/s] 27%|██▋       | 212/780 [01:26<02:46,  3.40it/s] 27%|██▋       | 213/780 [01:26<02:46,  3.40it/s] 27%|██▋       | 214/780 [01:27<02:46,  3.40it/s] 28%|██▊       | 215/780 [01:27<02:45,  3.40it/s] 28%|██▊       | 216/780 [01:27<02:45,  3.41it/s] 28%|██▊       | 217/780 [01:28<02:45,  3.41it/s] 28%|██▊       | 218/780 [01:28<02:45,  3.39it/s] 28%|██▊       | 219/780 [01:28<02:45,  3.40it/s] 28%|██▊       | 220/780 [01:28<02:44,  3.40it/s] 28%|██▊       | 221/780 [01:29<02:44,  3.40it/s] 28%|██▊       | 222/780 [01:29<02:44,  3.40it/s] 29%|██▊       | 223/780 [01:29<02:43,  3.40it/s] 29%|██▊       | 224/780 [01:30<02:42,  3.42it/s] 29%|██▉       | 225/780 [01:30<02:41,  3.43it/s] 29%|██▉       | 226/780 [01:30<02:40,  3.45it/s] 29%|██▉       | 227/780 [01:30<02:40,  3.46it/s] 29%|██▉       | 228/780 [01:31<02:39,  3.46it/s] 29%|██▉       | 229/780 [01:31<02:39,  3.45it/s] 29%|██▉       | 230/780 [01:31<02:39,  3.46it/s] 30%|██▉       | 231/780 [01:32<02:38,  3.46it/s] 30%|██▉       | 232/780 [01:32<02:38,  3.47it/s] 30%|██▉       | 233/780 [01:32<02:37,  3.47it/s] 30%|███       | 234/780 [01:33<02:37,  3.47it/s] 30%|███       | 235/780 [01:33<02:36,  3.47it/s] 30%|███       | 236/780 [01:33<02:36,  3.47it/s] 30%|███       | 237/780 [01:33<02:36,  3.48it/s] 31%|███       | 238/780 [01:34<02:35,  3.48it/s] 31%|███       | 239/780 [01:34<02:35,  3.48it/s] 31%|███       | 240/780 [01:34<02:36,  3.46it/s] 31%|███       | 241/780 [01:35<02:35,  3.46it/s] 31%|███       | 242/780 [01:35<02:35,  3.47it/s] 31%|███       | 243/780 [01:35<02:34,  3.47it/s] 31%|███▏      | 244/780 [01:35<02:34,  3.47it/s] 31%|███▏      | 245/780 [01:36<02:33,  3.48it/s] 32%|███▏      | 246/780 [01:36<02:33,  3.47it/s] 32%|███▏      | 247/780 [01:36<02:33,  3.48it/s] 32%|███▏      | 248/780 [01:37<02:32,  3.48it/s] 32%|███▏      | 249/780 [01:37<02:32,  3.48it/s] 32%|███▏      | 250/780 [01:37<02:32,  3.48it/s] 32%|███▏      | 251/780 [01:37<02:32,  3.46it/s] 32%|███▏      | 252/780 [01:38<02:32,  3.46it/s] 32%|███▏      | 253/780 [01:38<02:32,  3.47it/s] 33%|███▎      | 254/780 [01:38<02:31,  3.47it/s] 33%|███▎      | 255/780 [01:39<02:31,  3.47it/s] 33%|███▎      | 256/780 [01:39<02:30,  3.47it/s] 33%|███▎      | 257/780 [01:39<02:30,  3.48it/s] 33%|███▎      | 258/780 [01:39<02:30,  3.47it/s] 33%|███▎      | 259/780 [01:40<02:29,  3.48it/s] 33%|███▎      | 260/780 [01:40<02:29,  3.48it/s] 33%|███▎      | 261/780 [01:40<02:29,  3.48it/s] 34%|███▎      | 262/780 [01:41<02:30,  3.45it/s] 34%|███▎      | 263/780 [01:41<02:29,  3.46it/s] 34%|███▍      | 264/780 [01:41<02:28,  3.46it/s] 34%|███▍      | 265/780 [01:41<02:28,  3.46it/s] 34%|███▍      | 266/780 [01:42<02:28,  3.46it/s] 34%|███▍      | 267/780 [01:42<02:28,  3.47it/s] 34%|███▍      | 268/780 [01:42<02:27,  3.47it/s] 34%|███▍      | 269/780 [01:43<02:27,  3.47it/s] 35%|███▍      | 270/780 [01:43<02:26,  3.47it/s] 35%|███▍      | 271/780 [01:43<02:26,  3.48it/s] 35%|███▍      | 272/780 [01:43<02:26,  3.47it/s] 35%|███▌      | 273/780 [01:44<02:26,  3.46it/s] 35%|███▌      | 274/780 [01:44<02:26,  3.46it/s] 35%|███▌      | 275/780 [01:44<02:25,  3.47it/s] 35%|███▌      | 276/780 [01:45<02:25,  3.47it/s] 36%|███▌      | 277/780 [01:45<02:24,  3.47it/s] 36%|███▌      | 278/780 [01:45<02:24,  3.47it/s] 36%|███▌      | 279/780 [01:45<02:24,  3.47it/s] 36%|███▌      | 280/780 [01:46<02:23,  3.47it/s] 36%|███▌      | 281/780 [01:46<02:23,  3.47it/s] 36%|███▌      | 282/780 [01:46<02:23,  3.47it/s] 36%|███▋      | 283/780 [01:47<02:22,  3.48it/s] 36%|███▋      | 284/780 [01:47<02:23,  3.45it/s] 37%|███▋      | 285/780 [01:47<02:23,  3.46it/s] 37%|███▋      | 286/780 [01:47<02:22,  3.46it/s] 37%|███▋      | 287/780 [01:48<02:22,  3.46it/s] 37%|███▋      | 288/780 [01:48<02:21,  3.47it/s] 37%|███▋      | 289/780 [01:48<02:21,  3.47it/s] 37%|███▋      | 290/780 [01:49<02:21,  3.47it/s] 37%|███▋      | 291/780 [01:49<02:20,  3.47it/s] 37%|███▋      | 292/780 [01:49<02:24,  3.37it/s] 38%|███▊      | 293/780 [01:50<02:22,  3.41it/s] 38%|███▊      | 294/780 [01:50<02:21,  3.43it/s] 38%|███▊      | 295/780 [01:50<02:21,  3.42it/s] 38%|███▊      | 296/780 [01:50<02:20,  3.43it/s] 38%|███▊      | 297/780 [01:51<02:20,  3.45it/s] 38%|███▊      | 298/780 [01:51<02:19,  3.46it/s] 38%|███▊      | 299/780 [01:51<02:18,  3.46it/s] 38%|███▊      | 300/780 [01:52<02:18,  3.47it/s] 39%|███▊      | 301/780 [01:52<02:18,  3.47it/s] 39%|███▊      | 302/780 [01:52<02:17,  3.47it/s] 39%|███▉      | 303/780 [01:52<02:17,  3.47it/s] 39%|███▉      | 304/780 [01:53<02:17,  3.47it/s] 39%|███▉      | 305/780 [01:53<02:16,  3.47it/s] 39%|███▉      | 306/780 [01:53<02:16,  3.47it/s] 39%|███▉      | 307/780 [01:54<02:16,  3.47it/s] 39%|███▉      | 308/780 [01:54<02:15,  3.47it/s] 40%|███▉      | 309/780 [01:54<02:15,  3.47it/s] 40%|███▉      | 310/780 [01:54<02:15,  3.47it/s] 40%|███▉      | 311/780 [01:55<02:14,  3.48it/s] 40%|████      | 312/780 [01:55<02:14,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 15:49:35,382 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:49:35,382 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 15:49:35,382 >>   Batch size = 8
{'eval_loss': 0.9624547362327576, 'eval_runtime': 16.8664, 'eval_samples_per_second': 347.674, 'eval_steps_per_second': 43.459, 'epoch': 1.0}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 54.17it/s][A
  2%|▏         | 12/733 [00:00<00:15, 46.99it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.44it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.69it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.20it/s][A
  4%|▍         | 32/733 [00:00<00:15, 44.00it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.82it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.60it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.69it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.70it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.71it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.63it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.60it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.55it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.56it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.54it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.52it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.50it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.64it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.70it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.66it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.57it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.56it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.52it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.44it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.49it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.50it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.65it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.71it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.61it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.61it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.58it/s][A
 23%|██▎       | 167/733 [00:03<00:13, 43.48it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.49it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.54it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.60it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.59it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.63it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.59it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.56it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.52it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.52it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.52it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.60it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.62it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.63it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.58it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.54it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.51it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.53it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.53it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.55it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.60it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.65it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.62it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.57it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.55it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.58it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.54it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.57it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.64it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.67it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.61it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.53it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.51it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.50it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.51it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.50it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.62it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.65it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.56it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.53it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.58it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.55it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.47it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.38it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.49it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.64it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.62it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.53it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.55it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.58it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.53it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.48it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.55it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.61it/s][A
 60%|█████▉    | 437/733 [00:10<00:06, 43.66it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.62it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.56it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.60it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.55it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.56it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.55it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.62it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.64it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.63it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.57it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.60it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.57it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.58it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.56it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.56it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.57it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.63it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.56it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.61it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.60it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.55it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.60it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.57it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.57it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.64it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.60it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.52it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.60it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.60it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.52it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.52it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.54it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.66it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.60it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.51it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.54it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.54it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.52it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.53it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.54it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.54it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.56it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.57it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.60it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.57it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.54it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.52it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.60it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.59it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.55it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.56it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.58it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.57it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.48it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.53it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.49it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.64it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.56it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.57it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.57it/s][A 40%|████      | 312/780 [02:12<02:14,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 15:49:52,270 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 15:49:52,291 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:49:54,197 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:49:54,213 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:49:54,228 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:19<57:24,  7.38s/it] 40%|████      | 314/780 [02:19<40:47,  5.25s/it] 40%|████      | 315/780 [02:20<29:10,  3.76s/it] 41%|████      | 316/780 [02:20<21:03,  2.72s/it] 41%|████      | 317/780 [02:20<15:23,  1.99s/it] 41%|████      | 318/780 [02:20<11:25,  1.48s/it] 41%|████      | 319/780 [02:21<08:39,  1.13s/it] 41%|████      | 320/780 [02:21<06:43,  1.14it/s] 41%|████      | 321/780 [02:21<05:22,  1.42it/s] 41%|████▏     | 322/780 [02:22<04:25,  1.73it/s] 41%|████▏     | 323/780 [02:22<03:45,  2.02it/s] 42%|████▏     | 324/780 [02:22<03:17,  2.30it/s] 42%|████▏     | 325/780 [02:22<02:58,  2.55it/s] 42%|████▏     | 326/780 [02:23<02:44,  2.76it/s] 42%|████▏     | 327/780 [02:23<02:34,  2.92it/s] 42%|████▏     | 328/780 [02:23<02:28,  3.05it/s] 42%|████▏     | 329/780 [02:24<02:23,  3.15it/s] 42%|████▏     | 330/780 [02:24<02:19,  3.22it/s] 42%|████▏     | 331/780 [02:24<02:17,  3.27it/s] 43%|████▎     | 332/780 [02:25<02:15,  3.31it/s] 43%|████▎     | 333/780 [02:25<02:13,  3.34it/s] 43%|████▎     | 334/780 [02:25<02:12,  3.36it/s] 43%|████▎     | 335/780 [02:25<02:11,  3.38it/s] 43%|████▎     | 336/780 [02:26<02:10,  3.39it/s] 43%|████▎     | 337/780 [02:26<02:09,  3.42it/s] 43%|████▎     | 338/780 [02:26<02:08,  3.43it/s] 43%|████▎     | 339/780 [02:27<02:08,  3.45it/s] 44%|████▎     | 340/780 [02:27<02:07,  3.45it/s] 44%|████▎     | 341/780 [02:27<02:07,  3.45it/s] 44%|████▍     | 342/780 [02:27<02:06,  3.46it/s] 44%|████▍     | 343/780 [02:28<02:06,  3.46it/s] 44%|████▍     | 344/780 [02:28<02:06,  3.46it/s] 44%|████▍     | 345/780 [02:28<02:05,  3.46it/s] 44%|████▍     | 346/780 [02:29<02:05,  3.47it/s] 44%|████▍     | 347/780 [02:29<02:05,  3.46it/s] 45%|████▍     | 348/780 [02:29<02:04,  3.46it/s] 45%|████▍     | 349/780 [02:29<02:04,  3.47it/s] 45%|████▍     | 350/780 [02:30<02:04,  3.47it/s] 45%|████▌     | 351/780 [02:30<02:03,  3.47it/s] 45%|████▌     | 352/780 [02:30<02:03,  3.47it/s] 45%|████▌     | 353/780 [02:31<02:02,  3.47it/s] 45%|████▌     | 354/780 [02:31<02:02,  3.48it/s] 46%|████▌     | 355/780 [02:31<02:02,  3.47it/s] 46%|████▌     | 356/780 [02:31<02:02,  3.45it/s] 46%|████▌     | 357/780 [02:32<02:02,  3.44it/s] 46%|████▌     | 358/780 [02:32<02:03,  3.40it/s] 46%|████▌     | 359/780 [02:32<02:03,  3.40it/s] 46%|████▌     | 360/780 [02:33<02:03,  3.40it/s] 46%|████▋     | 361/780 [02:33<02:03,  3.40it/s] 46%|████▋     | 362/780 [02:33<02:02,  3.40it/s] 47%|████▋     | 363/780 [02:34<02:02,  3.40it/s] 47%|████▋     | 364/780 [02:34<02:02,  3.41it/s] 47%|████▋     | 365/780 [02:34<02:01,  3.40it/s] 47%|████▋     | 366/780 [02:34<02:01,  3.40it/s] 47%|████▋     | 367/780 [02:35<02:01,  3.41it/s] 47%|████▋     | 368/780 [02:35<02:00,  3.41it/s] 47%|████▋     | 369/780 [02:35<02:01,  3.40it/s] 47%|████▋     | 370/780 [02:36<02:00,  3.40it/s] 48%|████▊     | 371/780 [02:36<02:00,  3.40it/s] 48%|████▊     | 372/780 [02:36<01:59,  3.40it/s] 48%|████▊     | 373/780 [02:36<02:00,  3.39it/s] 48%|████▊     | 374/780 [02:37<01:59,  3.39it/s] 48%|████▊     | 375/780 [02:37<01:59,  3.40it/s] 48%|████▊     | 376/780 [02:37<01:58,  3.40it/s] 48%|████▊     | 377/780 [02:38<01:58,  3.40it/s] 48%|████▊     | 378/780 [02:38<01:58,  3.40it/s] 49%|████▊     | 379/780 [02:38<01:57,  3.40it/s] 49%|████▊     | 380/780 [02:39<01:58,  3.38it/s] 49%|████▉     | 381/780 [02:39<01:57,  3.39it/s] 49%|████▉     | 382/780 [02:39<01:57,  3.40it/s] 49%|████▉     | 383/780 [02:39<01:56,  3.40it/s] 49%|████▉     | 384/780 [02:40<01:56,  3.40it/s] 49%|████▉     | 385/780 [02:40<01:56,  3.40it/s] 49%|████▉     | 386/780 [02:40<01:55,  3.42it/s] 50%|████▉     | 387/780 [02:41<01:54,  3.43it/s] 50%|████▉     | 388/780 [02:41<01:53,  3.44it/s] 50%|████▉     | 389/780 [02:41<01:53,  3.45it/s] 50%|█████     | 390/780 [02:41<01:52,  3.46it/s] 50%|█████     | 391/780 [02:42<01:52,  3.45it/s] 50%|█████     | 392/780 [02:42<01:52,  3.45it/s] 50%|█████     | 393/780 [02:42<01:51,  3.46it/s] 51%|█████     | 394/780 [02:43<01:51,  3.46it/s] 51%|█████     | 395/780 [02:43<01:51,  3.47it/s] 51%|█████     | 396/780 [02:43<01:50,  3.47it/s] 51%|█████     | 397/780 [02:43<01:50,  3.47it/s] 51%|█████     | 398/780 [02:44<01:50,  3.47it/s] 51%|█████     | 399/780 [02:44<01:49,  3.47it/s] 51%|█████▏    | 400/780 [02:44<01:49,  3.47it/s] 51%|█████▏    | 401/780 [02:45<01:49,  3.47it/s] 52%|█████▏    | 402/780 [02:45<01:48,  3.47it/s] 52%|█████▏    | 403/780 [02:45<01:48,  3.47it/s] 52%|█████▏    | 404/780 [02:45<01:48,  3.47it/s] 52%|█████▏    | 405/780 [02:46<01:47,  3.47it/s] 52%|█████▏    | 406/780 [02:46<01:47,  3.48it/s] 52%|█████▏    | 407/780 [02:46<01:47,  3.48it/s] 52%|█████▏    | 408/780 [02:47<01:47,  3.46it/s] 52%|█████▏    | 409/780 [02:47<01:46,  3.47it/s] 53%|█████▎    | 410/780 [02:47<01:46,  3.47it/s] 53%|█████▎    | 411/780 [02:47<01:46,  3.47it/s] 53%|█████▎    | 412/780 [02:48<01:45,  3.47it/s] 53%|█████▎    | 413/780 [02:48<01:46,  3.46it/s] 53%|█████▎    | 414/780 [02:48<01:45,  3.47it/s] 53%|█████▎    | 415/780 [02:49<01:45,  3.47it/s] 53%|█████▎    | 416/780 [02:49<01:44,  3.47it/s] 53%|█████▎    | 417/780 [02:49<01:44,  3.47it/s] 54%|█████▎    | 418/780 [02:50<01:48,  3.34it/s] 54%|█████▎    | 419/780 [02:50<01:47,  3.37it/s] 54%|█████▍    | 420/780 [02:50<01:45,  3.40it/s] 54%|█████▍    | 421/780 [02:50<01:44,  3.42it/s] 54%|█████▍    | 422/780 [02:51<01:44,  3.44it/s] 54%|█████▍    | 423/780 [02:51<01:43,  3.45it/s] 54%|█████▍    | 424/780 [02:51<01:43,  3.46it/s] 54%|█████▍    | 425/780 [02:52<01:42,  3.46it/s] 55%|█████▍    | 426/780 [02:52<01:42,  3.47it/s] 55%|█████▍    | 427/780 [02:52<01:41,  3.47it/s] 55%|█████▍    | 428/780 [02:52<01:41,  3.47it/s] 55%|█████▌    | 429/780 [02:53<01:41,  3.47it/s] 55%|█████▌    | 430/780 [02:53<01:41,  3.46it/s] 55%|█████▌    | 431/780 [02:53<01:40,  3.46it/s] 55%|█████▌    | 432/780 [02:54<01:40,  3.47it/s] 56%|█████▌    | 433/780 [02:54<01:39,  3.47it/s] 56%|█████▌    | 434/780 [02:54<01:39,  3.47it/s] 56%|█████▌    | 435/780 [02:54<01:39,  3.47it/s] 56%|█████▌    | 436/780 [02:55<01:39,  3.47it/s] 56%|█████▌    | 437/780 [02:55<01:38,  3.47it/s] 56%|█████▌    | 438/780 [02:55<01:38,  3.47it/s] 56%|█████▋    | 439/780 [02:56<01:38,  3.47it/s] 56%|█████▋    | 440/780 [02:56<01:37,  3.47it/s] 57%|█████▋    | 441/780 [02:56<01:37,  3.46it/s] 57%|█████▋    | 442/780 [02:56<01:37,  3.47it/s] 57%|█████▋    | 443/780 [02:57<01:37,  3.47it/s] 57%|█████▋    | 444/780 [02:57<01:37,  3.46it/s] 57%|█████▋    | 445/780 [02:57<01:36,  3.47it/s] 57%|█████▋    | 446/780 [02:58<01:36,  3.47it/s] 57%|█████▋    | 447/780 [02:58<01:35,  3.47it/s] 57%|█████▋    | 448/780 [02:58<01:35,  3.47it/s] 58%|█████▊    | 449/780 [02:58<01:35,  3.47it/s] 58%|█████▊    | 450/780 [02:59<01:35,  3.47it/s] 58%|█████▊    | 451/780 [02:59<01:34,  3.47it/s] 58%|█████▊    | 452/780 [02:59<01:34,  3.47it/s] 58%|█████▊    | 453/780 [03:00<01:34,  3.47it/s] 58%|█████▊    | 454/780 [03:00<01:33,  3.47it/s] 58%|█████▊    | 455/780 [03:00<01:33,  3.47it/s] 58%|█████▊    | 456/780 [03:00<01:33,  3.47it/s] 59%|█████▊    | 457/780 [03:01<01:32,  3.47it/s] 59%|█████▊    | 458/780 [03:01<01:32,  3.47it/s] 59%|█████▉    | 459/780 [03:01<01:32,  3.47it/s] 59%|█████▉    | 460/780 [03:02<01:32,  3.47it/s] 59%|█████▉    | 461/780 [03:02<01:31,  3.47it/s] 59%|█████▉    | 462/780 [03:02<01:31,  3.47it/s] 59%|█████▉    | 463/780 [03:03<01:31,  3.46it/s] 59%|█████▉    | 464/780 [03:03<01:31,  3.47it/s] 60%|█████▉    | 465/780 [03:03<01:30,  3.47it/s] 60%|█████▉    | 466/780 [03:03<01:30,  3.47it/s] 60%|█████▉    | 467/780 [03:04<01:30,  3.47it/s] 60%|██████    | 468/780 [03:04<01:29,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 15:50:44,313 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:50:44,313 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 15:50:44,313 >>   Batch size = 8
{'eval_loss': 0.9783291816711426, 'eval_runtime': 16.8621, 'eval_samples_per_second': 347.761, 'eval_steps_per_second': 43.47, 'epoch': 2.0}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 54.41it/s][A
  2%|▏         | 12/733 [00:00<00:15, 46.91it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.16it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.51it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.17it/s][A
  4%|▍         | 32/733 [00:00<00:15, 43.89it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.85it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.74it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.81it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.85it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.66it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.52it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.43it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.50it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.60it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.57it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.60it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.70it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.66it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.55it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.46it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.41it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.51it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.54it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.51it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.69it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.71it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.65it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.54it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.50it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.52it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.45it/s][A
 23%|██▎       | 167/733 [00:03<00:13, 43.53it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.60it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.71it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.65it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.57it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.57it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.52it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.51it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.51it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.54it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.58it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.65it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.68it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.60it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.57it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.53it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.53it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.50it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.55it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.62it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.66it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.60it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.57it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.49it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.61it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.53it/s][A
 41%|████      | 297/733 [00:06<00:10, 43.59it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.59it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.64it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.60it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.51it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.43it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.43it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.58it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.59it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.68it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.67it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.67it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.53it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.52it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.52it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.49it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.58it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.66it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.67it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.70it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.63it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.58it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.53it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.46it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.54it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.57it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.57it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.63it/s][A
 60%|█████▉    | 437/733 [00:10<00:06, 43.66it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.52it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.52it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.42it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.47it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.59it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.53it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.56it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.68it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.70it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.63it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.55it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.46it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.52it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.57it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.54it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.65it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.67it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.67it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.63it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.53it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.42it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.52it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.54it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.62it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.61it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.62it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.62it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.56it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.44it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.43it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.52it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.59it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.58it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.59it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.68it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.65it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.50it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.44it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.51it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.60it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.57it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.57it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.65it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.65it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.58it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.51it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.54it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.59it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.57it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.56it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.61it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.59it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.55it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.54it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.51it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.56it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.60it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.61it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.68it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.68it/s][A 60%|██████    | 468/780 [03:21<01:29,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 15:51:01,207 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 15:51:01,234 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:51:03,473 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:51:03,491 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:51:03,505 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:29<40:02,  7.72s/it] 60%|██████    | 470/780 [03:29<28:23,  5.50s/it] 60%|██████    | 471/780 [03:30<20:15,  3.94s/it] 61%|██████    | 472/780 [03:30<14:35,  2.84s/it] 61%|██████    | 473/780 [03:30<10:37,  2.08s/it] 61%|██████    | 474/780 [03:30<07:52,  1.54s/it] 61%|██████    | 475/780 [03:31<05:56,  1.17s/it] 61%|██████    | 476/780 [03:31<04:35,  1.10it/s] 61%|██████    | 477/780 [03:31<03:38,  1.39it/s] 61%|██████▏   | 478/780 [03:32<02:59,  1.69it/s] 61%|██████▏   | 479/780 [03:32<02:31,  1.99it/s] 62%|██████▏   | 480/780 [03:32<02:12,  2.27it/s] 62%|██████▏   | 481/780 [03:33<01:58,  2.51it/s] 62%|██████▏   | 482/780 [03:33<01:49,  2.73it/s] 62%|██████▏   | 483/780 [03:33<01:42,  2.90it/s] 62%|██████▏   | 484/780 [03:33<01:37,  3.04it/s] 62%|██████▏   | 485/780 [03:34<01:33,  3.14it/s] 62%|██████▏   | 486/780 [03:34<01:31,  3.22it/s] 62%|██████▏   | 487/780 [03:34<01:29,  3.27it/s] 63%|██████▎   | 488/780 [03:35<01:28,  3.31it/s] 63%|██████▎   | 489/780 [03:35<01:27,  3.34it/s] 63%|██████▎   | 490/780 [03:35<01:26,  3.36it/s] 63%|██████▎   | 491/780 [03:35<01:25,  3.38it/s] 63%|██████▎   | 492/780 [03:36<01:25,  3.38it/s] 63%|██████▎   | 493/780 [03:36<01:24,  3.39it/s] 63%|██████▎   | 494/780 [03:36<01:24,  3.39it/s] 63%|██████▎   | 495/780 [03:37<01:23,  3.40it/s] 64%|██████▎   | 496/780 [03:37<01:23,  3.40it/s] 64%|██████▎   | 497/780 [03:37<01:23,  3.40it/s] 64%|██████▍   | 498/780 [03:38<01:22,  3.40it/s] 64%|██████▍   | 499/780 [03:38<01:22,  3.40it/s] 64%|██████▍   | 500/780 [03:38<01:22,  3.41it/s]                                                  64%|██████▍   | 500/780 [03:38<01:22,  3.41it/s] 64%|██████▍   | 501/780 [03:38<01:21,  3.40it/s] 64%|██████▍   | 502/780 [03:39<01:21,  3.41it/s] 64%|██████▍   | 503/780 [03:39<01:21,  3.39it/s] 65%|██████▍   | 504/780 [03:39<01:21,  3.40it/s] 65%|██████▍   | 505/780 [03:40<01:20,  3.40it/s] 65%|██████▍   | 506/780 [03:40<01:20,  3.40it/s] 65%|██████▌   | 507/780 [03:40<01:20,  3.40it/s] 65%|██████▌   | 508/780 [03:40<01:19,  3.40it/s] 65%|██████▌   | 509/780 [03:41<01:19,  3.40it/s] 65%|██████▌   | 510/780 [03:41<01:19,  3.41it/s] 66%|██████▌   | 511/780 [03:41<01:18,  3.41it/s] 66%|██████▌   | 512/780 [03:42<01:18,  3.43it/s] 66%|██████▌   | 513/780 [03:42<01:17,  3.44it/s] 66%|██████▌   | 514/780 [03:42<01:17,  3.44it/s] 66%|██████▌   | 515/780 [03:43<01:16,  3.45it/s] 66%|██████▌   | 516/780 [03:43<01:16,  3.46it/s] 66%|██████▋   | 517/780 [03:43<01:15,  3.46it/s] 66%|██████▋   | 518/780 [03:43<01:15,  3.47it/s] 67%|██████▋   | 519/780 [03:44<01:15,  3.47it/s] 67%|██████▋   | 520/780 [03:44<01:14,  3.47it/s] 67%|██████▋   | 521/780 [03:44<01:14,  3.47it/s] 67%|██████▋   | 522/780 [03:45<01:14,  3.47it/s] 67%|██████▋   | 523/780 [03:45<01:14,  3.47it/s] 67%|██████▋   | 524/780 [03:45<01:13,  3.47it/s] 67%|██████▋   | 525/780 [03:45<01:13,  3.47it/s] 67%|██████▋   | 526/780 [03:46<01:13,  3.47it/s] 68%|██████▊   | 527/780 [03:46<01:12,  3.47it/s] 68%|██████▊   | 528/780 [03:46<01:12,  3.47it/s] 68%|██████▊   | 529/780 [03:47<01:12,  3.47it/s] 68%|██████▊   | 530/780 [03:47<01:11,  3.48it/s] 68%|██████▊   | 531/780 [03:47<01:11,  3.48it/s] 68%|██████▊   | 532/780 [03:47<01:11,  3.48it/s] 68%|██████▊   | 533/780 [03:48<01:11,  3.48it/s] 68%|██████▊   | 534/780 [03:48<01:10,  3.47it/s] 69%|██████▊   | 535/780 [03:48<01:10,  3.46it/s] 69%|██████▊   | 536/780 [03:49<01:10,  3.47it/s] 69%|██████▉   | 537/780 [03:49<01:10,  3.47it/s] 69%|██████▉   | 538/780 [03:49<01:09,  3.47it/s] 69%|██████▉   | 539/780 [03:49<01:09,  3.47it/s] 69%|██████▉   | 540/780 [03:50<01:11,  3.37it/s] 69%|██████▉   | 541/780 [03:50<01:10,  3.40it/s] 69%|██████▉   | 542/780 [03:50<01:09,  3.42it/s] 70%|██████▉   | 543/780 [03:51<01:08,  3.44it/s] 70%|██████▉   | 544/780 [03:51<01:08,  3.45it/s] 70%|██████▉   | 545/780 [03:51<01:08,  3.43it/s] 70%|███████   | 546/780 [03:51<01:07,  3.45it/s] 70%|███████   | 547/780 [03:52<01:07,  3.46it/s] 70%|███████   | 548/780 [03:52<01:07,  3.46it/s] 70%|███████   | 549/780 [03:52<01:06,  3.46it/s] 71%|███████   | 550/780 [03:53<01:06,  3.47it/s] 71%|███████   | 551/780 [03:53<01:06,  3.47it/s] 71%|███████   | 552/780 [03:53<01:05,  3.47it/s] 71%|███████   | 553/780 [03:53<01:05,  3.47it/s] 71%|███████   | 554/780 [03:54<01:05,  3.47it/s] 71%|███████   | 555/780 [03:54<01:04,  3.47it/s] 71%|███████▏  | 556/780 [03:54<01:04,  3.46it/s] 71%|███████▏  | 557/780 [03:55<01:04,  3.47it/s] 72%|███████▏  | 558/780 [03:55<01:04,  3.47it/s] 72%|███████▏  | 559/780 [03:55<01:03,  3.47it/s] 72%|███████▏  | 560/780 [03:56<01:03,  3.47it/s] 72%|███████▏  | 561/780 [03:56<01:03,  3.47it/s] 72%|███████▏  | 562/780 [03:56<01:02,  3.48it/s] 72%|███████▏  | 563/780 [03:56<01:02,  3.47it/s] 72%|███████▏  | 564/780 [03:57<01:02,  3.47it/s] 72%|███████▏  | 565/780 [03:57<01:01,  3.47it/s] 73%|███████▎  | 566/780 [03:57<01:01,  3.47it/s] 73%|███████▎  | 567/780 [03:58<01:01,  3.45it/s] 73%|███████▎  | 568/780 [03:58<01:01,  3.46it/s] 73%|███████▎  | 569/780 [03:58<01:00,  3.46it/s] 73%|███████▎  | 570/780 [03:58<01:00,  3.47it/s] 73%|███████▎  | 571/780 [03:59<01:00,  3.47it/s] 73%|███████▎  | 572/780 [03:59<00:59,  3.47it/s] 73%|███████▎  | 573/780 [03:59<00:59,  3.47it/s] 74%|███████▎  | 574/780 [04:00<00:59,  3.47it/s] 74%|███████▎  | 575/780 [04:00<00:59,  3.47it/s] 74%|███████▍  | 576/780 [04:00<00:58,  3.48it/s] 74%|███████▍  | 577/780 [04:00<00:58,  3.47it/s] 74%|███████▍  | 578/780 [04:01<00:58,  3.45it/s] 74%|███████▍  | 579/780 [04:01<00:58,  3.46it/s] 74%|███████▍  | 580/780 [04:01<00:57,  3.46it/s] 74%|███████▍  | 581/780 [04:02<00:57,  3.46it/s] 75%|███████▍  | 582/780 [04:02<00:57,  3.47it/s] 75%|███████▍  | 583/780 [04:02<00:56,  3.47it/s] 75%|███████▍  | 584/780 [04:02<00:56,  3.47it/s] 75%|███████▌  | 585/780 [04:03<00:56,  3.47it/s] 75%|███████▌  | 586/780 [04:03<00:55,  3.47it/s] 75%|███████▌  | 587/780 [04:03<00:55,  3.47it/s] 75%|███████▌  | 588/780 [04:04<00:55,  3.47it/s] 76%|███████▌  | 589/780 [04:04<00:55,  3.45it/s] 76%|███████▌  | 590/780 [04:04<00:54,  3.46it/s] 76%|███████▌  | 591/780 [04:04<00:54,  3.46it/s] 76%|███████▌  | 592/780 [04:05<00:54,  3.47it/s] 76%|███████▌  | 593/780 [04:05<00:53,  3.47it/s] 76%|███████▌  | 594/780 [04:05<00:53,  3.47it/s] 76%|███████▋  | 595/780 [04:06<00:53,  3.47it/s] 76%|███████▋  | 596/780 [04:06<00:52,  3.47it/s] 77%|███████▋  | 597/780 [04:06<00:52,  3.48it/s] 77%|███████▋  | 598/780 [04:06<00:52,  3.48it/s] 77%|███████▋  | 599/780 [04:07<00:52,  3.48it/s] 77%|███████▋  | 600/780 [04:07<00:52,  3.46it/s] 77%|███████▋  | 601/780 [04:07<00:51,  3.46it/s] 77%|███████▋  | 602/780 [04:08<00:51,  3.47it/s] 77%|███████▋  | 603/780 [04:08<00:51,  3.47it/s] 77%|███████▋  | 604/780 [04:08<00:50,  3.47it/s] 78%|███████▊  | 605/780 [04:08<00:50,  3.47it/s] 78%|███████▊  | 606/780 [04:09<00:50,  3.47it/s] 78%|███████▊  | 607/780 [04:09<00:49,  3.47it/s] 78%|███████▊  | 608/780 [04:09<00:49,  3.48it/s] 78%|███████▊  | 609/780 [04:10<00:49,  3.48it/s] 78%|███████▊  | 610/780 [04:10<00:48,  3.48it/s] 78%|███████▊  | 611/780 [04:10<00:48,  3.45it/s] 78%|███████▊  | 612/780 [04:11<00:48,  3.46it/s] 79%|███████▊  | 613/780 [04:11<00:48,  3.46it/s] 79%|███████▊  | 614/780 [04:11<00:47,  3.47it/s] 79%|███████▉  | 615/780 [04:11<00:47,  3.47it/s] 79%|███████▉  | 616/780 [04:12<00:47,  3.47it/s] 79%|███████▉  | 617/780 [04:12<00:46,  3.47it/s] 79%|███████▉  | 618/780 [04:12<00:46,  3.47it/s] 79%|███████▉  | 619/780 [04:13<00:46,  3.47it/s] 79%|███████▉  | 620/780 [04:13<00:46,  3.47it/s] 80%|███████▉  | 621/780 [04:13<00:45,  3.47it/s] 80%|███████▉  | 622/780 [04:13<00:45,  3.45it/s] 80%|███████▉  | 623/780 [04:14<00:45,  3.46it/s] 80%|████████  | 624/780 [04:14<00:45,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 15:51:54,331 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:51:54,331 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 15:51:54,331 >>   Batch size = 8
{'eval_loss': 0.9917067885398865, 'eval_runtime': 16.8619, 'eval_samples_per_second': 347.766, 'eval_steps_per_second': 43.471, 'epoch': 3.0}
{'loss': 0.5221, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 54.66it/s][A
  2%|▏         | 12/733 [00:00<00:15, 47.33it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.53it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.78it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.33it/s][A
  4%|▍         | 32/733 [00:00<00:15, 44.02it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.82it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.65it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.74it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.87it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.83it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.69it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.62it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.66it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.56it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.52it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.52it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.71it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.73it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.68it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.66it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.67it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.59it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.51it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.51it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.57it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.67it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.66it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.68it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.62it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.56it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.55it/s][A
 23%|██▎       | 167/733 [00:03<00:12, 43.58it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.51it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.57it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.65it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.70it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.65it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.54it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.56it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.60it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.62it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.51it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.57it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.70it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.69it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.61it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.57it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.56it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.59it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.52it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.53it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.60it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.72it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.69it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.65it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.62it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.59it/s][A
 41%|████      | 297/733 [00:06<00:09, 43.63it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.51it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.54it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.63it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.68it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.63it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.56it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.57it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.63it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.57it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.53it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.55it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.65it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.68it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.57it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.56it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.56it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.60it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.54it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.57it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.64it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.63it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.60it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.61it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.56it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.51it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.51it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.57it/s][A
 60%|█████▉    | 437/733 [00:09<00:06, 43.60it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.61it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.58it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.57it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.52it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.53it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.60it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.64it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.68it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.63it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.63it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.61it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.53it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.55it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.57it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.54it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.62it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.63it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.66it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.48it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.52it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.56it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.64it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.55it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.57it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.63it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.62it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.64it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.55it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.53it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.53it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.60it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.60it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.64it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.58it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.57it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.49it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.55it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.53it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.57it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.61it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.65it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.59it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.56it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.52it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.61it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.56it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.58it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.60it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.65it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.62it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.59it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.58it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.62it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.57it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.56it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.61it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.64it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.58it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.47it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.47it/s][A 80%|████████  | 624/780 [04:31<00:45,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 15:52:11,209 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 15:52:11,244 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:52:13,637 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:52:13,662 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:52:13,683 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:39<19:50,  7.68s/it] 80%|████████  | 626/780 [04:39<14:01,  5.46s/it] 80%|████████  | 627/780 [04:39<09:58,  3.91s/it] 81%|████████  | 628/780 [04:40<07:09,  2.83s/it] 81%|████████  | 629/780 [04:40<05:12,  2.07s/it] 81%|████████  | 630/780 [04:40<03:50,  1.53s/it] 81%|████████  | 631/780 [04:41<02:53,  1.16s/it] 81%|████████  | 632/780 [04:41<02:13,  1.11it/s] 81%|████████  | 633/780 [04:41<01:45,  1.39it/s] 81%|████████▏ | 634/780 [04:42<01:26,  1.69it/s] 81%|████████▏ | 635/780 [04:42<01:12,  1.99it/s] 82%|████████▏ | 636/780 [04:42<01:03,  2.27it/s] 82%|████████▏ | 637/780 [04:42<00:56,  2.52it/s] 82%|████████▏ | 638/780 [04:43<00:51,  2.73it/s] 82%|████████▏ | 639/780 [04:43<00:48,  2.91it/s] 82%|████████▏ | 640/780 [04:43<00:46,  3.04it/s] 82%|████████▏ | 641/780 [04:44<00:44,  3.14it/s] 82%|████████▏ | 642/780 [04:44<00:42,  3.22it/s] 82%|████████▏ | 643/780 [04:44<00:41,  3.27it/s] 83%|████████▎ | 644/780 [04:44<00:41,  3.31it/s] 83%|████████▎ | 645/780 [04:45<00:40,  3.34it/s] 83%|████████▎ | 646/780 [04:45<00:39,  3.36it/s] 83%|████████▎ | 647/780 [04:45<00:39,  3.37it/s] 83%|████████▎ | 648/780 [04:46<00:39,  3.38it/s] 83%|████████▎ | 649/780 [04:46<00:38,  3.39it/s] 83%|████████▎ | 650/780 [04:46<00:38,  3.39it/s] 83%|████████▎ | 651/780 [04:47<00:37,  3.40it/s] 84%|████████▎ | 652/780 [04:47<00:37,  3.40it/s] 84%|████████▎ | 653/780 [04:47<00:37,  3.40it/s] 84%|████████▍ | 654/780 [04:47<00:37,  3.40it/s] 84%|████████▍ | 655/780 [04:48<00:36,  3.41it/s] 84%|████████▍ | 656/780 [04:48<00:36,  3.41it/s] 84%|████████▍ | 657/780 [04:48<00:36,  3.40it/s] 84%|████████▍ | 658/780 [04:49<00:35,  3.40it/s] 84%|████████▍ | 659/780 [04:49<00:35,  3.41it/s] 85%|████████▍ | 660/780 [04:49<00:35,  3.41it/s] 85%|████████▍ | 661/780 [04:49<00:35,  3.39it/s] 85%|████████▍ | 662/780 [04:50<00:35,  3.28it/s] 85%|████████▌ | 663/780 [04:50<00:35,  3.32it/s] 85%|████████▌ | 664/780 [04:50<00:34,  3.35it/s] 85%|████████▌ | 665/780 [04:51<00:34,  3.37it/s] 85%|████████▌ | 666/780 [04:51<00:33,  3.38it/s] 86%|████████▌ | 667/780 [04:51<00:33,  3.39it/s] 86%|████████▌ | 668/780 [04:52<00:33,  3.39it/s] 86%|████████▌ | 669/780 [04:52<00:32,  3.40it/s] 86%|████████▌ | 670/780 [04:52<00:32,  3.40it/s] 86%|████████▌ | 671/780 [04:52<00:32,  3.40it/s] 86%|████████▌ | 672/780 [04:53<00:31,  3.39it/s] 86%|████████▋ | 673/780 [04:53<00:31,  3.40it/s] 86%|████████▋ | 674/780 [04:53<00:31,  3.40it/s] 87%|████████▋ | 675/780 [04:54<00:30,  3.40it/s] 87%|████████▋ | 676/780 [04:54<00:30,  3.40it/s] 87%|████████▋ | 677/780 [04:54<00:30,  3.40it/s] 87%|████████▋ | 678/780 [04:54<00:29,  3.40it/s] 87%|████████▋ | 679/780 [04:55<00:29,  3.40it/s] 87%|████████▋ | 680/780 [04:55<00:29,  3.41it/s] 87%|████████▋ | 681/780 [04:55<00:29,  3.41it/s] 87%|████████▋ | 682/780 [04:56<00:28,  3.40it/s] 88%|████████▊ | 683/780 [04:56<00:28,  3.39it/s] 88%|████████▊ | 684/780 [04:56<00:28,  3.40it/s] 88%|████████▊ | 685/780 [04:57<00:27,  3.40it/s] 88%|████████▊ | 686/780 [04:57<00:27,  3.40it/s] 88%|████████▊ | 687/780 [04:57<00:27,  3.40it/s] 88%|████████▊ | 688/780 [04:57<00:27,  3.40it/s] 88%|████████▊ | 689/780 [04:58<00:26,  3.40it/s] 88%|████████▊ | 690/780 [04:58<00:26,  3.40it/s] 89%|████████▊ | 691/780 [04:58<00:26,  3.40it/s] 89%|████████▊ | 692/780 [04:59<00:25,  3.40it/s] 89%|████████▉ | 693/780 [04:59<00:25,  3.40it/s] 89%|████████▉ | 694/780 [04:59<00:25,  3.39it/s] 89%|████████▉ | 695/780 [04:59<00:25,  3.40it/s] 89%|████████▉ | 696/780 [05:00<00:24,  3.40it/s] 89%|████████▉ | 697/780 [05:00<00:24,  3.40it/s] 89%|████████▉ | 698/780 [05:00<00:24,  3.40it/s] 90%|████████▉ | 699/780 [05:01<00:23,  3.40it/s] 90%|████████▉ | 700/780 [05:01<00:23,  3.40it/s] 90%|████████▉ | 701/780 [05:01<00:23,  3.40it/s] 90%|█████████ | 702/780 [05:02<00:22,  3.40it/s] 90%|█████████ | 703/780 [05:02<00:22,  3.42it/s] 90%|█████████ | 704/780 [05:02<00:22,  3.44it/s] 90%|█████████ | 705/780 [05:02<00:21,  3.43it/s] 91%|█████████ | 706/780 [05:03<00:21,  3.44it/s] 91%|█████████ | 707/780 [05:03<00:21,  3.45it/s] 91%|█████████ | 708/780 [05:03<00:20,  3.46it/s] 91%|█████████ | 709/780 [05:04<00:20,  3.46it/s] 91%|█████████ | 710/780 [05:04<00:20,  3.47it/s] 91%|█████████ | 711/780 [05:04<00:19,  3.47it/s] 91%|█████████▏| 712/780 [05:04<00:19,  3.47it/s] 91%|█████████▏| 713/780 [05:05<00:19,  3.47it/s] 92%|█████████▏| 714/780 [05:05<00:18,  3.47it/s] 92%|█████████▏| 715/780 [05:05<00:18,  3.48it/s] 92%|█████████▏| 716/780 [05:06<00:18,  3.46it/s] 92%|█████████▏| 717/780 [05:06<00:18,  3.46it/s] 92%|█████████▏| 718/780 [05:06<00:17,  3.47it/s] 92%|█████████▏| 719/780 [05:06<00:17,  3.47it/s] 92%|█████████▏| 720/780 [05:07<00:17,  3.47it/s] 92%|█████████▏| 721/780 [05:07<00:17,  3.47it/s] 93%|█████████▎| 722/780 [05:07<00:16,  3.47it/s] 93%|█████████▎| 723/780 [05:08<00:16,  3.48it/s] 93%|█████████▎| 724/780 [05:08<00:16,  3.47it/s] 93%|█████████▎| 725/780 [05:08<00:15,  3.47it/s] 93%|█████████▎| 726/780 [05:08<00:15,  3.48it/s] 93%|█████████▎| 727/780 [05:09<00:15,  3.47it/s] 93%|█████████▎| 728/780 [05:09<00:14,  3.47it/s] 93%|█████████▎| 729/780 [05:09<00:14,  3.47it/s] 94%|█████████▎| 730/780 [05:10<00:14,  3.47it/s] 94%|█████████▎| 731/780 [05:10<00:14,  3.47it/s] 94%|█████████▍| 732/780 [05:10<00:13,  3.47it/s] 94%|█████████▍| 733/780 [05:10<00:13,  3.47it/s] 94%|█████████▍| 734/780 [05:11<00:13,  3.48it/s] 94%|█████████▍| 735/780 [05:11<00:12,  3.48it/s] 94%|█████████▍| 736/780 [05:11<00:12,  3.47it/s] 94%|█████████▍| 737/780 [05:12<00:12,  3.47it/s] 95%|█████████▍| 738/780 [05:12<00:12,  3.45it/s] 95%|█████████▍| 739/780 [05:12<00:11,  3.46it/s] 95%|█████████▍| 740/780 [05:13<00:11,  3.46it/s] 95%|█████████▌| 741/780 [05:13<00:11,  3.47it/s] 95%|█████████▌| 742/780 [05:13<00:10,  3.47it/s] 95%|█████████▌| 743/780 [05:13<00:10,  3.47it/s] 95%|█████████▌| 744/780 [05:14<00:10,  3.47it/s] 96%|█████████▌| 745/780 [05:14<00:10,  3.47it/s] 96%|█████████▌| 746/780 [05:14<00:09,  3.47it/s] 96%|█████████▌| 747/780 [05:15<00:09,  3.47it/s] 96%|█████████▌| 748/780 [05:15<00:09,  3.47it/s] 96%|█████████▌| 749/780 [05:15<00:08,  3.45it/s] 96%|█████████▌| 750/780 [05:15<00:08,  3.46it/s] 96%|█████████▋| 751/780 [05:16<00:08,  3.46it/s] 96%|█████████▋| 752/780 [05:16<00:08,  3.47it/s] 97%|█████████▋| 753/780 [05:16<00:07,  3.47it/s] 97%|█████████▋| 754/780 [05:17<00:07,  3.47it/s] 97%|█████████▋| 755/780 [05:17<00:07,  3.47it/s] 97%|█████████▋| 756/780 [05:17<00:06,  3.48it/s] 97%|█████████▋| 757/780 [05:17<00:06,  3.47it/s] 97%|█████████▋| 758/780 [05:18<00:06,  3.48it/s] 97%|█████████▋| 759/780 [05:18<00:06,  3.48it/s] 97%|█████████▋| 760/780 [05:18<00:05,  3.48it/s] 98%|█████████▊| 761/780 [05:19<00:05,  3.48it/s] 98%|█████████▊| 762/780 [05:19<00:05,  3.47it/s] 98%|█████████▊| 763/780 [05:19<00:04,  3.48it/s] 98%|█████████▊| 764/780 [05:19<00:04,  3.48it/s] 98%|█████████▊| 765/780 [05:20<00:04,  3.47it/s] 98%|█████████▊| 766/780 [05:20<00:04,  3.46it/s] 98%|█████████▊| 767/780 [05:20<00:03,  3.47it/s] 98%|█████████▊| 768/780 [05:21<00:03,  3.47it/s] 99%|█████████▊| 769/780 [05:21<00:03,  3.47it/s] 99%|█████████▊| 770/780 [05:21<00:02,  3.47it/s] 99%|█████████▉| 771/780 [05:21<00:02,  3.47it/s] 99%|█████████▉| 772/780 [05:22<00:02,  3.47it/s] 99%|█████████▉| 773/780 [05:22<00:02,  3.47it/s] 99%|█████████▉| 774/780 [05:22<00:01,  3.47it/s] 99%|█████████▉| 775/780 [05:23<00:01,  3.47it/s] 99%|█████████▉| 776/780 [05:23<00:01,  3.47it/s]100%|█████████▉| 777/780 [05:23<00:00,  3.45it/s]100%|█████████▉| 778/780 [05:23<00:00,  3.46it/s]100%|█████████▉| 779/780 [05:24<00:00,  3.46it/s]100%|██████████| 780/780 [05:24<00:00,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 15:53:04,369 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:53:04,370 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 15:53:04,370 >>   Batch size = 8
{'eval_loss': 0.9982721209526062, 'eval_runtime': 16.8525, 'eval_samples_per_second': 347.961, 'eval_steps_per_second': 43.495, 'epoch': 4.0}

  0%|          | 0/733 [00:00<?, ?it/s][A
  1%|          | 6/733 [00:00<00:13, 54.57it/s][A
  2%|▏         | 12/733 [00:00<00:15, 47.18it/s][A
  2%|▏         | 17/733 [00:00<00:15, 45.50it/s][A
  3%|▎         | 22/733 [00:00<00:15, 44.78it/s][A
  4%|▎         | 27/733 [00:00<00:15, 44.36it/s][A
  4%|▍         | 32/733 [00:00<00:15, 43.90it/s][A
  5%|▌         | 37/733 [00:00<00:15, 43.84it/s][A
  6%|▌         | 42/733 [00:00<00:15, 43.77it/s][A
  6%|▋         | 47/733 [00:01<00:15, 43.74it/s][A
  7%|▋         | 52/733 [00:01<00:15, 43.80it/s][A
  8%|▊         | 57/733 [00:01<00:15, 43.79it/s][A
  8%|▊         | 62/733 [00:01<00:15, 43.49it/s][A
  9%|▉         | 67/733 [00:01<00:15, 43.71it/s][A
 10%|▉         | 72/733 [00:01<00:15, 43.62it/s][A
 11%|█         | 77/733 [00:01<00:15, 43.59it/s][A
 11%|█         | 82/733 [00:01<00:14, 43.58it/s][A
 12%|█▏        | 87/733 [00:01<00:14, 43.57it/s][A
 13%|█▎        | 92/733 [00:02<00:14, 43.65it/s][A
 13%|█▎        | 97/733 [00:02<00:14, 43.65it/s][A
 14%|█▍        | 102/733 [00:02<00:14, 43.53it/s][A
 15%|█▍        | 107/733 [00:02<00:14, 43.66it/s][A
 15%|█▌        | 112/733 [00:02<00:14, 43.53it/s][A
 16%|█▌        | 117/733 [00:02<00:14, 43.53it/s][A
 17%|█▋        | 122/733 [00:02<00:14, 43.58it/s][A
 17%|█▋        | 127/733 [00:02<00:13, 43.55it/s][A
 18%|█▊        | 132/733 [00:03<00:13, 43.67it/s][A
 19%|█▊        | 137/733 [00:03<00:13, 43.69it/s][A
 19%|█▉        | 142/733 [00:03<00:13, 43.70it/s][A
 20%|██        | 147/733 [00:03<00:13, 43.63it/s][A
 21%|██        | 152/733 [00:03<00:13, 43.65it/s][A
 21%|██▏       | 157/733 [00:03<00:13, 43.54it/s][A
 22%|██▏       | 162/733 [00:03<00:13, 43.54it/s][A
 23%|██▎       | 167/733 [00:03<00:13, 43.54it/s][A
 23%|██▎       | 172/733 [00:03<00:12, 43.62it/s][A
 24%|██▍       | 177/733 [00:04<00:12, 43.66it/s][A
 25%|██▍       | 182/733 [00:04<00:12, 43.61it/s][A
 26%|██▌       | 187/733 [00:04<00:12, 43.66it/s][A
 26%|██▌       | 192/733 [00:04<00:12, 43.69it/s][A
 27%|██▋       | 197/733 [00:04<00:12, 43.60it/s][A
 28%|██▊       | 202/733 [00:04<00:12, 43.49it/s][A
 28%|██▊       | 207/733 [00:04<00:12, 43.52it/s][A
 29%|██▉       | 212/733 [00:04<00:11, 43.54it/s][A
 30%|██▉       | 217/733 [00:04<00:11, 43.60it/s][A
 30%|███       | 222/733 [00:05<00:11, 43.62it/s][A
 31%|███       | 227/733 [00:05<00:11, 43.63it/s][A
 32%|███▏      | 232/733 [00:05<00:11, 43.69it/s][A
 32%|███▏      | 237/733 [00:05<00:11, 43.66it/s][A
 33%|███▎      | 242/733 [00:05<00:11, 43.51it/s][A
 34%|███▎      | 247/733 [00:05<00:11, 43.41it/s][A
 34%|███▍      | 252/733 [00:05<00:11, 43.51it/s][A
 35%|███▌      | 257/733 [00:05<00:10, 43.55it/s][A
 36%|███▌      | 262/733 [00:05<00:10, 43.59it/s][A
 36%|███▋      | 267/733 [00:06<00:10, 43.58it/s][A
 37%|███▋      | 272/733 [00:06<00:10, 43.54it/s][A
 38%|███▊      | 277/733 [00:06<00:10, 43.59it/s][A
 38%|███▊      | 282/733 [00:06<00:10, 43.54it/s][A
 39%|███▉      | 287/733 [00:06<00:10, 43.54it/s][A
 40%|███▉      | 292/733 [00:06<00:10, 43.61it/s][A
 41%|████      | 297/733 [00:06<00:09, 43.61it/s][A
 41%|████      | 302/733 [00:06<00:09, 43.64it/s][A
 42%|████▏     | 307/733 [00:07<00:09, 43.62it/s][A
 43%|████▎     | 312/733 [00:07<00:09, 43.64it/s][A
 43%|████▎     | 317/733 [00:07<00:09, 43.62it/s][A
 44%|████▍     | 322/733 [00:07<00:09, 43.60it/s][A
 45%|████▍     | 327/733 [00:07<00:09, 43.49it/s][A
 45%|████▌     | 332/733 [00:07<00:09, 43.50it/s][A
 46%|████▌     | 337/733 [00:07<00:09, 43.58it/s][A
 47%|████▋     | 342/733 [00:07<00:08, 43.66it/s][A
 47%|████▋     | 347/733 [00:07<00:08, 43.61it/s][A
 48%|████▊     | 352/733 [00:08<00:08, 43.62it/s][A
 49%|████▊     | 357/733 [00:08<00:08, 43.61it/s][A
 49%|████▉     | 362/733 [00:08<00:08, 43.61it/s][A
 50%|█████     | 367/733 [00:08<00:08, 43.52it/s][A
 51%|█████     | 372/733 [00:08<00:08, 43.41it/s][A
 51%|█████▏    | 377/733 [00:08<00:08, 43.46it/s][A
 52%|█████▏    | 382/733 [00:08<00:08, 43.60it/s][A
 53%|█████▎    | 387/733 [00:08<00:07, 43.60it/s][A
 53%|█████▎    | 392/733 [00:08<00:07, 43.54it/s][A
 54%|█████▍    | 397/733 [00:09<00:07, 43.56it/s][A
 55%|█████▍    | 402/733 [00:09<00:07, 43.63it/s][A
 56%|█████▌    | 407/733 [00:09<00:07, 43.60it/s][A
 56%|█████▌    | 412/733 [00:09<00:07, 43.54it/s][A
 57%|█████▋    | 417/733 [00:09<00:07, 43.56it/s][A
 58%|█████▊    | 422/733 [00:09<00:07, 43.64it/s][A
 58%|█████▊    | 427/733 [00:09<00:07, 43.61it/s][A
 59%|█████▉    | 432/733 [00:09<00:06, 43.54it/s][A
 60%|█████▉    | 437/733 [00:10<00:06, 43.55it/s][A
 60%|██████    | 442/733 [00:10<00:06, 43.61it/s][A
 61%|██████    | 447/733 [00:10<00:06, 43.62it/s][A
 62%|██████▏   | 452/733 [00:10<00:06, 43.54it/s][A
 62%|██████▏   | 457/733 [00:10<00:06, 43.56it/s][A
 63%|██████▎   | 462/733 [00:10<00:06, 43.55it/s][A
 64%|██████▎   | 467/733 [00:10<00:06, 43.56it/s][A
 64%|██████▍   | 472/733 [00:10<00:05, 43.59it/s][A
 65%|██████▌   | 477/733 [00:10<00:05, 43.57it/s][A
 66%|██████▌   | 482/733 [00:11<00:05, 43.61it/s][A
 66%|██████▋   | 487/733 [00:11<00:05, 43.62it/s][A
 67%|██████▋   | 492/733 [00:11<00:05, 43.66it/s][A
 68%|██████▊   | 497/733 [00:11<00:05, 43.60it/s][A
 68%|██████▊   | 502/733 [00:11<00:05, 43.54it/s][A
 69%|██████▉   | 507/733 [00:11<00:05, 43.53it/s][A
 70%|██████▉   | 512/733 [00:11<00:05, 43.54it/s][A
 71%|███████   | 517/733 [00:11<00:04, 43.57it/s][A
 71%|███████   | 522/733 [00:11<00:04, 43.61it/s][A
 72%|███████▏  | 527/733 [00:12<00:04, 43.59it/s][A
 73%|███████▎  | 532/733 [00:12<00:04, 43.65it/s][A
 73%|███████▎  | 537/733 [00:12<00:04, 43.63it/s][A
 74%|███████▍  | 542/733 [00:12<00:04, 43.56it/s][A
 75%|███████▍  | 547/733 [00:12<00:04, 43.53it/s][A
 75%|███████▌  | 552/733 [00:12<00:04, 43.51it/s][A
 76%|███████▌  | 557/733 [00:12<00:04, 43.59it/s][A
 77%|███████▋  | 562/733 [00:12<00:03, 43.59it/s][A
 77%|███████▋  | 567/733 [00:12<00:03, 43.61it/s][A
 78%|███████▊  | 572/733 [00:13<00:03, 43.68it/s][A
 79%|███████▊  | 577/733 [00:13<00:03, 43.70it/s][A
 79%|███████▉  | 582/733 [00:13<00:03, 43.64it/s][A
 80%|████████  | 587/733 [00:13<00:03, 43.53it/s][A
 81%|████████  | 592/733 [00:13<00:03, 43.56it/s][A
 81%|████████▏ | 597/733 [00:13<00:03, 43.55it/s][A
 82%|████████▏ | 602/733 [00:13<00:03, 43.56it/s][A
 83%|████████▎ | 607/733 [00:13<00:02, 43.61it/s][A
 83%|████████▎ | 612/733 [00:14<00:02, 43.66it/s][A
 84%|████████▍ | 617/733 [00:14<00:02, 43.65it/s][A
 85%|████████▍ | 622/733 [00:14<00:02, 43.58it/s][A
 86%|████████▌ | 627/733 [00:14<00:02, 43.50it/s][A
 86%|████████▌ | 632/733 [00:14<00:02, 43.43it/s][A
 87%|████████▋ | 637/733 [00:14<00:02, 43.49it/s][A
 88%|████████▊ | 642/733 [00:14<00:02, 43.52it/s][A
 88%|████████▊ | 647/733 [00:14<00:01, 43.55it/s][A
 89%|████████▉ | 652/733 [00:14<00:01, 43.54it/s][A
 90%|████████▉ | 657/733 [00:15<00:01, 43.60it/s][A
 90%|█████████ | 662/733 [00:15<00:01, 43.63it/s][A
 91%|█████████ | 667/733 [00:15<00:01, 43.61it/s][A
 92%|█████████▏| 672/733 [00:15<00:01, 43.57it/s][A
 92%|█████████▏| 677/733 [00:15<00:01, 43.54it/s][A
 93%|█████████▎| 682/733 [00:15<00:01, 43.56it/s][A
 94%|█████████▎| 687/733 [00:15<00:01, 43.61it/s][A
 94%|█████████▍| 692/733 [00:15<00:00, 43.58it/s][A
 95%|█████████▌| 697/733 [00:15<00:00, 43.57it/s][A
 96%|█████████▌| 702/733 [00:16<00:00, 43.55it/s][A
 96%|█████████▋| 707/733 [00:16<00:00, 43.61it/s][A
 97%|█████████▋| 712/733 [00:16<00:00, 43.54it/s][A
 98%|█████████▊| 717/733 [00:16<00:00, 43.58it/s][A
 98%|█████████▊| 722/733 [00:16<00:00, 43.61it/s][A
 99%|█████████▉| 727/733 [00:16<00:00, 43.64it/s][A
100%|█████████▉| 732/733 [00:16<00:00, 43.59it/s][A
                                                 [A                                                 
100%|██████████| 733/733 [00:16<00:00, 43.59it/s][A100%|██████████| 780/780 [05:41<00:00,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 15:53:21,214 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 15:53:21,239 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:53:23,326 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:53:23,347 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:53:23,363 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 15:53:27,846 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 15:53:27,846 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156 (score: 0.9624547362327576).
                                                 100%|██████████| 780/780 [05:50<00:00,  3.47it/s]100%|██████████| 780/780 [05:50<00:00,  2.23it/s]
[INFO|trainer.py:1894] 2023-08-29 15:53:29,916 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 15:53:29,939 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:53:32,512 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:53:32,527 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:53:32,537 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 15:53:32,814 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:32,814 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:32,814 >>   train_loss               =      0.512
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:32,814 >>   train_runtime            = 0:05:50.07
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:32,814 >>   train_samples            =       9999
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:32,814 >>   train_samples_per_second =    142.811
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:32,814 >>   train_steps_per_second   =      2.228
{'eval_loss': 1.00216805934906, 'eval_runtime': 16.8196, 'eval_samples_per_second': 348.64, 'eval_steps_per_second': 43.58, 'epoch': 5.0}
{'train_runtime': 350.079, 'train_samples_per_second': 142.811, 'train_steps_per_second': 2.228, 'train_loss': 0.5119951492700822, 'epoch': 5.0}
08/29/2023 15:53:32 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 15:53:32,864 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:53:32,864 >>   Num examples = 5864
[INFO|trainer.py:2145] 2023-08-29 15:53:32,864 >>   Batch size = 8
  0%|          | 0/733 [00:00<?, ?it/s]  1%|          | 6/733 [00:00<00:13, 55.28it/s]  2%|▏         | 12/733 [00:00<00:15, 47.92it/s]  2%|▏         | 17/733 [00:00<00:15, 46.15it/s]  3%|▎         | 22/733 [00:00<00:15, 45.77it/s]  4%|▎         | 27/733 [00:00<00:15, 45.27it/s]  4%|▍         | 32/733 [00:00<00:15, 44.88it/s]  5%|▌         | 37/733 [00:00<00:15, 44.64it/s]  6%|▌         | 42/733 [00:00<00:15, 44.00it/s]  6%|▋         | 47/733 [00:01<00:15, 43.31it/s]  7%|▋         | 52/733 [00:01<00:15, 43.22it/s]  8%|▊         | 57/733 [00:01<00:15, 43.35it/s]  8%|▊         | 62/733 [00:01<00:15, 43.62it/s]  9%|▉         | 67/733 [00:01<00:15, 43.76it/s] 10%|▉         | 72/733 [00:01<00:15, 43.81it/s] 11%|█         | 77/733 [00:01<00:14, 43.95it/s] 11%|█         | 82/733 [00:01<00:14, 43.87it/s] 12%|█▏        | 87/733 [00:01<00:14, 43.44it/s] 13%|█▎        | 92/733 [00:02<00:14, 43.15it/s] 13%|█▎        | 97/733 [00:02<00:14, 43.13it/s] 14%|█▍        | 102/733 [00:02<00:14, 43.33it/s] 15%|█▍        | 107/733 [00:02<00:14, 43.57it/s] 15%|█▌        | 112/733 [00:02<00:14, 43.68it/s] 16%|█▌        | 117/733 [00:02<00:14, 43.87it/s] 17%|█▋        | 122/733 [00:02<00:13, 43.89it/s] 17%|█▋        | 127/733 [00:02<00:13, 43.65it/s] 18%|█▊        | 132/733 [00:02<00:13, 43.35it/s] 19%|█▊        | 137/733 [00:03<00:13, 43.16it/s] 19%|█▉        | 142/733 [00:03<00:13, 43.24it/s] 20%|██        | 147/733 [00:03<00:13, 43.39it/s] 21%|██        | 152/733 [00:03<00:13, 43.54it/s] 21%|██▏       | 157/733 [00:03<00:13, 43.75it/s] 22%|██▏       | 162/733 [00:03<00:13, 43.81it/s] 23%|██▎       | 167/733 [00:03<00:12, 43.80it/s] 23%|██▎       | 172/733 [00:03<00:12, 43.59it/s] 24%|██▍       | 177/733 [00:04<00:12, 43.31it/s] 25%|██▍       | 182/733 [00:04<00:12, 43.16it/s] 26%|██▌       | 187/733 [00:04<00:12, 43.31it/s] 26%|██▌       | 192/733 [00:04<00:12, 43.47it/s] 27%|██▋       | 197/733 [00:04<00:12, 43.67it/s] 28%|██▊       | 202/733 [00:04<00:12, 43.77it/s] 28%|██▊       | 207/733 [00:04<00:12, 43.80it/s] 29%|██▉       | 212/733 [00:04<00:11, 43.71it/s] 30%|██▉       | 217/733 [00:04<00:11, 43.50it/s] 30%|███       | 222/733 [00:05<00:11, 43.29it/s] 31%|███       | 227/733 [00:05<00:11, 43.28it/s] 32%|███▏      | 232/733 [00:05<00:11, 43.35it/s] 32%|███▏      | 237/733 [00:05<00:11, 43.53it/s] 33%|███▎      | 242/733 [00:05<00:11, 43.67it/s] 34%|███▎      | 247/733 [00:05<00:11, 43.75it/s] 34%|███▍      | 252/733 [00:05<00:10, 43.79it/s] 35%|███▌      | 257/733 [00:05<00:10, 43.62it/s] 36%|███▌      | 262/733 [00:05<00:10, 43.43it/s] 36%|███▋      | 267/733 [00:06<00:10, 43.33it/s] 37%|███▋      | 272/733 [00:06<00:10, 43.28it/s] 38%|███▊      | 277/733 [00:06<00:10, 43.50it/s] 38%|███▊      | 282/733 [00:06<00:10, 43.55it/s] 39%|███▉      | 287/733 [00:06<00:10, 43.67it/s] 40%|███▉      | 292/733 [00:06<00:10, 43.78it/s] 41%|████      | 297/733 [00:06<00:09, 43.72it/s] 41%|████      | 302/733 [00:06<00:09, 43.53it/s] 42%|████▏     | 307/733 [00:07<00:09, 43.40it/s] 43%|████▎     | 312/733 [00:07<00:09, 43.37it/s] 43%|████▎     | 317/733 [00:07<00:09, 43.42it/s] 44%|████▍     | 322/733 [00:07<00:09, 43.54it/s] 45%|████▍     | 327/733 [00:07<00:09, 43.59it/s] 45%|████▌     | 332/733 [00:07<00:09, 43.68it/s] 46%|████▌     | 337/733 [00:07<00:09, 43.72it/s] 47%|████▋     | 342/733 [00:07<00:08, 43.59it/s] 47%|████▋     | 347/733 [00:07<00:08, 43.47it/s] 48%|████▊     | 352/733 [00:08<00:08, 43.43it/s] 49%|████▊     | 357/733 [00:08<00:08, 43.37it/s] 49%|████▉     | 362/733 [00:08<00:08, 43.42it/s] 50%|█████     | 367/733 [00:08<00:08, 43.55it/s] 51%|█████     | 372/733 [00:08<00:08, 43.68it/s] 51%|█████▏    | 377/733 [00:08<00:08, 43.69it/s] 52%|█████▏    | 382/733 [00:08<00:08, 43.61it/s] 53%|█████▎    | 387/733 [00:08<00:07, 43.53it/s] 53%|█████▎    | 392/733 [00:08<00:07, 43.48it/s] 54%|█████▍    | 397/733 [00:09<00:07, 43.40it/s] 55%|█████▍    | 402/733 [00:09<00:07, 43.34it/s] 56%|█████▌    | 407/733 [00:09<00:07, 43.41it/s] 56%|█████▌    | 412/733 [00:09<00:07, 43.58it/s] 57%|█████▋    | 417/733 [00:09<00:07, 43.66it/s] 58%|█████▊    | 422/733 [00:09<00:07, 43.63it/s] 58%|█████▊    | 427/733 [00:09<00:07, 43.61it/s] 59%|█████▉    | 432/733 [00:09<00:06, 43.57it/s] 60%|█████▉    | 437/733 [00:10<00:06, 43.52it/s] 60%|██████    | 442/733 [00:10<00:06, 43.40it/s] 61%|██████    | 447/733 [00:10<00:06, 43.25it/s] 62%|██████▏   | 452/733 [00:10<00:06, 43.48it/s] 62%|██████▏   | 457/733 [00:10<00:06, 43.57it/s] 63%|██████▎   | 462/733 [00:10<00:06, 43.56it/s] 64%|██████▎   | 467/733 [00:10<00:06, 43.59it/s] 64%|██████▍   | 472/733 [00:10<00:05, 43.61it/s] 65%|██████▌   | 477/733 [00:10<00:05, 43.54it/s] 66%|██████▌   | 482/733 [00:11<00:05, 43.43it/s] 66%|██████▋   | 487/733 [00:11<00:05, 43.30it/s] 67%|██████▋   | 492/733 [00:11<00:05, 43.40it/s] 68%|██████▊   | 497/733 [00:11<00:05, 43.51it/s] 68%|██████▊   | 502/733 [00:11<00:05, 43.57it/s] 69%|██████▉   | 507/733 [00:11<00:05, 43.60it/s] 70%|██████▉   | 512/733 [00:11<00:05, 43.65it/s] 71%|███████   | 517/733 [00:11<00:04, 43.60it/s] 71%|███████   | 522/733 [00:11<00:04, 43.52it/s] 72%|███████▏  | 527/733 [00:12<00:04, 43.39it/s] 73%|███████▎  | 532/733 [00:12<00:04, 43.37it/s] 73%|███████▎  | 537/733 [00:12<00:04, 43.42it/s] 74%|███████▍  | 542/733 [00:12<00:04, 43.51it/s] 75%|███████▍  | 547/733 [00:12<00:04, 43.54it/s] 75%|███████▌  | 552/733 [00:12<00:04, 43.60it/s] 76%|███████▌  | 557/733 [00:12<00:04, 43.61it/s] 77%|███████▋  | 562/733 [00:12<00:03, 43.54it/s] 77%|███████▋  | 567/733 [00:12<00:03, 43.46it/s] 78%|███████▊  | 572/733 [00:13<00:03, 43.43it/s] 79%|███████▊  | 577/733 [00:13<00:03, 43.42it/s] 79%|███████▉  | 582/733 [00:13<00:03, 43.42it/s] 80%|████████  | 587/733 [00:13<00:03, 43.46it/s] 81%|████████  | 592/733 [00:13<00:03, 43.59it/s] 81%|████████▏ | 597/733 [00:13<00:03, 43.51it/s] 82%|████████▏ | 602/733 [00:13<00:03, 43.53it/s] 83%|████████▎ | 607/733 [00:13<00:02, 43.52it/s] 83%|████████▎ | 612/733 [00:14<00:02, 43.48it/s] 84%|████████▍ | 617/733 [00:14<00:02, 43.48it/s] 85%|████████▍ | 622/733 [00:14<00:02, 43.47it/s] 86%|████████▌ | 627/733 [00:14<00:02, 43.44it/s] 86%|████████▌ | 632/733 [00:14<00:02, 43.48it/s] 87%|████████▋ | 637/733 [00:14<00:02, 43.42it/s] 88%|████████▊ | 642/733 [00:14<00:02, 43.42it/s] 88%|████████▊ | 647/733 [00:14<00:01, 43.53it/s] 89%|████████▉ | 652/733 [00:14<00:01, 43.53it/s] 90%|████████▉ | 657/733 [00:15<00:01, 43.51it/s] 90%|█████████ | 662/733 [00:15<00:01, 43.50it/s] 91%|█████████ | 667/733 [00:15<00:01, 43.50it/s] 92%|█████████▏| 672/733 [00:15<00:01, 43.51it/s] 92%|█████████▏| 677/733 [00:15<00:01, 43.43it/s] 93%|█████████▎| 682/733 [00:15<00:01, 43.42it/s] 94%|█████████▎| 687/733 [00:15<00:01, 43.48it/s] 94%|█████████▍| 692/733 [00:15<00:00, 43.59it/s] 95%|█████████▌| 697/733 [00:15<00:00, 43.62it/s] 96%|█████████▌| 702/733 [00:16<00:00, 43.62it/s] 96%|█████████▋| 707/733 [00:16<00:00, 43.69it/s] 97%|█████████▋| 712/733 [00:16<00:00, 43.70it/s] 98%|█████████▊| 717/733 [00:16<00:00, 43.59it/s] 98%|█████████▊| 722/733 [00:16<00:00, 43.59it/s] 99%|█████████▉| 727/733 [00:16<00:00, 43.66it/s]100%|█████████▉| 732/733 [00:16<00:00, 43.71it/s]100%|██████████| 733/733 [00:16<00:00, 43.58it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 15:53:49,701 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:49,702 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:49,702 >>   eval_loss               =     0.9625
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:49,702 >>   eval_runtime            = 0:00:16.83
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:49,702 >>   eval_samples            =       5864
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:49,702 >>   eval_samples_per_second =    348.279
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:49,702 >>   eval_steps_per_second   =     43.535
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:53:49,702 >>   perplexity              =     2.6181
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-780
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl', 'labels': ['inception', 'located on terrain feature', 'military branch', 'occupant', 'occupation'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14932
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15032, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
  File "wrapper.py", line 752, in run_eval
    model.predict(path_in, path_out, labels)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/extractor.py", line 235, in predict
    model = self.get_model(args)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/extractor.py", line 125, in get_model
    model = JointModel(config)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/models/base.py", line 26, in __init__
    self.set_embedding_layer()
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/models/joint_models.py", line 253, in set_embedding_layer
    self.token_embedding = AllEmbedding(self.config)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/layers/encodings/embeddings.py", line 101, in __init__
    self.lm_embedding = PreEmbeddedLM(self.config)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/layers/encodings/lm_embeddings.py", line 248, in __init__
    self.lm_emb_path, layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean")]
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/layers/encodings/lm_embeddings.py", line 43, in __init__
    super().__init__()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/flair/embeddings.py", line 2225, in __init__
    self.tokenizer = BertTokenizer.from_pretrained(bert_model_or_path)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/transformers/tokenization_utils_base.py", line 1680, in from_pretrained
    user_agent=user_agent,
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/transformers/file_utils.py", line 1337, in cached_path
    local_files_only=local_files_only,
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt
