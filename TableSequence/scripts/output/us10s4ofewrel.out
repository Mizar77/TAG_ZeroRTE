Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_4', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:50, 16.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:22, 15.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:49<03:23, 16.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:04<02:56, 16.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:19<02:37, 15.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:35<02:22, 15.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:51<02:06, 15.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:09<01:54, 16.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:22<01:33, 15.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:37<01:16, 15.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:52<01:00, 15.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:07<00:45, 15.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:22<00:29, 14.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:36<00:14, 14.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:56<00:00, 16.36s/it]Generating: 100%|██████████| 15/15 [03:56<00:00, 15.76s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 100, 'raw': 160}
{'target': 600, 'success': 124, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 180, 'raw': 288}
{'target': 600, 'success': 200, 'raw': 320}
{'target': 600, 'success': 218, 'raw': 352}
{'target': 600, 'success': 240, 'raw': 384}
{'target': 600, 'success': 258, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 307, 'raw': 480}
{'target': 600, 'success': 328, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 374, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 418, 'raw': 640}
{'target': 600, 'success': 440, 'raw': 672}
{'target': 600, 'success': 460, 'raw': 704}
{'target': 600, 'success': 487, 'raw': 736}
{'target': 600, 'success': 506, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 569, 'raw': 864}
{'target': 600, 'success': 595, 'raw': 896}
{'target': 600, 'success': 617, 'raw': 928}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.6648706896551724, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'2012 MTV Video Music Video Awards\', \'nominated for\', \'\', \'He was in a songwriting competition on " The Simpsons " at the 2012 MTV Video Music Video Awards , winning in one of his four categories .\')', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 402, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 579, 'raw': 800}
{'target': 600, 'success': 605, 'raw': 832}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.7271634615384616, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : architect .', 'success_rate': 0.7877604166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 398, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 496, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 551, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 594, 'raw': 800}
{'target': 600, 'success': 616, 'raw': 832}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7403846153846154, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a new development project called " Bifurcio " , which was developed by the Swiss developers , EMC , for Bifurcio . Head Entity : Bifurcio , Tail Entity : Ericsson .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.80078125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 472, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 619, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.77375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Pristiniki\', \'member of political party\', \'\', \'In 2005 , she became the first female politician to serve in the parliament of Bulgaria , as first and leader of " Pristiniki " in the new parliament .\')'}}
['Relation : operator . Context : Later in the year , the company built a high - speed commuter railway crossing the Bordeaux via Bordeaux Station to Marseille , France . Head Entity : Marseille , Tail Entity : Ralf Rummel .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.842391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 76, 'raw': 128}
{'target': 600, 'success': 92, 'raw': 160}
{'target': 600, 'success': 111, 'raw': 192}
{'target': 600, 'success': 130, 'raw': 224}
{'target': 600, 'success': 144, 'raw': 256}
{'target': 600, 'success': 158, 'raw': 288}
{'target': 600, 'success': 176, 'raw': 320}
{'target': 600, 'success': 191, 'raw': 352}
{'target': 600, 'success': 208, 'raw': 384}
{'target': 600, 'success': 226, 'raw': 416}
{'target': 600, 'success': 245, 'raw': 448}
{'target': 600, 'success': 267, 'raw': 480}
{'target': 600, 'success': 284, 'raw': 512}
{'target': 600, 'success': 298, 'raw': 544}
{'target': 600, 'success': 315, 'raw': 576}
{'target': 600, 'success': 332, 'raw': 608}
{'target': 600, 'success': 349, 'raw': 640}
{'target': 600, 'success': 372, 'raw': 672}
{'target': 600, 'success': 388, 'raw': 704}
{'target': 600, 'success': 406, 'raw': 736}
{'target': 600, 'success': 424, 'raw': 768}
{'target': 600, 'success': 439, 'raw': 800}
{'target': 600, 'success': 454, 'raw': 832}
{'target': 600, 'success': 473, 'raw': 864}
{'target': 600, 'success': 490, 'raw': 896}
{'target': 600, 'success': 511, 'raw': 928}
{'target': 600, 'success': 530, 'raw': 960}
{'target': 600, 'success': 551, 'raw': 992}
{'target': 600, 'success': 567, 'raw': 1024}
{'target': 600, 'success': 586, 'raw': 1056}
{'target': 600, 'success': 608, 'raw': 1088}
{'prompt': 'Relation : position held .', 'success_rate': 0.5588235294117647, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 14871
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14971, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.43s/it]Extractor Estimating: 2it [00:18,  7.87s/it]Extractor Estimating: 3it [00:19,  4.86s/it]Extractor Estimating: 4it [00:20,  3.19s/it]Extractor Estimating: 5it [00:21,  2.24s/it]Extractor Estimating: 6it [00:21,  1.69s/it]Extractor Estimating: 7it [00:22,  1.35s/it]Extractor Estimating: 8it [00:22,  1.10s/it]Extractor Estimating: 9it [00:23,  1.03it/s]Extractor Estimating: 10it [00:24,  1.18it/s]Extractor Estimating: 11it [00:26,  1.29s/it]Extractor Estimating: 12it [00:27,  1.08s/it]Extractor Estimating: 13it [00:27,  1.04it/s]Extractor Estimating: 14it [00:28,  1.15it/s]Extractor Estimating: 15it [00:29,  1.24it/s]Extractor Estimating: 16it [00:29,  1.30it/s]Extractor Estimating: 17it [00:30,  1.30it/s]Extractor Estimating: 18it [00:31,  1.33it/s]Extractor Estimating: 19it [00:31,  1.46it/s]Extractor Estimating: 20it [00:32,  1.49it/s]Extractor Estimating: 21it [00:32,  1.54it/s]Extractor Estimating: 22it [00:33,  1.61it/s]Extractor Estimating: 23it [00:34,  1.58it/s]Extractor Estimating: 24it [00:34,  1.53it/s]Extractor Estimating: 25it [00:35,  1.59it/s]Extractor Estimating: 26it [00:36,  1.56it/s]Extractor Estimating: 27it [00:36,  1.57it/s]Extractor Estimating: 28it [00:37,  1.62it/s]Extractor Estimating: 29it [00:37,  1.65it/s]Extractor Estimating: 30it [00:38,  1.65it/s]Extractor Estimating: 31it [00:39,  1.70it/s]Extractor Estimating: 32it [00:39,  1.72it/s]Extractor Estimating: 33it [00:40,  1.67it/s]Extractor Estimating: 34it [00:41,  1.58it/s]Extractor Estimating: 35it [00:41,  1.63it/s]Extractor Estimating: 36it [00:42,  1.65it/s]Extractor Estimating: 37it [00:42,  1.54it/s]Extractor Estimating: 38it [00:43,  1.58it/s]Extractor Estimating: 39it [00:44,  1.59it/s]Extractor Estimating: 40it [00:44,  1.57it/s]Extractor Estimating: 41it [00:45,  1.56it/s]Extractor Estimating: 42it [00:46,  1.55it/s]Extractor Estimating: 43it [00:46,  1.57it/s]Extractor Estimating: 44it [00:47,  1.52it/s]Extractor Estimating: 45it [00:48,  1.53it/s]Extractor Estimating: 46it [00:48,  1.58it/s]Extractor Estimating: 47it [00:49,  1.60it/s]Extractor Estimating: 48it [00:49,  1.56it/s]Extractor Estimating: 49it [00:50,  1.61it/s]Extractor Estimating: 50it [00:51,  1.58it/s]Extractor Estimating: 51it [00:51,  1.57it/s]Extractor Estimating: 52it [00:52,  1.55it/s]Extractor Estimating: 53it [00:53,  1.56it/s]Extractor Estimating: 54it [00:53,  1.51it/s]Extractor Estimating: 55it [00:54,  1.48it/s]Extractor Estimating: 56it [00:55,  1.49it/s]Extractor Estimating: 57it [00:55,  1.48it/s]Extractor Estimating: 58it [00:56,  1.50it/s]Extractor Estimating: 59it [00:57,  1.46it/s]Extractor Estimating: 60it [00:57,  1.49it/s]Extractor Estimating: 61it [00:58,  1.45it/s]Extractor Estimating: 62it [00:59,  1.48it/s]Extractor Estimating: 63it [00:59,  1.46it/s]Extractor Estimating: 64it [01:00,  1.51it/s]Extractor Estimating: 65it [01:01,  1.36it/s]Extractor Estimating: 66it [01:02,  1.40it/s]Extractor Estimating: 67it [01:02,  1.43it/s]Extractor Estimating: 68it [01:03,  1.51it/s]Extractor Estimating: 69it [01:03,  1.55it/s]Extractor Estimating: 70it [01:04,  1.54it/s]Extractor Estimating: 71it [01:05,  1.51it/s]Extractor Estimating: 72it [01:05,  1.52it/s]Extractor Estimating: 73it [01:06,  1.50it/s]Extractor Estimating: 74it [01:07,  1.55it/s]Extractor Estimating: 75it [01:07,  1.53it/s]Extractor Estimating: 76it [01:08,  1.58it/s]Extractor Estimating: 77it [01:09,  1.62it/s]Extractor Estimating: 78it [01:09,  1.69it/s]Extractor Estimating: 79it [01:10,  1.71it/s]Extractor Estimating: 80it [01:10,  1.76it/s]Extractor Estimating: 81it [01:11,  1.75it/s]Extractor Estimating: 82it [01:11,  1.71it/s]Extractor Estimating: 83it [01:12,  1.71it/s]Extractor Estimating: 84it [01:13,  1.71it/s]Extractor Estimating: 85it [01:13,  1.69it/s]Extractor Estimating: 86it [01:14,  1.71it/s]Extractor Estimating: 87it [01:14,  1.70it/s]Extractor Estimating: 88it [01:15,  1.76it/s]Extractor Estimating: 89it [01:15,  1.75it/s]Extractor Estimating: 90it [01:16,  1.77it/s]Extractor Estimating: 91it [01:17,  1.73it/s]Extractor Estimating: 92it [01:17,  1.75it/s]Extractor Estimating: 93it [01:18,  1.76it/s]Extractor Estimating: 94it [01:18,  1.65it/s]Extractor Estimating: 95it [01:19,  1.46it/s]Extractor Estimating: 96it [01:20,  1.51it/s]Extractor Estimating: 97it [01:20,  1.57it/s]Extractor Estimating: 98it [01:21,  1.64it/s]Extractor Estimating: 99it [01:22,  1.66it/s]Extractor Estimating: 100it [01:22,  1.74it/s]Extractor Estimating: 101it [01:23,  1.76it/s]Extractor Estimating: 102it [01:23,  1.76it/s]Extractor Estimating: 103it [01:24,  1.77it/s]Extractor Estimating: 104it [01:24,  1.83it/s]Extractor Estimating: 105it [01:25,  1.82it/s]Extractor Estimating: 106it [01:25,  1.80it/s]Extractor Estimating: 107it [01:26,  1.85it/s]Extractor Estimating: 108it [01:26,  1.91it/s]Extractor Estimating: 109it [01:27,  1.86it/s]Extractor Estimating: 110it [01:28,  1.88it/s]Extractor Estimating: 111it [01:28,  1.88it/s]Extractor Estimating: 112it [01:29,  1.88it/s]Extractor Estimating: 113it [01:29,  1.82it/s]Extractor Estimating: 114it [01:30,  1.84it/s]Extractor Estimating: 115it [01:30,  1.83it/s]Extractor Estimating: 116it [01:31,  1.84it/s]Extractor Estimating: 117it [01:32,  1.46it/s]Extractor Estimating: 118it [01:32,  1.53it/s]Extractor Estimating: 119it [01:33,  1.55it/s]Extractor Estimating: 120it [01:34,  1.66it/s]Extractor Estimating: 121it [01:34,  1.65it/s]Extractor Estimating: 122it [01:35,  1.59it/s]Extractor Estimating: 123it [01:35,  1.69it/s]Extractor Estimating: 124it [01:36,  1.75it/s]Extractor Estimating: 125it [01:36,  1.75it/s]Extractor Estimating: 126it [01:37,  1.74it/s]Extractor Estimating: 127it [01:38,  1.69it/s]Extractor Estimating: 128it [01:38,  1.74it/s]Extractor Estimating: 129it [01:39,  1.76it/s]Extractor Estimating: 130it [01:39,  1.78it/s]Extractor Estimating: 131it [01:40,  1.75it/s]Extractor Estimating: 132it [01:40,  1.75it/s]Extractor Estimating: 133it [01:41,  1.70it/s]Extractor Estimating: 134it [01:42,  1.76it/s]Extractor Estimating: 135it [01:42,  1.71it/s]Extractor Estimating: 136it [01:43,  1.69it/s]Extractor Estimating: 137it [01:43,  1.68it/s]Extractor Estimating: 138it [01:44,  1.70it/s]Extractor Estimating: 139it [01:45,  1.69it/s]Extractor Estimating: 140it [01:45,  1.68it/s]Extractor Estimating: 141it [01:46,  1.66it/s]Extractor Estimating: 142it [01:46,  1.69it/s]Extractor Estimating: 143it [01:47,  1.75it/s]Extractor Estimating: 144it [01:47,  1.73it/s]Extractor Estimating: 145it [01:48,  1.73it/s]Extractor Estimating: 146it [01:49,  1.75it/s]Extractor Estimating: 147it [01:49,  1.71it/s]Extractor Estimating: 148it [01:50,  1.65it/s]Extractor Estimating: 149it [01:51,  1.64it/s]Extractor Estimating: 150it [01:51,  1.66it/s]Extractor Estimating: 151it [01:52,  1.62it/s]Extractor Estimating: 152it [01:52,  1.59it/s]Extractor Estimating: 153it [01:53,  1.60it/s]Extractor Estimating: 154it [01:54,  1.64it/s]Extractor Estimating: 155it [01:54,  1.66it/s]Extractor Estimating: 156it [01:55,  1.69it/s]Extractor Estimating: 157it [01:55,  1.74it/s]Extractor Estimating: 158it [01:56,  1.66it/s]Extractor Estimating: 159it [01:56,  1.73it/s]Extractor Estimating: 160it [01:57,  1.70it/s]Extractor Estimating: 161it [01:58,  1.67it/s]Extractor Estimating: 162it [01:58,  1.65it/s]Extractor Estimating: 163it [01:59,  1.58it/s]Extractor Estimating: 164it [02:00,  1.62it/s]Extractor Estimating: 165it [02:00,  1.57it/s]Extractor Estimating: 166it [02:01,  1.57it/s]Extractor Estimating: 167it [02:02,  1.61it/s]Extractor Estimating: 168it [02:02,  1.43it/s]Extractor Estimating: 169it [02:03,  1.48it/s]Extractor Estimating: 170it [02:04,  1.57it/s]Extractor Estimating: 171it [02:04,  1.54it/s]Extractor Estimating: 172it [02:05,  1.56it/s]Extractor Estimating: 173it [02:05,  1.57it/s]Extractor Estimating: 174it [02:06,  1.51it/s]Extractor Estimating: 175it [02:07,  1.57it/s]Extractor Estimating: 176it [02:07,  1.53it/s]Extractor Estimating: 177it [02:08,  1.61it/s]Extractor Estimating: 178it [02:09,  1.61it/s]Extractor Estimating: 179it [02:09,  1.68it/s]Extractor Estimating: 180it [02:10,  1.72it/s]Extractor Estimating: 181it [02:10,  1.68it/s]Extractor Estimating: 182it [02:11,  1.69it/s]Extractor Estimating: 183it [02:12,  1.63it/s]Extractor Estimating: 184it [02:12,  1.56it/s]Extractor Estimating: 185it [02:13,  1.57it/s]Extractor Estimating: 186it [02:14,  1.61it/s]Extractor Estimating: 187it [02:14,  1.64it/s]Extractor Estimating: 188it [02:15,  1.62it/s]Extractor Estimating: 189it [02:15,  1.68it/s]Extractor Estimating: 190it [02:16,  1.69it/s]Extractor Estimating: 191it [02:17,  1.64it/s]Extractor Estimating: 192it [02:17,  1.66it/s]Extractor Estimating: 193it [02:18,  1.62it/s]Extractor Estimating: 194it [02:18,  1.60it/s]Extractor Estimating: 195it [02:19,  1.62it/s]Extractor Estimating: 196it [02:20,  1.64it/s]Extractor Estimating: 197it [02:20,  1.64it/s]Extractor Estimating: 198it [02:21,  1.60it/s]Extractor Estimating: 199it [02:22,  1.55it/s]Extractor Estimating: 200it [02:22,  1.63it/s]Extractor Estimating: 201it [02:23,  1.64it/s]Extractor Estimating: 202it [02:23,  1.62it/s]Extractor Estimating: 203it [02:24,  1.63it/s]Extractor Estimating: 204it [02:25,  1.53it/s]Extractor Estimating: 205it [02:25,  1.55it/s]Extractor Estimating: 206it [02:26,  1.60it/s]Extractor Estimating: 207it [02:27,  1.53it/s]Extractor Estimating: 208it [02:27,  1.54it/s]Extractor Estimating: 209it [02:28,  1.54it/s]Extractor Estimating: 210it [02:29,  1.56it/s]Extractor Estimating: 211it [02:29,  1.62it/s]Extractor Estimating: 212it [02:30,  1.59it/s]Extractor Estimating: 213it [02:30,  1.59it/s]Extractor Estimating: 214it [02:31,  1.56it/s]Extractor Estimating: 215it [02:32,  1.62it/s]Extractor Estimating: 216it [02:32,  1.62it/s]Extractor Estimating: 217it [02:33,  1.59it/s]Extractor Estimating: 218it [02:34,  1.37it/s]Extractor Estimating: 219it [02:34,  1.42it/s]Extractor Estimating: 220it [02:35,  1.49it/s]Extractor Estimating: 221it [02:36,  1.49it/s]Extractor Estimating: 222it [02:36,  1.58it/s]Extractor Estimating: 223it [02:37,  1.64it/s]Extractor Estimating: 224it [02:37,  1.67it/s]Extractor Estimating: 225it [02:38,  1.65it/s]Extractor Estimating: 226it [02:39,  1.60it/s]Extractor Estimating: 227it [02:39,  1.58it/s]Extractor Estimating: 228it [02:40,  1.60it/s]Extractor Estimating: 229it [02:41,  1.57it/s]Extractor Estimating: 230it [02:41,  1.54it/s]Extractor Estimating: 231it [02:42,  1.52it/s]Extractor Estimating: 232it [02:43,  1.53it/s]Extractor Estimating: 233it [02:43,  1.54it/s]Extractor Estimating: 234it [02:44,  1.57it/s]Extractor Estimating: 235it [02:45,  1.55it/s]Extractor Estimating: 236it [02:45,  1.53it/s]Extractor Estimating: 237it [02:46,  1.56it/s]Extractor Estimating: 238it [02:47,  1.51it/s]Extractor Estimating: 239it [02:47,  1.49it/s]Extractor Estimating: 240it [02:48,  1.53it/s]Extractor Estimating: 241it [02:48,  1.54it/s]Extractor Estimating: 242it [02:49,  1.55it/s]Extractor Estimating: 243it [02:50,  1.56it/s]Extractor Estimating: 244it [02:50,  1.52it/s]Extractor Estimating: 245it [02:51,  1.48it/s]Extractor Estimating: 246it [02:52,  1.53it/s]Extractor Estimating: 247it [02:53,  1.41it/s]Extractor Estimating: 248it [02:53,  1.41it/s]Extractor Estimating: 249it [02:54,  1.46it/s]Extractor Estimating: 250it [02:55,  1.50it/s]Extractor Estimating: 251it [02:55,  1.60it/s]Extractor Estimating: 252it [02:56,  1.70it/s]Extractor Estimating: 253it [02:56,  1.63it/s]Extractor Estimating: 254it [02:57,  1.70it/s]Extractor Estimating: 255it [02:57,  1.76it/s]Extractor Estimating: 256it [02:58,  1.77it/s]Extractor Estimating: 257it [02:58,  1.80it/s]Extractor Estimating: 258it [02:59,  1.77it/s]Extractor Estimating: 259it [03:00,  1.73it/s]Extractor Estimating: 260it [03:00,  1.65it/s]Extractor Estimating: 261it [03:01,  1.66it/s]Extractor Estimating: 262it [03:01,  1.71it/s]Extractor Estimating: 263it [03:02,  1.70it/s]Extractor Estimating: 264it [03:03,  1.74it/s]Extractor Estimating: 265it [03:03,  1.74it/s]Extractor Estimating: 266it [03:04,  1.81it/s]Extractor Estimating: 267it [03:04,  1.82it/s]Extractor Estimating: 268it [03:05,  1.67it/s]Extractor Estimating: 269it [03:05,  1.71it/s]Extractor Estimating: 270it [03:06,  1.69it/s]Extractor Estimating: 271it [03:07,  1.71it/s]Extractor Estimating: 272it [03:07,  1.74it/s]Extractor Estimating: 273it [03:08,  1.79it/s]Extractor Estimating: 274it [03:08,  1.76it/s]Extractor Estimating: 275it [03:09,  1.77it/s]Extractor Estimating: 276it [03:09,  1.74it/s]Extractor Estimating: 277it [03:10,  1.68it/s]Extractor Estimating: 278it [03:11,  1.65it/s]Extractor Estimating: 279it [03:11,  1.62it/s]Extractor Estimating: 280it [03:12,  1.64it/s]Extractor Estimating: 281it [03:13,  1.64it/s]Extractor Estimating: 282it [03:13,  1.57it/s]Extractor Estimating: 283it [03:14,  1.59it/s]Extractor Estimating: 284it [03:15,  1.55it/s]Extractor Estimating: 285it [03:15,  1.56it/s]Extractor Estimating: 286it [03:16,  1.60it/s]Extractor Estimating: 287it [03:16,  1.61it/s]Extractor Estimating: 288it [03:17,  1.68it/s]Extractor Estimating: 289it [03:18,  1.65it/s]Extractor Estimating: 290it [03:18,  1.68it/s]Extractor Estimating: 291it [03:19,  1.68it/s]Extractor Estimating: 292it [03:19,  1.69it/s]Extractor Estimating: 293it [03:20,  1.67it/s]Extractor Estimating: 294it [03:21,  1.65it/s]Extractor Estimating: 295it [03:21,  1.64it/s]Extractor Estimating: 296it [03:22,  1.63it/s]Extractor Estimating: 297it [03:22,  1.63it/s]Extractor Estimating: 298it [03:23,  1.64it/s]Extractor Estimating: 299it [03:24,  1.66it/s]Extractor Estimating: 300it [03:24,  1.66it/s]Extractor Estimating: 301it [03:25,  1.66it/s]Extractor Estimating: 302it [03:25,  1.61it/s]Extractor Estimating: 303it [03:26,  1.61it/s]Extractor Estimating: 304it [03:27,  1.63it/s]Extractor Estimating: 305it [03:27,  1.62it/s]Extractor Estimating: 306it [03:28,  1.68it/s]Extractor Estimating: 307it [03:28,  1.68it/s]Extractor Estimating: 308it [03:29,  1.69it/s]Extractor Estimating: 309it [03:30,  1.69it/s]Extractor Estimating: 310it [03:30,  1.63it/s]Extractor Estimating: 311it [03:31,  1.57it/s]Extractor Estimating: 312it [03:32,  1.51it/s]Extractor Estimating: 313it [03:32,  1.49it/s]Extractor Estimating: 314it [03:33,  1.56it/s]Extractor Estimating: 315it [03:34,  1.55it/s]Extractor Estimating: 316it [03:34,  1.59it/s]Extractor Estimating: 317it [03:35,  1.58it/s]Extractor Estimating: 318it [03:35,  1.60it/s]Extractor Estimating: 319it [03:36,  1.33it/s]Extractor Estimating: 320it [03:37,  1.44it/s]Extractor Estimating: 321it [03:38,  1.48it/s]Extractor Estimating: 322it [03:38,  1.50it/s]Extractor Estimating: 323it [03:39,  1.54it/s]Extractor Estimating: 324it [03:40,  1.54it/s]Extractor Estimating: 325it [03:40,  1.53it/s]Extractor Estimating: 326it [03:41,  1.45it/s]Extractor Estimating: 327it [03:42,  1.44it/s]Extractor Estimating: 328it [03:42,  1.47it/s]Extractor Estimating: 329it [03:43,  1.48it/s]Extractor Estimating: 330it [03:44,  1.48it/s]Extractor Estimating: 331it [03:44,  1.49it/s]Extractor Estimating: 332it [03:45,  1.58it/s]Extractor Estimating: 333it [03:46,  1.59it/s]Extractor Estimating: 334it [03:46,  1.63it/s]Extractor Estimating: 335it [03:47,  1.59it/s]Extractor Estimating: 336it [03:47,  1.60it/s]Extractor Estimating: 337it [03:48,  1.54it/s]Extractor Estimating: 338it [03:49,  1.56it/s]Extractor Estimating: 339it [03:49,  1.57it/s]Extractor Estimating: 340it [03:50,  1.63it/s]Extractor Estimating: 341it [03:51,  1.64it/s]Extractor Estimating: 342it [03:51,  1.64it/s]Extractor Estimating: 343it [03:52,  1.63it/s]Extractor Estimating: 344it [03:52,  1.60it/s]Extractor Estimating: 345it [03:53,  1.58it/s]Extractor Estimating: 346it [03:54,  1.49it/s]Extractor Estimating: 347it [03:54,  1.50it/s]Extractor Estimating: 348it [03:55,  1.57it/s]Extractor Estimating: 349it [03:56,  1.58it/s]Extractor Estimating: 350it [03:56,  1.56it/s]Extractor Estimating: 351it [03:57,  1.60it/s]Extractor Estimating: 352it [03:58,  1.59it/s]Extractor Estimating: 353it [03:58,  1.63it/s]Extractor Estimating: 354it [03:59,  1.62it/s]Extractor Estimating: 355it [03:59,  1.62it/s]Extractor Estimating: 356it [04:00,  1.61it/s]Extractor Estimating: 357it [04:01,  1.63it/s]Extractor Estimating: 358it [04:01,  1.64it/s]Extractor Estimating: 359it [04:02,  1.65it/s]Extractor Estimating: 360it [04:02,  1.61it/s]Extractor Estimating: 361it [04:03,  1.62it/s]Extractor Estimating: 362it [04:04,  1.66it/s]Extractor Estimating: 363it [04:04,  1.63it/s]Extractor Estimating: 364it [04:05,  1.66it/s]Extractor Estimating: 365it [04:05,  1.66it/s]Extractor Estimating: 366it [04:06,  1.63it/s]Extractor Estimating: 367it [04:07,  1.62it/s]Extractor Estimating: 368it [04:07,  1.66it/s]Extractor Estimating: 369it [04:08,  1.68it/s]Extractor Estimating: 370it [04:08,  1.67it/s]Extractor Estimating: 371it [04:09,  1.66it/s]Extractor Estimating: 372it [04:10,  1.74it/s]Extractor Estimating: 373it [04:10,  1.74it/s]Extractor Estimating: 374it [04:11,  1.76it/s]Extractor Estimating: 375it [04:11,  1.69it/s]Extractor Estimating: 375it [04:11,  1.49it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 1513 mean pseudo reward: 0.9648745038749349
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 14292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=14392, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 36, avg_time 1.268, loss:255.8988
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 8, avg_time 0.955, loss:201.9753
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 44, avg_time 0.959, loss:165.9645
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 16, avg_time 0.939, loss:132.6918
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 52, avg_time 0.960, loss:125.4208
>> valid entity prec:0.5128, rec:0.4872, f1:0.4997
>> valid relation prec:0.1664, rec:0.1102, f1:0.1326
>> valid relation with NER prec:0.1664, rec:0.1102, f1:0.1326
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 24, avg_time 2.206, loss:98.6596
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 60, avg_time 0.958, loss:98.9070
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 32, avg_time 0.953, loss:90.3052
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 4, avg_time 0.948, loss:103.2794
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 40, avg_time 0.961, loss:94.4229
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4947, rec:0.5642, f1:0.5272
>> valid relation prec:0.1484, rec:0.1388, f1:0.1434
>> valid relation with NER prec:0.1484, rec:0.1388, f1:0.1434
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 12, avg_time 2.183, loss:90.0994
g_step 1200, step 48, avg_time 0.951, loss:86.7797
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 18:21:54 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 18:21:54 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_18-21-54_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 18:21:57 - WARNING - datasets.builder -   Using custom data configuration default-555e681c7aa1e2c3
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-555e681c7aa1e2c3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  1.45 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 18:22:05,434 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:22:05,494 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:22:05,495 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:22:05,496 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:22:05,735 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:22:06,826 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:22:06,826 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:22:06,826 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:22:06,826 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:22:06,826 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:22:06,826 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 18:22:08,615 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:22:11,779 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 18:22:11,836 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-555e681c7aa1e2c3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 18:22:11 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1549e30469e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:01<00:01,  1.84s/ba]100%|██████████| 2/2 [00:01<00:00,  1.20ba/s]100%|██████████| 2/2 [00:01<00:00,  1.02ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.53ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.10ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.62ba/s]100%|██████████| 4/4 [00:01<00:00,  4.71ba/s]100%|██████████| 4/4 [00:01<00:00,  3.99ba/s]
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  4.13ba/s]100%|██████████| 2/2 [00:00<00:00,  6.88ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.91ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.89ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  5.80ba/s]100%|██████████| 4/4 [00:00<00:00,  6.77ba/s]
[INFO|trainer.py:414] 2023-08-28 18:22:19,125 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 18:22:19,367 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 18:22:19,367 >>   Num examples = 1513
[INFO|trainer.py:1149] 2023-08-28 18:22:19,367 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 18:22:19,367 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 18:22:19,367 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 18:22:19,367 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 18:22:19,367 >>   Total optimization steps = 120
  0%|          | 0/120 [00:00<?, ?it/s]  1%|          | 1/120 [00:00<00:53,  2.21it/s]  2%|▏         | 2/120 [00:00<00:41,  2.82it/s]  2%|▎         | 3/120 [00:01<00:38,  3.06it/s]  3%|▎         | 4/120 [00:01<00:36,  3.19it/s]  4%|▍         | 5/120 [00:01<00:35,  3.27it/s]  5%|▌         | 6/120 [00:01<00:34,  3.31it/s]  6%|▌         | 7/120 [00:02<00:33,  3.34it/s]  7%|▋         | 8/120 [00:02<00:35,  3.19it/s]  8%|▊         | 9/120 [00:02<00:34,  3.26it/s]  8%|▊         | 10/120 [00:03<00:33,  3.31it/s]  9%|▉         | 11/120 [00:03<00:32,  3.35it/s] 10%|█         | 12/120 [00:03<00:32,  3.37it/s] 11%|█         | 13/120 [00:04<00:31,  3.39it/s] 12%|█▏        | 14/120 [00:04<00:31,  3.39it/s] 12%|█▎        | 15/120 [00:04<00:30,  3.39it/s] 13%|█▎        | 16/120 [00:04<00:30,  3.39it/s] 14%|█▍        | 17/120 [00:05<00:30,  3.39it/s] 15%|█▌        | 18/120 [00:05<00:30,  3.39it/s] 16%|█▌        | 19/120 [00:05<00:31,  3.23it/s] 17%|█▋        | 20/120 [00:06<00:30,  3.29it/s] 18%|█▊        | 21/120 [00:06<00:29,  3.32it/s] 18%|█▊        | 22/120 [00:06<00:29,  3.35it/s] 19%|█▉        | 23/120 [00:07<00:28,  3.37it/s] 20%|██        | 24/120 [00:07<00:25,  3.72it/s][INFO|trainer.py:2140] 2023-08-28 18:22:26,574 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:22:26,574 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:22:26,574 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.07it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.40it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.10it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.80it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.07it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.66it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.51it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.27it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.25it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.31it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.45it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.48it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 42.60it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.07it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.40it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.53it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.80it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.86it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.09it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.31it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.12it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.00it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.13it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.30it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.21it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.21it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.29it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.47it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.49it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.35it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.33it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.10it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.14it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.10it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.16it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.23it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.28it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.39it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.30it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.38it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.19it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.25it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.88it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.31it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.25it/s][A
 53%|█████▎    | 232/437 [00:05<00:05, 39.22it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 40.77it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 41.95it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 42.77it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.26it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.59it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.75it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.65it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.51it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.52it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.86it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.06it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.05it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.34it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.43it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.46it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.16it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.04it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.86it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 37.34it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 39.54it/s][A
 77%|███████▋  | 337/437 [00:07<00:03, 30.15it/s][A
 78%|███████▊  | 341/437 [00:08<00:05, 17.74it/s][A
 79%|███████▊  | 344/437 [00:08<00:04, 18.81it/s][A
 79%|███████▉  | 347/437 [00:08<00:04, 19.65it/s][A
 81%|████████  | 353/437 [00:08<00:03, 25.92it/s][A
 82%|████████▏ | 358/437 [00:08<00:02, 30.03it/s][A
 83%|████████▎ | 363/437 [00:08<00:02, 33.50it/s][A
 84%|████████▍ | 368/437 [00:09<00:01, 36.38it/s][A
 85%|████████▌ | 373/437 [00:09<00:01, 38.63it/s][A
 86%|████████▋ | 378/437 [00:09<00:01, 40.37it/s][A
 88%|████████▊ | 383/437 [00:09<00:01, 41.37it/s][A
 89%|████████▉ | 388/437 [00:09<00:01, 41.87it/s][A
 90%|████████▉ | 393/437 [00:09<00:01, 42.30it/s][A
 91%|█████████ | 398/437 [00:09<00:00, 42.76it/s][A
 92%|█████████▏| 403/437 [00:09<00:00, 43.34it/s][A
 93%|█████████▎| 408/437 [00:09<00:00, 43.70it/s][A
 95%|█████████▍| 413/437 [00:10<00:00, 44.07it/s][A
 96%|█████████▌| 418/437 [00:10<00:00, 44.25it/s][A
 97%|█████████▋| 423/437 [00:10<00:00, 44.26it/s][A
 98%|█████████▊| 428/437 [00:10<00:00, 44.13it/s][A
 99%|█████████▉| 433/437 [00:10<00:00, 43.86it/s][A                                                
                                                 [A 20%|██        | 24/120 [00:17<00:25,  3.72it/s]
100%|██████████| 437/437 [00:10<00:00, 43.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:22:37,895 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-24
[INFO|configuration_utils.py:351] 2023-08-28 18:22:38,269 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-24/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:24:05,428 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-24/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:24:05,907 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-24/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:24:06,005 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-24/special_tokens_map.json
 21%|██        | 25/120 [02:26<1:06:18, 41.88s/it] 22%|██▏       | 26/120 [02:26<46:05, 29.42s/it]   22%|██▎       | 27/120 [02:26<32:03, 20.68s/it] 23%|██▎       | 28/120 [02:27<22:19, 14.56s/it] 24%|██▍       | 29/120 [02:27<15:35, 10.28s/it] 25%|██▌       | 30/120 [02:27<10:55,  7.28s/it] 26%|██▌       | 31/120 [02:27<07:41,  5.19s/it] 27%|██▋       | 32/120 [02:28<05:29,  3.75s/it] 28%|██▊       | 33/120 [02:28<03:55,  2.71s/it] 28%|██▊       | 34/120 [02:28<02:50,  1.99s/it] 29%|██▉       | 35/120 [02:29<02:05,  1.48s/it] 30%|███       | 36/120 [02:29<01:34,  1.12s/it] 31%|███       | 37/120 [02:29<01:12,  1.15it/s] 32%|███▏      | 38/120 [02:30<00:57,  1.44it/s] 32%|███▎      | 39/120 [02:30<00:46,  1.74it/s] 33%|███▎      | 40/120 [02:30<00:38,  2.05it/s] 34%|███▍      | 41/120 [02:30<00:33,  2.34it/s] 35%|███▌      | 42/120 [02:31<00:31,  2.47it/s] 36%|███▌      | 43/120 [02:31<00:28,  2.70it/s] 37%|███▋      | 44/120 [02:31<00:26,  2.89it/s] 38%|███▊      | 45/120 [02:32<00:24,  3.04it/s] 38%|███▊      | 46/120 [02:32<00:23,  3.17it/s] 39%|███▉      | 47/120 [02:32<00:22,  3.26it/s] 40%|████      | 48/120 [02:32<00:19,  3.64it/s][INFO|trainer.py:2140] 2023-08-28 18:24:52,322 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:24:52,322 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:24:52,322 >>   Batch size = 8
{'eval_loss': 1.0159531831741333, 'eval_runtime': 10.6538, 'eval_samples_per_second': 327.864, 'eval_steps_per_second': 41.018, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.07it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.95it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.87it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.15it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.51it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.01it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.58it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.31it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.33it/s][A
 12%|█▏        | 52/437 [00:01<00:13, 29.00it/s][A
 13%|█▎        | 57/437 [00:01<00:11, 32.54it/s][A
 14%|█▍        | 62/437 [00:01<00:10, 35.60it/s][A
 15%|█▌        | 67/437 [00:01<00:09, 38.07it/s][A
 16%|█▋        | 72/437 [00:01<00:09, 40.00it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 41.48it/s][A
 19%|█▉        | 82/437 [00:02<00:08, 42.53it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 43.23it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.23it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.29it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.30it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.88it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.33it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.33it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.66it/s][A
 29%|██▉       | 127/437 [00:03<00:06, 44.84it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.76it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.44it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.38it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.25it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.41it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.51it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.79it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.88it/s][A
 39%|███▉      | 172/437 [00:04<00:05, 44.98it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.81it/s][A
 42%|████▏     | 182/437 [00:04<00:08, 29.27it/s][A
 43%|████▎     | 187/437 [00:04<00:07, 32.70it/s][A
 44%|████▍     | 192/437 [00:04<00:06, 35.64it/s][A
 45%|████▌     | 197/437 [00:04<00:06, 38.06it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 39.97it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 41.42it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 42.39it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 43.05it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.13it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.23it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.46it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.91it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.25it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.63it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 44.61it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 44.86it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.54it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.29it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.10it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.06it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.28it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.40it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.50it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 44.65it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.74it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.59it/s][A
 71%|███████▏  | 312/437 [00:07<00:03, 38.88it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 40.67it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 41.95it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 42.69it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.37it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.94it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 44.30it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.32it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.98it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.78it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.68it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.08it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.45it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.66it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.77it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 44.82it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.62it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.30it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.93it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.92it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.17it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.48it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.58it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.76it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.74it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.79it/s][A                                                
                                                 [A 40%|████      | 48/120 [02:43<00:19,  3.64it/s]
100%|██████████| 437/437 [00:10<00:00, 44.79it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:25:02,672 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-48
[INFO|configuration_utils.py:351] 2023-08-28 18:25:03,119 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-48/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:25:17,272 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-48/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:25:19,034 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-48/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:25:19,934 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-48/special_tokens_map.json
 41%|████      | 49/120 [03:19<16:51, 14.25s/it] 42%|████▏     | 50/120 [03:20<11:51, 10.17s/it] 42%|████▎     | 51/120 [03:20<08:17,  7.21s/it] 43%|████▎     | 52/120 [03:21<05:48,  5.13s/it] 44%|████▍     | 53/120 [03:21<04:06,  3.68s/it] 45%|████▌     | 54/120 [03:21<02:55,  2.66s/it] 46%|████▌     | 55/120 [03:21<02:06,  1.95s/it] 47%|████▋     | 56/120 [03:22<01:33,  1.46s/it] 48%|████▊     | 57/120 [03:22<01:09,  1.11s/it] 48%|████▊     | 58/120 [03:22<00:53,  1.16it/s] 49%|████▉     | 59/120 [03:23<00:42,  1.45it/s] 50%|█████     | 60/120 [03:23<00:42,  1.40it/s] 51%|█████     | 61/120 [03:24<00:34,  1.70it/s] 52%|█████▏    | 62/120 [03:24<00:28,  2.00it/s] 52%|█████▎    | 63/120 [03:24<00:24,  2.29it/s] 53%|█████▎    | 64/120 [03:25<00:22,  2.54it/s] 54%|█████▍    | 65/120 [03:25<00:20,  2.75it/s] 55%|█████▌    | 66/120 [03:25<00:18,  2.92it/s] 56%|█████▌    | 67/120 [03:25<00:17,  3.05it/s] 57%|█████▋    | 68/120 [03:26<00:16,  3.15it/s] 57%|█████▊    | 69/120 [03:26<00:16,  3.14it/s] 58%|█████▊    | 70/120 [03:26<00:15,  3.22it/s] 59%|█████▉    | 71/120 [03:27<00:14,  3.28it/s] 60%|██████    | 72/120 [03:27<00:13,  3.65it/s][INFO|trainer.py:2140] 2023-08-28 18:25:46,687 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:25:46,687 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:25:46,687 >>   Batch size = 8
{'eval_loss': 1.0025359392166138, 'eval_runtime': 10.2348, 'eval_samples_per_second': 341.285, 'eval_steps_per_second': 42.697, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.57it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.13it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.54it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.76it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.34it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.96it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.70it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.45it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.52it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.59it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.62it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.41it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.37it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.51it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.47it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.37it/s][A
 20%|█▉        | 87/437 [00:02<00:07, 44.35it/s][A
 21%|██        | 92/437 [00:02<00:23, 14.85it/s][A
 22%|██▏       | 97/437 [00:02<00:18, 18.59it/s][A
 23%|██▎       | 102/437 [00:03<00:14, 22.58it/s][A
 24%|██▍       | 107/437 [00:03<00:12, 26.51it/s][A
 26%|██▌       | 112/437 [00:03<00:10, 30.29it/s][A
 27%|██▋       | 117/437 [00:03<00:09, 33.40it/s][A
 28%|██▊       | 122/437 [00:03<00:08, 36.09it/s][A
 29%|██▉       | 127/437 [00:03<00:08, 38.18it/s][A
 30%|███       | 132/437 [00:03<00:07, 39.61it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 40.68it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 41.69it/s][A
 34%|███▎      | 147/437 [00:04<00:06, 42.56it/s][A
 35%|███▍      | 152/437 [00:04<00:06, 43.19it/s][A
 36%|███▌      | 157/437 [00:04<00:06, 43.72it/s][A
 37%|███▋      | 162/437 [00:04<00:06, 44.02it/s][A
 38%|███▊      | 167/437 [00:04<00:06, 44.39it/s][A
 39%|███▉      | 172/437 [00:04<00:05, 44.20it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.98it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.92it/s][A
 43%|████▎     | 187/437 [00:05<00:05, 44.00it/s][A
 44%|████▍     | 192/437 [00:05<00:06, 37.22it/s][A
 45%|████▌     | 197/437 [00:05<00:06, 39.19it/s][A
 46%|████▌     | 202/437 [00:05<00:05, 40.84it/s][A
 47%|████▋     | 207/437 [00:05<00:05, 42.07it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 42.93it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 43.49it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.95it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.03it/s][A
 53%|█████▎    | 232/437 [00:06<00:04, 43.72it/s][A
 54%|█████▍    | 237/437 [00:06<00:04, 43.54it/s][A
 55%|█████▌    | 242/437 [00:06<00:04, 43.69it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 43.98it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 44.26it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 44.53it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.59it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.66it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.08it/s][A
 63%|██████▎   | 277/437 [00:07<00:03, 43.81it/s][A
 65%|██████▍   | 282/437 [00:07<00:03, 43.85it/s][A
 66%|██████▌   | 287/437 [00:07<00:03, 43.76it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 44.09it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 44.18it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.42it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.51it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.63it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.53it/s][A
 74%|███████▎  | 322/437 [00:08<00:02, 44.04it/s][A
 75%|███████▍  | 327/437 [00:08<00:03, 33.97it/s][A
 76%|███████▌  | 332/437 [00:08<00:02, 36.64it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 38.64it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 40.47it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 41.64it/s][A
 81%|████████  | 352/437 [00:08<00:01, 42.62it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.33it/s][A
 83%|████████▎ | 362/437 [00:09<00:01, 43.63it/s][A
 84%|████████▍ | 367/437 [00:09<00:01, 43.44it/s][A
 85%|████████▌ | 372/437 [00:09<00:01, 43.38it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 43.60it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 43.90it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 44.19it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.39it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.52it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.56it/s][A
 93%|█████████▎| 407/437 [00:10<00:00, 44.26it/s][A
 94%|█████████▍| 412/437 [00:10<00:00, 43.94it/s][A
 95%|█████████▌| 417/437 [00:10<00:00, 43.53it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 43.60it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 43.94it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.31it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.56it/s][A                                                
                                                 [A 60%|██████    | 72/120 [03:38<00:13,  3.65it/s]
100%|██████████| 437/437 [00:10<00:00, 44.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:25:58,715 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-72
[INFO|configuration_utils.py:351] 2023-08-28 18:25:59,849 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-72/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:26:09,243 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-72/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:26:10,886 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-72/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:26:11,607 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-72/special_tokens_map.json
 61%|██████    | 73/120 [04:11<10:37, 13.57s/it] 62%|██████▏   | 74/120 [04:12<07:24,  9.66s/it] 62%|██████▎   | 75/120 [04:12<05:08,  6.85s/it] 63%|██████▎   | 76/120 [04:13<03:34,  4.88s/it] 64%|██████▍   | 77/120 [04:13<02:30,  3.51s/it] 65%|██████▌   | 78/120 [04:13<01:46,  2.54s/it] 66%|██████▌   | 79/120 [04:13<01:16,  1.87s/it] 67%|██████▋   | 80/120 [04:14<00:55,  1.39s/it] 68%|██████▊   | 81/120 [04:14<00:41,  1.06s/it] 68%|██████▊   | 82/120 [04:14<00:31,  1.20it/s] 69%|██████▉   | 83/120 [04:15<00:25,  1.46it/s] 70%|███████   | 84/120 [04:15<00:20,  1.73it/s] 71%|███████   | 85/120 [04:15<00:17,  2.03it/s] 72%|███████▏  | 86/120 [04:16<00:14,  2.32it/s] 72%|███████▎  | 87/120 [04:16<00:12,  2.57it/s] 73%|███████▎  | 88/120 [04:16<00:11,  2.77it/s] 74%|███████▍  | 89/120 [04:16<00:10,  2.94it/s] 75%|███████▌  | 90/120 [04:17<00:09,  3.07it/s] 76%|███████▌  | 91/120 [04:17<00:09,  3.16it/s] 77%|███████▋  | 92/120 [04:17<00:08,  3.23it/s] 78%|███████▊  | 93/120 [04:18<00:08,  3.28it/s] 78%|███████▊  | 94/120 [04:18<00:07,  3.31it/s] 79%|███████▉  | 95/120 [04:18<00:07,  3.13it/s] 80%|████████  | 96/120 [04:18<00:06,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 18:26:38,326 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:26:38,326 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:26:38,326 >>   Batch size = 8
{'eval_loss': 1.0042213201522827, 'eval_runtime': 10.7719, 'eval_samples_per_second': 324.27, 'eval_steps_per_second': 40.569, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.75it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.69it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.20it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.65it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.96it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.60it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.24it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.28it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.33it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.37it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.60it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.59it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.38it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.99it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.99it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.70it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.84it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.84it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.22it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.50it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.52it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.34it/s][A
 27%|██▋       | 117/437 [00:02<00:10, 31.42it/s][A
 28%|██▊       | 122/437 [00:02<00:09, 34.54it/s][A
 29%|██▉       | 127/437 [00:02<00:08, 37.02it/s][A
 30%|███       | 132/437 [00:03<00:07, 39.10it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 40.66it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 41.83it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 42.63it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.13it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.03it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.00it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.22it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 43.60it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.90it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.10it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.23it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.37it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.17it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.96it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.70it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.69it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.15it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.34it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.23it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.38it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.37it/s][A
 55%|█████▌    | 242/437 [00:06<00:04, 44.38it/s][A
 57%|█████▋    | 247/437 [00:06<00:09, 19.75it/s][A
 58%|█████▊    | 252/437 [00:06<00:07, 23.74it/s][A
 59%|█████▉    | 257/437 [00:06<00:06, 27.61it/s][A
 60%|█████▉    | 262/437 [00:06<00:05, 31.20it/s][A
 61%|██████    | 267/437 [00:06<00:04, 34.34it/s][A
 62%|██████▏   | 272/437 [00:06<00:04, 36.92it/s][A
 63%|██████▎   | 277/437 [00:06<00:04, 38.96it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 40.40it/s][A
 66%|██████▌   | 287/437 [00:07<00:03, 41.12it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 41.81it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 42.43it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 43.16it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.64it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.96it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.09it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.26it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.06it/s][A
 76%|███████▌  | 332/437 [00:08<00:02, 43.79it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 43.58it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 43.67it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 43.86it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.17it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.43it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 41.94it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 42.59it/s][A
 85%|████████▌ | 372/437 [00:09<00:01, 42.98it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 43.10it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 43.33it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 43.56it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.00it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.11it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.20it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.16it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.26it/s][A
 95%|█████████▌| 417/437 [00:10<00:00, 44.14it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 44.07it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 44.02it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.01it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.16it/s][A                                                
                                                 [A 80%|████████  | 96/120 [04:29<00:06,  3.52it/s]
100%|██████████| 437/437 [00:10<00:00, 44.16it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:26:49,407 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-96
[INFO|configuration_utils.py:351] 2023-08-28 18:26:50,403 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-96/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:27:02,121 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-96/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:27:02,787 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-96/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:27:03,050 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-96/special_tokens_map.json
 81%|████████  | 97/120 [05:06<05:31, 14.42s/it] 82%|████████▏ | 98/120 [05:06<03:45, 10.23s/it] 82%|████████▎ | 99/120 [05:07<02:32,  7.25s/it] 83%|████████▎ | 100/120 [05:07<01:43,  5.16s/it] 84%|████████▍ | 101/120 [05:07<01:10,  3.70s/it] 85%|████████▌ | 102/120 [05:07<00:48,  2.68s/it] 86%|████████▌ | 103/120 [05:08<00:33,  1.96s/it] 87%|████████▋ | 104/120 [05:08<00:23,  1.46s/it] 88%|████████▊ | 105/120 [05:08<00:16,  1.11s/it] 88%|████████▊ | 106/120 [05:09<00:12,  1.16it/s] 89%|████████▉ | 107/120 [05:09<00:09,  1.44it/s] 90%|█████████ | 108/120 [05:09<00:07,  1.71it/s] 91%|█████████ | 109/120 [05:10<00:05,  2.01it/s] 92%|█████████▏| 110/120 [05:10<00:04,  2.29it/s] 92%|█████████▎| 111/120 [05:10<00:03,  2.54it/s] 93%|█████████▎| 112/120 [05:10<00:02,  2.75it/s] 94%|█████████▍| 113/120 [05:11<00:02,  2.93it/s] 95%|█████████▌| 114/120 [05:11<00:01,  3.07it/s] 96%|█████████▌| 115/120 [05:11<00:01,  3.18it/s] 97%|█████████▋| 116/120 [05:12<00:01,  3.26it/s] 98%|█████████▊| 117/120 [05:12<00:00,  3.31it/s] 98%|█████████▊| 118/120 [05:12<00:00,  3.36it/s] 99%|█████████▉| 119/120 [05:13<00:00,  2.66it/s]100%|██████████| 120/120 [05:13<00:00,  3.09it/s][INFO|trainer.py:2140] 2023-08-28 18:27:32,831 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:27:32,831 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:27:32,831 >>   Batch size = 8
{'eval_loss': 1.0083131790161133, 'eval_runtime': 10.516, 'eval_samples_per_second': 332.16, 'eval_steps_per_second': 41.556, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.55it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.81it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.98it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.69it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.04it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.73it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.46it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.24it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.39it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.44it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.57it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.60it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.53it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.38it/s][A
 18%|█▊        | 77/437 [00:01<00:11, 31.92it/s][A
 19%|█▉        | 82/437 [00:01<00:10, 34.95it/s][A
 20%|█▉        | 87/437 [00:02<00:09, 37.38it/s][A
 21%|██        | 92/437 [00:02<00:08, 39.39it/s][A
 22%|██▏       | 97/437 [00:02<00:08, 40.96it/s][A
 23%|██▎       | 102/437 [00:02<00:08, 39.72it/s][A
 24%|██▍       | 107/437 [00:02<00:08, 41.20it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 42.10it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 42.44it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 42.61it/s][A
 29%|██▉       | 127/437 [00:03<00:07, 43.11it/s][A
 30%|███       | 132/437 [00:03<00:06, 43.69it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.04it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.02it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.16it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.43it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.27it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.18it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.93it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 44.16it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.26it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.39it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.41it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.42it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.60it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.41it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.03it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.05it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 43.96it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.12it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.30it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.23it/s][A
 54%|█████▍    | 237/437 [00:05<00:05, 38.51it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 40.26it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 41.62it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 42.61it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.20it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 43.51it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.70it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.72it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.57it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.64it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.90it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.15it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.29it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.50it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.58it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.19it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.00it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.86it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.97it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.06it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.24it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.29it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.46it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.59it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.34it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.19it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.00it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 38.05it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 39.85it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 41.26it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 42.41it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.16it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.64it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.08it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.12it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.80it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.68it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.77it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.97it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.32it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.50it/s][A                                                 
                                                 [A100%|██████████| 120/120 [05:23<00:00,  3.09it/s]
100%|██████████| 437/437 [00:10<00:00, 44.50it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:27:43,902 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-120
[INFO|configuration_utils.py:351] 2023-08-28 18:27:44,367 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-120/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:27:56,157 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:27:56,595 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:27:56,801 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-120/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:28:25,640 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:28:25,912 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-48 (score: 1.0025359392166138).
                                                 100%|██████████| 120/120 [06:22<00:00,  3.09it/s]100%|██████████| 120/120 [06:22<00:00,  3.19s/it]
[INFO|trainer.py:1894] 2023-08-28 18:28:42,254 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 18:28:42,412 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:28:50,755 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:28:51,180 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:28:51,440 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:28:53,098 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:53,099 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:53,099 >>   train_loss               =     0.5803
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:53,099 >>   train_runtime            = 0:06:22.84
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:53,099 >>   train_samples            =       1513
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:53,099 >>   train_samples_per_second =      19.76
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:53,099 >>   train_steps_per_second   =      0.313
{'eval_loss': 1.0094892978668213, 'eval_runtime': 10.1277, 'eval_samples_per_second': 344.895, 'eval_steps_per_second': 43.149, 'epoch': 5.0}
{'train_runtime': 382.8468, 'train_samples_per_second': 19.76, 'train_steps_per_second': 0.313, 'train_loss': 0.5802829742431641, 'epoch': 5.0}
08/28/2023 18:28:54 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:28:54,392 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:28:54,392 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:28:54,393 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 54.90it/s]  3%|▎         | 12/437 [00:00<00:08, 49.24it/s]  4%|▍         | 17/437 [00:00<00:08, 47.39it/s]  5%|▌         | 22/437 [00:00<00:08, 46.30it/s]  6%|▌         | 27/437 [00:00<00:08, 46.02it/s]  7%|▋         | 32/437 [00:00<00:08, 45.67it/s]  8%|▊         | 37/437 [00:00<00:08, 45.41it/s] 10%|▉         | 42/437 [00:00<00:08, 45.09it/s] 11%|█         | 47/437 [00:01<00:08, 44.56it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.18it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.37it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.46it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.59it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.66it/s] 18%|█▊        | 77/437 [00:01<00:08, 40.17it/s] 19%|█▉        | 82/437 [00:01<00:08, 41.50it/s] 20%|█▉        | 87/437 [00:01<00:08, 42.43it/s] 21%|██        | 92/437 [00:02<00:08, 42.91it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.35it/s] 23%|██▎       | 102/437 [00:02<00:07, 43.66it/s] 24%|██▍       | 107/437 [00:02<00:07, 43.99it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.28it/s] 27%|██▋       | 117/437 [00:02<00:07, 43.97it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.10it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.34it/s] 30%|███       | 132/437 [00:02<00:06, 44.51it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.55it/s] 32%|███▏      | 142/437 [00:03<00:06, 44.46it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.44it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.51it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.37it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.22it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.17it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.27it/s] 41%|████      | 177/437 [00:03<00:05, 44.29it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.40it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.30it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.29it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.29it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.06it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.19it/s] 49%|████▊     | 212/437 [00:04<00:05, 38.32it/s] 50%|████▉     | 217/437 [00:04<00:05, 40.14it/s] 51%|█████     | 222/437 [00:05<00:05, 41.40it/s] 52%|█████▏    | 227/437 [00:05<00:04, 42.33it/s] 53%|█████▎    | 232/437 [00:05<00:04, 43.01it/s] 54%|█████▍    | 237/437 [00:05<00:04, 43.57it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.12it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.24it/s] 58%|█████▊    | 252/437 [00:05<00:04, 43.86it/s] 59%|█████▉    | 257/437 [00:05<00:04, 43.71it/s] 60%|█████▉    | 262/437 [00:05<00:03, 43.88it/s] 61%|██████    | 267/437 [00:06<00:03, 44.08it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.28it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.43it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.59it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.63it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.45it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.25it/s] 69%|██████▉   | 302/437 [00:06<00:03, 43.98it/s] 70%|███████   | 307/437 [00:06<00:02, 43.93it/s] 71%|███████▏  | 312/437 [00:07<00:02, 43.99it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.29it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.44it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.57it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.67it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.28it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.26it/s] 79%|███████▉  | 347/437 [00:08<00:02, 33.44it/s] 81%|████████  | 352/437 [00:08<00:02, 36.12it/s] 82%|████████▏ | 357/437 [00:08<00:02, 38.45it/s] 83%|████████▎ | 362/437 [00:08<00:01, 40.17it/s] 84%|████████▍ | 367/437 [00:08<00:01, 41.50it/s] 85%|████████▌ | 372/437 [00:08<00:01, 42.46it/s] 86%|████████▋ | 377/437 [00:08<00:01, 43.17it/s] 87%|████████▋ | 382/437 [00:08<00:01, 43.38it/s] 89%|████████▊ | 387/437 [00:08<00:01, 43.30it/s] 90%|████████▉ | 392/437 [00:09<00:01, 43.14it/s] 91%|█████████ | 397/437 [00:09<00:00, 43.47it/s] 92%|█████████▏| 402/437 [00:09<00:00, 43.69it/s] 93%|█████████▎| 407/437 [00:09<00:00, 43.99it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.30it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.37it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.54it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.45it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.06it/s]100%|██████████| 437/437 [00:10<00:00, 43.99it/s]100%|██████████| 437/437 [00:10<00:00, 43.53it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:29:04,450 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:29:04,450 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:29:04,450 >>   eval_loss               =     1.0025
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:29:04,450 >>   eval_runtime            = 0:00:10.05
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:29:04,450 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:29:04,450 >>   eval_samples_per_second =    347.305
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:29:04,450 >>   eval_steps_per_second   =      43.45
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:29:04,450 >>   perplexity              =     2.7252
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:29:45,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:29:45,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:29:45,788 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:29:45,788 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:29:45,788 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:29:46,560 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:29:46,561 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:29:47,199 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:29:48,223 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:29:48,677 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:29:53,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:29:53,227 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:29:53,228 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:29:53,228 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:29:53,228 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:29:53,939 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:29:53,940 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:29:54,540 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:29:54,714 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:29:54,714 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-96
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-120
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-24
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-72
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/checkpoint-48
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.75it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.76it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:04,  1.54it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:06,  1.56it/s]Extractor Predicting: 12it [00:07,  1.42it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:08,  1.57it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.64it/s]Extractor Predicting: 17it [00:10,  1.48it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.59it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.63it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:16,  1.71it/s]Extractor Predicting: 27it [00:17,  1.35it/s]Extractor Predicting: 28it [00:17,  1.46it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.50it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:21,  1.52it/s]Extractor Predicting: 35it [00:22,  1.57it/s]Extractor Predicting: 36it [00:23,  1.46it/s]Extractor Predicting: 37it [00:23,  1.49it/s]Extractor Predicting: 38it [00:24,  1.51it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:26,  1.41it/s]Extractor Predicting: 42it [00:27,  1.39it/s]Extractor Predicting: 43it [00:27,  1.44it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:29,  1.57it/s]Extractor Predicting: 46it [00:30,  1.32it/s]Extractor Predicting: 47it [00:30,  1.37it/s]Extractor Predicting: 48it [00:31,  1.42it/s]Extractor Predicting: 49it [00:32,  1.46it/s]Extractor Predicting: 50it [00:32,  1.51it/s]Extractor Predicting: 51it [00:33,  1.37it/s]Extractor Predicting: 52it [00:34,  1.41it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:35,  1.51it/s]Extractor Predicting: 55it [00:36,  1.52it/s]Extractor Predicting: 56it [00:36,  1.44it/s]Extractor Predicting: 57it [00:37,  1.45it/s]Extractor Predicting: 58it [00:38,  1.47it/s]Extractor Predicting: 59it [00:38,  1.48it/s]Extractor Predicting: 60it [00:39,  1.51it/s]Extractor Predicting: 61it [00:40,  1.29it/s]Extractor Predicting: 62it [00:41,  1.36it/s]Extractor Predicting: 63it [00:41,  1.44it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:43,  1.53it/s]Extractor Predicting: 66it [00:43,  1.41it/s]Extractor Predicting: 67it [00:44,  1.45it/s]Extractor Predicting: 68it [00:45,  1.49it/s]Extractor Predicting: 69it [00:45,  1.52it/s]Extractor Predicting: 70it [00:46,  1.53it/s]Extractor Predicting: 71it [00:47,  1.48it/s]Extractor Predicting: 72it [00:47,  1.53it/s]Extractor Predicting: 73it [00:48,  1.55it/s]Extractor Predicting: 74it [00:49,  1.59it/s]Extractor Predicting: 75it [00:49,  1.62it/s]Extractor Predicting: 76it [00:50,  1.32it/s]Extractor Predicting: 77it [00:51,  1.39it/s]Extractor Predicting: 78it [00:51,  1.43it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:53,  1.51it/s]Extractor Predicting: 81it [00:53,  1.50it/s]Extractor Predicting: 82it [00:54,  1.52it/s]Extractor Predicting: 83it [00:55,  1.51it/s]Extractor Predicting: 84it [00:55,  1.54it/s]Extractor Predicting: 85it [00:56,  1.59it/s]Extractor Predicting: 86it [00:57,  1.59it/s]Extractor Predicting: 87it [00:57,  1.67it/s]Extractor Predicting: 88it [00:58,  1.69it/s]Extractor Predicting: 89it [00:58,  1.69it/s]Extractor Predicting: 90it [00:59,  1.71it/s]Extractor Predicting: 91it [00:59,  1.70it/s]Extractor Predicting: 92it [01:00,  1.60it/s]Extractor Predicting: 93it [01:01,  1.67it/s]Extractor Predicting: 94it [01:01,  1.71it/s]Extractor Predicting: 95it [01:02,  1.71it/s]Extractor Predicting: 96it [01:02,  1.72it/s]Extractor Predicting: 97it [01:03,  1.68it/s]Extractor Predicting: 98it [01:04,  1.61it/s]Extractor Predicting: 99it [01:04,  1.61it/s]Extractor Predicting: 100it [01:05,  1.65it/s]Extractor Predicting: 101it [01:05,  1.69it/s]Extractor Predicting: 102it [01:06,  1.67it/s]Extractor Predicting: 103it [01:07,  1.64it/s]Extractor Predicting: 104it [01:07,  1.65it/s]Extractor Predicting: 105it [01:08,  1.72it/s]Extractor Predicting: 106it [01:08,  1.69it/s]Extractor Predicting: 107it [01:09,  1.70it/s]Extractor Predicting: 108it [01:10,  1.71it/s]Extractor Predicting: 109it [01:11,  1.32it/s]Extractor Predicting: 110it [01:11,  1.41it/s]Extractor Predicting: 111it [01:12,  1.48it/s]Extractor Predicting: 112it [01:13,  1.52it/s]Extractor Predicting: 113it [01:13,  1.57it/s]Extractor Predicting: 114it [01:14,  1.20it/s]Extractor Predicting: 115it [01:15,  1.32it/s]Extractor Predicting: 116it [01:16,  1.39it/s]Extractor Predicting: 117it [01:16,  1.44it/s]Extractor Predicting: 118it [01:17,  1.48it/s]Extractor Predicting: 119it [01:17,  1.53it/s]Extractor Predicting: 120it [01:18,  1.58it/s]Extractor Predicting: 121it [01:19,  1.59it/s]Extractor Predicting: 122it [01:19,  1.59it/s]Extractor Predicting: 123it [01:20,  1.52it/s]Extractor Predicting: 124it [01:21,  1.53it/s]Extractor Predicting: 125it [01:22,  1.26it/s]Extractor Predicting: 126it [01:22,  1.36it/s]Extractor Predicting: 127it [01:23,  1.41it/s]Extractor Predicting: 128it [01:24,  1.34it/s]Extractor Predicting: 129it [01:25,  1.29it/s]Extractor Predicting: 130it [01:25,  1.38it/s]Extractor Predicting: 131it [01:26,  1.45it/s]Extractor Predicting: 132it [01:27,  1.50it/s]Extractor Predicting: 133it [01:27,  1.53it/s]Extractor Predicting: 134it [01:28,  1.49it/s]Extractor Predicting: 135it [01:29,  1.52it/s]Extractor Predicting: 136it [01:29,  1.54it/s]Extractor Predicting: 137it [01:30,  1.55it/s]Extractor Predicting: 138it [01:30,  1.55it/s]Extractor Predicting: 139it [01:31,  1.54it/s]Extractor Predicting: 140it [01:32,  1.60it/s]Extractor Predicting: 141it [01:32,  1.59it/s]Extractor Predicting: 142it [01:33,  1.68it/s]Extractor Predicting: 142it [01:33,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:53,848 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:53,885 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:53,885 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:53,885 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:53,885 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:31:54,470 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:31:54,471 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:31:54,785 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:31:56,258 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:31:56,258 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:57,990 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:57,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:57,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:57,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:57,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:31:58,841 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:31:58,842 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:31:59,149 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:31:59,374 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:31:59,374 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.17177314211212516,
  "recall": 0.1508731749212711,
  "score": 0.16064624295076968,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:05,  1.71it/s]Extractor Predicting: 11it [00:06,  1.56it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:08,  1.57it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:18,  1.58it/s]Extractor Predicting: 31it [00:19,  1.49it/s]Extractor Predicting: 32it [00:20,  1.51it/s]Extractor Predicting: 33it [00:20,  1.55it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:21,  1.57it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:23,  1.67it/s]Extractor Predicting: 39it [00:24,  1.66it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:25,  1.59it/s]Extractor Predicting: 42it [00:26,  1.58it/s]Extractor Predicting: 43it [00:26,  1.59it/s]Extractor Predicting: 44it [00:27,  1.59it/s]Extractor Predicting: 45it [00:28,  1.56it/s]Extractor Predicting: 46it [00:28,  1.59it/s]Extractor Predicting: 47it [00:29,  1.60it/s]Extractor Predicting: 48it [00:30,  1.61it/s]Extractor Predicting: 49it [00:30,  1.62it/s]Extractor Predicting: 50it [00:31,  1.57it/s]Extractor Predicting: 51it [00:31,  1.60it/s]Extractor Predicting: 52it [00:32,  1.62it/s]Extractor Predicting: 53it [00:33,  1.62it/s]Extractor Predicting: 54it [00:33,  1.62it/s]Extractor Predicting: 55it [00:34,  1.53it/s]Extractor Predicting: 56it [00:35,  1.56it/s]Extractor Predicting: 57it [00:35,  1.61it/s]Extractor Predicting: 58it [00:36,  1.65it/s]Extractor Predicting: 59it [00:36,  1.66it/s]Extractor Predicting: 60it [00:37,  1.58it/s]Extractor Predicting: 61it [00:38,  1.59it/s]Extractor Predicting: 62it [00:38,  1.62it/s]Extractor Predicting: 63it [00:39,  1.64it/s]Extractor Predicting: 64it [00:40,  1.61it/s]Extractor Predicting: 65it [00:40,  1.52it/s]Extractor Predicting: 66it [00:41,  1.54it/s]Extractor Predicting: 67it [00:41,  1.58it/s]Extractor Predicting: 68it [00:42,  1.65it/s]Extractor Predicting: 69it [00:43,  1.64it/s]Extractor Predicting: 70it [00:43,  1.61it/s]Extractor Predicting: 71it [00:44,  1.62it/s]Extractor Predicting: 72it [00:45,  1.64it/s]Extractor Predicting: 73it [00:45,  1.60it/s]Extractor Predicting: 74it [00:46,  1.61it/s]Extractor Predicting: 75it [00:46,  1.58it/s]Extractor Predicting: 76it [00:47,  1.60it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:48,  1.60it/s]Extractor Predicting: 79it [00:49,  1.60it/s]Extractor Predicting: 80it [00:50,  1.58it/s]Extractor Predicting: 81it [00:50,  1.61it/s]Extractor Predicting: 82it [00:51,  1.59it/s]Extractor Predicting: 83it [00:51,  1.61it/s]Extractor Predicting: 84it [00:52,  1.51it/s]Extractor Predicting: 85it [00:53,  1.56it/s]Extractor Predicting: 86it [00:53,  1.56it/s]Extractor Predicting: 87it [00:54,  1.58it/s]Extractor Predicting: 88it [00:55,  1.55it/s]Extractor Predicting: 89it [00:55,  1.54it/s]Extractor Predicting: 90it [00:56,  1.58it/s]Extractor Predicting: 91it [00:57,  1.57it/s]Extractor Predicting: 92it [00:57,  1.58it/s]Extractor Predicting: 93it [00:58,  1.55it/s]Extractor Predicting: 94it [00:59,  1.50it/s]Extractor Predicting: 95it [00:59,  1.52it/s]Extractor Predicting: 96it [01:00,  1.54it/s]Extractor Predicting: 97it [01:01,  1.56it/s]Extractor Predicting: 98it [01:01,  1.55it/s]Extractor Predicting: 99it [01:02,  1.36it/s]Extractor Predicting: 100it [01:03,  1.42it/s]Extractor Predicting: 101it [01:03,  1.46it/s]Extractor Predicting: 102it [01:04,  1.49it/s]Extractor Predicting: 103it [01:05,  1.53it/s]Extractor Predicting: 104it [01:05,  1.50it/s]Extractor Predicting: 105it [01:06,  1.51it/s]Extractor Predicting: 106it [01:07,  1.54it/s]Extractor Predicting: 107it [01:07,  1.52it/s]Extractor Predicting: 108it [01:08,  1.52it/s]Extractor Predicting: 109it [01:09,  1.33it/s]Extractor Predicting: 110it [01:10,  1.37it/s]Extractor Predicting: 111it [01:10,  1.41it/s]Extractor Predicting: 112it [01:11,  1.49it/s]Extractor Predicting: 113it [01:11,  1.55it/s]Extractor Predicting: 114it [01:12,  1.49it/s]Extractor Predicting: 115it [01:13,  1.51it/s]Extractor Predicting: 116it [01:13,  1.55it/s]Extractor Predicting: 117it [01:14,  1.59it/s]Extractor Predicting: 118it [01:15,  1.62it/s]Extractor Predicting: 119it [01:15,  1.48it/s]Extractor Predicting: 120it [01:16,  1.54it/s]Extractor Predicting: 121it [01:17,  1.57it/s]Extractor Predicting: 122it [01:17,  1.50it/s]Extractor Predicting: 123it [01:18,  1.54it/s]Extractor Predicting: 124it [01:19,  1.50it/s]Extractor Predicting: 125it [01:19,  1.54it/s]Extractor Predicting: 126it [01:20,  1.56it/s]Extractor Predicting: 127it [01:20,  1.60it/s]Extractor Predicting: 128it [01:21,  1.63it/s]Extractor Predicting: 129it [01:22,  1.56it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:23,  1.57it/s]Extractor Predicting: 132it [01:24,  1.60it/s]Extractor Predicting: 133it [01:24,  1.59it/s]Extractor Predicting: 134it [01:25,  1.60it/s]Extractor Predicting: 135it [01:25,  1.64it/s]Extractor Predicting: 136it [01:26,  1.56it/s]Extractor Predicting: 137it [01:27,  1.57it/s]Extractor Predicting: 138it [01:27,  1.60it/s]Extractor Predicting: 139it [01:28,  1.65it/s]Extractor Predicting: 140it [01:29,  1.62it/s]Extractor Predicting: 141it [01:29,  1.51it/s]Extractor Predicting: 142it [01:30,  1.58it/s]Extractor Predicting: 143it [01:31,  1.60it/s]Extractor Predicting: 144it [01:31,  1.59it/s]Extractor Predicting: 145it [01:32,  1.61it/s]Extractor Predicting: 146it [01:32,  1.57it/s]Extractor Predicting: 147it [01:33,  1.55it/s]Extractor Predicting: 148it [01:34,  1.57it/s]Extractor Predicting: 149it [01:34,  1.58it/s]Extractor Predicting: 150it [01:35,  1.61it/s]Extractor Predicting: 151it [01:36,  1.57it/s]Extractor Predicting: 152it [01:36,  1.59it/s]Extractor Predicting: 153it [01:37,  1.61it/s]Extractor Predicting: 154it [01:37,  1.61it/s]Extractor Predicting: 155it [01:38,  1.60it/s]Extractor Predicting: 156it [01:39,  1.56it/s]Extractor Predicting: 157it [01:39,  1.55it/s]Extractor Predicting: 158it [01:40,  1.57it/s]Extractor Predicting: 159it [01:41,  1.58it/s]Extractor Predicting: 160it [01:41,  1.57it/s]Extractor Predicting: 161it [01:42,  1.58it/s]Extractor Predicting: 162it [01:43,  1.61it/s]Extractor Predicting: 163it [01:43,  1.61it/s]Extractor Predicting: 164it [01:44,  1.63it/s]Extractor Predicting: 165it [01:44,  1.59it/s]Extractor Predicting: 166it [01:45,  1.57it/s]Extractor Predicting: 167it [01:46,  1.61it/s]Extractor Predicting: 168it [01:46,  1.61it/s]Extractor Predicting: 169it [01:47,  1.61it/s]Extractor Predicting: 170it [01:48,  1.57it/s]Extractor Predicting: 171it [01:48,  1.52it/s]Extractor Predicting: 172it [01:49,  1.54it/s]Extractor Predicting: 173it [01:50,  1.56it/s]Extractor Predicting: 174it [01:50,  1.51it/s]Extractor Predicting: 175it [01:51,  1.47it/s]Extractor Predicting: 176it [01:52,  1.48it/s]Extractor Predicting: 177it [01:52,  1.49it/s]Extractor Predicting: 178it [01:53,  1.55it/s]Extractor Predicting: 179it [01:53,  1.55it/s]Extractor Predicting: 180it [01:54,  1.62it/s]Extractor Predicting: 181it [01:55,  1.59it/s]Extractor Predicting: 182it [01:55,  1.61it/s]Extractor Predicting: 183it [01:56,  1.60it/s]Extractor Predicting: 184it [01:57,  1.59it/s]Extractor Predicting: 185it [01:57,  1.63it/s]Extractor Predicting: 186it [01:58,  1.64it/s]Extractor Predicting: 187it [01:58,  1.65it/s]Extractor Predicting: 188it [01:59,  1.65it/s]Extractor Predicting: 189it [02:00,  1.44it/s]Extractor Predicting: 190it [02:00,  1.48it/s]Extractor Predicting: 191it [02:01,  1.48it/s]Extractor Predicting: 192it [02:02,  1.54it/s]Extractor Predicting: 193it [02:02,  1.59it/s]Extractor Predicting: 194it [02:03,  1.35it/s]Extractor Predicting: 195it [02:04,  1.42it/s]Extractor Predicting: 196it [02:05,  1.51it/s]Extractor Predicting: 197it [02:05,  1.56it/s]Extractor Predicting: 198it [02:06,  1.56it/s]Extractor Predicting: 199it [02:07,  1.35it/s]Extractor Predicting: 200it [02:07,  1.42it/s]Extractor Predicting: 201it [02:08,  1.49it/s]Extractor Predicting: 202it [02:08,  1.57it/s]Extractor Predicting: 203it [02:09,  1.62it/s]Extractor Predicting: 204it [02:10,  1.48it/s]Extractor Predicting: 205it [02:11,  1.48it/s]Extractor Predicting: 206it [02:11,  1.52it/s]Extractor Predicting: 207it [02:12,  1.57it/s]Extractor Predicting: 208it [02:12,  1.62it/s]Extractor Predicting: 209it [02:13,  1.40it/s]Extractor Predicting: 210it [02:14,  1.44it/s]Extractor Predicting: 211it [02:15,  1.49it/s]Extractor Predicting: 212it [02:15,  1.55it/s]Extractor Predicting: 213it [02:16,  1.59it/s]Extractor Predicting: 214it [02:17,  1.39it/s]Extractor Predicting: 215it [02:17,  1.45it/s]Extractor Predicting: 216it [02:18,  1.51it/s]Extractor Predicting: 217it [02:18,  1.56it/s]Extractor Predicting: 218it [02:19,  1.53it/s]Extractor Predicting: 219it [02:20,  1.33it/s]Extractor Predicting: 220it [02:21,  1.41it/s]Extractor Predicting: 221it [02:21,  1.43it/s]Extractor Predicting: 222it [02:22,  1.48it/s]Extractor Predicting: 223it [02:23,  1.51it/s]Extractor Predicting: 224it [02:24,  1.38it/s]Extractor Predicting: 225it [02:24,  1.26it/s]Extractor Predicting: 226it [02:25,  1.38it/s]Extractor Predicting: 227it [02:26,  1.48it/s]Extractor Predicting: 228it [02:26,  1.50it/s]Extractor Predicting: 229it [02:27,  1.54it/s]Extractor Predicting: 230it [02:28,  1.23it/s]Extractor Predicting: 231it [02:29,  1.32it/s]Extractor Predicting: 232it [02:30,  1.27it/s]Extractor Predicting: 233it [02:30,  1.39it/s]Extractor Predicting: 234it [02:31,  1.23it/s]Extractor Predicting: 235it [02:32,  1.33it/s]Extractor Predicting: 236it [02:32,  1.38it/s]Extractor Predicting: 237it [02:33,  1.44it/s]Extractor Predicting: 238it [02:34,  1.51it/s]Extractor Predicting: 239it [02:34,  1.47it/s]Extractor Predicting: 240it [02:35,  1.50it/s]Extractor Predicting: 241it [02:36,  1.53it/s]Extractor Predicting: 242it [02:36,  1.55it/s]Extractor Predicting: 243it [02:37,  1.54it/s]Extractor Predicting: 244it [02:38,  1.49it/s]Extractor Predicting: 245it [02:38,  1.57it/s]Extractor Predicting: 246it [02:39,  1.58it/s]Extractor Predicting: 247it [02:39,  1.61it/s]Extractor Predicting: 248it [02:40,  1.60it/s]Extractor Predicting: 249it [02:41,  1.59it/s]Extractor Predicting: 250it [02:41,  1.60it/s]Extractor Predicting: 251it [02:42,  1.57it/s]Extractor Predicting: 252it [02:43,  1.58it/s]Extractor Predicting: 253it [02:43,  1.59it/s]Extractor Predicting: 254it [02:44,  1.46it/s]Extractor Predicting: 255it [02:45,  1.50it/s]Extractor Predicting: 256it [02:45,  1.50it/s]Extractor Predicting: 257it [02:46,  1.55it/s]Extractor Predicting: 258it [02:46,  1.56it/s]Extractor Predicting: 259it [02:47,  1.46it/s]Extractor Predicting: 260it [02:48,  1.50it/s]Extractor Predicting: 261it [02:48,  1.55it/s]Extractor Predicting: 262it [02:49,  1.56it/s]Extractor Predicting: 263it [02:50,  1.55it/s]Extractor Predicting: 264it [02:50,  1.54it/s]Extractor Predicting: 265it [02:51,  1.54it/s]Extractor Predicting: 266it [02:52,  1.54it/s]Extractor Predicting: 267it [02:52,  1.51it/s]Extractor Predicting: 268it [02:53,  1.53it/s]Extractor Predicting: 269it [02:54,  1.48it/s]Extractor Predicting: 270it [02:54,  1.49it/s]Extractor Predicting: 271it [02:55,  1.49it/s]Extractor Predicting: 272it [02:56,  1.52it/s]Extractor Predicting: 273it [02:56,  1.52it/s]Extractor Predicting: 274it [02:57,  1.54it/s]Extractor Predicting: 275it [02:58,  1.59it/s]Extractor Predicting: 276it [02:58,  1.44it/s]Extractor Predicting: 277it [02:59,  1.47it/s]Extractor Predicting: 278it [03:00,  1.50it/s]Extractor Predicting: 279it [03:00,  1.52it/s]Extractor Predicting: 280it [03:01,  1.54it/s]Extractor Predicting: 281it [03:02,  1.47it/s]Extractor Predicting: 282it [03:02,  1.51it/s]Extractor Predicting: 283it [03:03,  1.48it/s]Extractor Predicting: 284it [03:04,  1.48it/s]Extractor Predicting: 285it [03:04,  1.47it/s]Extractor Predicting: 286it [03:05,  1.39it/s]Extractor Predicting: 287it [03:06,  1.44it/s]Extractor Predicting: 288it [03:06,  1.91it/s]Extractor Predicting: 288it [03:06,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:28,970 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:29,043 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:29,043 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:29,043 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:29,043 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:35:30,346 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:35:30,347 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:35:31,069 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:35:32,126 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:35:32,603 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:37,004 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:37,261 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:37,261 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:37,261 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:37,262 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:35:37,987 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:35:37,988 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:35:38,697 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:35:38,872 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:35:38,872 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.30637210151542815,
  "recall": 0.2435767165045725,
  "score": 0.27138929322335437,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:01,  1.94it/s]Extractor Predicting: 3it [00:01,  1.78it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:35:44,413 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:35:44,414 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:35:44,475 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:35:44,476 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:35:44,504 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:36:23,350 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:36:23,407 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:36:24,061 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:36:24,062 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:36:24,622 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:36:25,502 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:36:25,503 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:36:25,503 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:36:25,503 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:36:25,503 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:36:25,503 >> loading file outputs/wrapper/fewrel/unseen_10_seed_4/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3902439024390244,
  "recall": 0.14414414414414414,
  "score": 0.2105263157894737,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:36:26,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:27,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:28,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:28,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:29,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:29,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:30,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:31,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:31,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:32,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:32,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:33,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:34,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:34,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:35,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:35,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:36,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:37,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:37,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:38,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:39,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:39,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:15, 13.94s/it][WARNING|generation_utils.py:914] 2023-08-28 18:36:40,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:41,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:41,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:42,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:42,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:43,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:44,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:44,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:45,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:45,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:46,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:46,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:47,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:48,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:48,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:49,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:50,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:51,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:52,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:52,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:53,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:56, 13.56s/it][WARNING|generation_utils.py:914] 2023-08-28 18:36:53,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:54,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:55,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:55,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:56,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:56,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:57,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:58,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:59,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:59,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:00,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:00,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:01,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:02,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:02,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:03,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:03,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:04,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:05,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:05,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:06,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:07,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:07,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:41<02:48, 14.03s/it][WARNING|generation_utils.py:914] 2023-08-28 18:37:08,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:09,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:09,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:10,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:10,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:11,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:12,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:12,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:13,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:13,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:14,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:15,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:16,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:16,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:17,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:18,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:18,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:19,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:20,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:20,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:21,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:55<02:30, 13.73s/it][WARNING|generation_utils.py:914] 2023-08-28 18:37:21,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:22,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:22,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:23,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:23,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:24,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:24,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:25,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:26,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:26,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:27,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:27,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:28,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:29,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:29,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:30,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:30,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:31,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:31,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:32,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:33,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:34,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:34,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:08<02:16, 13.68s/it][WARNING|generation_utils.py:914] 2023-08-28 18:37:35,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:35,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:36,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:37,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:37,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:38,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:39,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:39,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:40,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:41,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:41,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:42,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:43,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:44,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:44,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:45,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:45,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:46,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:47,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:48,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:49,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:23<02:06, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-28 18:37:50,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:50,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:51,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:52,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:52,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:53,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:54,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:54,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:55,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:56,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:56,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:57,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:58,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:58,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:59,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:00,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:01,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:01,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:02,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:02,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:03,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:37<01:52, 14.04s/it][WARNING|generation_utils.py:914] 2023-08-28 18:38:04,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:04,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:05,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:06,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:06,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:07,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:08,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:09,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:09,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:10,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:10,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:11,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:11,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:12,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:12,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:13,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:14,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:14,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:15,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:16,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:17,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:18,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:18,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:52<01:41, 14.46s/it][WARNING|generation_utils.py:914] 2023-08-28 18:38:19,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:20,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:20,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:21,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:22,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:22,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:23,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:23,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:24,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:24,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:25,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:26,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:26,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:27,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:28,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:28,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:29,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:30,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:30,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:31,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:31,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:05<01:23, 13.98s/it][WARNING|generation_utils.py:914] 2023-08-28 18:38:32,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:33,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:34,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:34,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:35,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:35,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:36,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:37,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:37,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:38,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:38,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:39,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:40,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:40,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:41,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:42,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:42,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:43,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:44,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:44,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:45,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:19<01:09, 13.89s/it][WARNING|generation_utils.py:914] 2023-08-28 18:38:46,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:46,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:47,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:47,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:48,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:48,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:49,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:50,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:50,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:51,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:51,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:52,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:52,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:53,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:53,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:54,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:54,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:55,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:56,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:56,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:30<00:52, 13.10s/it][WARNING|generation_utils.py:914] 2023-08-28 18:38:57,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:58,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:58,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:59,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:59,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:01,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:01,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:02,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:02,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:03,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:04,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:04,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:05,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:05,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:06,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:06,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:07,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:08,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:08,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:09,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:10,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:10,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:44<00:39, 13.32s/it][WARNING|generation_utils.py:914] 2023-08-28 18:39:11,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:11,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:12,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:12,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:13,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:13,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:14,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:15,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:15,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:16,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:17,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:17,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:18,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:18,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:19,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:19,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:20,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:21,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:21,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:22,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:22,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:56<00:26, 13.04s/it][WARNING|generation_utils.py:914] 2023-08-28 18:39:23,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:24,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:24,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:25,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:26,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:26,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:27,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:28,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:28,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:29,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:29,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:31,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:31,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:32,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:32,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:33,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:34,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:34,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:35,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:35,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:37,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:11<00:13, 13.34s/it][WARNING|generation_utils.py:914] 2023-08-28 18:39:37,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:38,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:38,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:39,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:39,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:40,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:41,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:41,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:42,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:43,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:43,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:44,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:44,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:45,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:46,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:46,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:47,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:47,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:48,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:49,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:50,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:50,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:51,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:51,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:52,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:52,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:53,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:54,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:54,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:28<00:00, 14.70s/it]Generating: 100%|██████████| 15/15 [03:28<00:00, 13.93s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:10,213 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:10,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:10,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:10,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:10,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:40:11,213 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:40:11,214 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:40:11,932 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:40:13,654 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:40:13,914 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:17,537 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:17,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:17,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:17,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:17,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:40:19,437 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:40:19,438 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:40:20,271 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:40:20,649 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:40:20,784 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : nominated for . Context : Later in 2008 , he won the Music Award for Best Post - Album , for his song " I Am Legend " . Head Entity : Music Award for Best Post - Album , Tail Entity : John Ritchie .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8464673913043478, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.8165760869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : architect .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9032738095238095, 'errors': {''}}
['Relation : country of origin . Context : Later in 2008 , the country became a part of a new parliamentary committee headed by former President Andrzej Plevcic , led by former Minister of Foreign Affairs Andrzej Plevkovski . Head Entity : parliamentary committee , Tail Entity : Czech Republic .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.8464673913043478, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : follows . Context : Later in the year , the band formed with former member and producer Brian Michael Bendis . Head Entity : Brian Michael Bendis , Tail Entity : The Band .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9300595238095238, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.953125, 'errors': {'', "('Robert Ehrlich', 'located in or next to body of water', '', 'The area is named for local explorer Robert Ehrlich who was the first person to arrive in the area .')"}}
['Relation : member of political party . Context : Later in 2008 , the party became a minority government under former Prime Minister Niki Osoura , who was led by former Prime Minister Asakusa Obiang . Head Entity : Asakusa Obiang , Tail Entity : National Democratic Alliance .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8792613636363636, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : operator .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 104, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 172, 'raw': 256}
{'target': 600, 'success': 191, 'raw': 288}
{'target': 600, 'success': 215, 'raw': 320}
{'target': 600, 'success': 236, 'raw': 352}
{'target': 600, 'success': 255, 'raw': 384}
{'target': 600, 'success': 275, 'raw': 416}
{'target': 600, 'success': 301, 'raw': 448}
{'target': 600, 'success': 321, 'raw': 480}
{'target': 600, 'success': 345, 'raw': 512}
{'target': 600, 'success': 363, 'raw': 544}
{'target': 600, 'success': 386, 'raw': 576}
{'target': 600, 'success': 408, 'raw': 608}
{'target': 600, 'success': 426, 'raw': 640}
{'target': 600, 'success': 445, 'raw': 672}
{'target': 600, 'success': 466, 'raw': 704}
{'target': 600, 'success': 483, 'raw': 736}
{'target': 600, 'success': 502, 'raw': 768}
{'target': 600, 'success': 525, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 568, 'raw': 864}
{'target': 600, 'success': 587, 'raw': 896}
{'target': 600, 'success': 611, 'raw': 928}
{'prompt': 'Relation : position held .', 'success_rate': 0.6584051724137931, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 11671
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11771, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.40it/s]Extractor Estimating: 2it [00:01,  1.52it/s]Extractor Estimating: 3it [00:02,  1.49it/s]Extractor Estimating: 4it [00:02,  1.55it/s]Extractor Estimating: 5it [00:03,  1.48it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.68it/s]Extractor Estimating: 8it [00:05,  1.64it/s]Extractor Estimating: 9it [00:05,  1.67it/s]Extractor Estimating: 10it [00:06,  1.46it/s]Extractor Estimating: 11it [00:07,  1.52it/s]Extractor Estimating: 12it [00:07,  1.58it/s]Extractor Estimating: 13it [00:08,  1.31it/s]Extractor Estimating: 14it [00:09,  1.44it/s]Extractor Estimating: 15it [00:09,  1.51it/s]Extractor Estimating: 16it [00:10,  1.58it/s]Extractor Estimating: 17it [00:11,  1.61it/s]Extractor Estimating: 18it [00:12,  1.24it/s]Extractor Estimating: 19it [00:12,  1.35it/s]Extractor Estimating: 20it [00:13,  1.44it/s]Extractor Estimating: 21it [00:14,  1.45it/s]Extractor Estimating: 22it [00:14,  1.47it/s]Extractor Estimating: 23it [00:15,  1.49it/s]Extractor Estimating: 24it [00:16,  1.53it/s]Extractor Estimating: 25it [00:16,  1.58it/s]Extractor Estimating: 26it [00:17,  1.62it/s]Extractor Estimating: 27it [00:17,  1.67it/s]Extractor Estimating: 28it [00:18,  1.69it/s]Extractor Estimating: 29it [00:18,  1.68it/s]Extractor Estimating: 30it [00:19,  1.73it/s]Extractor Estimating: 31it [00:20,  1.75it/s]Extractor Estimating: 32it [00:20,  1.80it/s]Extractor Estimating: 33it [00:21,  1.54it/s]Extractor Estimating: 34it [00:21,  1.63it/s]Extractor Estimating: 35it [00:22,  1.69it/s]Extractor Estimating: 36it [00:23,  1.73it/s]Extractor Estimating: 37it [00:23,  1.81it/s]Extractor Estimating: 38it [00:24,  1.80it/s]Extractor Estimating: 39it [00:24,  1.69it/s]Extractor Estimating: 40it [00:25,  1.65it/s]Extractor Estimating: 41it [00:26,  1.63it/s]Extractor Estimating: 42it [00:26,  1.67it/s]Extractor Estimating: 43it [00:27,  1.70it/s]Extractor Estimating: 44it [00:27,  1.68it/s]Extractor Estimating: 45it [00:28,  1.69it/s]Extractor Estimating: 46it [00:28,  1.72it/s]Extractor Estimating: 47it [00:29,  1.81it/s]Extractor Estimating: 48it [00:29,  1.82it/s]Extractor Estimating: 49it [00:30,  1.80it/s]Extractor Estimating: 50it [00:31,  1.62it/s]Extractor Estimating: 51it [00:31,  1.56it/s]Extractor Estimating: 52it [00:32,  1.61it/s]Extractor Estimating: 53it [00:33,  1.62it/s]Extractor Estimating: 54it [00:33,  1.60it/s]Extractor Estimating: 55it [00:34,  1.51it/s]Extractor Estimating: 56it [00:35,  1.55it/s]Extractor Estimating: 57it [00:35,  1.57it/s]Extractor Estimating: 58it [00:36,  1.56it/s]Extractor Estimating: 59it [00:37,  1.56it/s]Extractor Estimating: 60it [00:37,  1.41it/s]Extractor Estimating: 61it [00:38,  1.44it/s]Extractor Estimating: 62it [00:39,  1.50it/s]Extractor Estimating: 63it [00:39,  1.59it/s]Extractor Estimating: 64it [00:40,  1.61it/s]Extractor Estimating: 65it [00:40,  1.60it/s]Extractor Estimating: 66it [00:41,  1.64it/s]Extractor Estimating: 67it [00:42,  1.52it/s]Extractor Estimating: 68it [00:42,  1.56it/s]Extractor Estimating: 69it [00:43,  1.59it/s]Extractor Estimating: 70it [00:44,  1.59it/s]Extractor Estimating: 71it [00:44,  1.61it/s]Extractor Estimating: 72it [00:45,  1.42it/s]Extractor Estimating: 73it [00:46,  1.47it/s]Extractor Estimating: 74it [00:46,  1.49it/s]Extractor Estimating: 75it [00:47,  1.49it/s]Extractor Estimating: 76it [00:48,  1.59it/s]Extractor Estimating: 77it [00:48,  1.51it/s]Extractor Estimating: 78it [00:49,  1.62it/s]Extractor Estimating: 79it [00:50,  1.57it/s]Extractor Estimating: 80it [00:50,  1.60it/s]Extractor Estimating: 81it [00:51,  1.71it/s]Extractor Estimating: 82it [00:51,  1.64it/s]Extractor Estimating: 83it [00:52,  1.68it/s]Extractor Estimating: 84it [00:52,  1.79it/s]Extractor Estimating: 85it [00:53,  1.88it/s]Extractor Estimating: 86it [00:53,  1.87it/s]Extractor Estimating: 87it [00:54,  1.88it/s]Extractor Estimating: 88it [00:55,  1.78it/s]Extractor Estimating: 89it [00:55,  1.83it/s]Extractor Estimating: 90it [00:56,  1.82it/s]Extractor Estimating: 91it [00:56,  1.87it/s]Extractor Estimating: 92it [00:57,  1.90it/s]Extractor Estimating: 93it [00:57,  1.88it/s]Extractor Estimating: 94it [00:58,  1.58it/s]Extractor Estimating: 95it [00:59,  1.63it/s]Extractor Estimating: 96it [00:59,  1.72it/s]Extractor Estimating: 97it [01:00,  1.80it/s]Extractor Estimating: 98it [01:00,  1.80it/s]Extractor Estimating: 99it [01:01,  1.86it/s]Extractor Estimating: 100it [01:01,  1.77it/s]Extractor Estimating: 101it [01:02,  1.74it/s]Extractor Estimating: 102it [01:02,  1.78it/s]Extractor Estimating: 103it [01:03,  1.84it/s]Extractor Estimating: 104it [01:03,  1.90it/s]Extractor Estimating: 105it [01:04,  1.91it/s]Extractor Estimating: 106it [01:05,  1.63it/s]Extractor Estimating: 107it [01:05,  1.67it/s]Extractor Estimating: 108it [01:06,  1.78it/s]Extractor Estimating: 109it [01:06,  1.81it/s]Extractor Estimating: 110it [01:07,  1.78it/s]Extractor Estimating: 111it [01:07,  1.80it/s]Extractor Estimating: 112it [01:08,  1.78it/s]Extractor Estimating: 113it [01:09,  1.79it/s]Extractor Estimating: 114it [01:09,  1.79it/s]Extractor Estimating: 115it [01:10,  1.56it/s]Extractor Estimating: 116it [01:11,  1.58it/s]Extractor Estimating: 117it [01:11,  1.64it/s]Extractor Estimating: 118it [01:12,  1.73it/s]Extractor Estimating: 119it [01:12,  1.56it/s]Extractor Estimating: 120it [01:13,  1.63it/s]Extractor Estimating: 121it [01:14,  1.64it/s]Extractor Estimating: 122it [01:14,  1.70it/s]Extractor Estimating: 123it [01:15,  1.73it/s]Extractor Estimating: 124it [01:16,  1.44it/s]Extractor Estimating: 125it [01:16,  1.56it/s]Extractor Estimating: 126it [01:17,  1.58it/s]Extractor Estimating: 127it [01:17,  1.58it/s]Extractor Estimating: 128it [01:18,  1.60it/s]Extractor Estimating: 129it [01:19,  1.33it/s]Extractor Estimating: 130it [01:20,  1.42it/s]Extractor Estimating: 131it [01:20,  1.55it/s]Extractor Estimating: 132it [01:21,  1.54it/s]Extractor Estimating: 133it [01:21,  1.59it/s]Extractor Estimating: 134it [01:22,  1.57it/s]Extractor Estimating: 135it [01:23,  1.66it/s]Extractor Estimating: 136it [01:23,  1.68it/s]Extractor Estimating: 137it [01:24,  1.74it/s]Extractor Estimating: 138it [01:24,  1.73it/s]Extractor Estimating: 139it [01:25,  1.71it/s]Extractor Estimating: 140it [01:25,  1.65it/s]Extractor Estimating: 141it [01:26,  1.62it/s]Extractor Estimating: 142it [01:27,  1.67it/s]Extractor Estimating: 143it [01:27,  1.72it/s]Extractor Estimating: 144it [01:28,  1.74it/s]Extractor Estimating: 145it [01:28,  1.78it/s]Extractor Estimating: 146it [01:29,  1.57it/s]Extractor Estimating: 147it [01:30,  1.57it/s]Extractor Estimating: 148it [01:30,  1.62it/s]Extractor Estimating: 149it [01:31,  1.70it/s]Extractor Estimating: 150it [01:31,  1.73it/s]Extractor Estimating: 151it [01:32,  1.74it/s]Extractor Estimating: 152it [01:33,  1.78it/s]Extractor Estimating: 153it [01:33,  1.72it/s]Extractor Estimating: 154it [01:34,  1.77it/s]Extractor Estimating: 155it [01:34,  1.82it/s]Extractor Estimating: 156it [01:35,  1.82it/s]Extractor Estimating: 157it [01:35,  1.66it/s]Extractor Estimating: 158it [01:36,  1.73it/s]Extractor Estimating: 159it [01:37,  1.75it/s]Extractor Estimating: 160it [01:37,  1.83it/s]Extractor Estimating: 161it [01:38,  1.78it/s]Extractor Estimating: 162it [01:38,  1.83it/s]Extractor Estimating: 163it [01:39,  1.46it/s]Extractor Estimating: 164it [01:40,  1.56it/s]Extractor Estimating: 165it [01:40,  1.63it/s]Extractor Estimating: 166it [01:41,  1.70it/s]Extractor Estimating: 167it [01:41,  1.72it/s]Extractor Estimating: 168it [01:42,  1.68it/s]Extractor Estimating: 169it [01:42,  1.71it/s]Extractor Estimating: 170it [01:43,  1.66it/s]Extractor Estimating: 171it [01:44,  1.66it/s]Extractor Estimating: 172it [01:44,  1.77it/s]Extractor Estimating: 173it [01:45,  1.77it/s]Extractor Estimating: 174it [01:45,  1.74it/s]Extractor Estimating: 175it [01:46,  1.75it/s]Extractor Estimating: 176it [01:47,  1.46it/s]Extractor Estimating: 177it [01:47,  1.53it/s]Extractor Estimating: 178it [01:48,  1.58it/s]Extractor Estimating: 179it [01:49,  1.61it/s]Extractor Estimating: 180it [01:49,  1.68it/s]Extractor Estimating: 181it [01:50,  1.54it/s]Extractor Estimating: 182it [01:51,  1.58it/s]Extractor Estimating: 183it [01:51,  1.62it/s]Extractor Estimating: 184it [01:52,  1.66it/s]Extractor Estimating: 185it [01:52,  1.69it/s]Extractor Estimating: 186it [01:53,  1.66it/s]Extractor Estimating: 187it [01:53,  1.70it/s]Extractor Estimating: 188it [01:54,  1.76it/s]Extractor Estimating: 189it [01:55,  1.74it/s]Extractor Estimating: 190it [01:55,  1.80it/s]Extractor Estimating: 191it [01:56,  1.88it/s]Extractor Estimating: 192it [01:56,  1.78it/s]Extractor Estimating: 193it [01:57,  1.70it/s]Extractor Estimating: 194it [01:57,  1.66it/s]Extractor Estimating: 195it [01:58,  1.65it/s]Extractor Estimating: 196it [01:59,  1.67it/s]Extractor Estimating: 197it [02:00,  1.19it/s]Extractor Estimating: 198it [02:01,  1.31it/s]Extractor Estimating: 199it [02:01,  1.43it/s]Extractor Estimating: 200it [02:02,  1.51it/s]Extractor Estimating: 201it [02:03,  1.46it/s]Extractor Estimating: 202it [02:04,  1.16it/s]Extractor Estimating: 203it [02:04,  1.25it/s]Extractor Estimating: 204it [02:05,  1.35it/s]Extractor Estimating: 205it [02:06,  1.34it/s]Extractor Estimating: 206it [02:06,  1.39it/s]Extractor Estimating: 207it [02:07,  1.44it/s]Extractor Estimating: 208it [02:08,  1.54it/s]Extractor Estimating: 209it [02:08,  1.56it/s]Extractor Estimating: 210it [02:09,  1.48it/s]Extractor Estimating: 211it [02:10,  1.57it/s]Extractor Estimating: 212it [02:11,  1.00s/it]Extractor Estimating: 213it [02:12,  1.13it/s]Extractor Estimating: 214it [02:13,  1.22it/s]Extractor Estimating: 215it [02:13,  1.30it/s]Extractor Estimating: 216it [02:14,  1.31it/s]Extractor Estimating: 217it [02:15,  1.42it/s]Extractor Estimating: 218it [02:15,  1.48it/s]Extractor Estimating: 219it [02:16,  1.51it/s]Extractor Estimating: 220it [02:16,  1.57it/s]Extractor Estimating: 221it [02:17,  1.52it/s]Extractor Estimating: 222it [02:18,  1.56it/s]Extractor Estimating: 223it [02:18,  1.57it/s]Extractor Estimating: 224it [02:19,  1.58it/s]Extractor Estimating: 225it [02:20,  1.59it/s]Extractor Estimating: 226it [02:20,  1.52it/s]Extractor Estimating: 227it [02:21,  1.50it/s]Extractor Estimating: 228it [02:22,  1.56it/s]Extractor Estimating: 229it [02:22,  1.59it/s]Extractor Estimating: 230it [02:23,  1.57it/s]Extractor Estimating: 231it [02:24,  1.41it/s]Extractor Estimating: 232it [02:24,  1.46it/s]Extractor Estimating: 233it [02:25,  1.44it/s]Extractor Estimating: 234it [02:26,  1.48it/s]Extractor Estimating: 235it [02:26,  1.51it/s]Extractor Estimating: 236it [02:27,  1.30it/s]Extractor Estimating: 237it [02:28,  1.35it/s]Extractor Estimating: 238it [02:29,  1.37it/s]Extractor Estimating: 239it [02:29,  1.43it/s]Extractor Estimating: 240it [02:30,  1.39it/s]Extractor Estimating: 241it [02:31,  1.41it/s]Extractor Estimating: 242it [02:31,  1.47it/s]Extractor Estimating: 243it [02:32,  1.51it/s]Extractor Estimating: 244it [02:33,  1.51it/s]Extractor Estimating: 245it [02:34,  1.40it/s]Extractor Estimating: 246it [02:34,  1.48it/s]Extractor Estimating: 247it [02:35,  1.51it/s]Extractor Estimating: 248it [02:35,  1.51it/s]Extractor Estimating: 249it [02:36,  1.35it/s]Extractor Estimating: 250it [02:37,  1.23it/s]Extractor Estimating: 251it [02:38,  1.42it/s]Extractor Estimating: 252it [02:38,  1.57it/s]Extractor Estimating: 253it [02:39,  1.66it/s]Extractor Estimating: 254it [02:39,  1.74it/s]Extractor Estimating: 255it [02:40,  1.86it/s]Extractor Estimating: 256it [02:40,  1.85it/s]Extractor Estimating: 257it [02:41,  1.95it/s]Extractor Estimating: 258it [02:41,  1.80it/s]Extractor Estimating: 259it [02:42,  1.87it/s]Extractor Estimating: 260it [02:42,  1.92it/s]Extractor Estimating: 261it [02:43,  1.97it/s]Extractor Estimating: 262it [02:43,  2.09it/s]Extractor Estimating: 263it [02:44,  2.06it/s]Extractor Estimating: 264it [02:44,  1.95it/s]Extractor Estimating: 265it [02:45,  2.02it/s]Extractor Estimating: 266it [02:45,  2.06it/s]Extractor Estimating: 267it [02:46,  2.11it/s]Extractor Estimating: 268it [02:46,  2.15it/s]Extractor Estimating: 269it [02:47,  2.08it/s]Extractor Estimating: 270it [02:47,  2.04it/s]Extractor Estimating: 271it [02:48,  2.01it/s]Extractor Estimating: 272it [02:48,  2.09it/s]Extractor Estimating: 273it [02:49,  2.13it/s]Extractor Estimating: 274it [02:49,  2.04it/s]Extractor Estimating: 275it [02:50,  2.06it/s]Extractor Estimating: 276it [02:50,  1.98it/s]Extractor Estimating: 277it [02:51,  1.93it/s]Extractor Estimating: 278it [02:52,  1.49it/s]Extractor Estimating: 279it [02:52,  1.54it/s]Extractor Estimating: 280it [02:53,  1.52it/s]Extractor Estimating: 281it [02:54,  1.57it/s]Extractor Estimating: 282it [02:54,  1.61it/s]Extractor Estimating: 283it [02:55,  1.26it/s]Extractor Estimating: 284it [02:56,  1.36it/s]Extractor Estimating: 285it [02:57,  1.44it/s]Extractor Estimating: 286it [02:57,  1.48it/s]Extractor Estimating: 287it [02:58,  1.43it/s]Extractor Estimating: 288it [02:59,  1.49it/s]Extractor Estimating: 289it [02:59,  1.58it/s]Extractor Estimating: 290it [03:00,  1.62it/s]Extractor Estimating: 291it [03:00,  1.66it/s]Extractor Estimating: 292it [03:01,  1.61it/s]Extractor Estimating: 293it [03:02,  1.65it/s]Extractor Estimating: 294it [03:02,  1.73it/s]Extractor Estimating: 295it [03:03,  1.69it/s]Extractor Estimating: 296it [03:03,  1.67it/s]Extractor Estimating: 297it [03:04,  1.65it/s]Extractor Estimating: 298it [03:05,  1.27it/s]Extractor Estimating: 299it [03:06,  1.39it/s]Extractor Estimating: 300it [03:06,  1.48it/s]Extractor Estimating: 301it [03:07,  1.52it/s]Extractor Estimating: 302it [03:07,  1.54it/s]Extractor Estimating: 303it [03:08,  1.46it/s]Extractor Estimating: 304it [03:09,  1.49it/s]Extractor Estimating: 305it [03:09,  1.54it/s]Extractor Estimating: 306it [03:10,  1.57it/s]Extractor Estimating: 307it [03:11,  1.61it/s]Extractor Estimating: 308it [03:11,  1.47it/s]Extractor Estimating: 309it [03:13,  1.20it/s]Extractor Estimating: 310it [03:13,  1.32it/s]Extractor Estimating: 311it [03:14,  1.43it/s]Extractor Estimating: 312it [03:15,  1.44it/s]Extractor Estimating: 313it [03:15,  1.46it/s]Extractor Estimating: 314it [03:16,  1.56it/s]Extractor Estimating: 315it [03:16,  1.61it/s]Extractor Estimating: 316it [03:17,  1.69it/s]Extractor Estimating: 317it [03:17,  1.69it/s]Extractor Estimating: 318it [03:18,  1.69it/s]Extractor Estimating: 319it [03:19,  1.58it/s]Extractor Estimating: 320it [03:19,  1.66it/s]Extractor Estimating: 321it [03:20,  1.66it/s]Extractor Estimating: 322it [03:20,  1.65it/s]Extractor Estimating: 323it [03:21,  1.62it/s]Extractor Estimating: 324it [03:22,  1.43it/s]Extractor Estimating: 325it [03:23,  1.44it/s]Extractor Estimating: 326it [03:23,  1.50it/s]Extractor Estimating: 327it [03:24,  1.52it/s]Extractor Estimating: 328it [03:25,  1.59it/s]Extractor Estimating: 329it [03:25,  1.37it/s]Extractor Estimating: 330it [03:26,  1.41it/s]Extractor Estimating: 331it [03:27,  1.48it/s]Extractor Estimating: 332it [03:28,  1.41it/s]Extractor Estimating: 333it [03:28,  1.38it/s]Extractor Estimating: 334it [03:29,  1.47it/s]Extractor Estimating: 335it [03:29,  1.51it/s]Extractor Estimating: 336it [03:30,  1.55it/s]Extractor Estimating: 337it [03:31,  1.55it/s]Extractor Estimating: 338it [03:32,  1.43it/s]Extractor Estimating: 339it [03:32,  1.43it/s]Extractor Estimating: 340it [03:33,  1.53it/s]Extractor Estimating: 341it [03:33,  1.55it/s]Extractor Estimating: 342it [03:34,  1.57it/s]Extractor Estimating: 343it [03:35,  1.35it/s]Extractor Estimating: 344it [03:36,  1.41it/s]Extractor Estimating: 345it [03:36,  1.49it/s]Extractor Estimating: 346it [03:37,  1.55it/s]Extractor Estimating: 347it [03:37,  1.58it/s]Extractor Estimating: 348it [03:38,  1.52it/s]Extractor Estimating: 349it [03:39,  1.59it/s]Extractor Estimating: 350it [03:39,  1.61it/s]Extractor Estimating: 351it [03:40,  1.58it/s]Extractor Estimating: 352it [03:41,  1.60it/s]Extractor Estimating: 353it [03:42,  1.31it/s]Extractor Estimating: 354it [03:42,  1.44it/s]Extractor Estimating: 355it [03:43,  1.54it/s]Extractor Estimating: 356it [03:43,  1.60it/s]Extractor Estimating: 357it [03:44,  1.65it/s]Extractor Estimating: 358it [03:44,  1.66it/s]Extractor Estimating: 359it [03:45,  1.68it/s]Extractor Estimating: 360it [03:46,  1.71it/s]Extractor Estimating: 361it [03:46,  1.65it/s]Extractor Estimating: 362it [03:47,  1.72it/s]Extractor Estimating: 363it [03:47,  1.73it/s]Extractor Estimating: 364it [03:48,  1.77it/s]Extractor Estimating: 365it [03:49,  1.71it/s]Extractor Estimating: 366it [03:49,  1.71it/s]Extractor Estimating: 367it [03:50,  1.68it/s]Extractor Estimating: 368it [03:50,  1.73it/s]Extractor Estimating: 369it [03:51,  1.74it/s]Extractor Estimating: 370it [03:51,  1.73it/s]Extractor Estimating: 371it [03:52,  1.75it/s]Extractor Estimating: 372it [03:53,  1.74it/s]Extractor Estimating: 373it [03:53,  1.54it/s]Extractor Estimating: 374it [03:54,  1.60it/s]Extractor Estimating: 375it [03:55,  1.62it/s]Extractor Estimating: 375it [03:55,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:57,229 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:57,293 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:57,293 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:57,293 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:57,293 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:44:58,347 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:44:58,348 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:44:59,349 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:45:00,821 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:45:00,946 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:04,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:04,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:04,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:04,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:04,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:45:05,616 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:45:05,618 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:45:06,748 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:45:07,228 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:45:07,228 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 19:35:20,176 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 19:35:21,437 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 3011 mean pseudo reward: 0.991103445276181
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 15241
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15341, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15341, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.954, loss:298.3579
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 74, avg_time 0.932, loss:233.1089
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 48, avg_time 0.940, loss:210.0204
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 22, avg_time 0.939, loss:209.5378
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 122, avg_time 0.946, loss:188.9793
>> valid entity prec:0.5459, rec:0.5117, f1:0.5282
>> valid relation prec:0.1738, rec:0.1114, f1:0.1358
>> valid relation with NER prec:0.1738, rec:0.1114, f1:0.1358
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 96, avg_time 2.157, loss:159.3076
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 70, avg_time 0.942, loss:155.3852
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 44, avg_time 0.943, loss:152.7275
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 18, avg_time 0.941, loss:157.3691
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 118, avg_time 0.956, loss:154.4807
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5047, rec:0.5330, f1:0.5184
>> valid relation prec:0.1684, rec:0.0882, f1:0.1157
>> valid relation with NER prec:0.1684, rec:0.0882, f1:0.1157
g_step 1100, step 92, avg_time 2.172, loss:147.4703
g_step 1200, step 66, avg_time 0.942, loss:140.4511
g_step 1300, step 40, avg_time 0.965, loss:126.4501
g_step 1400, step 14, avg_time 0.953, loss:119.5815
g_step 1500, step 114, avg_time 0.965, loss:128.5914
>> valid entity prec:0.5206, rec:0.5007, f1:0.5104
>> valid relation prec:0.1531, rec:0.1068, f1:0.1258
>> valid relation with NER prec:0.1531, rec:0.1068, f1:0.1258
g_step 1600, step 88, avg_time 2.142, loss:113.1422
g_step 1700, step 62, avg_time 0.950, loss:117.9946
g_step 1800, step 36, avg_time 0.953, loss:126.1140
g_step 1900, step 10, avg_time 0.954, loss:97.5022
g_step 2000, step 110, avg_time 0.964, loss:98.9981
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4908, rec:0.4963, f1:0.4936
>> valid relation prec:0.1451, rec:0.0993, f1:0.1179
>> valid relation with NER prec:0.1451, rec:0.0993, f1:0.1179
g_step 2100, step 84, avg_time 2.164, loss:93.5322
g_step 2200, step 58, avg_time 0.957, loss:89.1160
g_step 2300, step 32, avg_time 0.952, loss:96.2798
g_step 2400, step 6, avg_time 0.955, loss:80.5800
g_step 2500, step 106, avg_time 0.957, loss:74.4227
>> valid entity prec:0.5227, rec:0.5046, f1:0.5135
>> valid relation prec:0.1566, rec:0.1054, f1:0.1260
>> valid relation with NER prec:0.1566, rec:0.1054, f1:0.1260
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:35:21 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:35:21 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-35-20_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:35:24 - WARNING - datasets.builder -   Using custom data configuration default-97db85f1af07d1c7
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-97db85f1af07d1c7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:35:31,498 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:35:31,814 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:35:31,815 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:35:31,816 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:35:32,128 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:32,326 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:32,327 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:32,327 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:32,327 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:32,327 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:32,327 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:35:33,426 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:35:36,670 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:35:36,670 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-97db85f1af07d1c7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.04ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.11ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.75ba/s]100%|██████████| 4/4 [00:00<00:00,  4.43ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.25ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.17ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.61ba/s]100%|██████████| 4/4 [00:01<00:00,  4.67ba/s]100%|██████████| 4/4 [00:01<00:00,  3.93ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.64ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.04ba/s]100%|██████████| 4/4 [00:00<00:00,  9.91ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.64ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.44ba/s]100%|██████████| 4/4 [00:00<00:00,  4.77ba/s]
[INFO|trainer.py:414] 2023-08-28 19:35:44,373 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:35:45,416 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:35:45,416 >>   Num examples = 3011
[INFO|trainer.py:1149] 2023-08-28 19:35:45,416 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:35:45,416 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:35:45,416 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:35:45,416 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:35:45,416 >>   Total optimization steps = 235
  0%|          | 0/235 [00:00<?, ?it/s]  0%|          | 1/235 [00:00<01:11,  3.28it/s]  1%|          | 2/235 [00:00<01:09,  3.36it/s]  1%|▏         | 3/235 [00:00<01:08,  3.39it/s]  2%|▏         | 4/235 [00:01<01:08,  3.40it/s]  2%|▏         | 5/235 [00:01<01:07,  3.41it/s]  3%|▎         | 6/235 [00:01<01:07,  3.41it/s]  3%|▎         | 7/235 [00:02<01:06,  3.42it/s]  3%|▎         | 8/235 [00:02<01:06,  3.42it/s]  4%|▍         | 9/235 [00:02<01:06,  3.42it/s]  4%|▍         | 10/235 [00:02<01:05,  3.42it/s]  5%|▍         | 11/235 [00:03<01:05,  3.42it/s]  5%|▌         | 12/235 [00:03<01:05,  3.42it/s]  6%|▌         | 13/235 [00:03<01:04,  3.42it/s]  6%|▌         | 14/235 [00:04<01:04,  3.41it/s]  6%|▋         | 15/235 [00:04<01:04,  3.42it/s]  7%|▋         | 16/235 [00:04<01:04,  3.42it/s]  7%|▋         | 17/235 [00:04<01:04,  3.41it/s]  8%|▊         | 18/235 [00:05<01:03,  3.41it/s]  8%|▊         | 19/235 [00:05<01:08,  3.13it/s]  9%|▊         | 20/235 [00:05<01:06,  3.22it/s]  9%|▉         | 21/235 [00:06<01:05,  3.28it/s]  9%|▉         | 22/235 [00:06<01:04,  3.31it/s] 10%|▉         | 23/235 [00:06<01:03,  3.34it/s] 10%|█         | 24/235 [00:07<01:02,  3.36it/s] 11%|█         | 25/235 [00:07<01:02,  3.37it/s] 11%|█         | 26/235 [00:07<01:01,  3.39it/s] 11%|█▏        | 27/235 [00:08<01:01,  3.40it/s] 12%|█▏        | 28/235 [00:08<01:00,  3.40it/s] 12%|█▏        | 29/235 [00:08<01:00,  3.40it/s] 13%|█▎        | 30/235 [00:08<01:00,  3.40it/s] 13%|█▎        | 31/235 [00:09<01:00,  3.40it/s] 14%|█▎        | 32/235 [00:09<00:59,  3.40it/s] 14%|█▍        | 33/235 [00:09<00:59,  3.40it/s] 14%|█▍        | 34/235 [00:10<00:59,  3.41it/s] 15%|█▍        | 35/235 [00:10<00:58,  3.41it/s] 15%|█▌        | 36/235 [00:10<00:58,  3.41it/s] 16%|█▌        | 37/235 [00:10<00:58,  3.40it/s] 16%|█▌        | 38/235 [00:11<00:57,  3.41it/s] 17%|█▋        | 39/235 [00:11<00:57,  3.40it/s] 17%|█▋        | 40/235 [00:11<00:57,  3.41it/s] 17%|█▋        | 41/235 [00:12<00:56,  3.40it/s] 18%|█▊        | 42/235 [00:12<00:56,  3.40it/s] 18%|█▊        | 43/235 [00:12<00:56,  3.40it/s] 19%|█▊        | 44/235 [00:12<00:56,  3.40it/s] 19%|█▉        | 45/235 [00:13<00:55,  3.40it/s] 20%|█▉        | 46/235 [00:13<00:55,  3.40it/s] 20%|██        | 47/235 [00:13<00:55,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 19:35:59,336 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:35:59,336 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:35:59,336 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.21it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.74it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.13it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.92it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.23it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.82it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.48it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.32it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.21it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.39it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.43it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.53it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.20it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.16it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.16it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.16it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.04it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.06it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.22it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.34it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.56it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.41it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.33it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.27it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.14it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.11it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.18it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.12it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.38it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.43it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.39it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.38it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.27it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.13it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.02it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.13it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.19it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.38it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.44it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.44it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.39it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.26it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.07it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.01it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.15it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.22it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.21it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.22it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.39it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.29it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.95it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.92it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.05it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.11it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.31it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.32it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.45it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.43it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.36it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.15it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.11it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.13it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.24it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.25it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.17it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.40it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.43it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.29it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.12it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.99it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.01it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.13it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.17it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.11it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.37it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.45it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.18it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.08it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.04it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.04it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.12it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.22it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.23it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.44it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.42it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.26it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.08it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 44.08it/s][A 20%|██        | 47/235 [00:23<00:55,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:36:09,487 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-47
[INFO|configuration_utils.py:351] 2023-08-28 19:36:09,858 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-47/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:36:19,501 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-47/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:36:21,210 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-47/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:36:21,497 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-47/special_tokens_map.json
 20%|██        | 48/235 [00:53<38:04, 12.21s/it] 21%|██        | 49/235 [00:54<26:53,  8.68s/it] 21%|██▏       | 50/235 [00:54<18:59,  6.16s/it] 22%|██▏       | 51/235 [00:54<13:29,  4.40s/it] 22%|██▏       | 52/235 [00:55<09:39,  3.17s/it] 23%|██▎       | 53/235 [00:55<06:59,  2.30s/it] 23%|██▎       | 54/235 [00:55<05:07,  1.70s/it] 23%|██▎       | 55/235 [00:56<03:49,  1.28s/it] 24%|██▍       | 56/235 [00:56<02:55,  1.02it/s] 24%|██▍       | 57/235 [00:56<02:17,  1.29it/s] 25%|██▍       | 58/235 [00:56<01:51,  1.59it/s] 25%|██▌       | 59/235 [00:57<01:39,  1.76it/s] 26%|██▌       | 60/235 [00:57<01:24,  2.06it/s] 26%|██▌       | 61/235 [00:57<01:14,  2.35it/s] 26%|██▋       | 62/235 [00:58<01:06,  2.59it/s] 27%|██▋       | 63/235 [00:58<01:01,  2.81it/s] 27%|██▋       | 64/235 [00:58<00:57,  2.98it/s] 28%|██▊       | 65/235 [00:59<00:54,  3.11it/s] 28%|██▊       | 66/235 [00:59<00:52,  3.20it/s] 29%|██▊       | 67/235 [00:59<00:51,  3.28it/s] 29%|██▉       | 68/235 [00:59<00:50,  3.33it/s] 29%|██▉       | 69/235 [01:00<00:53,  3.12it/s] 30%|██▉       | 70/235 [01:00<00:51,  3.21it/s] 30%|███       | 71/235 [01:00<00:50,  3.28it/s] 31%|███       | 72/235 [01:01<00:49,  3.32it/s] 31%|███       | 73/235 [01:01<00:48,  3.36it/s] 31%|███▏      | 74/235 [01:01<00:47,  3.39it/s] 32%|███▏      | 75/235 [01:02<00:46,  3.41it/s] 32%|███▏      | 76/235 [01:02<00:46,  3.42it/s] 33%|███▎      | 77/235 [01:02<00:46,  3.43it/s] 33%|███▎      | 78/235 [01:02<00:45,  3.43it/s] 34%|███▎      | 79/235 [01:03<00:45,  3.44it/s] 34%|███▍      | 80/235 [01:03<00:47,  3.25it/s] 34%|███▍      | 81/235 [01:03<00:46,  3.31it/s] 35%|███▍      | 82/235 [01:04<00:45,  3.35it/s] 35%|███▌      | 83/235 [01:04<00:44,  3.38it/s] 36%|███▌      | 84/235 [01:04<00:44,  3.41it/s] 36%|███▌      | 85/235 [01:05<00:43,  3.42it/s] 37%|███▋      | 86/235 [01:05<00:43,  3.42it/s] 37%|███▋      | 87/235 [01:05<00:43,  3.43it/s] 37%|███▋      | 88/235 [01:05<00:42,  3.44it/s] 38%|███▊      | 89/235 [01:06<00:42,  3.44it/s] 38%|███▊      | 90/235 [01:06<00:42,  3.44it/s] 39%|███▊      | 91/235 [01:06<00:46,  3.08it/s] 39%|███▉      | 92/235 [01:07<00:46,  3.05it/s] 40%|███▉      | 93/235 [01:07<00:44,  3.16it/s] 40%|████      | 94/235 [01:07<00:43,  3.24it/s][INFO|trainer.py:2140] 2023-08-28 19:36:53,254 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:36:53,255 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:36:53,255 >>   Batch size = 8
{'eval_loss': 1.0386451482772827, 'eval_runtime': 9.873, 'eval_samples_per_second': 353.793, 'eval_steps_per_second': 44.262, 'epoch': 0.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.91it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.95it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.90it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.96it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.36it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.95it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.64it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.31it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.26it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.44it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.70it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.55it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.43it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.18it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.04it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.98it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.80it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.93it/s][A
 22%|██▏       | 97/437 [00:02<00:08, 41.71it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 42.56it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.27it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.75it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.86it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.81it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.00it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.95it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.77it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.96it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.28it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.47it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.54it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.64it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.45it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.23it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.02it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.90it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.03it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.27it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.53it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.58it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.54it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.47it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.31it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.04it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.98it/s][A
 53%|█████▎    | 232/437 [00:05<00:05, 36.00it/s][A
 54%|█████▍    | 237/437 [00:05<00:05, 38.30it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 40.05it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 41.36it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 42.36it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.05it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 43.62it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.67it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.32it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.19it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.62it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.95it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.29it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.52it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.63it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.54it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.11it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.96it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.86it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.85it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.17it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.19it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.43it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.68it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.34it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.30it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 38.24it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 39.90it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 41.21it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 42.10it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 42.98it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.48it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.96it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.05it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.53it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.57it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.79it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.05it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.26it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.39it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.49it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.45it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:10<00:00, 44.45it/s][A 40%|████      | 94/235 [01:17<00:43,  3.24it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:37:03,679 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 19:37:04,121 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:37:13,203 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:37:13,431 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:37:13,573 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-94/special_tokens_map.json
 40%|████      | 95/235 [01:52<31:35, 13.54s/it] 41%|████      | 96/235 [01:52<22:16,  9.61s/it] 41%|████▏     | 97/235 [01:52<15:40,  6.82s/it] 42%|████▏     | 98/235 [01:53<11:05,  4.86s/it] 42%|████▏     | 99/235 [01:53<07:54,  3.49s/it] 43%|████▎     | 100/235 [01:53<05:41,  2.53s/it] 43%|████▎     | 101/235 [01:54<04:08,  1.86s/it] 43%|████▎     | 102/235 [01:54<03:04,  1.39s/it] 44%|████▍     | 103/235 [01:54<02:19,  1.06s/it] 44%|████▍     | 104/235 [01:54<01:48,  1.21it/s] 45%|████▍     | 105/235 [01:55<01:26,  1.50it/s] 45%|████▌     | 106/235 [01:55<01:12,  1.77it/s] 46%|████▌     | 107/235 [01:55<01:01,  2.07it/s] 46%|████▌     | 108/235 [01:56<00:53,  2.36it/s] 46%|████▋     | 109/235 [01:56<00:48,  2.61it/s] 47%|████▋     | 110/235 [01:56<00:44,  2.81it/s] 47%|████▋     | 111/235 [01:57<00:41,  2.98it/s] 48%|████▊     | 112/235 [01:57<00:39,  3.10it/s] 48%|████▊     | 113/235 [01:57<00:38,  3.20it/s] 49%|████▊     | 114/235 [01:57<00:36,  3.27it/s] 49%|████▉     | 115/235 [01:58<00:36,  3.33it/s] 49%|████▉     | 116/235 [01:58<00:35,  3.36it/s] 50%|████▉     | 117/235 [01:59<00:42,  2.76it/s] 50%|█████     | 118/235 [01:59<00:39,  2.94it/s] 51%|█████     | 119/235 [01:59<00:37,  3.07it/s] 51%|█████     | 120/235 [01:59<00:36,  3.18it/s] 51%|█████▏    | 121/235 [02:00<00:35,  3.25it/s] 52%|█████▏    | 122/235 [02:00<00:34,  3.32it/s] 52%|█████▏    | 123/235 [02:00<00:33,  3.36it/s] 53%|█████▎    | 124/235 [02:01<00:32,  3.39it/s] 53%|█████▎    | 125/235 [02:01<00:32,  3.41it/s] 54%|█████▎    | 126/235 [02:01<00:31,  3.43it/s] 54%|█████▍    | 127/235 [02:02<00:39,  2.73it/s] 54%|█████▍    | 128/235 [02:02<00:36,  2.91it/s] 55%|█████▍    | 129/235 [02:02<00:34,  3.06it/s] 55%|█████▌    | 130/235 [02:03<00:33,  3.17it/s] 56%|█████▌    | 131/235 [02:03<00:32,  3.25it/s] 56%|█████▌    | 132/235 [02:03<00:31,  3.31it/s] 57%|█████▋    | 133/235 [02:03<00:30,  3.36it/s] 57%|█████▋    | 134/235 [02:04<00:29,  3.39it/s] 57%|█████▋    | 135/235 [02:04<00:29,  3.41it/s] 58%|█████▊    | 136/235 [02:04<00:28,  3.43it/s] 58%|█████▊    | 137/235 [02:05<00:31,  3.08it/s] 59%|█████▊    | 138/235 [02:05<00:30,  3.19it/s] 59%|█████▉    | 139/235 [02:05<00:29,  3.27it/s] 60%|█████▉    | 140/235 [02:06<00:28,  3.32it/s] 60%|██████    | 141/235 [02:06<00:27,  3.36it/s][INFO|trainer.py:2140] 2023-08-28 19:37:51,758 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:37:51,758 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:37:51,758 >>   Batch size = 8
{'eval_loss': 1.0468918085098267, 'eval_runtime': 10.0206, 'eval_samples_per_second': 348.583, 'eval_steps_per_second': 43.61, 'epoch': 1.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.77it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.64it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.73it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.60it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.12it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.74it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.54it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.13it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.21it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.26it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.50it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.42it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.30it/s][A
 16%|█▋        | 72/437 [00:01<00:14, 25.40it/s][A
 18%|█▊        | 77/437 [00:02<00:12, 29.21it/s][A
 19%|█▊        | 81/437 [00:02<00:28, 12.31it/s][A
 20%|█▉        | 86/437 [00:02<00:22, 15.92it/s][A
 21%|██        | 91/437 [00:03<00:17, 19.90it/s][A
 22%|██▏       | 96/437 [00:03<00:14, 23.96it/s][A
 23%|██▎       | 101/437 [00:03<00:12, 27.91it/s][A
 24%|██▍       | 106/437 [00:03<00:10, 31.49it/s][A
 25%|██▌       | 111/437 [00:03<00:09, 34.56it/s][A
 27%|██▋       | 116/437 [00:03<00:08, 37.02it/s][A
 28%|██▊       | 121/437 [00:03<00:08, 38.70it/s][A
 29%|██▉       | 126/437 [00:03<00:07, 39.89it/s][A
 30%|██▉       | 131/437 [00:03<00:07, 41.07it/s][A
 31%|███       | 136/437 [00:04<00:07, 42.03it/s][A
 32%|███▏      | 141/437 [00:04<00:06, 42.87it/s][A
 33%|███▎      | 146/437 [00:04<00:06, 43.39it/s][A
 35%|███▍      | 151/437 [00:04<00:06, 43.81it/s][A
 36%|███▌      | 156/437 [00:04<00:06, 43.99it/s][A
 37%|███▋      | 161/437 [00:04<00:06, 43.96it/s][A
 38%|███▊      | 166/437 [00:04<00:06, 43.73it/s][A
 39%|███▉      | 171/437 [00:04<00:06, 43.67it/s][A
 40%|████      | 176/437 [00:05<00:05, 43.74it/s][A
 41%|████▏     | 181/437 [00:05<00:10, 23.81it/s][A
 43%|████▎     | 186/437 [00:05<00:09, 27.70it/s][A
 44%|████▎     | 191/437 [00:05<00:07, 31.28it/s][A
 45%|████▍     | 196/437 [00:05<00:07, 34.40it/s][A
 46%|████▌     | 201/437 [00:05<00:06, 36.99it/s][A
 47%|████▋     | 206/437 [00:06<00:05, 39.00it/s][A
 48%|████▊     | 211/437 [00:06<00:05, 40.60it/s][A
 49%|████▉     | 216/437 [00:06<00:05, 41.61it/s][A
 51%|█████     | 221/437 [00:06<00:05, 41.99it/s][A
 52%|█████▏    | 226/437 [00:06<00:04, 42.44it/s][A
 53%|█████▎    | 231/437 [00:06<00:04, 42.93it/s][A
 54%|█████▍    | 236/437 [00:06<00:04, 43.52it/s][A
 55%|█████▌    | 241/437 [00:06<00:04, 43.89it/s][A
 56%|█████▋    | 246/437 [00:06<00:04, 44.14it/s][A
 57%|█████▋    | 251/437 [00:07<00:04, 44.31it/s][A
 59%|█████▊    | 256/437 [00:07<00:04, 44.39it/s][A
 60%|█████▉    | 261/437 [00:07<00:03, 44.21it/s][A
 61%|██████    | 266/437 [00:07<00:03, 43.96it/s][A
 62%|██████▏   | 271/437 [00:07<00:03, 43.90it/s][A
 63%|██████▎   | 276/437 [00:07<00:03, 43.95it/s][A
 64%|██████▍   | 281/437 [00:07<00:03, 44.29it/s][A
 65%|██████▌   | 286/437 [00:07<00:03, 44.41it/s][A
 67%|██████▋   | 291/437 [00:07<00:03, 44.27it/s][A
 68%|██████▊   | 296/437 [00:08<00:03, 44.43it/s][A
 69%|██████▉   | 301/437 [00:08<00:03, 39.30it/s][A
 70%|███████   | 306/437 [00:08<00:03, 40.87it/s][A
 71%|███████   | 311/437 [00:08<00:03, 41.82it/s][A
 72%|███████▏  | 316/437 [00:08<00:02, 42.52it/s][A
 73%|███████▎  | 321/437 [00:08<00:02, 43.11it/s][A
 75%|███████▍  | 326/437 [00:08<00:02, 43.44it/s][A
 76%|███████▌  | 331/437 [00:08<00:02, 43.99it/s][A
 77%|███████▋  | 336/437 [00:08<00:02, 44.14it/s][A
 78%|███████▊  | 341/437 [00:09<00:02, 43.73it/s][A
 79%|███████▉  | 346/437 [00:09<00:02, 43.81it/s][A
 80%|████████  | 351/437 [00:09<00:01, 44.05it/s][A
 81%|████████▏ | 356/437 [00:09<00:01, 44.14it/s][A
 83%|████████▎ | 361/437 [00:09<00:01, 44.28it/s][A
 84%|████████▍ | 366/437 [00:09<00:01, 44.31it/s][A
 85%|████████▍ | 371/437 [00:09<00:01, 44.16it/s][A
 86%|████████▌ | 376/437 [00:09<00:01, 44.22it/s][A
 87%|████████▋ | 381/437 [00:10<00:01, 44.20it/s][A
 88%|████████▊ | 386/437 [00:10<00:01, 44.08it/s][A
 89%|████████▉ | 391/437 [00:10<00:01, 44.09it/s][A
 91%|█████████ | 396/437 [00:10<00:00, 44.27it/s][A
 92%|█████████▏| 401/437 [00:10<00:00, 44.23it/s][A
 93%|█████████▎| 406/437 [00:10<00:00, 44.42it/s][A
 94%|█████████▍| 411/437 [00:10<00:00, 44.41it/s][A
 95%|█████████▌| 416/437 [00:10<00:00, 44.43it/s][A
 96%|█████████▋| 421/437 [00:10<00:00, 44.30it/s][A
 97%|█████████▋| 426/437 [00:11<00:00, 44.23it/s][A
 99%|█████████▊| 431/437 [00:11<00:00, 44.16it/s][A
100%|█████████▉| 436/437 [00:11<00:00, 29.56it/s][A                                                 
                                                 [A 60%|██████    | 141/235 [02:17<00:27,  3.36it/s]
100%|██████████| 437/437 [00:11<00:00, 29.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:38:03,542 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-141
[INFO|configuration_utils.py:351] 2023-08-28 19:38:03,890 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-141/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:38:17,671 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-141/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:38:18,012 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-141/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:38:18,356 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-141/special_tokens_map.json
 60%|██████    | 142/235 [02:56<23:27, 15.13s/it] 61%|██████    | 143/235 [02:56<16:23, 10.69s/it] 61%|██████▏   | 144/235 [02:56<11:29,  7.57s/it] 62%|██████▏   | 145/235 [02:56<08:05,  5.39s/it] 62%|██████▏   | 146/235 [02:57<05:43,  3.86s/it] 63%|██████▎   | 147/235 [02:57<04:05,  2.79s/it] 63%|██████▎   | 148/235 [02:57<02:57,  2.04s/it] 63%|██████▎   | 149/235 [02:58<02:10,  1.52s/it] 64%|██████▍   | 150/235 [02:58<01:37,  1.15s/it] 64%|██████▍   | 151/235 [02:58<01:14,  1.12it/s] 65%|██████▍   | 152/235 [02:59<00:59,  1.41it/s] 65%|██████▌   | 153/235 [02:59<00:47,  1.71it/s] 66%|██████▌   | 154/235 [02:59<00:42,  1.90it/s] 66%|██████▌   | 155/235 [02:59<00:36,  2.19it/s] 66%|██████▋   | 156/235 [03:00<00:32,  2.46it/s] 67%|██████▋   | 157/235 [03:00<00:28,  2.70it/s] 67%|██████▋   | 158/235 [03:00<00:26,  2.89it/s] 68%|██████▊   | 159/235 [03:01<00:24,  3.04it/s] 68%|██████▊   | 160/235 [03:01<00:23,  3.16it/s] 69%|██████▊   | 161/235 [03:01<00:22,  3.24it/s] 69%|██████▉   | 162/235 [03:02<00:22,  3.30it/s] 69%|██████▉   | 163/235 [03:02<00:21,  3.34it/s] 70%|██████▉   | 164/235 [03:02<00:21,  3.38it/s] 70%|███████   | 165/235 [03:02<00:22,  3.18it/s] 71%|███████   | 166/235 [03:03<00:21,  3.27it/s] 71%|███████   | 167/235 [03:03<00:20,  3.32it/s] 71%|███████▏  | 168/235 [03:03<00:19,  3.36it/s] 72%|███████▏  | 169/235 [03:04<00:19,  3.38it/s] 72%|███████▏  | 170/235 [03:04<00:19,  3.40it/s] 73%|███████▎  | 171/235 [03:04<00:18,  3.41it/s] 73%|███████▎  | 172/235 [03:04<00:18,  3.43it/s] 74%|███████▎  | 173/235 [03:05<00:18,  3.43it/s] 74%|███████▍  | 174/235 [03:05<00:17,  3.44it/s] 74%|███████▍  | 175/235 [03:05<00:17,  3.45it/s] 75%|███████▍  | 176/235 [03:06<00:21,  2.69it/s] 75%|███████▌  | 177/235 [03:06<00:20,  2.88it/s] 76%|███████▌  | 178/235 [03:06<00:18,  3.04it/s] 76%|███████▌  | 179/235 [03:07<00:17,  3.15it/s] 77%|███████▋  | 180/235 [03:07<00:16,  3.24it/s] 77%|███████▋  | 181/235 [03:07<00:16,  3.31it/s] 77%|███████▋  | 182/235 [03:08<00:15,  3.35it/s] 78%|███████▊  | 183/235 [03:08<00:15,  3.38it/s] 78%|███████▊  | 184/235 [03:08<00:14,  3.41it/s] 79%|███████▊  | 185/235 [03:09<00:14,  3.42it/s] 79%|███████▉  | 186/235 [03:09<00:15,  3.19it/s] 80%|███████▉  | 187/235 [03:09<00:14,  3.26it/s] 80%|████████  | 188/235 [03:09<00:14,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 19:38:55,764 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:38:55,764 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:38:55,764 >>   Batch size = 8
{'eval_loss': 1.0591404438018799, 'eval_runtime': 11.471, 'eval_samples_per_second': 304.508, 'eval_steps_per_second': 38.096, 'epoch': 2.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.44it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.14it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.34it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.35it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.95it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.67it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.43it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.68it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.13it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 43.99it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.16it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.10it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.32it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.42it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.38it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.50it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.22it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.86it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.88it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.27it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.37it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.59it/s][A
 27%|██▋       | 117/437 [00:02<00:12, 26.60it/s][A
 28%|██▊       | 122/437 [00:02<00:10, 30.32it/s][A
 29%|██▉       | 127/437 [00:03<00:09, 33.62it/s][A
 30%|███       | 132/437 [00:03<00:08, 36.38it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 38.44it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 40.30it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 41.55it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 42.34it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 42.49it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 42.77it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.12it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 43.58it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.72it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.10it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.37it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.55it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.34it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.04it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.91it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 43.89it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.15it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.40it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.59it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.65it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.52it/s][A
 55%|█████▌    | 242/437 [00:05<00:07, 27.71it/s][A
 57%|█████▋    | 247/437 [00:06<00:06, 31.34it/s][A
 58%|█████▊    | 252/437 [00:06<00:05, 34.43it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 37.07it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 39.08it/s][A
 61%|██████    | 267/437 [00:06<00:04, 40.53it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 41.84it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 42.58it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 42.71it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 42.90it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 43.30it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 43.70it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.11it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.36it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.54it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.49it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.35it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.99it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.86it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 43.94it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 44.00it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.18it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.50it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.54it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.46it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 40.64it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 41.62it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 42.43it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 42.95it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 43.47it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.78it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.14it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.20it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.89it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.90it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.91it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.12it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 44.24it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.44it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.56it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.56it/s][A 80%|████████  | 188/235 [03:20<00:14,  3.32it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:39:06,522 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 19:39:06,840 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:39:18,844 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:39:19,431 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:39:19,602 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-188/special_tokens_map.json
 80%|████████  | 189/235 [04:11<14:12, 18.53s/it] 81%|████████  | 190/235 [04:11<09:56, 13.26s/it] 81%|████████▏ | 191/235 [04:12<06:52,  9.37s/it] 82%|████████▏ | 192/235 [04:12<04:45,  6.64s/it] 82%|████████▏ | 193/235 [04:12<03:19,  4.74s/it] 83%|████████▎ | 194/235 [04:13<02:19,  3.40s/it] 83%|████████▎ | 195/235 [04:13<01:38,  2.47s/it] 83%|████████▎ | 196/235 [04:13<01:10,  1.82s/it] 84%|████████▍ | 197/235 [04:14<00:51,  1.36s/it] 84%|████████▍ | 198/235 [04:14<00:39,  1.07s/it] 85%|████████▍ | 199/235 [04:14<00:30,  1.19it/s] 85%|████████▌ | 200/235 [04:14<00:23,  1.48it/s] 86%|████████▌ | 201/235 [04:15<00:19,  1.78it/s] 86%|████████▌ | 202/235 [04:15<00:15,  2.08it/s] 86%|████████▋ | 203/235 [04:15<00:13,  2.36it/s] 87%|████████▋ | 204/235 [04:16<00:11,  2.60it/s] 87%|████████▋ | 205/235 [04:16<00:10,  2.80it/s] 88%|████████▊ | 206/235 [04:17<00:16,  1.79it/s] 88%|████████▊ | 207/235 [04:17<00:13,  2.09it/s] 89%|████████▊ | 208/235 [04:18<00:11,  2.36it/s] 89%|████████▉ | 209/235 [04:18<00:10,  2.60it/s] 89%|████████▉ | 210/235 [04:18<00:08,  2.80it/s] 90%|████████▉ | 211/235 [04:18<00:08,  2.96it/s] 90%|█████████ | 212/235 [04:19<00:07,  3.08it/s] 91%|█████████ | 213/235 [04:19<00:06,  3.18it/s] 91%|█████████ | 214/235 [04:19<00:07,  2.98it/s] 91%|█████████▏| 215/235 [04:20<00:06,  3.09it/s] 92%|█████████▏| 216/235 [04:20<00:05,  3.18it/s] 92%|█████████▏| 217/235 [04:20<00:05,  3.24it/s] 93%|█████████▎| 218/235 [04:21<00:05,  3.29it/s] 93%|█████████▎| 219/235 [04:21<00:04,  3.32it/s] 94%|█████████▎| 220/235 [04:21<00:04,  3.35it/s] 94%|█████████▍| 221/235 [04:21<00:04,  3.37it/s] 94%|█████████▍| 222/235 [04:22<00:03,  3.39it/s] 95%|█████████▍| 223/235 [04:22<00:03,  3.39it/s] 95%|█████████▌| 224/235 [04:23<00:04,  2.74it/s] 96%|█████████▌| 225/235 [04:23<00:03,  2.92it/s] 96%|█████████▌| 226/235 [04:23<00:02,  3.07it/s] 97%|█████████▋| 227/235 [04:23<00:02,  3.18it/s] 97%|█████████▋| 228/235 [04:24<00:02,  3.26it/s] 97%|█████████▋| 229/235 [04:24<00:01,  3.31it/s] 98%|█████████▊| 230/235 [04:24<00:01,  3.36it/s] 98%|█████████▊| 231/235 [04:25<00:01,  3.38it/s] 99%|█████████▊| 232/235 [04:25<00:00,  3.40it/s] 99%|█████████▉| 233/235 [04:25<00:00,  3.42it/s]100%|█████████▉| 234/235 [04:26<00:00,  2.79it/s]100%|██████████| 235/235 [04:26<00:00,  2.96it/s][INFO|trainer.py:2140] 2023-08-28 19:40:11,923 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:40:11,923 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:40:11,923 >>   Batch size = 8
{'eval_loss': 1.0646039247512817, 'eval_runtime': 10.3447, 'eval_samples_per_second': 337.661, 'eval_steps_per_second': 42.244, 'epoch': 3.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.74it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.86it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.61it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.34it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.10it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.75it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.67it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.31it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.42it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.63it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.69it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.45it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.40it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.24it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.37it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.15it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.20it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.36it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.46it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.42it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 41.30it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 42.40it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.08it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.39it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.64it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.89it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.20it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.24it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.85it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.95it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.04it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.19it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.16it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.22it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.26it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.39it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.32it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.95it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.16it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.27it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.29it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.26it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.38it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.43it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.37it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.17it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.16it/s][A
 55%|█████▌    | 242/437 [00:05<00:06, 30.85it/s][A
 57%|█████▋    | 247/437 [00:05<00:05, 34.05it/s][A
 58%|█████▊    | 252/437 [00:05<00:05, 36.72it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 38.75it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 40.44it/s][A
 61%|██████    | 267/437 [00:06<00:04, 41.72it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 42.29it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 42.66it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 42.71it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 42.97it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.35it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.72it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.97it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.27it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.36it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.44it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.14it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.77it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.79it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.85it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.13it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.31it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.44it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.56it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.35it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.23it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 39.26it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 40.77it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 41.80it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 42.49it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.17it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.40it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.83it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.81it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.59it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.35it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.58it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.02it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.29it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.48it/s][A                                                 
                                                 [A100%|██████████| 235/235 [04:36<00:00,  2.96it/s]
100%|██████████| 437/437 [00:10<00:00, 44.48it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:40:22,901 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-235
[INFO|configuration_utils.py:351] 2023-08-28 19:40:23,775 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-235/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:40:36,060 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-235/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:40:37,362 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-235/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:40:37,690 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-235/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 19:41:14,238 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 19:41:14,664 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-47 (score: 1.0386451482772827).
                                                 100%|██████████| 235/235 [06:05<00:00,  2.96it/s]100%|██████████| 235/235 [06:05<00:00,  1.56s/it]
[INFO|trainer.py:1894] 2023-08-28 19:41:51,360 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 19:41:52,905 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:42:04,444 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:42:05,103 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:42:05,272 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:42:08,545 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:08,647 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:08,647 >>   train_loss               =      0.468
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:08,647 >>   train_runtime            = 0:06:05.72
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:08,648 >>   train_samples            =       3011
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:08,648 >>   train_samples_per_second =     41.165
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:08,648 >>   train_steps_per_second   =      0.643
{'eval_loss': 1.0706281661987305, 'eval_runtime': 10.0992, 'eval_samples_per_second': 345.869, 'eval_steps_per_second': 43.271, 'epoch': 4.99}
{'train_runtime': 365.7247, 'train_samples_per_second': 41.165, 'train_steps_per_second': 0.643, 'train_loss': 0.46801225378158245, 'epoch': 4.99}
08/28/2023 19:42:09 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 19:42:09,587 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:42:09,587 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:42:09,587 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.36it/s]  3%|▎         | 12/437 [00:00<00:08, 49.05it/s]  4%|▍         | 17/437 [00:00<00:31, 13.46it/s]  5%|▍         | 20/437 [00:01<00:38, 10.83it/s]  6%|▌         | 25/437 [00:01<00:26, 15.45it/s]  7%|▋         | 29/437 [00:01<00:26, 15.12it/s]  7%|▋         | 32/437 [00:01<00:25, 16.09it/s]  8%|▊         | 36/437 [00:02<00:20, 19.64it/s]  9%|▉         | 41/437 [00:02<00:16, 24.72it/s] 10%|█         | 45/437 [00:02<00:15, 26.13it/s] 12%|█▏        | 51/437 [00:02<00:12, 32.02it/s] 13%|█▎        | 56/437 [00:02<00:12, 31.49it/s] 14%|█▍        | 62/437 [00:02<00:10, 35.96it/s] 15%|█▌        | 66/437 [00:02<00:11, 31.58it/s] 16%|█▌        | 71/437 [00:02<00:10, 34.88it/s] 17%|█▋        | 76/437 [00:03<00:09, 37.39it/s] 19%|█▊        | 81/437 [00:03<00:09, 39.47it/s] 20%|█▉        | 86/437 [00:03<00:17, 19.92it/s] 21%|██        | 91/437 [00:03<00:14, 23.96it/s] 22%|██▏       | 96/437 [00:03<00:12, 27.85it/s] 23%|██▎       | 101/437 [00:04<00:10, 31.41it/s] 24%|██▍       | 106/437 [00:04<00:09, 34.55it/s] 25%|██▌       | 111/437 [00:04<00:08, 37.06it/s] 27%|██▋       | 116/437 [00:04<00:08, 39.05it/s] 28%|██▊       | 121/437 [00:04<00:07, 40.46it/s] 29%|██▉       | 126/437 [00:04<00:07, 41.12it/s] 30%|██▉       | 131/437 [00:04<00:07, 41.49it/s] 31%|███       | 136/437 [00:04<00:07, 42.26it/s] 32%|███▏      | 141/437 [00:04<00:06, 43.00it/s] 33%|███▎      | 146/437 [00:05<00:06, 43.48it/s] 35%|███▍      | 151/437 [00:05<00:06, 43.82it/s] 36%|███▌      | 156/437 [00:05<00:06, 44.10it/s] 37%|███▋      | 161/437 [00:05<00:06, 44.33it/s] 38%|███▊      | 166/437 [00:05<00:06, 43.99it/s] 39%|███▉      | 171/437 [00:05<00:06, 43.85it/s] 40%|████      | 176/437 [00:05<00:05, 43.71it/s] 41%|████▏     | 181/437 [00:05<00:05, 43.89it/s] 43%|████▎     | 186/437 [00:06<00:05, 44.18it/s] 44%|████▎     | 191/437 [00:06<00:05, 44.36it/s] 45%|████▍     | 196/437 [00:06<00:05, 44.47it/s] 46%|████▌     | 201/437 [00:06<00:05, 39.37it/s] 47%|████▋     | 206/437 [00:06<00:05, 40.85it/s] 48%|████▊     | 211/437 [00:06<00:05, 41.99it/s] 49%|████▉     | 216/437 [00:06<00:05, 42.68it/s] 51%|█████     | 221/437 [00:06<00:05, 43.03it/s] 52%|█████▏    | 226/437 [00:06<00:04, 43.44it/s] 53%|█████▎    | 231/437 [00:07<00:04, 43.79it/s] 54%|█████▍    | 236/437 [00:07<00:04, 43.88it/s] 55%|█████▌    | 241/437 [00:07<00:04, 43.61it/s] 56%|█████▋    | 246/437 [00:07<00:04, 43.63it/s] 57%|█████▋    | 251/437 [00:07<00:04, 43.79it/s] 59%|█████▊    | 256/437 [00:07<00:04, 44.18it/s] 60%|█████▉    | 261/437 [00:07<00:03, 44.23it/s] 61%|██████    | 266/437 [00:07<00:03, 44.22it/s] 62%|██████▏   | 271/437 [00:07<00:03, 44.33it/s] 63%|██████▎   | 276/437 [00:08<00:03, 44.24it/s] 64%|██████▍   | 281/437 [00:08<00:03, 44.22it/s] 65%|██████▌   | 286/437 [00:08<00:03, 43.85it/s] 67%|██████▋   | 291/437 [00:08<00:03, 43.88it/s] 68%|██████▊   | 296/437 [00:08<00:03, 43.95it/s] 69%|██████▉   | 301/437 [00:08<00:03, 44.19it/s] 70%|███████   | 306/437 [00:08<00:02, 44.23it/s] 71%|███████   | 311/437 [00:08<00:02, 44.28it/s] 72%|███████▏  | 316/437 [00:09<00:02, 44.33it/s] 73%|███████▎  | 321/437 [00:09<00:02, 44.36it/s] 75%|███████▍  | 326/437 [00:09<00:02, 44.07it/s] 76%|███████▌  | 331/437 [00:09<00:02, 43.89it/s] 77%|███████▋  | 336/437 [00:09<00:02, 40.06it/s] 78%|███████▊  | 341/437 [00:09<00:02, 41.39it/s] 79%|███████▉  | 346/437 [00:09<00:02, 42.32it/s] 80%|████████  | 351/437 [00:09<00:01, 43.08it/s] 81%|████████▏ | 356/437 [00:09<00:01, 43.61it/s] 83%|████████▎ | 361/437 [00:10<00:01, 43.88it/s] 84%|████████▍ | 366/437 [00:10<00:01, 43.96it/s] 85%|████████▍ | 371/437 [00:10<00:01, 43.88it/s] 86%|████████▌ | 376/437 [00:10<00:01, 43.60it/s] 87%|████████▋ | 381/437 [00:10<00:01, 43.42it/s] 88%|████████▊ | 386/437 [00:10<00:01, 43.68it/s] 89%|████████▉ | 391/437 [00:10<00:01, 43.84it/s] 91%|█████████ | 396/437 [00:10<00:00, 44.06it/s] 92%|█████████▏| 401/437 [00:10<00:00, 44.09it/s] 93%|█████████▎| 406/437 [00:11<00:00, 44.26it/s] 94%|█████████▍| 411/437 [00:11<00:00, 44.18it/s] 95%|█████████▌| 416/437 [00:11<00:00, 43.77it/s] 96%|█████████▋| 421/437 [00:11<00:00, 43.86it/s] 97%|█████████▋| 426/437 [00:11<00:00, 43.78it/s] 99%|█████████▊| 431/437 [00:11<00:00, 43.88it/s]100%|█████████▉| 436/437 [00:11<00:00, 44.11it/s]100%|██████████| 437/437 [00:11<00:00, 37.07it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:42:21,395 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:21,395 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:21,395 >>   eval_loss               =     1.0386
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:21,395 >>   eval_runtime            = 0:00:11.80
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:21,395 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:21,395 >>   eval_samples_per_second =    295.816
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:21,395 >>   eval_steps_per_second   =     37.009
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:21,395 >>   perplexity              =     2.8254
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:42:40,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:42:40,736 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:42:40,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:42:40,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:42:40,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:42:41,979 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:42:41,980 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:42:42,520 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:42:44,230 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:42:44,402 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:42:47,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:42:47,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:42:47,829 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:42:47,829 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:42:47,829 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:42:49,505 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:42:49,506 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:42:49,980 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:42:50,236 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:42:50,236 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-235
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-94
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-47
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/checkpoint-141
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.82it/s]Extractor Predicting: 3it [00:01,  1.75it/s]Extractor Predicting: 4it [00:02,  1.77it/s]Extractor Predicting: 5it [00:03,  1.19it/s]Extractor Predicting: 6it [00:04,  1.29it/s]Extractor Predicting: 7it [00:04,  1.42it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.18it/s]Extractor Predicting: 10it [00:07,  1.26it/s]Extractor Predicting: 11it [00:07,  1.35it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:09,  1.42it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:10,  1.55it/s]Extractor Predicting: 16it [00:10,  1.61it/s]Extractor Predicting: 17it [00:11,  1.65it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:13,  1.50it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:14,  1.56it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:15,  1.56it/s]Extractor Predicting: 24it [00:16,  1.61it/s]Extractor Predicting: 25it [00:16,  1.63it/s]Extractor Predicting: 26it [00:17,  1.71it/s]Extractor Predicting: 27it [00:17,  1.69it/s]Extractor Predicting: 28it [00:18,  1.75it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:20,  1.54it/s]Extractor Predicting: 32it [00:21,  1.56it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:22,  1.43it/s]Extractor Predicting: 35it [00:23,  1.48it/s]Extractor Predicting: 36it [00:23,  1.50it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:25,  1.51it/s]Extractor Predicting: 39it [00:26,  1.43it/s]Extractor Predicting: 40it [00:26,  1.48it/s]Extractor Predicting: 41it [00:27,  1.49it/s]Extractor Predicting: 42it [00:27,  1.52it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:29,  1.55it/s]Extractor Predicting: 45it [00:29,  1.58it/s]Extractor Predicting: 46it [00:30,  1.56it/s]Extractor Predicting: 47it [00:31,  1.54it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.49it/s]Extractor Predicting: 50it [00:33,  1.52it/s]Extractor Predicting: 51it [00:33,  1.55it/s]Extractor Predicting: 52it [00:34,  1.54it/s]Extractor Predicting: 53it [00:34,  1.59it/s]Extractor Predicting: 54it [00:35,  1.57it/s]Extractor Predicting: 55it [00:36,  1.55it/s]Extractor Predicting: 56it [00:37,  1.47it/s]Extractor Predicting: 57it [00:37,  1.47it/s]Extractor Predicting: 58it [00:38,  1.49it/s]Extractor Predicting: 59it [00:39,  1.50it/s]Extractor Predicting: 60it [00:39,  1.40it/s]Extractor Predicting: 61it [00:40,  1.38it/s]Extractor Predicting: 62it [00:41,  1.43it/s]Extractor Predicting: 63it [00:41,  1.50it/s]Extractor Predicting: 64it [00:42,  1.53it/s]Extractor Predicting: 65it [00:43,  1.53it/s]Extractor Predicting: 66it [00:43,  1.56it/s]Extractor Predicting: 67it [00:44,  1.57it/s]Extractor Predicting: 68it [00:45,  1.56it/s]Extractor Predicting: 69it [00:45,  1.57it/s]Extractor Predicting: 70it [00:46,  1.55it/s]Extractor Predicting: 71it [00:47,  1.35it/s]Extractor Predicting: 72it [00:47,  1.43it/s]Extractor Predicting: 73it [00:48,  1.47it/s]Extractor Predicting: 74it [00:49,  1.53it/s]Extractor Predicting: 75it [00:49,  1.56it/s]Extractor Predicting: 76it [00:50,  1.49it/s]Extractor Predicting: 77it [00:51,  1.52it/s]Extractor Predicting: 78it [00:51,  1.53it/s]Extractor Predicting: 79it [00:52,  1.55it/s]Extractor Predicting: 80it [00:53,  1.55it/s]Extractor Predicting: 81it [00:53,  1.40it/s]Extractor Predicting: 82it [00:54,  1.45it/s]Extractor Predicting: 83it [00:55,  1.47it/s]Extractor Predicting: 84it [00:55,  1.51it/s]Extractor Predicting: 85it [00:56,  1.56it/s]Extractor Predicting: 86it [00:57,  1.33it/s]Extractor Predicting: 87it [00:57,  1.45it/s]Extractor Predicting: 88it [00:58,  1.52it/s]Extractor Predicting: 89it [00:59,  1.56it/s]Extractor Predicting: 90it [00:59,  1.62it/s]Extractor Predicting: 91it [01:00,  1.27it/s]Extractor Predicting: 92it [01:01,  1.35it/s]Extractor Predicting: 93it [01:02,  1.47it/s]Extractor Predicting: 94it [01:02,  1.55it/s]Extractor Predicting: 95it [01:03,  1.60it/s]Extractor Predicting: 96it [01:03,  1.50it/s]Extractor Predicting: 97it [01:04,  1.46it/s]Extractor Predicting: 98it [01:05,  1.49it/s]Extractor Predicting: 99it [01:05,  1.53it/s]Extractor Predicting: 100it [01:06,  1.59it/s]Extractor Predicting: 101it [01:07,  1.64it/s]Extractor Predicting: 102it [01:07,  1.56it/s]Extractor Predicting: 103it [01:08,  1.59it/s]Extractor Predicting: 104it [01:08,  1.61it/s]Extractor Predicting: 105it [01:09,  1.66it/s]Extractor Predicting: 106it [01:10,  1.65it/s]Extractor Predicting: 107it [01:10,  1.57it/s]Extractor Predicting: 108it [01:11,  1.59it/s]Extractor Predicting: 109it [01:12,  1.66it/s]Extractor Predicting: 110it [01:12,  1.67it/s]Extractor Predicting: 111it [01:13,  1.67it/s]Extractor Predicting: 112it [01:13,  1.59it/s]Extractor Predicting: 113it [01:14,  1.62it/s]Extractor Predicting: 114it [01:15,  1.59it/s]Extractor Predicting: 115it [01:15,  1.61it/s]Extractor Predicting: 116it [01:16,  1.60it/s]Extractor Predicting: 117it [01:17,  1.53it/s]Extractor Predicting: 118it [01:17,  1.62it/s]Extractor Predicting: 119it [01:18,  1.63it/s]Extractor Predicting: 120it [01:18,  1.65it/s]Extractor Predicting: 121it [01:19,  1.63it/s]Extractor Predicting: 122it [01:20,  1.57it/s]Extractor Predicting: 123it [01:20,  1.58it/s]Extractor Predicting: 124it [01:21,  1.56it/s]Extractor Predicting: 125it [01:22,  1.57it/s]Extractor Predicting: 126it [01:22,  1.58it/s]Extractor Predicting: 127it [01:23,  1.50it/s]Extractor Predicting: 128it [01:24,  1.40it/s]Extractor Predicting: 129it [01:24,  1.42it/s]Extractor Predicting: 130it [01:25,  1.47it/s]Extractor Predicting: 131it [01:26,  1.50it/s]Extractor Predicting: 132it [01:26,  1.52it/s]Extractor Predicting: 133it [01:27,  1.55it/s]Extractor Predicting: 134it [01:28,  1.57it/s]Extractor Predicting: 135it [01:28,  1.57it/s]Extractor Predicting: 136it [01:29,  1.58it/s]Extractor Predicting: 137it [01:30,  1.37it/s]Extractor Predicting: 138it [01:30,  1.43it/s]Extractor Predicting: 139it [01:31,  1.47it/s]Extractor Predicting: 140it [01:32,  1.54it/s]Extractor Predicting: 141it [01:32,  1.54it/s]Extractor Predicting: 142it [01:33,  1.55it/s]Extractor Predicting: 142it [01:33,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:45:03,881 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:45:03,962 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:45:03,962 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:45:03,963 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:45:03,963 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:45:06,565 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:45:06,566 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:45:07,721 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:45:09,555 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:45:09,555 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:45:13,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:45:13,918 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:45:13,919 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:45:13,919 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:45:13,919 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:45:15,748 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:45:15,749 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:45:16,507 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:45:16,752 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:45:16,752 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2012310606060606,
  "recall": 0.1216719152590896,
  "score": 0.15165031222123104,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.41it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:07,  1.27it/s]Extractor Predicting: 12it [00:07,  1.38it/s]Extractor Predicting: 13it [00:08,  1.43it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.55it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.61it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.39it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.50it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:18,  1.62it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.34it/s]Extractor Predicting: 32it [00:21,  1.40it/s]Extractor Predicting: 33it [00:21,  1.46it/s]Extractor Predicting: 34it [00:22,  1.42it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:23,  1.44it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.56it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:26,  1.59it/s]Extractor Predicting: 41it [00:27,  1.50it/s]Extractor Predicting: 42it [00:27,  1.52it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:29,  1.58it/s]Extractor Predicting: 46it [00:30,  1.20it/s]Extractor Predicting: 47it [00:31,  1.30it/s]Extractor Predicting: 48it [00:32,  1.40it/s]Extractor Predicting: 49it [00:32,  1.46it/s]Extractor Predicting: 50it [00:33,  1.45it/s]Extractor Predicting: 51it [00:33,  1.52it/s]Extractor Predicting: 52it [00:34,  1.56it/s]Extractor Predicting: 53it [00:35,  1.58it/s]Extractor Predicting: 54it [00:35,  1.60it/s]Extractor Predicting: 55it [00:36,  1.47it/s]Extractor Predicting: 56it [00:37,  1.53it/s]Extractor Predicting: 57it [00:37,  1.58it/s]Extractor Predicting: 58it [00:38,  1.63it/s]Extractor Predicting: 59it [00:38,  1.64it/s]Extractor Predicting: 60it [00:39,  1.54it/s]Extractor Predicting: 61it [00:40,  1.57it/s]Extractor Predicting: 62it [00:40,  1.60it/s]Extractor Predicting: 63it [00:41,  1.62it/s]Extractor Predicting: 64it [00:42,  1.59it/s]Extractor Predicting: 65it [00:42,  1.56it/s]Extractor Predicting: 66it [00:43,  1.57it/s]Extractor Predicting: 67it [00:44,  1.59it/s]Extractor Predicting: 68it [00:44,  1.65it/s]Extractor Predicting: 69it [00:45,  1.64it/s]Extractor Predicting: 70it [00:45,  1.56it/s]Extractor Predicting: 71it [00:46,  1.57it/s]Extractor Predicting: 72it [00:47,  1.60it/s]Extractor Predicting: 73it [00:47,  1.56it/s]Extractor Predicting: 74it [00:48,  1.56it/s]Extractor Predicting: 75it [00:49,  1.59it/s]Extractor Predicting: 76it [00:49,  1.60it/s]Extractor Predicting: 77it [00:50,  1.48it/s]Extractor Predicting: 78it [00:51,  1.53it/s]Extractor Predicting: 79it [00:51,  1.55it/s]Extractor Predicting: 80it [00:52,  1.57it/s]Extractor Predicting: 81it [00:52,  1.59it/s]Extractor Predicting: 82it [00:53,  1.51it/s]Extractor Predicting: 83it [00:54,  1.55it/s]Extractor Predicting: 84it [00:54,  1.57it/s]Extractor Predicting: 85it [00:55,  1.59it/s]Extractor Predicting: 86it [00:56,  1.57it/s]Extractor Predicting: 87it [00:57,  1.40it/s]Extractor Predicting: 88it [00:57,  1.43it/s]Extractor Predicting: 89it [00:58,  1.49it/s]Extractor Predicting: 90it [00:58,  1.54it/s]Extractor Predicting: 91it [00:59,  1.53it/s]Extractor Predicting: 92it [01:00,  1.19it/s]Extractor Predicting: 93it [01:01,  1.26it/s]Extractor Predicting: 94it [01:02,  1.33it/s]Extractor Predicting: 95it [01:02,  1.40it/s]Extractor Predicting: 96it [01:03,  1.34it/s]Extractor Predicting: 97it [01:04,  1.41it/s]Extractor Predicting: 98it [01:04,  1.44it/s]Extractor Predicting: 99it [01:05,  1.49it/s]Extractor Predicting: 100it [01:06,  1.51it/s]Extractor Predicting: 101it [01:06,  1.45it/s]Extractor Predicting: 102it [01:07,  1.36it/s]Extractor Predicting: 103it [01:08,  1.42it/s]Extractor Predicting: 104it [01:09,  1.46it/s]Extractor Predicting: 105it [01:09,  1.47it/s]Extractor Predicting: 106it [01:10,  1.48it/s]Extractor Predicting: 107it [01:11,  1.48it/s]Extractor Predicting: 108it [01:11,  1.49it/s]Extractor Predicting: 109it [01:12,  1.52it/s]Extractor Predicting: 110it [01:13,  1.49it/s]Extractor Predicting: 111it [01:13,  1.42it/s]Extractor Predicting: 112it [01:14,  1.49it/s]Extractor Predicting: 113it [01:15,  1.54it/s]Extractor Predicting: 114it [01:15,  1.57it/s]Extractor Predicting: 115it [01:16,  1.57it/s]Extractor Predicting: 116it [01:16,  1.56it/s]Extractor Predicting: 117it [01:17,  1.59it/s]Extractor Predicting: 118it [01:18,  1.60it/s]Extractor Predicting: 119it [01:18,  1.59it/s]Extractor Predicting: 120it [01:19,  1.62it/s]Extractor Predicting: 121it [01:19,  1.61it/s]Extractor Predicting: 122it [01:20,  1.66it/s]Extractor Predicting: 123it [01:21,  1.65it/s]Extractor Predicting: 124it [01:22,  1.38it/s]Extractor Predicting: 125it [01:22,  1.45it/s]Extractor Predicting: 126it [01:23,  1.49it/s]Extractor Predicting: 127it [01:24,  1.54it/s]Extractor Predicting: 128it [01:24,  1.58it/s]Extractor Predicting: 129it [01:25,  1.42it/s]Extractor Predicting: 130it [01:26,  1.49it/s]Extractor Predicting: 131it [01:26,  1.52it/s]Extractor Predicting: 132it [01:27,  1.55it/s]Extractor Predicting: 133it [01:27,  1.57it/s]Extractor Predicting: 134it [01:29,  1.21it/s]Extractor Predicting: 135it [01:29,  1.33it/s]Extractor Predicting: 136it [01:30,  1.41it/s]Extractor Predicting: 137it [01:30,  1.47it/s]Extractor Predicting: 138it [01:31,  1.37it/s]Extractor Predicting: 139it [01:32,  1.48it/s]Extractor Predicting: 140it [01:32,  1.52it/s]Extractor Predicting: 141it [01:33,  1.54it/s]Extractor Predicting: 142it [01:34,  1.60it/s]Extractor Predicting: 143it [01:34,  1.52it/s]Extractor Predicting: 144it [01:35,  1.54it/s]Extractor Predicting: 145it [01:36,  1.57it/s]Extractor Predicting: 146it [01:36,  1.58it/s]Extractor Predicting: 147it [01:37,  1.57it/s]Extractor Predicting: 148it [01:38,  1.51it/s]Extractor Predicting: 149it [01:38,  1.53it/s]Extractor Predicting: 150it [01:39,  1.57it/s]Extractor Predicting: 151it [01:39,  1.61it/s]Extractor Predicting: 152it [01:40,  1.63it/s]Extractor Predicting: 153it [01:41,  1.54it/s]Extractor Predicting: 154it [01:41,  1.55it/s]Extractor Predicting: 155it [01:42,  1.56it/s]Extractor Predicting: 156it [01:43,  1.57it/s]Extractor Predicting: 157it [01:43,  1.56it/s]Extractor Predicting: 158it [01:44,  1.46it/s]Extractor Predicting: 159it [01:45,  1.51it/s]Extractor Predicting: 160it [01:45,  1.54it/s]Extractor Predicting: 161it [01:46,  1.58it/s]Extractor Predicting: 162it [01:47,  1.60it/s]Extractor Predicting: 163it [01:47,  1.40it/s]Extractor Predicting: 164it [01:48,  1.47it/s]Extractor Predicting: 165it [01:49,  1.48it/s]Extractor Predicting: 166it [01:49,  1.53it/s]Extractor Predicting: 167it [01:50,  1.57it/s]Extractor Predicting: 168it [01:51,  1.57it/s]Extractor Predicting: 169it [01:51,  1.58it/s]Extractor Predicting: 170it [01:52,  1.43it/s]Extractor Predicting: 171it [01:53,  1.43it/s]Extractor Predicting: 172it [01:53,  1.47it/s]Extractor Predicting: 173it [01:54,  1.49it/s]Extractor Predicting: 174it [01:55,  1.47it/s]Extractor Predicting: 175it [01:56,  1.38it/s]Extractor Predicting: 176it [01:56,  1.43it/s]Extractor Predicting: 177it [01:57,  1.47it/s]Extractor Predicting: 178it [01:57,  1.53it/s]Extractor Predicting: 179it [01:58,  1.53it/s]Extractor Predicting: 180it [01:59,  1.48it/s]Extractor Predicting: 181it [01:59,  1.50it/s]Extractor Predicting: 182it [02:00,  1.55it/s]Extractor Predicting: 183it [02:01,  1.56it/s]Extractor Predicting: 184it [02:01,  1.61it/s]Extractor Predicting: 185it [02:02,  1.53it/s]Extractor Predicting: 186it [02:03,  1.43it/s]Extractor Predicting: 187it [02:03,  1.49it/s]Extractor Predicting: 188it [02:04,  1.53it/s]Extractor Predicting: 189it [02:05,  1.57it/s]Extractor Predicting: 190it [02:05,  1.48it/s]Extractor Predicting: 191it [02:06,  1.48it/s]Extractor Predicting: 192it [02:07,  1.54it/s]Extractor Predicting: 193it [02:07,  1.60it/s]Extractor Predicting: 194it [02:08,  1.60it/s]Extractor Predicting: 195it [02:09,  1.53it/s]Extractor Predicting: 196it [02:09,  1.58it/s]Extractor Predicting: 197it [02:10,  1.61it/s]Extractor Predicting: 198it [02:10,  1.59it/s]Extractor Predicting: 199it [02:11,  1.61it/s]Extractor Predicting: 200it [02:12,  1.51it/s]Extractor Predicting: 201it [02:12,  1.55it/s]Extractor Predicting: 202it [02:13,  1.61it/s]Extractor Predicting: 203it [02:14,  1.64it/s]Extractor Predicting: 204it [02:14,  1.63it/s]Extractor Predicting: 205it [02:15,  1.51it/s]Extractor Predicting: 206it [02:16,  1.55it/s]Extractor Predicting: 207it [02:16,  1.59it/s]Extractor Predicting: 208it [02:17,  1.61it/s]Extractor Predicting: 209it [02:17,  1.57it/s]Extractor Predicting: 210it [02:18,  1.47it/s]Extractor Predicting: 211it [02:19,  1.51it/s]Extractor Predicting: 212it [02:19,  1.49it/s]Extractor Predicting: 213it [02:20,  1.53it/s]Extractor Predicting: 214it [02:21,  1.52it/s]Extractor Predicting: 215it [02:21,  1.53it/s]Extractor Predicting: 216it [02:22,  1.58it/s]Extractor Predicting: 217it [02:23,  1.50it/s]Extractor Predicting: 218it [02:23,  1.48it/s]Extractor Predicting: 219it [02:24,  1.52it/s]Extractor Predicting: 220it [02:25,  1.55it/s]Extractor Predicting: 221it [02:25,  1.53it/s]Extractor Predicting: 222it [02:26,  1.45it/s]Extractor Predicting: 223it [02:27,  1.48it/s]Extractor Predicting: 224it [02:27,  1.54it/s]Extractor Predicting: 225it [02:28,  1.55it/s]Extractor Predicting: 226it [02:29,  1.62it/s]Extractor Predicting: 227it [02:29,  1.51it/s]Extractor Predicting: 228it [02:30,  1.53it/s]Extractor Predicting: 229it [02:31,  1.57it/s]Extractor Predicting: 230it [02:31,  1.56it/s]Extractor Predicting: 231it [02:32,  1.56it/s]Extractor Predicting: 232it [02:33,  1.41it/s]Extractor Predicting: 233it [02:33,  1.50it/s]Extractor Predicting: 234it [02:34,  1.52it/s]Extractor Predicting: 235it [02:34,  1.56it/s]Extractor Predicting: 236it [02:35,  1.56it/s]Extractor Predicting: 237it [02:36,  1.44it/s]Extractor Predicting: 238it [02:37,  1.51it/s]Extractor Predicting: 239it [02:37,  1.55it/s]Extractor Predicting: 240it [02:38,  1.57it/s]Extractor Predicting: 241it [02:38,  1.58it/s]Extractor Predicting: 242it [02:39,  1.30it/s]Extractor Predicting: 243it [02:40,  1.35it/s]Extractor Predicting: 244it [02:41,  1.42it/s]Extractor Predicting: 245it [02:41,  1.50it/s]Extractor Predicting: 246it [02:42,  1.53it/s]Extractor Predicting: 247it [02:43,  1.47it/s]Extractor Predicting: 248it [02:43,  1.52it/s]Extractor Predicting: 249it [02:44,  1.56it/s]Extractor Predicting: 250it [02:45,  1.57it/s]Extractor Predicting: 251it [02:45,  1.55it/s]Extractor Predicting: 252it [02:46,  1.48it/s]Extractor Predicting: 253it [02:47,  1.52it/s]Extractor Predicting: 254it [02:47,  1.56it/s]Extractor Predicting: 255it [02:48,  1.56it/s]Extractor Predicting: 256it [02:48,  1.56it/s]Extractor Predicting: 257it [02:49,  1.56it/s]Extractor Predicting: 258it [02:50,  1.57it/s]Extractor Predicting: 259it [02:50,  1.53it/s]Extractor Predicting: 260it [02:51,  1.56it/s]Extractor Predicting: 261it [02:52,  1.59it/s]Extractor Predicting: 262it [02:52,  1.58it/s]Extractor Predicting: 263it [02:53,  1.56it/s]Extractor Predicting: 264it [02:54,  1.46it/s]Extractor Predicting: 265it [02:54,  1.49it/s]Extractor Predicting: 266it [02:55,  1.49it/s]Extractor Predicting: 267it [02:56,  1.48it/s]Extractor Predicting: 268it [02:56,  1.50it/s]Extractor Predicting: 269it [02:57,  1.46it/s]Extractor Predicting: 270it [02:58,  1.46it/s]Extractor Predicting: 271it [02:58,  1.49it/s]Extractor Predicting: 272it [02:59,  1.51it/s]Extractor Predicting: 273it [03:00,  1.51it/s]Extractor Predicting: 274it [03:00,  1.48it/s]Extractor Predicting: 275it [03:01,  1.53it/s]Extractor Predicting: 276it [03:02,  1.55it/s]Extractor Predicting: 277it [03:02,  1.54it/s]Extractor Predicting: 278it [03:03,  1.54it/s]Extractor Predicting: 279it [03:04,  1.51it/s]Extractor Predicting: 280it [03:04,  1.52it/s]Extractor Predicting: 281it [03:05,  1.51it/s]Extractor Predicting: 282it [03:06,  1.54it/s]Extractor Predicting: 283it [03:06,  1.50it/s]Extractor Predicting: 284it [03:07,  1.43it/s]Extractor Predicting: 285it [03:08,  1.46it/s]Extractor Predicting: 286it [03:08,  1.47it/s]Extractor Predicting: 287it [03:09,  1.49it/s]Extractor Predicting: 288it [03:09,  1.97it/s]Extractor Predicting: 288it [03:09,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:50,305 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:50,307 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:50,308 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:50,308 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:50,308 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:48:50,922 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:48:50,923 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:48:51,779 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:48:52,861 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:48:53,100 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:56,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:56,841 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:56,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:56,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:48:56,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:48:57,536 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:48:57,537 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:48:58,561 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:48:58,721 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:48:58,721 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.410844629822732,
  "recall": 0.2287705037015532,
  "score": 0.29389277389277385,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.97it/s]Extractor Predicting: 3it [00:01,  1.81it/s]
[INFO|configuration_utils.py:515] 2023-08-28 19:49:06,964 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:49:07,049 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:49:07,220 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:49:07,221 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 19:49:07,261 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:49:54,250 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 19:49:54,292 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 19:49:55,855 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:49:55,856 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:49:56,510 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:49:56,892 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:49:56,893 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:49:56,893 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:49:56,893 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:49:56,893 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:49:56,893 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.36666666666666664,
  "recall": 0.0990990990990991,
  "score": 0.15602836879432622,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 19:49:58,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:49:58,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:49:59,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:00,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:00,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:01,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:02,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:02,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:03,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:03,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:04,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:05,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:06,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:06,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:07,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:07,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:08,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:09,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:09,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:10,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:10,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:07, 13.41s/it][WARNING|generation_utils.py:914] 2023-08-28 19:50:11,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:12,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:12,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:13,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:13,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:14,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:15,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:16,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:16,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:17,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:17,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:18,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:19,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:19,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:20,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:21,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:21,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:22,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:22,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:23,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:25<02:45, 12.70s/it][WARNING|generation_utils.py:914] 2023-08-28 19:50:23,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:24,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:25,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:25,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:26,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:27,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:28,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:28,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:29,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:29,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:30,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:31,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:32,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:32,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:33,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:34,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:34,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:35,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:36,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:36,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:37,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:38,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:40<02:44, 13.75s/it][WARNING|generation_utils.py:914] 2023-08-28 19:50:38,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:39,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:40,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:41,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:41,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:42,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:43,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:43,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:44,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:44,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:45,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:46,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:46,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:47,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:47,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:48,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:49,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:49,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:50,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:51,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:51,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:54<02:30, 13.66s/it][WARNING|generation_utils.py:914] 2023-08-28 19:50:52,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:52,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:53,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:54,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:54,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:55,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:55,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:56,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:56,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:57,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:58,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:59,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:50:59,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:00,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:00,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:01,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:01,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:02,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:02,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:03,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:03,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:06<02:11, 13.16s/it][WARNING|generation_utils.py:914] 2023-08-28 19:51:04,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:05,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:05,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:06,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:07,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:08,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:08,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:09,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:09,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:10,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:11,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:11,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:12,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:12,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:13,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:14,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:15,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:15,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:16,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:16,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:17,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:19<01:59, 13.24s/it][WARNING|generation_utils.py:914] 2023-08-28 19:51:18,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:18,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:19,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:20,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:20,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:21,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:22,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:22,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:23,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:23,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:24,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:25,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:25,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:26,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:27,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:28,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:29,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:29,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:30,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:31,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:33<01:46, 13.37s/it][WARNING|generation_utils.py:914] 2023-08-28 19:51:31,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:32,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:32,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:33,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:34,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:34,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:35,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:35,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:36,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:37,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:37,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:38,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:38,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:39,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:40,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:40,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:41,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:41,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:42,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:42,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:43,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:44,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:44,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:47<01:34, 13.45s/it][WARNING|generation_utils.py:914] 2023-08-28 19:51:45,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:45,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:46,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:47,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:47,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:48,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:49,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:49,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:50,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:51,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:52,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:52,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:53,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:53,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:54,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:55,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:55,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:56,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:57,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:57,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:58,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:00<01:20, 13.44s/it][WARNING|generation_utils.py:914] 2023-08-28 19:51:58,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:59,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:00,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:00,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:01,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:01,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:02,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:03,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:03,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:04,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:05,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:06,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:06,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:07,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:07,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:08,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:09,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:10,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:10,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:11,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:14<01:07, 13.49s/it][WARNING|generation_utils.py:914] 2023-08-28 19:52:12,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:12,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:13,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:13,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:14,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:15,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:15,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:16,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:16,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:17,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:18,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:18,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:19,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:19,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:20,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:20,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:21,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:21,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:22,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:22,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:25<00:51, 12.75s/it][WARNING|generation_utils.py:914] 2023-08-28 19:52:23,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:24,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:24,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:25,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:26,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:27,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:27,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:28,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:29,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:30,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:30,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:31,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:31,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:32,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:33,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:33,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:34,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:34,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:35,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:36,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:36,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:37,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:39<00:39, 13.32s/it][WARNING|generation_utils.py:914] 2023-08-28 19:52:38,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:38,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:39,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:40,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:40,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:41,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:42,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:42,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:43,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:44,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:45,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:46,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:46,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:47,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:48,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:48,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:49,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:50,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:50,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:51,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:52,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:54<00:27, 13.67s/it][WARNING|generation_utils.py:914] 2023-08-28 19:52:53,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:53,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:54,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:54,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:55,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:56,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:57,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:57,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:58,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:58,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:59,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:00,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:01,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:01,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:02,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:03,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:03,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:04,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:05,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:05,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:07<00:13, 13.66s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:06,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:06,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:07,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:08,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:08,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:09,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:09,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:11,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:11,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:12,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:12,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:13,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:14,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:14,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:15,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:15,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:16,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:17,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:18,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:18,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:19,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:20,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:20,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:21,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:22,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:22,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:23,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:24,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:26<00:00, 15.11s/it]Generating: 100%|██████████| 15/15 [03:26<00:00, 13.76s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:35,499 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:35,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:35,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:35,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:35,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:53:35,934 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:53:35,935 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:53:36,231 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:53:37,322 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:53:37,322 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:39,533 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:39,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:39,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:39,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:39,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:53:40,867 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:53:40,868 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:53:41,241 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:53:41,414 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:53:41,414 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8928571428571429, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9300595238095238, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : architect .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.8288043478260869, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9609375, 'errors': {''}}
['Relation : member of political party . Context : On 31 March 2014 , the party announced that it would be joining the coalition government at the end of 2015 , led by Premier Yashwant K. Singh , led by Chief Minister Manohar Chaudhary . Head Entity : Yashwant K Singh , Tail Entity : Premier .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8778409090909091, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : operator .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9578125, 'errors': {''}}
['Relation : position held . Context : On 31 March 2014 , the Brazilian national football team won their first match of the 2014 FIFA Confederations Cup , beating FC Porto in the quarter - final of the game . Head Entity : FC Porto , Tail Entity : striker .\n']
['Relation : position held . Context : On 31 March 2014 , the Brazilian national football team won their first match of the 2014 FIFA Confederations Cup , beating FC Porto in the quarter - final of the game . Head Entity : FC Porto , Tail Entity : striker .\n', 'Relation : position held . Context : After he was drafted by the Denver Broncos in 1999 , he entered the NFL Draft with the 49ers , who selected him with the No. 5 overall pick . Head Entity : 49ers , Tail Entity : cornerback .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 163, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 226, 'raw': 352}
{'target': 600, 'success': 248, 'raw': 384}
{'target': 600, 'success': 268, 'raw': 416}
{'target': 600, 'success': 291, 'raw': 448}
{'target': 600, 'success': 311, 'raw': 480}
{'target': 600, 'success': 331, 'raw': 512}
{'target': 600, 'success': 349, 'raw': 544}
{'target': 600, 'success': 372, 'raw': 576}
{'target': 600, 'success': 394, 'raw': 608}
{'target': 600, 'success': 419, 'raw': 640}
{'target': 600, 'success': 442, 'raw': 672}
{'target': 600, 'success': 465, 'raw': 704}
{'target': 600, 'success': 484, 'raw': 736}
{'target': 600, 'success': 506, 'raw': 768}
{'target': 600, 'success': 528, 'raw': 800}
{'target': 600, 'success': 553, 'raw': 832}
{'target': 600, 'success': 575, 'raw': 864}
{'target': 600, 'success': 600, 'raw': 896}
{'prompt': 'Relation : position held .', 'success_rate': 0.6696428571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 10175
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10275, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.54it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:01,  1.66it/s]Extractor Estimating: 4it [00:02,  1.70it/s]Extractor Estimating: 5it [00:02,  1.76it/s]Extractor Estimating: 6it [00:03,  1.63it/s]Extractor Estimating: 7it [00:04,  1.61it/s]Extractor Estimating: 8it [00:04,  1.69it/s]Extractor Estimating: 9it [00:05,  1.66it/s]Extractor Estimating: 10it [00:06,  1.70it/s]Extractor Estimating: 11it [00:06,  1.61it/s]Extractor Estimating: 12it [00:07,  1.68it/s]Extractor Estimating: 13it [00:07,  1.67it/s]Extractor Estimating: 14it [00:08,  1.72it/s]Extractor Estimating: 15it [00:08,  1.75it/s]Extractor Estimating: 16it [00:09,  1.75it/s]Extractor Estimating: 17it [00:10,  1.48it/s]Extractor Estimating: 18it [00:11,  1.50it/s]Extractor Estimating: 19it [00:11,  1.56it/s]Extractor Estimating: 20it [00:12,  1.59it/s]Extractor Estimating: 21it [00:12,  1.65it/s]Extractor Estimating: 22it [00:13,  1.67it/s]Extractor Estimating: 23it [00:14,  1.54it/s]Extractor Estimating: 24it [00:14,  1.56it/s]Extractor Estimating: 25it [00:15,  1.49it/s]Extractor Estimating: 26it [00:16,  1.59it/s]Extractor Estimating: 27it [00:16,  1.64it/s]Extractor Estimating: 28it [00:17,  1.35it/s]Extractor Estimating: 29it [00:18,  1.49it/s]Extractor Estimating: 30it [00:18,  1.55it/s]Extractor Estimating: 31it [00:19,  1.60it/s]Extractor Estimating: 32it [00:19,  1.69it/s]Extractor Estimating: 33it [00:21,  1.28it/s]Extractor Estimating: 34it [00:21,  1.36it/s]Extractor Estimating: 35it [00:22,  1.48it/s]Extractor Estimating: 36it [00:22,  1.57it/s]Extractor Estimating: 37it [00:23,  1.59it/s]Extractor Estimating: 38it [00:24,  1.28it/s]Extractor Estimating: 39it [00:25,  1.43it/s]Extractor Estimating: 40it [00:25,  1.47it/s]Extractor Estimating: 41it [00:26,  1.58it/s]Extractor Estimating: 42it [00:26,  1.57it/s]Extractor Estimating: 43it [00:27,  1.35it/s]Extractor Estimating: 44it [00:28,  1.46it/s]Extractor Estimating: 45it [00:29,  1.44it/s]Extractor Estimating: 46it [00:29,  1.50it/s]Extractor Estimating: 47it [00:30,  1.55it/s]Extractor Estimating: 48it [00:31,  1.49it/s]Extractor Estimating: 49it [00:31,  1.59it/s]Extractor Estimating: 50it [00:32,  1.63it/s]Extractor Estimating: 51it [00:32,  1.51it/s]Extractor Estimating: 52it [00:33,  1.47it/s]Extractor Estimating: 53it [00:34,  1.20it/s]Extractor Estimating: 54it [00:35,  1.25it/s]Extractor Estimating: 55it [00:36,  1.33it/s]Extractor Estimating: 56it [00:36,  1.38it/s]Extractor Estimating: 57it [00:37,  1.35it/s]Extractor Estimating: 58it [00:38,  1.41it/s]Extractor Estimating: 59it [00:38,  1.44it/s]Extractor Estimating: 60it [00:39,  1.46it/s]Extractor Estimating: 61it [00:40,  1.52it/s]Extractor Estimating: 62it [00:41,  1.33it/s]Extractor Estimating: 63it [00:41,  1.33it/s]Extractor Estimating: 64it [00:42,  1.37it/s]Extractor Estimating: 65it [00:43,  1.42it/s]Extractor Estimating: 66it [00:43,  1.45it/s]Extractor Estimating: 67it [00:44,  1.52it/s]Extractor Estimating: 68it [00:45,  1.48it/s]Extractor Estimating: 69it [00:45,  1.47it/s]Extractor Estimating: 70it [00:46,  1.49it/s]Extractor Estimating: 71it [00:47,  1.50it/s]Extractor Estimating: 72it [00:47,  1.56it/s]Extractor Estimating: 73it [00:48,  1.38it/s]Extractor Estimating: 74it [00:49,  1.40it/s]Extractor Estimating: 75it [00:50,  1.43it/s]Extractor Estimating: 76it [00:50,  1.52it/s]Extractor Estimating: 77it [00:51,  1.60it/s]Extractor Estimating: 78it [00:51,  1.55it/s]Extractor Estimating: 79it [00:52,  1.66it/s]Extractor Estimating: 80it [00:52,  1.74it/s]Extractor Estimating: 81it [00:53,  1.78it/s]Extractor Estimating: 82it [00:53,  1.81it/s]Extractor Estimating: 83it [00:54,  1.87it/s]Extractor Estimating: 84it [00:54,  1.86it/s]Extractor Estimating: 85it [00:55,  1.90it/s]Extractor Estimating: 86it [00:55,  1.89it/s]Extractor Estimating: 87it [00:56,  1.90it/s]Extractor Estimating: 88it [00:56,  2.00it/s]Extractor Estimating: 89it [00:57,  1.97it/s]Extractor Estimating: 90it [00:58,  1.85it/s]Extractor Estimating: 91it [00:58,  1.91it/s]Extractor Estimating: 92it [00:59,  1.91it/s]Extractor Estimating: 93it [00:59,  1.90it/s]Extractor Estimating: 94it [01:00,  1.86it/s]Extractor Estimating: 95it [01:00,  1.88it/s]Extractor Estimating: 96it [01:01,  1.50it/s]Extractor Estimating: 97it [01:02,  1.62it/s]Extractor Estimating: 98it [01:02,  1.67it/s]Extractor Estimating: 99it [01:03,  1.71it/s]Extractor Estimating: 100it [01:03,  1.66it/s]Extractor Estimating: 101it [01:04,  1.63it/s]Extractor Estimating: 102it [01:05,  1.68it/s]Extractor Estimating: 103it [01:05,  1.69it/s]Extractor Estimating: 104it [01:06,  1.70it/s]Extractor Estimating: 105it [01:06,  1.76it/s]Extractor Estimating: 106it [01:07,  1.79it/s]Extractor Estimating: 107it [01:08,  1.48it/s]Extractor Estimating: 108it [01:08,  1.57it/s]Extractor Estimating: 109it [01:09,  1.66it/s]Extractor Estimating: 110it [01:09,  1.68it/s]Extractor Estimating: 111it [01:10,  1.77it/s]Extractor Estimating: 112it [01:11,  1.67it/s]Extractor Estimating: 113it [01:11,  1.72it/s]Extractor Estimating: 114it [01:12,  1.67it/s]Extractor Estimating: 115it [01:12,  1.72it/s]Extractor Estimating: 116it [01:13,  1.74it/s]Extractor Estimating: 117it [01:13,  1.80it/s]Extractor Estimating: 118it [01:14,  1.87it/s]Extractor Estimating: 119it [01:14,  1.86it/s]Extractor Estimating: 120it [01:15,  1.48it/s]Extractor Estimating: 121it [01:16,  1.58it/s]Extractor Estimating: 122it [01:16,  1.70it/s]Extractor Estimating: 123it [01:17,  1.74it/s]Extractor Estimating: 124it [01:18,  1.79it/s]Extractor Estimating: 125it [01:19,  1.46it/s]Extractor Estimating: 126it [01:19,  1.53it/s]Extractor Estimating: 127it [01:20,  1.61it/s]Extractor Estimating: 128it [01:20,  1.62it/s]Extractor Estimating: 129it [01:21,  1.69it/s]Extractor Estimating: 130it [01:22,  1.50it/s]Extractor Estimating: 131it [01:22,  1.59it/s]Extractor Estimating: 132it [01:23,  1.69it/s]Extractor Estimating: 133it [01:23,  1.71it/s]Extractor Estimating: 134it [01:24,  1.69it/s]Extractor Estimating: 135it [01:24,  1.65it/s]Extractor Estimating: 136it [01:25,  1.69it/s]Extractor Estimating: 137it [01:26,  1.71it/s]Extractor Estimating: 138it [01:26,  1.76it/s]Extractor Estimating: 139it [01:27,  1.80it/s]Extractor Estimating: 140it [01:27,  1.81it/s]Extractor Estimating: 141it [01:28,  1.71it/s]Extractor Estimating: 142it [01:28,  1.70it/s]Extractor Estimating: 143it [01:29,  1.70it/s]Extractor Estimating: 144it [01:30,  1.67it/s]Extractor Estimating: 145it [01:30,  1.72it/s]Extractor Estimating: 146it [01:31,  1.62it/s]Extractor Estimating: 147it [01:32,  1.61it/s]Extractor Estimating: 148it [01:32,  1.63it/s]Extractor Estimating: 149it [01:33,  1.65it/s]Extractor Estimating: 150it [01:33,  1.72it/s]Extractor Estimating: 151it [01:34,  1.70it/s]Extractor Estimating: 152it [01:34,  1.76it/s]Extractor Estimating: 153it [01:35,  1.83it/s]Extractor Estimating: 154it [01:35,  1.76it/s]Extractor Estimating: 155it [01:36,  1.80it/s]Extractor Estimating: 156it [01:36,  1.86it/s]Extractor Estimating: 157it [01:37,  1.80it/s]Extractor Estimating: 158it [01:38,  1.88it/s]Extractor Estimating: 159it [01:38,  1.94it/s]Extractor Estimating: 160it [01:39,  1.87it/s]Extractor Estimating: 161it [01:39,  1.87it/s]Extractor Estimating: 162it [01:40,  1.90it/s]Extractor Estimating: 163it [01:40,  1.81it/s]Extractor Estimating: 164it [01:41,  1.85it/s]Extractor Estimating: 165it [01:41,  1.89it/s]Extractor Estimating: 166it [01:42,  1.88it/s]Extractor Estimating: 167it [01:42,  1.83it/s]Extractor Estimating: 168it [01:43,  1.83it/s]Extractor Estimating: 169it [01:44,  1.83it/s]Extractor Estimating: 170it [01:44,  1.81it/s]Extractor Estimating: 171it [01:45,  1.82it/s]Extractor Estimating: 172it [01:45,  1.84it/s]Extractor Estimating: 173it [01:46,  1.84it/s]Extractor Estimating: 174it [01:46,  1.84it/s]Extractor Estimating: 175it [01:47,  1.85it/s]Extractor Estimating: 176it [01:47,  1.79it/s]Extractor Estimating: 177it [01:48,  1.83it/s]Extractor Estimating: 178it [01:48,  1.84it/s]Extractor Estimating: 179it [01:49,  1.66it/s]Extractor Estimating: 180it [01:50,  1.74it/s]Extractor Estimating: 181it [01:50,  1.76it/s]Extractor Estimating: 182it [01:51,  1.79it/s]Extractor Estimating: 183it [01:51,  1.79it/s]Extractor Estimating: 184it [01:52,  1.78it/s]Extractor Estimating: 185it [01:53,  1.66it/s]Extractor Estimating: 186it [01:53,  1.71it/s]Extractor Estimating: 187it [01:54,  1.75it/s]Extractor Estimating: 188it [01:54,  1.79it/s]Extractor Estimating: 189it [01:55,  1.79it/s]Extractor Estimating: 190it [01:55,  1.75it/s]Extractor Estimating: 191it [01:56,  1.63it/s]Extractor Estimating: 192it [01:57,  1.72it/s]Extractor Estimating: 193it [01:57,  1.74it/s]Extractor Estimating: 194it [01:58,  1.78it/s]Extractor Estimating: 195it [01:58,  1.80it/s]Extractor Estimating: 196it [01:59,  1.77it/s]Extractor Estimating: 197it [02:00,  1.53it/s]Extractor Estimating: 198it [02:00,  1.62it/s]Extractor Estimating: 199it [02:01,  1.66it/s]Extractor Estimating: 200it [02:01,  1.70it/s]Extractor Estimating: 201it [02:02,  1.64it/s]Extractor Estimating: 202it [02:03,  1.30it/s]Extractor Estimating: 203it [02:04,  1.36it/s]Extractor Estimating: 204it [02:04,  1.44it/s]Extractor Estimating: 205it [02:05,  1.49it/s]Extractor Estimating: 206it [02:06,  1.56it/s]Extractor Estimating: 207it [02:06,  1.43it/s]Extractor Estimating: 208it [02:07,  1.48it/s]Extractor Estimating: 209it [02:08,  1.46it/s]Extractor Estimating: 210it [02:08,  1.51it/s]Extractor Estimating: 211it [02:09,  1.54it/s]Extractor Estimating: 212it [02:10,  1.40it/s]Extractor Estimating: 213it [02:10,  1.48it/s]Extractor Estimating: 214it [02:11,  1.51it/s]Extractor Estimating: 215it [02:12,  1.57it/s]Extractor Estimating: 216it [02:12,  1.62it/s]Extractor Estimating: 217it [02:14,  1.08it/s]Extractor Estimating: 218it [02:14,  1.20it/s]Extractor Estimating: 219it [02:15,  1.28it/s]Extractor Estimating: 220it [02:16,  1.35it/s]Extractor Estimating: 221it [02:17,  1.27it/s]Extractor Estimating: 222it [02:17,  1.39it/s]Extractor Estimating: 223it [02:18,  1.47it/s]Extractor Estimating: 224it [02:18,  1.49it/s]Extractor Estimating: 225it [02:19,  1.53it/s]Extractor Estimating: 226it [02:20,  1.36it/s]Extractor Estimating: 227it [02:21,  1.41it/s]Extractor Estimating: 228it [02:21,  1.46it/s]Extractor Estimating: 229it [02:22,  1.46it/s]Extractor Estimating: 230it [02:23,  1.47it/s]Extractor Estimating: 231it [02:23,  1.44it/s]Extractor Estimating: 232it [02:24,  1.48it/s]Extractor Estimating: 233it [02:25,  1.51it/s]Extractor Estimating: 234it [02:25,  1.50it/s]Extractor Estimating: 235it [02:26,  1.52it/s]Extractor Estimating: 236it [02:27,  1.44it/s]Extractor Estimating: 237it [02:27,  1.45it/s]Extractor Estimating: 238it [02:28,  1.48it/s]Extractor Estimating: 239it [02:29,  1.47it/s]Extractor Estimating: 240it [02:29,  1.51it/s]Extractor Estimating: 241it [02:30,  1.33it/s]Extractor Estimating: 242it [02:31,  1.43it/s]Extractor Estimating: 243it [02:32,  1.42it/s]Extractor Estimating: 244it [02:32,  1.39it/s]Extractor Estimating: 245it [02:33,  1.40it/s]Extractor Estimating: 246it [02:34,  1.45it/s]Extractor Estimating: 247it [02:34,  1.44it/s]Extractor Estimating: 248it [02:35,  1.45it/s]Extractor Estimating: 249it [02:36,  1.47it/s]Extractor Estimating: 250it [02:37,  1.38it/s]Extractor Estimating: 251it [02:37,  1.59it/s]Extractor Estimating: 252it [02:37,  1.80it/s]Extractor Estimating: 253it [02:38,  1.96it/s]Extractor Estimating: 254it [02:38,  2.05it/s]Extractor Estimating: 255it [02:39,  2.14it/s]Extractor Estimating: 256it [02:39,  2.21it/s]Extractor Estimating: 257it [02:40,  2.00it/s]Extractor Estimating: 258it [02:40,  2.12it/s]Extractor Estimating: 259it [02:40,  2.14it/s]Extractor Estimating: 260it [02:41,  2.12it/s]Extractor Estimating: 261it [02:41,  2.11it/s]Extractor Estimating: 262it [02:42,  2.09it/s]Extractor Estimating: 263it [02:42,  2.16it/s]Extractor Estimating: 264it [02:43,  1.93it/s]Extractor Estimating: 265it [02:43,  2.04it/s]Extractor Estimating: 266it [02:44,  1.92it/s]Extractor Estimating: 267it [02:44,  2.01it/s]Extractor Estimating: 268it [02:45,  2.11it/s]Extractor Estimating: 269it [02:45,  2.22it/s]Extractor Estimating: 270it [02:46,  2.27it/s]Extractor Estimating: 271it [02:46,  2.23it/s]Extractor Estimating: 272it [02:47,  2.24it/s]Extractor Estimating: 273it [02:47,  2.15it/s]Extractor Estimating: 274it [02:48,  2.16it/s]Extractor Estimating: 275it [02:48,  2.17it/s]Extractor Estimating: 276it [02:49,  1.97it/s]Extractor Estimating: 277it [02:49,  1.83it/s]Extractor Estimating: 278it [02:50,  1.79it/s]Extractor Estimating: 279it [02:51,  1.66it/s]Extractor Estimating: 280it [02:51,  1.69it/s]Extractor Estimating: 281it [02:52,  1.68it/s]Extractor Estimating: 282it [02:52,  1.62it/s]Extractor Estimating: 283it [02:53,  1.61it/s]Extractor Estimating: 284it [02:54,  1.40it/s]Extractor Estimating: 285it [02:55,  1.48it/s]Extractor Estimating: 286it [02:55,  1.51it/s]Extractor Estimating: 287it [02:56,  1.56it/s]Extractor Estimating: 288it [02:56,  1.58it/s]Extractor Estimating: 289it [02:57,  1.36it/s]Extractor Estimating: 290it [02:58,  1.43it/s]Extractor Estimating: 291it [02:59,  1.42it/s]Extractor Estimating: 292it [02:59,  1.46it/s]Extractor Estimating: 293it [03:00,  1.53it/s]Extractor Estimating: 294it [03:01,  1.51it/s]Extractor Estimating: 295it [03:01,  1.58it/s]Extractor Estimating: 296it [03:02,  1.58it/s]Extractor Estimating: 297it [03:02,  1.64it/s]Extractor Estimating: 298it [03:03,  1.61it/s]Extractor Estimating: 299it [03:04,  1.47it/s]Extractor Estimating: 300it [03:04,  1.54it/s]Extractor Estimating: 301it [03:05,  1.59it/s]Extractor Estimating: 302it [03:06,  1.58it/s]Extractor Estimating: 303it [03:06,  1.57it/s]Extractor Estimating: 304it [03:07,  1.53it/s]Extractor Estimating: 305it [03:08,  1.54it/s]Extractor Estimating: 306it [03:08,  1.58it/s]Extractor Estimating: 307it [03:09,  1.60it/s]Extractor Estimating: 308it [03:09,  1.57it/s]Extractor Estimating: 309it [03:10,  1.48it/s]Extractor Estimating: 310it [03:11,  1.54it/s]Extractor Estimating: 311it [03:11,  1.58it/s]Extractor Estimating: 312it [03:12,  1.61it/s]Extractor Estimating: 313it [03:13,  1.63it/s]Extractor Estimating: 314it [03:13,  1.50it/s]Extractor Estimating: 315it [03:14,  1.53it/s]Extractor Estimating: 316it [03:15,  1.57it/s]Extractor Estimating: 317it [03:15,  1.63it/s]Extractor Estimating: 318it [03:16,  1.66it/s]Extractor Estimating: 319it [03:16,  1.70it/s]Extractor Estimating: 320it [03:17,  1.71it/s]Extractor Estimating: 321it [03:17,  1.72it/s]Extractor Estimating: 322it [03:18,  1.39it/s]Extractor Estimating: 323it [03:19,  1.44it/s]Extractor Estimating: 324it [03:20,  1.51it/s]Extractor Estimating: 325it [03:20,  1.61it/s]Extractor Estimating: 326it [03:21,  1.62it/s]Extractor Estimating: 327it [03:22,  1.39it/s]Extractor Estimating: 328it [03:22,  1.45it/s]Extractor Estimating: 329it [03:23,  1.47it/s]Extractor Estimating: 330it [03:24,  1.54it/s]Extractor Estimating: 331it [03:24,  1.52it/s]Extractor Estimating: 332it [03:25,  1.44it/s]Extractor Estimating: 333it [03:26,  1.52it/s]Extractor Estimating: 334it [03:26,  1.59it/s]Extractor Estimating: 335it [03:27,  1.58it/s]Extractor Estimating: 336it [03:27,  1.61it/s]Extractor Estimating: 337it [03:29,  1.33it/s]Extractor Estimating: 338it [03:29,  1.47it/s]Extractor Estimating: 339it [03:30,  1.56it/s]Extractor Estimating: 340it [03:30,  1.61it/s]Extractor Estimating: 341it [03:31,  1.64it/s]Extractor Estimating: 342it [03:32,  1.45it/s]Extractor Estimating: 343it [03:32,  1.53it/s]Extractor Estimating: 344it [03:33,  1.50it/s]Extractor Estimating: 345it [03:33,  1.56it/s]Extractor Estimating: 346it [03:34,  1.65it/s]Extractor Estimating: 347it [03:35,  1.56it/s]Extractor Estimating: 348it [03:35,  1.63it/s]Extractor Estimating: 349it [03:36,  1.62it/s]Extractor Estimating: 350it [03:37,  1.59it/s]Extractor Estimating: 351it [03:37,  1.62it/s]Extractor Estimating: 352it [03:38,  1.55it/s]Extractor Estimating: 353it [03:38,  1.59it/s]Extractor Estimating: 354it [03:39,  1.60it/s]Extractor Estimating: 355it [03:40,  1.64it/s]Extractor Estimating: 356it [03:40,  1.60it/s]Extractor Estimating: 357it [03:41,  1.32it/s]Extractor Estimating: 358it [03:42,  1.44it/s]Extractor Estimating: 359it [03:43,  1.50it/s]Extractor Estimating: 360it [03:43,  1.57it/s]Extractor Estimating: 361it [03:44,  1.60it/s]Extractor Estimating: 362it [03:44,  1.63it/s]Extractor Estimating: 363it [03:45,  1.67it/s]Extractor Estimating: 364it [03:45,  1.68it/s]Extractor Estimating: 365it [03:46,  1.74it/s]Extractor Estimating: 366it [03:47,  1.71it/s]Extractor Estimating: 367it [03:47,  1.68it/s]Extractor Estimating: 368it [03:48,  1.74it/s]Extractor Estimating: 369it [03:48,  1.71it/s]Extractor Estimating: 370it [03:49,  1.57it/s]Extractor Estimating: 371it [03:50,  1.63it/s]Extractor Estimating: 372it [03:50,  1.65it/s]Extractor Estimating: 373it [03:51,  1.53it/s]Extractor Estimating: 374it [03:52,  1.53it/s]Extractor Estimating: 375it [03:52,  1.54it/s]Extractor Estimating: 375it [03:52,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:09,042 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:09,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:09,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:09,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:09,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:58:10,059 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:58:10,060 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:58:10,442 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:58:11,725 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:58:11,726 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:15,321 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:15,441 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:15,441 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:15,441 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:15,441 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:58:16,333 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:58:16,334 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:58:16,932 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:58:17,102 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:58:17,102 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:13:17,356 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:13:17,452 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 4500 mean pseudo reward: 0.9830684039957559
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 15661
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15761, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15761, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.946, loss:233.2170
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 12, avg_time 0.960, loss:229.5735
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 112, avg_time 0.974, loss:211.8445
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 24, avg_time 0.957, loss:211.7827
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 124, avg_time 0.973, loss:188.2418
>> valid entity prec:0.5131, rec:0.5354, f1:0.5240
>> valid relation prec:0.1646, rec:0.1245, f1:0.1418
>> valid relation with NER prec:0.1646, rec:0.1245, f1:0.1418
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 36, avg_time 2.209, loss:181.0045
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 136, avg_time 0.973, loss:188.6095
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 48, avg_time 0.957, loss:187.2915
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 148, avg_time 0.970, loss:185.2222
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 60, avg_time 0.958, loss:180.6692
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4776, rec:0.5532, f1:0.5127
>> valid relation prec:0.1521, rec:0.1371, f1:0.1442
>> valid relation with NER prec:0.1521, rec:0.1371, f1:0.1442
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 160, avg_time 2.185, loss:185.4576
g_step 1200, step 72, avg_time 0.965, loss:170.1605
g_step 1300, step 172, avg_time 0.972, loss:177.6800
g_step 1400, step 84, avg_time 0.964, loss:160.8954
g_step 1500, step 184, avg_time 0.968, loss:163.4321
>> valid entity prec:0.4874, rec:0.4865, f1:0.4869
>> valid relation prec:0.1478, rec:0.1142, f1:0.1289
>> valid relation with NER prec:0.1478, rec:0.1142, f1:0.1289
g_step 1600, step 96, avg_time 2.169, loss:137.8649
g_step 1700, step 8, avg_time 0.965, loss:154.5955
g_step 1800, step 108, avg_time 0.961, loss:137.8504
g_step 1900, step 20, avg_time 0.960, loss:135.8354
g_step 2000, step 120, avg_time 0.986, loss:125.6445
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5154, rec:0.4592, f1:0.4857
>> valid relation prec:0.1762, rec:0.1033, f1:0.1303
>> valid relation with NER prec:0.1762, rec:0.1033, f1:0.1303
g_step 2100, step 32, avg_time 2.151, loss:124.5070
g_step 2200, step 132, avg_time 0.975, loss:119.6708
g_step 2300, step 44, avg_time 0.962, loss:112.8443
g_step 2400, step 144, avg_time 0.967, loss:119.9161
g_step 2500, step 56, avg_time 0.964, loss:109.9624
>> valid entity prec:0.5154, rec:0.5309, f1:0.5230
>> valid relation prec:0.1647, rec:0.1308, f1:0.1458
>> valid relation with NER prec:0.1647, rec:0.1308, f1:0.1458
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 156, avg_time 2.201, loss:119.6534
g_step 2700, step 68, avg_time 0.961, loss:107.3502
g_step 2800, step 168, avg_time 0.977, loss:124.0261
g_step 2900, step 80, avg_time 0.959, loss:88.4803
g_step 3000, step 180, avg_time 0.965, loss:100.4183
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5206, rec:0.5299, f1:0.5252
>> valid relation prec:0.1677, rec:0.1337, f1:0.1488
>> valid relation with NER prec:0.1677, rec:0.1337, f1:0.1488
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 92, avg_time 2.187, loss:78.5422
g_step 3200, step 4, avg_time 0.971, loss:97.3573
g_step 3300, step 104, avg_time 0.959, loss:87.4097
g_step 3400, step 16, avg_time 0.966, loss:100.3963
g_step 3500, step 116, avg_time 0.973, loss:74.3188
>> valid entity prec:0.5357, rec:0.4995, f1:0.5169
>> valid relation prec:0.1704, rec:0.1157, f1:0.1378
>> valid relation with NER prec:0.1704, rec:0.1157, f1:0.1378
g_step 3600, step 28, avg_time 2.148, loss:84.6189
g_step 3700, step 128, avg_time 0.968, loss:76.6476
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:13:17 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:13:17 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-13-17_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:13:19 - WARNING - datasets.builder -   Using custom data configuration default-d6b6cc7eb2e8440e
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-d6b6cc7eb2e8440e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:13:26,539 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:13:26,875 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:13:26,875 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:13:26,876 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:13:27,902 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:28,047 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:28,047 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:28,047 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:28,047 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:28,048 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:28,048 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:13:29,719 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:13:32,938 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:13:32,966 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-d6b6cc7eb2e8440e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:03,  1.11ba/s] 40%|████      | 2/5 [00:01<00:01,  2.06ba/s] 60%|██████    | 3/5 [00:01<00:00,  2.83ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.41ba/s]100%|██████████| 5/5 [00:01<00:00,  4.46ba/s]100%|██████████| 5/5 [00:01<00:00,  3.14ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.82ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.36ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.47ba/s]100%|██████████| 4/4 [00:01<00:00,  2.19ba/s]100%|██████████| 4/4 [00:01<00:00,  2.02ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:01<00:04,  1.04s/ba] 60%|██████    | 3/5 [00:01<00:00,  2.97ba/s]100%|██████████| 5/5 [00:01<00:00,  5.01ba/s]100%|██████████| 5/5 [00:01<00:00,  3.67ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.52ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.96ba/s]100%|██████████| 4/4 [00:00<00:00,  5.84ba/s]100%|██████████| 4/4 [00:00<00:00,  4.37ba/s]
[INFO|trainer.py:414] 2023-08-28 21:13:44,078 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:13:44,685 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:13:44,685 >>   Num examples = 4500
[INFO|trainer.py:1149] 2023-08-28 21:13:44,685 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:13:44,685 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:13:44,685 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:13:44,685 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:13:44,685 >>   Total optimization steps = 350
  0%|          | 0/350 [00:00<?, ?it/s]  0%|          | 1/350 [00:00<01:47,  3.25it/s]  1%|          | 2/350 [00:00<01:44,  3.34it/s]  1%|          | 3/350 [00:00<01:42,  3.38it/s]  1%|          | 4/350 [00:01<01:42,  3.38it/s]  1%|▏         | 5/350 [00:01<01:41,  3.39it/s]  2%|▏         | 6/350 [00:01<01:41,  3.40it/s]  2%|▏         | 7/350 [00:02<01:40,  3.40it/s]  2%|▏         | 8/350 [00:02<01:40,  3.40it/s]  3%|▎         | 9/350 [00:02<01:40,  3.40it/s]  3%|▎         | 10/350 [00:03<01:45,  3.23it/s]  3%|▎         | 11/350 [00:03<01:43,  3.29it/s]  3%|▎         | 12/350 [00:03<01:41,  3.32it/s]  4%|▎         | 13/350 [00:03<01:40,  3.35it/s]  4%|▍         | 14/350 [00:04<01:39,  3.37it/s]  4%|▍         | 15/350 [00:04<01:39,  3.37it/s]  5%|▍         | 16/350 [00:04<01:38,  3.38it/s]  5%|▍         | 17/350 [00:05<01:38,  3.39it/s]  5%|▌         | 18/350 [00:05<01:37,  3.40it/s]  5%|▌         | 19/350 [00:05<01:37,  3.40it/s]  6%|▌         | 20/350 [00:05<01:37,  3.40it/s]  6%|▌         | 21/350 [00:06<01:42,  3.22it/s]  6%|▋         | 22/350 [00:06<01:40,  3.27it/s]  7%|▋         | 23/350 [00:06<01:38,  3.31it/s]  7%|▋         | 24/350 [00:07<01:37,  3.34it/s]  7%|▋         | 25/350 [00:07<01:36,  3.36it/s]  7%|▋         | 26/350 [00:07<01:36,  3.37it/s]  8%|▊         | 27/350 [00:08<01:35,  3.38it/s]  8%|▊         | 28/350 [00:08<01:35,  3.38it/s]  8%|▊         | 29/350 [00:08<01:34,  3.39it/s]  9%|▊         | 30/350 [00:08<01:34,  3.39it/s]  9%|▉         | 31/350 [00:09<01:34,  3.39it/s]  9%|▉         | 32/350 [00:09<01:33,  3.40it/s]  9%|▉         | 33/350 [00:09<01:33,  3.40it/s] 10%|▉         | 34/350 [00:10<01:32,  3.40it/s] 10%|█         | 35/350 [00:10<01:32,  3.40it/s] 10%|█         | 36/350 [00:10<01:32,  3.40it/s] 11%|█         | 37/350 [00:10<01:32,  3.40it/s] 11%|█         | 38/350 [00:11<01:34,  3.31it/s] 11%|█         | 39/350 [00:11<01:33,  3.34it/s] 11%|█▏        | 40/350 [00:11<01:32,  3.36it/s] 12%|█▏        | 41/350 [00:12<01:31,  3.37it/s] 12%|█▏        | 42/350 [00:12<01:31,  3.38it/s] 12%|█▏        | 43/350 [00:12<01:30,  3.39it/s] 13%|█▎        | 44/350 [00:13<01:30,  3.39it/s] 13%|█▎        | 45/350 [00:13<01:29,  3.39it/s] 13%|█▎        | 46/350 [00:13<01:29,  3.40it/s] 13%|█▎        | 47/350 [00:13<01:29,  3.40it/s] 14%|█▎        | 48/350 [00:14<01:28,  3.40it/s] 14%|█▍        | 49/350 [00:14<01:56,  2.59it/s] 14%|█▍        | 50/350 [00:15<01:47,  2.79it/s] 15%|█▍        | 51/350 [00:15<01:41,  2.95it/s] 15%|█▍        | 52/350 [00:15<01:37,  3.07it/s] 15%|█▌        | 53/350 [00:16<01:33,  3.16it/s] 15%|█▌        | 54/350 [00:16<01:31,  3.24it/s] 16%|█▌        | 55/350 [00:16<01:29,  3.29it/s] 16%|█▌        | 56/350 [00:16<01:28,  3.33it/s] 16%|█▋        | 57/350 [00:17<01:27,  3.35it/s] 17%|█▋        | 58/350 [00:17<01:26,  3.36it/s] 17%|█▋        | 59/350 [00:18<01:47,  2.72it/s] 17%|█▋        | 60/350 [00:18<01:40,  2.89it/s] 17%|█▋        | 61/350 [00:18<01:35,  3.03it/s] 18%|█▊        | 62/350 [00:19<01:53,  2.54it/s] 18%|█▊        | 63/350 [00:19<01:44,  2.75it/s] 18%|█▊        | 64/350 [00:19<01:37,  2.93it/s] 19%|█▊        | 65/350 [00:20<01:32,  3.07it/s] 19%|█▉        | 66/350 [00:20<01:29,  3.18it/s] 19%|█▉        | 67/350 [00:20<01:27,  3.25it/s] 19%|█▉        | 68/350 [00:21<02:04,  2.27it/s] 20%|█▉        | 69/350 [00:21<01:51,  2.53it/s] 20%|██        | 70/350 [00:21<01:41,  2.75it/s][INFO|trainer.py:2140] 2023-08-28 21:14:06,664 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:14:06,664 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:14:06,664 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.81it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.66it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.21it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.43it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.79it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.52it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.32it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.22it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.22it/s][A
 12%|█▏        | 52/437 [00:01<00:16, 22.79it/s][A
 13%|█▎        | 57/437 [00:01<00:16, 23.15it/s][A
 14%|█▍        | 61/437 [00:01<00:16, 23.50it/s][A
 15%|█▌        | 66/437 [00:01<00:13, 27.69it/s][A
 16%|█▌        | 71/437 [00:02<00:11, 31.30it/s][A
 17%|█▋        | 76/437 [00:02<00:10, 34.53it/s][A
 19%|█▊        | 81/437 [00:02<00:09, 37.10it/s][A
 20%|█▉        | 86/437 [00:02<00:08, 39.15it/s][A
 21%|██        | 91/437 [00:02<00:08, 40.65it/s][A
 22%|██▏       | 96/437 [00:02<00:08, 41.65it/s][A
 23%|██▎       | 101/437 [00:02<00:08, 42.00it/s][A
 24%|██▍       | 106/437 [00:02<00:07, 42.32it/s][A
 25%|██▌       | 111/437 [00:03<00:07, 42.74it/s][A
 27%|██▋       | 116/437 [00:03<00:07, 43.25it/s][A
 28%|██▊       | 121/437 [00:03<00:07, 43.71it/s][A
 29%|██▉       | 126/437 [00:03<00:07, 44.05it/s][A
 30%|██▉       | 131/437 [00:03<00:06, 44.17it/s][A
 31%|███       | 136/437 [00:03<00:06, 44.31it/s][A
 32%|███▏      | 141/437 [00:03<00:06, 44.11it/s][A
 33%|███▎      | 146/437 [00:03<00:06, 43.78it/s][A
 35%|███▍      | 151/437 [00:03<00:06, 43.52it/s][A
 36%|███▌      | 156/437 [00:04<00:06, 43.70it/s][A
 37%|███▋      | 161/437 [00:04<00:06, 43.81it/s][A
 38%|███▊      | 166/437 [00:04<00:06, 44.21it/s][A
 39%|███▉      | 171/437 [00:04<00:06, 44.13it/s][A
 40%|████      | 176/437 [00:05<00:12, 20.53it/s][A
 41%|████      | 180/437 [00:05<00:16, 16.00it/s][A
 42%|████▏     | 183/437 [00:05<00:17, 14.16it/s][A
 43%|████▎     | 188/437 [00:05<00:13, 18.49it/s][A
 44%|████▍     | 193/437 [00:05<00:10, 22.82it/s][A
 45%|████▌     | 198/437 [00:05<00:08, 27.03it/s][A
 46%|████▋     | 203/437 [00:06<00:07, 30.86it/s][A
 48%|████▊     | 208/437 [00:06<00:06, 34.10it/s][A
 49%|████▊     | 213/437 [00:06<00:06, 36.68it/s][A
 50%|████▉     | 218/437 [00:06<00:05, 38.95it/s][A
 51%|█████     | 223/437 [00:06<00:05, 40.08it/s][A
 52%|█████▏    | 228/437 [00:06<00:05, 40.81it/s][A
 53%|█████▎    | 233/437 [00:06<00:06, 32.96it/s][A
 54%|█████▍    | 238/437 [00:06<00:05, 35.82it/s][A
 56%|█████▌    | 243/437 [00:07<00:05, 38.09it/s][A
 57%|█████▋    | 248/437 [00:07<00:04, 39.93it/s][A
 58%|█████▊    | 253/437 [00:07<00:04, 41.28it/s][A
 59%|█████▉    | 258/437 [00:07<00:04, 42.22it/s][A
 60%|██████    | 263/437 [00:07<00:04, 42.97it/s][A
 61%|██████▏   | 268/437 [00:07<00:03, 43.28it/s][A
 62%|██████▏   | 273/437 [00:07<00:03, 43.04it/s][A
 64%|██████▎   | 278/437 [00:07<00:03, 42.93it/s][A
 65%|██████▍   | 283/437 [00:08<00:03, 43.04it/s][A
 66%|██████▌   | 288/437 [00:08<00:06, 21.60it/s][A
 67%|██████▋   | 293/437 [00:08<00:05, 25.85it/s][A
 68%|██████▊   | 298/437 [00:09<00:13, 10.21it/s][A
 69%|██████▉   | 303/437 [00:09<00:10, 13.38it/s][A
 70%|███████   | 308/437 [00:09<00:07, 16.93it/s][A
 72%|███████▏  | 313/437 [00:10<00:05, 20.83it/s][A
 73%|███████▎  | 318/437 [00:10<00:04, 24.76it/s][A
 74%|███████▍  | 323/437 [00:10<00:03, 28.58it/s][A
 75%|███████▌  | 328/437 [00:10<00:03, 31.98it/s][A
 76%|███████▌  | 333/437 [00:10<00:02, 35.02it/s][A
 77%|███████▋  | 338/437 [00:10<00:02, 37.30it/s][A
 78%|███████▊  | 343/437 [00:10<00:02, 38.76it/s][A
 80%|███████▉  | 348/437 [00:10<00:02, 39.96it/s][A
 81%|████████  | 353/437 [00:11<00:02, 41.12it/s][A
 82%|████████▏ | 358/437 [00:11<00:03, 26.17it/s][A
 83%|████████▎ | 363/437 [00:11<00:02, 29.88it/s][A
 84%|████████▍ | 368/437 [00:11<00:02, 33.25it/s][A
 85%|████████▌ | 373/437 [00:11<00:01, 35.97it/s][A
 86%|████████▋ | 378/437 [00:11<00:01, 38.22it/s][A
 88%|████████▊ | 383/437 [00:11<00:01, 39.70it/s][A
 89%|████████▉ | 388/437 [00:12<00:01, 41.23it/s][A
 90%|████████▉ | 393/437 [00:12<00:01, 41.91it/s][A
 91%|█████████ | 398/437 [00:12<00:00, 41.99it/s][A
 92%|█████████▏| 403/437 [00:12<00:00, 42.40it/s][A
 93%|█████████▎| 408/437 [00:12<00:00, 42.68it/s][A
 95%|█████████▍| 413/437 [00:12<00:00, 43.22it/s][A
 96%|█████████▌| 418/437 [00:12<00:00, 43.78it/s][A
 97%|█████████▋| 423/437 [00:12<00:00, 43.93it/s][A
 98%|█████████▊| 428/437 [00:12<00:00, 44.16it/s][A
 99%|█████████▉| 433/437 [00:13<00:00, 44.14it/s][A                                                
                                                 [A 20%|██        | 70/350 [00:35<01:41,  2.75it/s]
100%|██████████| 437/437 [00:13<00:00, 44.14it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:14:20,849 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-70
[INFO|configuration_utils.py:351] 2023-08-28 21:14:21,918 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-70/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:14:30,308 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-70/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:14:30,657 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-70/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:14:31,277 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-70/special_tokens_map.json
 20%|██        | 71/350 [01:09<1:07:13, 14.46s/it] 21%|██        | 72/350 [01:10<48:02, 10.37s/it]   21%|██        | 73/350 [01:10<33:54,  7.35s/it] 21%|██        | 74/350 [01:10<24:03,  5.23s/it] 21%|██▏       | 75/350 [01:10<17:10,  3.75s/it] 22%|██▏       | 76/350 [01:11<12:22,  2.71s/it] 22%|██▏       | 77/350 [01:11<09:01,  1.98s/it] 22%|██▏       | 78/350 [01:11<06:41,  1.48s/it] 23%|██▎       | 79/350 [01:12<05:03,  1.12s/it] 23%|██▎       | 80/350 [01:12<03:54,  1.15it/s] 23%|██▎       | 81/350 [01:12<03:17,  1.36it/s] 23%|██▎       | 82/350 [01:13<02:40,  1.67it/s] 24%|██▎       | 83/350 [01:13<02:15,  1.97it/s] 24%|██▍       | 84/350 [01:13<01:57,  2.27it/s] 24%|██▍       | 85/350 [01:13<01:44,  2.53it/s] 25%|██▍       | 86/350 [01:14<01:35,  2.75it/s] 25%|██▍       | 87/350 [01:14<01:29,  2.93it/s] 25%|██▌       | 88/350 [01:14<01:25,  3.08it/s] 25%|██▌       | 89/350 [01:15<01:22,  3.18it/s] 26%|██▌       | 90/350 [01:15<01:19,  3.26it/s] 26%|██▌       | 91/350 [01:15<01:25,  3.04it/s] 26%|██▋       | 92/350 [01:16<01:21,  3.16it/s] 27%|██▋       | 93/350 [01:16<01:19,  3.25it/s] 27%|██▋       | 94/350 [01:16<01:17,  3.31it/s] 27%|██▋       | 95/350 [01:16<01:16,  3.35it/s] 27%|██▋       | 96/350 [01:17<01:15,  3.38it/s] 28%|██▊       | 97/350 [01:17<01:14,  3.40it/s] 28%|██▊       | 98/350 [01:17<01:13,  3.42it/s] 28%|██▊       | 99/350 [01:18<01:13,  3.44it/s] 29%|██▊       | 100/350 [01:18<01:12,  3.45it/s] 29%|██▉       | 101/350 [01:18<01:12,  3.45it/s] 29%|██▉       | 102/350 [01:19<01:51,  2.23it/s] 29%|██▉       | 103/350 [01:19<01:38,  2.50it/s] 30%|██▉       | 104/350 [01:20<01:30,  2.73it/s] 30%|███       | 105/350 [01:20<01:24,  2.91it/s] 30%|███       | 106/350 [01:20<01:19,  3.06it/s] 31%|███       | 107/350 [01:20<01:16,  3.17it/s] 31%|███       | 108/350 [01:21<01:14,  3.25it/s] 31%|███       | 109/350 [01:21<01:12,  3.31it/s] 31%|███▏      | 110/350 [01:21<01:11,  3.36it/s] 32%|███▏      | 111/350 [01:22<01:31,  2.62it/s] 32%|███▏      | 112/350 [01:22<01:24,  2.83it/s] 32%|███▏      | 113/350 [01:22<01:19,  2.99it/s] 33%|███▎      | 114/350 [01:23<01:15,  3.12it/s] 33%|███▎      | 115/350 [01:23<01:13,  3.21it/s] 33%|███▎      | 116/350 [01:23<01:11,  3.28it/s] 33%|███▎      | 117/350 [01:24<01:09,  3.34it/s] 34%|███▎      | 118/350 [01:24<01:08,  3.37it/s] 34%|███▍      | 119/350 [01:24<01:08,  3.39it/s] 34%|███▍      | 120/350 [01:25<01:07,  3.41it/s] 35%|███▍      | 121/350 [01:26<01:59,  1.91it/s] 35%|███▍      | 122/350 [01:26<01:43,  2.20it/s] 35%|███▌      | 123/350 [01:26<01:31,  2.47it/s] 35%|███▌      | 124/350 [01:26<01:23,  2.70it/s] 36%|███▌      | 125/350 [01:27<01:18,  2.88it/s] 36%|███▌      | 126/350 [01:27<01:13,  3.03it/s] 36%|███▋      | 127/350 [01:27<01:10,  3.15it/s] 37%|███▋      | 128/350 [01:28<01:08,  3.24it/s] 37%|███▋      | 129/350 [01:29<01:53,  1.95it/s] 37%|███▋      | 130/350 [01:29<01:38,  2.24it/s] 37%|███▋      | 131/350 [01:29<01:27,  2.50it/s] 38%|███▊      | 132/350 [01:29<01:20,  2.72it/s] 38%|███▊      | 133/350 [01:33<04:34,  1.26s/it] 38%|███▊      | 134/350 [01:34<04:24,  1.22s/it] 39%|███▊      | 135/350 [01:34<03:23,  1.06it/s] 39%|███▉      | 136/350 [01:35<02:40,  1.33it/s] 39%|███▉      | 137/350 [01:35<02:10,  1.63it/s] 39%|███▉      | 138/350 [01:35<01:49,  1.93it/s] 40%|███▉      | 139/350 [01:35<01:35,  2.22it/s] 40%|████      | 140/350 [01:36<01:24,  2.48it/s][INFO|trainer.py:2140] 2023-08-28 21:15:20,949 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:15:20,949 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:15:20,949 >>   Batch size = 8
{'eval_loss': 1.0689884424209595, 'eval_runtime': 13.2208, 'eval_samples_per_second': 264.204, 'eval_steps_per_second': 33.054, 'epoch': 0.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.13it/s][A
  3%|▎         | 12/437 [00:00<00:10, 41.12it/s][A
  4%|▍         | 17/437 [00:00<00:09, 42.47it/s][A
  5%|▌         | 22/437 [00:00<00:09, 43.28it/s][A
  6%|▌         | 27/437 [00:00<00:09, 43.48it/s][A
  7%|▋         | 32/437 [00:00<00:09, 43.75it/s][A
  8%|▊         | 37/437 [00:00<00:09, 43.66it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.91it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.72it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 43.83it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.04it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.21it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.08it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.22it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.08it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.97it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.04it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.91it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.09it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.29it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.34it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.39it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.15it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.15it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.02it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.04it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.08it/s][A
 32%|███▏      | 142/437 [00:03<00:10, 26.96it/s][A
 34%|███▎      | 147/437 [00:03<00:09, 30.50it/s][A
 35%|███▍      | 152/437 [00:03<00:08, 33.63it/s][A
 36%|███▌      | 157/437 [00:03<00:07, 35.68it/s][A
 37%|███▋      | 162/437 [00:03<00:07, 38.06it/s][A
 38%|███▊      | 167/437 [00:04<00:06, 39.90it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 41.23it/s][A
 41%|████      | 177/437 [00:04<00:06, 41.92it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 42.26it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 42.33it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 42.64it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.01it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.58it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.87it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 44.20it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.36it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.36it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.98it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.81it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.69it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.79it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.91it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.10it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 44.39it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.47it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.37it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.09it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.94it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.83it/s][A
 66%|██████▌   | 287/437 [00:06<00:04, 31.88it/s][A
 67%|██████▋   | 292/437 [00:07<00:04, 34.70it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 37.14it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 39.24it/s][A
 70%|███████   | 307/437 [00:07<00:03, 40.77it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 41.95it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 42.75it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.06it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 42.99it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 42.93it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 43.02it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 43.31it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 43.60it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.90it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.05it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.26it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.27it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.89it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.64it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 43.43it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 43.66it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.90it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.22it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.34it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.41it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.26it/s][A
 95%|█████████▌| 417/437 [00:10<00:00, 28.86it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 32.19it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 35.15it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 37.52it/s][A
100%|██████████| 437/437 [00:10<00:00, 39.42it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 39.42it/s][A 40%|████      | 140/350 [01:46<01:24,  2.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:15:31,905 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-140
[INFO|configuration_utils.py:351] 2023-08-28 21:15:32,338 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-140/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:15:42,922 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-140/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:15:43,965 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:15:44,161 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-140/special_tokens_map.json
 40%|████      | 141/350 [02:27<55:01, 15.80s/it] 41%|████      | 142/350 [02:28<38:42, 11.17s/it] 41%|████      | 143/350 [02:28<27:16,  7.91s/it] 41%|████      | 144/350 [02:28<19:18,  5.62s/it] 41%|████▏     | 145/350 [02:29<13:44,  4.02s/it] 42%|████▏     | 146/350 [02:29<09:52,  2.90s/it] 42%|████▏     | 147/350 [02:29<07:10,  2.12s/it] 42%|████▏     | 148/350 [02:30<05:17,  1.57s/it] 43%|████▎     | 149/350 [02:30<04:03,  1.21s/it] 43%|████▎     | 150/350 [02:30<03:06,  1.07it/s] 43%|████▎     | 151/350 [02:31<02:27,  1.35it/s] 43%|████▎     | 152/350 [02:34<04:51,  1.47s/it] 44%|████▎     | 153/350 [02:34<03:58,  1.21s/it] 44%|████▍     | 154/350 [02:35<03:03,  1.07it/s] 44%|████▍     | 155/350 [02:35<02:24,  1.35it/s] 45%|████▍     | 156/350 [02:35<01:57,  1.65it/s] 45%|████▍     | 157/350 [02:35<01:38,  1.95it/s] 45%|████▌     | 158/350 [02:36<01:25,  2.24it/s] 45%|████▌     | 159/350 [02:36<01:16,  2.50it/s] 46%|████▌     | 160/350 [02:36<01:09,  2.72it/s] 46%|████▌     | 161/350 [02:37<01:05,  2.89it/s] 46%|████▋     | 162/350 [02:37<01:01,  3.03it/s] 47%|████▋     | 163/350 [02:37<01:05,  2.88it/s] 47%|████▋     | 164/350 [02:38<01:01,  3.02it/s] 47%|████▋     | 165/350 [02:38<00:59,  3.13it/s] 47%|████▋     | 166/350 [02:38<00:57,  3.21it/s] 48%|████▊     | 167/350 [02:38<00:56,  3.27it/s] 48%|████▊     | 168/350 [02:39<00:55,  3.30it/s] 48%|████▊     | 169/350 [02:39<00:54,  3.33it/s] 49%|████▊     | 170/350 [02:39<00:53,  3.35it/s] 49%|████▉     | 171/350 [02:40<00:53,  3.37it/s] 49%|████▉     | 172/350 [02:40<00:52,  3.37it/s] 49%|████▉     | 173/350 [02:40<00:54,  3.24it/s] 50%|████▉     | 174/350 [02:41<00:53,  3.29it/s] 50%|█████     | 175/350 [02:41<00:52,  3.32it/s] 50%|█████     | 176/350 [02:41<00:54,  3.19it/s] 51%|█████     | 177/350 [02:42<00:53,  3.25it/s] 51%|█████     | 178/350 [02:42<00:52,  3.29it/s] 51%|█████     | 179/350 [02:42<00:51,  3.32it/s] 51%|█████▏    | 180/350 [02:42<00:50,  3.35it/s] 52%|█████▏    | 181/350 [02:43<00:50,  3.36it/s] 52%|█████▏    | 182/350 [02:43<00:49,  3.37it/s] 52%|█████▏    | 183/350 [02:43<00:49,  3.38it/s] 53%|█████▎    | 184/350 [02:44<00:49,  3.38it/s] 53%|█████▎    | 185/350 [02:44<00:48,  3.39it/s] 53%|█████▎    | 186/350 [02:45<01:06,  2.46it/s] 53%|█████▎    | 187/350 [02:45<01:00,  2.68it/s] 54%|█████▎    | 188/350 [02:45<00:56,  2.86it/s] 54%|█████▍    | 189/350 [02:45<00:53,  3.00it/s] 54%|█████▍    | 190/350 [02:46<00:51,  3.11it/s] 55%|█████▍    | 191/350 [02:46<00:49,  3.19it/s] 55%|█████▍    | 192/350 [02:46<00:48,  3.25it/s] 55%|█████▌    | 193/350 [02:47<00:47,  3.30it/s] 55%|█████▌    | 194/350 [02:47<00:46,  3.32it/s] 56%|█████▌    | 195/350 [02:47<00:53,  2.88it/s] 56%|█████▌    | 196/350 [02:48<00:51,  3.02it/s] 56%|█████▋    | 197/350 [02:48<00:49,  3.12it/s] 57%|█████▋    | 198/350 [02:48<00:47,  3.20it/s] 57%|█████▋    | 199/350 [02:49<00:46,  3.26it/s] 57%|█████▋    | 200/350 [02:49<00:45,  3.30it/s] 57%|█████▋    | 201/350 [02:49<00:44,  3.33it/s] 58%|█████▊    | 202/350 [02:49<00:44,  3.34it/s] 58%|█████▊    | 203/350 [02:50<00:43,  3.36it/s] 58%|█████▊    | 204/350 [02:50<00:43,  3.37it/s] 59%|█████▊    | 205/350 [02:50<00:46,  3.09it/s] 59%|█████▉    | 206/350 [02:51<00:45,  3.18it/s] 59%|█████▉    | 207/350 [02:51<00:43,  3.25it/s] 59%|█████▉    | 208/350 [02:51<00:42,  3.31it/s] 60%|█████▉    | 209/350 [02:52<00:42,  3.34it/s] 60%|██████    | 210/350 [02:52<00:41,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 21:16:37,059 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:16:37,059 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:16:37,059 >>   Batch size = 8
{'eval_loss': 1.0771703720092773, 'eval_runtime': 10.5687, 'eval_samples_per_second': 330.505, 'eval_steps_per_second': 41.349, 'epoch': 1.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.51it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.15it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.99it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.06it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.44it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.16it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.06it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.15it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.27it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.21it/s][A
 13%|█▎        | 57/437 [00:01<00:09, 40.15it/s][A
 14%|█▍        | 62/437 [00:01<00:09, 41.47it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 42.41it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 42.96it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.18it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.45it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.68it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.92it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.74it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.88it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.06it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.23it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.08it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.13it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.16it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.25it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.10it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.05it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.05it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.99it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.06it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.13it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.83it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.15it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.13it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.86it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.00it/s][A
 44%|████▍     | 192/437 [00:04<00:07, 31.54it/s][A
 45%|████▌     | 197/437 [00:04<00:06, 34.41it/s][A
 46%|████▌     | 202/437 [00:04<00:06, 36.90it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 38.67it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 40.34it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 41.62it/s][A
 51%|█████     | 222/437 [00:05<00:05, 42.55it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 42.99it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.04it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 42.93it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.32it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.47it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.78it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.96it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.12it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.23it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.01it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.67it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.91it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.73it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.94it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.08it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.21it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.16it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.43it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.86it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 42.44it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 42.63it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 42.92it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.15it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.65it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 43.83it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.10it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.01it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.76it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.75it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.79it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.00it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.93it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.00it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.09it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.13it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.15it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.94it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.88it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.77it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.76it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.86it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.14it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.23it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.23it/s][A 60%|██████    | 210/350 [03:02<00:41,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:16:48,044 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-210
[INFO|configuration_utils.py:351] 2023-08-28 21:16:48,619 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-210/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:16:58,850 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-210/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:16:59,435 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-210/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:17:00,208 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-210/special_tokens_map.json
 60%|██████    | 211/350 [03:50<41:10, 17.77s/it] 61%|██████    | 212/350 [03:51<28:48, 12.53s/it] 61%|██████    | 213/350 [03:51<20:22,  8.92s/it] 61%|██████    | 214/350 [03:51<14:21,  6.33s/it] 61%|██████▏   | 215/350 [03:52<10:10,  4.52s/it] 62%|██████▏   | 216/350 [03:52<07:15,  3.25s/it] 62%|██████▏   | 217/350 [03:52<05:14,  2.36s/it] 62%|██████▏   | 218/350 [03:53<03:50,  1.74s/it] 63%|██████▎   | 219/350 [03:53<02:51,  1.31s/it] 63%|██████▎   | 220/350 [03:53<02:10,  1.00s/it] 63%|██████▎   | 221/350 [03:54<01:42,  1.26it/s] 63%|██████▎   | 222/350 [03:54<01:22,  1.56it/s] 64%|██████▎   | 223/350 [03:55<01:28,  1.43it/s] 64%|██████▍   | 224/350 [03:55<01:12,  1.73it/s] 64%|██████▍   | 225/350 [03:55<01:01,  2.04it/s] 65%|██████▍   | 226/350 [03:56<00:53,  2.33it/s] 65%|██████▍   | 227/350 [03:56<00:47,  2.58it/s] 65%|██████▌   | 228/350 [03:56<00:43,  2.79it/s] 65%|██████▌   | 229/350 [03:56<00:40,  2.96it/s] 66%|██████▌   | 230/350 [03:57<00:38,  3.09it/s] 66%|██████▌   | 231/350 [03:57<00:37,  3.18it/s] 66%|██████▋   | 232/350 [03:57<00:44,  2.68it/s] 67%|██████▋   | 233/350 [03:58<00:40,  2.86it/s] 67%|██████▋   | 234/350 [03:58<00:38,  3.01it/s] 67%|██████▋   | 235/350 [03:58<00:36,  3.11it/s] 67%|██████▋   | 236/350 [03:59<00:35,  3.19it/s] 68%|██████▊   | 237/350 [03:59<00:34,  3.25it/s] 68%|██████▊   | 238/350 [03:59<00:33,  3.29it/s] 68%|██████▊   | 239/350 [04:00<00:33,  3.32it/s] 69%|██████▊   | 240/350 [04:00<00:32,  3.35it/s] 69%|██████▉   | 241/350 [04:00<00:32,  3.36it/s] 69%|██████▉   | 242/350 [04:00<00:33,  3.22it/s] 69%|██████▉   | 243/350 [04:01<00:32,  3.27it/s] 70%|██████▉   | 244/350 [04:01<00:32,  3.31it/s] 70%|███████   | 245/350 [04:01<00:31,  3.33it/s] 70%|███████   | 246/350 [04:02<00:30,  3.36it/s] 71%|███████   | 247/350 [04:02<00:30,  3.37it/s] 71%|███████   | 248/350 [04:02<00:30,  3.38it/s] 71%|███████   | 249/350 [04:03<00:29,  3.38it/s] 71%|███████▏  | 250/350 [04:03<00:29,  3.39it/s] 72%|███████▏  | 251/350 [04:03<00:29,  3.39it/s] 72%|███████▏  | 252/350 [04:03<00:28,  3.40it/s] 72%|███████▏  | 253/350 [04:04<00:36,  2.65it/s] 73%|███████▎  | 254/350 [04:04<00:33,  2.84it/s] 73%|███████▎  | 255/350 [04:05<00:31,  2.99it/s] 73%|███████▎  | 256/350 [04:05<00:30,  3.10it/s] 73%|███████▎  | 257/350 [04:05<00:29,  3.19it/s] 74%|███████▎  | 258/350 [04:05<00:28,  3.25it/s] 74%|███████▍  | 259/350 [04:06<00:27,  3.29it/s] 74%|███████▍  | 260/350 [04:06<00:27,  3.32it/s] 75%|███████▍  | 261/350 [04:07<00:33,  2.66it/s] 75%|███████▍  | 262/350 [04:07<00:34,  2.52it/s] 75%|███████▌  | 263/350 [04:07<00:31,  2.73it/s] 75%|███████▌  | 264/350 [04:08<00:29,  2.91it/s] 76%|███████▌  | 265/350 [04:08<00:27,  3.04it/s] 76%|███████▌  | 266/350 [04:08<00:26,  3.14it/s] 76%|███████▋  | 267/350 [04:09<00:25,  3.22it/s] 77%|███████▋  | 268/350 [04:09<00:25,  3.27it/s] 77%|███████▋  | 269/350 [04:09<00:24,  3.31it/s] 77%|███████▋  | 270/350 [04:09<00:23,  3.34it/s] 77%|███████▋  | 271/350 [04:10<00:23,  3.35it/s] 78%|███████▊  | 272/350 [04:10<00:25,  3.04it/s] 78%|███████▊  | 273/350 [04:10<00:24,  3.13it/s] 78%|███████▊  | 274/350 [04:11<00:23,  3.20it/s] 79%|███████▊  | 275/350 [04:11<00:23,  3.25it/s] 79%|███████▉  | 276/350 [04:11<00:22,  3.29it/s] 79%|███████▉  | 277/350 [04:12<00:21,  3.32it/s] 79%|███████▉  | 278/350 [04:12<00:21,  3.34it/s] 80%|███████▉  | 279/350 [04:12<00:21,  3.36it/s] 80%|████████  | 280/350 [04:12<00:20,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 21:17:57,672 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:17:57,673 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:17:57,673 >>   Batch size = 8
{'eval_loss': 1.0932284593582153, 'eval_runtime': 10.1622, 'eval_samples_per_second': 343.726, 'eval_steps_per_second': 43.003, 'epoch': 2.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.13it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.36it/s][A
  4%|▍         | 17/437 [00:00<00:09, 43.49it/s][A
  5%|▌         | 22/437 [00:00<00:09, 44.00it/s][A
  6%|▌         | 27/437 [00:00<00:09, 43.95it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.05it/s][A
  8%|▊         | 37/437 [00:00<00:09, 43.92it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.92it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.98it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.26it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.17it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.28it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.34it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.29it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.16it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.09it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.07it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.21it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.27it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.09it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.20it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.13it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.01it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.96it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.08it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.04it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.22it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.24it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.25it/s][A
 35%|███▍      | 152/437 [00:03<00:10, 27.07it/s][A
 36%|███▌      | 157/437 [00:03<00:09, 30.60it/s][A
 37%|███▋      | 162/437 [00:03<00:08, 33.69it/s][A
 38%|███▊      | 167/437 [00:04<00:07, 36.36it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 38.47it/s][A
 41%|████      | 177/437 [00:04<00:06, 40.26it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 41.50it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 42.34it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 42.43it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 42.59it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 42.82it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.18it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 43.59it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.00it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.14it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.18it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.04it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.81it/s][A
 55%|█████▌    | 242/437 [00:05<00:05, 33.31it/s][A
 57%|█████▋    | 247/437 [00:05<00:05, 36.09it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 38.22it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 39.96it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 41.24it/s][A
 61%|██████    | 267/437 [00:06<00:04, 42.13it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.00it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.26it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.09it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.18it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.25it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 43.65it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.02it/s][A
 70%|███████   | 307/437 [00:07<00:05, 22.42it/s][A
 71%|███████▏  | 312/437 [00:07<00:04, 26.65it/s][A
 73%|███████▎  | 317/437 [00:07<00:03, 30.26it/s][A
 74%|███████▎  | 322/437 [00:07<00:03, 33.53it/s][A
 75%|███████▍  | 327/437 [00:08<00:03, 36.31it/s][A
 76%|███████▌  | 332/437 [00:08<00:02, 38.50it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 40.14it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 41.48it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 42.04it/s][A
 81%|████████  | 352/437 [00:09<00:02, 42.32it/s][A
 82%|████████▏ | 357/437 [00:09<00:04, 19.68it/s][A
 83%|████████▎ | 362/437 [00:09<00:03, 23.56it/s][A
 84%|████████▍ | 367/437 [00:09<00:02, 27.46it/s][A
 85%|████████▌ | 372/437 [00:09<00:02, 31.13it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 34.24it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 36.83it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 38.89it/s][A
 90%|████████▉ | 392/437 [00:10<00:01, 40.27it/s][A
 91%|█████████ | 397/437 [00:10<00:00, 41.06it/s][A
 92%|█████████▏| 402/437 [00:10<00:00, 41.64it/s][A
 93%|█████████▎| 407/437 [00:10<00:00, 42.25it/s][A
 94%|█████████▍| 412/437 [00:10<00:00, 42.82it/s][A
 95%|█████████▌| 417/437 [00:10<00:00, 43.36it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 43.74it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 43.89it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.17it/s][A
100%|██████████| 437/437 [00:11<00:00, 44.06it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:11<00:00, 44.06it/s][A 80%|████████  | 280/350 [04:24<00:20,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:18:09,540 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-280
[INFO|configuration_utils.py:351] 2023-08-28 21:18:11,731 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-280/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:18:24,692 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-280/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:18:24,931 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:18:26,046 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-280/special_tokens_map.json
 80%|████████  | 281/350 [05:10<20:10, 17.54s/it] 81%|████████  | 282/350 [05:11<14:13, 12.55s/it] 81%|████████  | 283/350 [05:11<09:54,  8.87s/it] 81%|████████  | 284/350 [05:12<06:55,  6.30s/it] 81%|████████▏ | 285/350 [05:12<04:52,  4.50s/it] 82%|████████▏ | 286/350 [05:12<03:27,  3.24s/it] 82%|████████▏ | 287/350 [05:13<02:28,  2.35s/it] 82%|████████▏ | 288/350 [05:13<01:47,  1.74s/it] 83%|████████▎ | 289/350 [05:13<01:19,  1.30s/it] 83%|████████▎ | 290/350 [05:13<01:00,  1.00s/it] 83%|████████▎ | 291/350 [05:14<00:58,  1.00it/s] 83%|████████▎ | 292/350 [05:15<00:45,  1.27it/s] 84%|████████▎ | 293/350 [05:15<00:36,  1.56it/s] 84%|████████▍ | 294/350 [05:15<00:30,  1.87it/s] 84%|████████▍ | 295/350 [05:16<00:25,  2.16it/s] 85%|████████▍ | 296/350 [05:16<00:22,  2.43it/s] 85%|████████▍ | 297/350 [05:16<00:19,  2.66it/s] 85%|████████▌ | 298/350 [05:17<00:18,  2.86it/s] 85%|████████▌ | 299/350 [05:17<00:19,  2.58it/s] 86%|████████▌ | 300/350 [05:17<00:17,  2.79it/s] 86%|████████▌ | 301/350 [05:18<00:16,  2.96it/s] 86%|████████▋ | 302/350 [05:18<00:15,  3.09it/s] 87%|████████▋ | 303/350 [05:18<00:14,  3.19it/s] 87%|████████▋ | 304/350 [05:18<00:14,  3.26it/s] 87%|████████▋ | 305/350 [05:19<00:13,  3.32it/s] 87%|████████▋ | 306/350 [05:19<00:13,  3.36it/s] 88%|████████▊ | 307/350 [05:19<00:12,  3.39it/s] 88%|████████▊ | 308/350 [05:20<00:12,  3.40it/s] 88%|████████▊ | 309/350 [05:20<00:13,  3.10it/s] 89%|████████▊ | 310/350 [05:20<00:12,  3.19it/s] 89%|████████▉ | 311/350 [05:21<00:11,  3.27it/s] 89%|████████▉ | 312/350 [05:21<00:11,  3.32it/s] 89%|████████▉ | 313/350 [05:21<00:10,  3.37it/s] 90%|████████▉ | 314/350 [05:21<00:10,  3.40it/s] 90%|█████████ | 315/350 [05:22<00:10,  3.42it/s] 90%|█████████ | 316/350 [05:22<00:09,  3.43it/s] 91%|█████████ | 317/350 [05:22<00:09,  3.44it/s] 91%|█████████ | 318/350 [05:23<00:09,  3.45it/s] 91%|█████████ | 319/350 [05:23<00:08,  3.46it/s] 91%|█████████▏| 320/350 [05:23<00:10,  2.94it/s] 92%|█████████▏| 321/350 [05:24<00:09,  3.07it/s] 92%|█████████▏| 322/350 [05:24<00:08,  3.18it/s] 92%|█████████▏| 323/350 [05:24<00:08,  3.26it/s] 93%|█████████▎| 324/350 [05:25<00:07,  3.32it/s] 93%|█████████▎| 325/350 [05:25<00:07,  3.36it/s] 93%|█████████▎| 326/350 [05:25<00:07,  3.39it/s] 93%|█████████▎| 327/350 [05:25<00:06,  3.42it/s] 94%|█████████▎| 328/350 [05:26<00:06,  3.43it/s] 94%|█████████▍| 329/350 [05:26<00:06,  3.43it/s] 94%|█████████▍| 330/350 [05:27<00:08,  2.25it/s] 95%|█████████▍| 331/350 [05:27<00:07,  2.52it/s] 95%|█████████▍| 332/350 [05:27<00:06,  2.74it/s] 95%|█████████▌| 333/350 [05:28<00:05,  2.92it/s] 95%|█████████▌| 334/350 [05:28<00:05,  3.06it/s] 96%|█████████▌| 335/350 [05:28<00:04,  3.17it/s] 96%|█████████▌| 336/350 [05:28<00:04,  3.25it/s] 96%|█████████▋| 337/350 [05:29<00:03,  3.31it/s] 97%|█████████▋| 338/350 [05:29<00:03,  3.35it/s] 97%|█████████▋| 339/350 [05:30<00:05,  2.05it/s] 97%|█████████▋| 340/350 [05:30<00:04,  2.19it/s] 97%|█████████▋| 341/350 [05:31<00:03,  2.45it/s] 98%|█████████▊| 342/350 [05:31<00:02,  2.69it/s] 98%|█████████▊| 343/350 [05:31<00:02,  2.88it/s] 98%|█████████▊| 344/350 [05:32<00:01,  3.02it/s] 99%|█████████▊| 345/350 [05:32<00:01,  3.14it/s] 99%|█████████▉| 346/350 [05:32<00:01,  3.23it/s] 99%|█████████▉| 347/350 [05:33<00:01,  2.51it/s] 99%|█████████▉| 348/350 [05:33<00:00,  2.73it/s]100%|█████████▉| 349/350 [05:33<00:00,  2.91it/s]100%|██████████| 350/350 [05:34<00:00,  3.05it/s][INFO|trainer.py:2140] 2023-08-28 21:19:18,788 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:19:18,788 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:19:18,788 >>   Batch size = 8
{'eval_loss': 1.1041642427444458, 'eval_runtime': 11.1199, 'eval_samples_per_second': 314.122, 'eval_steps_per_second': 39.299, 'epoch': 3.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.03it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.07it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.59it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.97it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.17it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.59it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.34it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.04it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.10it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.30it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.35it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.51it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.24it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.09it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 41.30it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 42.28it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 42.72it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.22it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.46it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.77it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.81it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.94it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.61it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.92it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.91it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.11it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.13it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.32it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.22it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.19it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.93it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.80it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.83it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.05it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.10it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.23it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.20it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.14it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.97it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.79it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.84it/s][A
 49%|████▊     | 212/437 [00:04<00:06, 36.45it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 38.64it/s][A
 51%|█████     | 222/437 [00:05<00:05, 40.35it/s][A
 52%|█████▏    | 227/437 [00:05<00:05, 41.48it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 42.44it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.01it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.55it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.68it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.36it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.02it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 43.46it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.68it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.12it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.19it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.29it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.40it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.16it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.88it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.69it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.74it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.91it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.93it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.19it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.29it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.28it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 44.16it/s][A
 78%|███████▊  | 342/437 [00:08<00:04, 22.53it/s][A
 79%|███████▉  | 347/437 [00:08<00:03, 26.39it/s][A
 81%|████████  | 352/437 [00:08<00:02, 30.07it/s][A
 82%|████████▏ | 357/437 [00:08<00:02, 33.41it/s][A
 83%|████████▎ | 362/437 [00:08<00:02, 36.18it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 38.39it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 40.12it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 41.17it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 41.59it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 41.79it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 42.29it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 42.81it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.36it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.68it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.03it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.22it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 44.09it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 43.72it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 43.37it/s][A
100%|██████████| 437/437 [00:10<00:00, 43.36it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 43.36it/s][A100%|██████████| 350/350 [05:44<00:00,  3.05it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:19:29,567 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-350
[INFO|configuration_utils.py:351] 2023-08-28 21:19:31,047 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-350/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:19:41,708 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-350/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:19:42,542 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-350/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:19:42,626 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-350/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:20:19,802 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:20:19,947 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-70 (score: 1.0689884424209595).
                                                 100%|██████████| 350/350 [07:15<00:00,  3.05it/s]100%|██████████| 350/350 [07:15<00:00,  1.24s/it]
[INFO|trainer.py:1894] 2023-08-28 21:21:00,199 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 21:21:00,895 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:21:15,026 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:21:16,351 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:21:16,940 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:21:21,174 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:21,231 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:21,231 >>   train_loss               =     0.4265
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:21,231 >>   train_runtime            = 0:07:15.32
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:21,231 >>   train_samples            =       4500
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:21,231 >>   train_samples_per_second =     51.686
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:21,231 >>   train_steps_per_second   =      0.804
{'eval_loss': 1.1089540719985962, 'eval_runtime': 10.3703, 'eval_samples_per_second': 336.828, 'eval_steps_per_second': 42.14, 'epoch': 4.99}
{'train_runtime': 435.3227, 'train_samples_per_second': 51.686, 'train_steps_per_second': 0.804, 'train_loss': 0.4265193830217634, 'epoch': 4.99}
08/28/2023 21:21:23 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:21:23,442 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:21:23,442 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:21:23,442 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:08, 49.69it/s]  3%|▎         | 11/437 [00:00<00:09, 46.82it/s]  4%|▎         | 16/437 [00:00<00:09, 46.04it/s]  5%|▍         | 21/437 [00:00<00:09, 45.25it/s]  6%|▌         | 26/437 [00:00<00:09, 45.16it/s]  7%|▋         | 31/437 [00:00<00:08, 45.16it/s]  8%|▊         | 36/437 [00:00<00:08, 45.06it/s]  9%|▉         | 41/437 [00:00<00:10, 38.92it/s] 11%|█         | 46/437 [00:01<00:09, 40.43it/s] 12%|█▏        | 51/437 [00:01<00:09, 41.41it/s] 13%|█▎        | 56/437 [00:01<00:09, 42.09it/s] 14%|█▍        | 61/437 [00:01<00:08, 42.97it/s] 15%|█▌        | 66/437 [00:01<00:08, 43.60it/s] 16%|█▌        | 71/437 [00:01<00:08, 44.01it/s] 17%|█▋        | 76/437 [00:01<00:08, 44.09it/s] 19%|█▊        | 81/437 [00:01<00:08, 43.80it/s] 20%|█▉        | 86/437 [00:01<00:07, 43.90it/s] 21%|██        | 91/437 [00:02<00:07, 43.88it/s] 22%|██▏       | 96/437 [00:02<00:07, 43.94it/s] 23%|██▎       | 101/437 [00:02<00:07, 43.86it/s] 24%|██▍       | 106/437 [00:02<00:07, 44.17it/s] 25%|██▌       | 111/437 [00:02<00:07, 44.35it/s] 27%|██▋       | 116/437 [00:02<00:07, 44.34it/s] 28%|██▊       | 121/437 [00:02<00:07, 44.17it/s] 29%|██▉       | 126/437 [00:02<00:07, 44.07it/s] 30%|██▉       | 131/437 [00:02<00:06, 43.99it/s] 31%|███       | 136/437 [00:03<00:06, 43.96it/s] 32%|███▏      | 141/437 [00:03<00:06, 44.04it/s] 33%|███▎      | 146/437 [00:03<00:06, 44.16it/s] 35%|███▍      | 151/437 [00:03<00:06, 44.31it/s] 36%|███▌      | 156/437 [00:03<00:06, 44.42it/s] 37%|███▋      | 161/437 [00:03<00:06, 44.43it/s] 38%|███▊      | 166/437 [00:03<00:06, 44.31it/s] 39%|███▉      | 171/437 [00:03<00:06, 43.89it/s] 40%|████      | 176/437 [00:04<00:06, 39.34it/s] 41%|████▏     | 181/437 [00:04<00:06, 40.77it/s] 43%|████▎     | 186/437 [00:04<00:05, 41.89it/s] 44%|████▎     | 191/437 [00:04<00:05, 42.76it/s] 45%|████▍     | 196/437 [00:04<00:05, 43.36it/s] 46%|████▌     | 201/437 [00:04<00:05, 43.75it/s] 47%|████▋     | 206/437 [00:04<00:05, 43.92it/s] 48%|████▊     | 211/437 [00:04<00:05, 43.83it/s] 49%|████▉     | 216/437 [00:04<00:05, 43.58it/s] 51%|█████     | 221/437 [00:05<00:04, 43.50it/s] 52%|█████▏    | 226/437 [00:05<00:04, 43.73it/s] 53%|█████▎    | 231/437 [00:05<00:04, 43.94it/s] 54%|█████▍    | 236/437 [00:05<00:04, 43.97it/s] 55%|█████▌    | 241/437 [00:05<00:04, 44.14it/s] 56%|█████▋    | 246/437 [00:05<00:04, 44.50it/s] 57%|█████▋    | 251/437 [00:05<00:04, 44.44it/s] 59%|█████▊    | 256/437 [00:05<00:04, 44.19it/s] 60%|█████▉    | 261/437 [00:05<00:04, 43.90it/s] 61%|██████    | 266/437 [00:06<00:03, 43.84it/s] 62%|██████▏   | 271/437 [00:06<00:03, 43.91it/s] 63%|██████▎   | 276/437 [00:06<00:03, 44.15it/s] 64%|██████▍   | 281/437 [00:06<00:03, 44.18it/s] 65%|██████▌   | 286/437 [00:06<00:03, 44.38it/s] 67%|██████▋   | 291/437 [00:06<00:03, 44.45it/s] 68%|██████▊   | 296/437 [00:06<00:03, 44.31it/s] 69%|██████▉   | 301/437 [00:06<00:03, 43.97it/s] 70%|███████   | 306/437 [00:06<00:02, 43.90it/s] 71%|███████   | 311/437 [00:07<00:03, 41.76it/s] 72%|███████▏  | 316/437 [00:07<00:02, 42.60it/s] 73%|███████▎  | 321/437 [00:07<00:02, 43.19it/s] 75%|███████▍  | 326/437 [00:07<00:02, 43.65it/s] 76%|███████▌  | 331/437 [00:07<00:02, 44.00it/s] 77%|███████▋  | 336/437 [00:07<00:02, 44.14it/s] 78%|███████▊  | 341/437 [00:07<00:02, 43.98it/s] 79%|███████▉  | 346/437 [00:07<00:02, 43.84it/s] 80%|████████  | 351/437 [00:08<00:01, 43.60it/s] 81%|████████▏ | 356/437 [00:08<00:01, 43.59it/s] 83%|████████▎ | 361/437 [00:08<00:01, 43.73it/s] 84%|████████▍ | 366/437 [00:08<00:01, 44.10it/s] 85%|████████▍ | 371/437 [00:08<00:01, 44.07it/s] 86%|████████▌ | 376/437 [00:08<00:01, 44.23it/s] 87%|████████▋ | 381/437 [00:08<00:01, 43.99it/s] 88%|████████▊ | 386/437 [00:08<00:01, 44.02it/s] 89%|████████▉ | 391/437 [00:08<00:01, 43.89it/s] 91%|█████████ | 396/437 [00:09<00:00, 43.70it/s] 92%|█████████▏| 401/437 [00:09<00:00, 43.78it/s] 93%|█████████▎| 406/437 [00:09<00:00, 43.96it/s] 94%|█████████▍| 411/437 [00:09<00:00, 44.16it/s] 95%|█████████▌| 416/437 [00:09<00:00, 44.17it/s] 96%|█████████▋| 421/437 [00:09<00:00, 44.11it/s] 97%|█████████▋| 426/437 [00:09<00:00, 44.23it/s] 99%|█████████▊| 431/437 [00:09<00:00, 44.01it/s]100%|█████████▉| 436/437 [00:09<00:00, 43.86it/s]100%|██████████| 437/437 [00:09<00:00, 43.70it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:21:33,691 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:33,691 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:33,691 >>   eval_loss               =      1.069
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:33,691 >>   eval_runtime            = 0:00:10.03
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:33,691 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:33,691 >>   eval_samples_per_second =    348.014
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:33,691 >>   eval_steps_per_second   =     43.539
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:33,691 >>   perplexity              =     2.9124
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:00,995 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:01,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:01,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:01,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:01,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:22:02,178 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:22:02,179 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:22:02,937 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:22:04,237 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:22:04,278 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:08,536 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:08,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:08,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:08,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:08,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:22:10,058 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:22:10,060 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:22:10,877 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:22:11,313 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:22:11,313 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-280
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-350
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-140
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-70
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/checkpoint-210
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.82it/s]Extractor Predicting: 3it [00:01,  1.76it/s]Extractor Predicting: 4it [00:02,  1.77it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.69it/s]Extractor Predicting: 18it [00:10,  1.56it/s]Extractor Predicting: 19it [00:11,  1.59it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:12,  1.57it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:16,  1.75it/s]Extractor Predicting: 29it [00:17,  1.73it/s]Extractor Predicting: 30it [00:18,  1.69it/s]Extractor Predicting: 31it [00:18,  1.52it/s]Extractor Predicting: 32it [00:19,  1.54it/s]Extractor Predicting: 33it [00:20,  1.53it/s]Extractor Predicting: 34it [00:20,  1.53it/s]Extractor Predicting: 35it [00:21,  1.56it/s]Extractor Predicting: 36it [00:22,  1.49it/s]Extractor Predicting: 37it [00:22,  1.50it/s]Extractor Predicting: 38it [00:23,  1.51it/s]Extractor Predicting: 39it [00:24,  1.54it/s]Extractor Predicting: 40it [00:24,  1.55it/s]Extractor Predicting: 41it [00:25,  1.50it/s]Extractor Predicting: 42it [00:26,  1.53it/s]Extractor Predicting: 43it [00:26,  1.53it/s]Extractor Predicting: 44it [00:27,  1.57it/s]Extractor Predicting: 45it [00:28,  1.58it/s]Extractor Predicting: 46it [00:28,  1.53it/s]Extractor Predicting: 47it [00:29,  1.53it/s]Extractor Predicting: 48it [00:30,  1.55it/s]Extractor Predicting: 49it [00:30,  1.53it/s]Extractor Predicting: 50it [00:31,  1.55it/s]Extractor Predicting: 51it [00:32,  1.49it/s]Extractor Predicting: 52it [00:32,  1.50it/s]Extractor Predicting: 53it [00:33,  1.56it/s]Extractor Predicting: 54it [00:33,  1.55it/s]Extractor Predicting: 55it [00:34,  1.55it/s]Extractor Predicting: 56it [00:35,  1.54it/s]Extractor Predicting: 57it [00:35,  1.51it/s]Extractor Predicting: 58it [00:36,  1.52it/s]Extractor Predicting: 59it [00:37,  1.50it/s]Extractor Predicting: 60it [00:37,  1.51it/s]Extractor Predicting: 61it [00:38,  1.54it/s]Extractor Predicting: 62it [00:39,  1.32it/s]Extractor Predicting: 63it [00:40,  1.41it/s]Extractor Predicting: 64it [00:40,  1.48it/s]Extractor Predicting: 65it [00:41,  1.50it/s]Extractor Predicting: 66it [00:41,  1.55it/s]Extractor Predicting: 67it [00:42,  1.42it/s]Extractor Predicting: 68it [00:43,  1.47it/s]Extractor Predicting: 69it [00:44,  1.50it/s]Extractor Predicting: 70it [00:44,  1.51it/s]Extractor Predicting: 71it [00:45,  1.53it/s]Extractor Predicting: 72it [00:46,  1.53it/s]Extractor Predicting: 73it [00:46,  1.54it/s]Extractor Predicting: 74it [00:47,  1.58it/s]Extractor Predicting: 75it [00:47,  1.61it/s]Extractor Predicting: 76it [00:48,  1.60it/s]Extractor Predicting: 77it [00:49,  1.52it/s]Extractor Predicting: 78it [00:49,  1.53it/s]Extractor Predicting: 79it [00:50,  1.56it/s]Extractor Predicting: 80it [00:51,  1.56it/s]Extractor Predicting: 81it [00:51,  1.61it/s]Extractor Predicting: 82it [00:52,  1.39it/s]Extractor Predicting: 83it [00:53,  1.43it/s]Extractor Predicting: 84it [00:53,  1.49it/s]Extractor Predicting: 85it [00:54,  1.54it/s]Extractor Predicting: 86it [00:55,  1.61it/s]Extractor Predicting: 87it [00:55,  1.53it/s]Extractor Predicting: 88it [00:56,  1.58it/s]Extractor Predicting: 89it [00:56,  1.61it/s]Extractor Predicting: 90it [00:57,  1.64it/s]Extractor Predicting: 91it [00:58,  1.63it/s]Extractor Predicting: 92it [00:59,  1.46it/s]Extractor Predicting: 93it [00:59,  1.55it/s]Extractor Predicting: 94it [01:00,  1.62it/s]Extractor Predicting: 95it [01:00,  1.63it/s]Extractor Predicting: 96it [01:01,  1.67it/s]Extractor Predicting: 97it [01:02,  1.57it/s]Extractor Predicting: 98it [01:02,  1.58it/s]Extractor Predicting: 99it [01:03,  1.40it/s]Extractor Predicting: 100it [01:04,  1.48it/s]Extractor Predicting: 101it [01:04,  1.57it/s]Extractor Predicting: 102it [01:05,  1.58it/s]Extractor Predicting: 103it [01:05,  1.61it/s]Extractor Predicting: 104it [01:06,  1.58it/s]Extractor Predicting: 105it [01:07,  1.65it/s]Extractor Predicting: 106it [01:07,  1.63it/s]Extractor Predicting: 107it [01:08,  1.53it/s]Extractor Predicting: 108it [01:09,  1.57it/s]Extractor Predicting: 109it [01:09,  1.60it/s]Extractor Predicting: 110it [01:10,  1.63it/s]Extractor Predicting: 111it [01:10,  1.63it/s]Extractor Predicting: 112it [01:11,  1.64it/s]Extractor Predicting: 113it [01:12,  1.65it/s]Extractor Predicting: 114it [01:12,  1.61it/s]Extractor Predicting: 115it [01:13,  1.53it/s]Extractor Predicting: 116it [01:14,  1.55it/s]Extractor Predicting: 117it [01:14,  1.55it/s]Extractor Predicting: 118it [01:15,  1.62it/s]Extractor Predicting: 119it [01:15,  1.62it/s]Extractor Predicting: 120it [01:16,  1.51it/s]Extractor Predicting: 121it [01:17,  1.54it/s]Extractor Predicting: 122it [01:17,  1.56it/s]Extractor Predicting: 123it [01:18,  1.57it/s]Extractor Predicting: 124it [01:19,  1.56it/s]Extractor Predicting: 125it [01:19,  1.50it/s]Extractor Predicting: 126it [01:20,  1.53it/s]Extractor Predicting: 127it [01:21,  1.55it/s]Extractor Predicting: 128it [01:21,  1.55it/s]Extractor Predicting: 129it [01:22,  1.52it/s]Extractor Predicting: 130it [01:23,  1.24it/s]Extractor Predicting: 131it [01:24,  1.33it/s]Extractor Predicting: 132it [01:24,  1.43it/s]Extractor Predicting: 133it [01:25,  1.48it/s]Extractor Predicting: 134it [01:26,  1.52it/s]Extractor Predicting: 135it [01:26,  1.44it/s]Extractor Predicting: 136it [01:27,  1.50it/s]Extractor Predicting: 137it [01:28,  1.52it/s]Extractor Predicting: 138it [01:28,  1.54it/s]Extractor Predicting: 139it [01:29,  1.56it/s]Extractor Predicting: 140it [01:30,  1.51it/s]Extractor Predicting: 141it [01:30,  1.52it/s]Extractor Predicting: 142it [01:31,  1.62it/s]Extractor Predicting: 142it [01:31,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:17,247 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:17,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:17,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:17,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:17,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:24:20,648 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:24:20,777 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:24:22,560 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:24:24,165 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:24:24,388 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:29,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:30,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:30,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:30,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:30,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:24:30,792 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:24:30,794 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:24:31,513 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:24:31,688 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:24:31,688 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.1889055472263868,
  "recall": 0.14428857715430862,
  "score": 0.16360980360331118,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 6it [00:03,  1.48it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.44it/s]Extractor Predicting: 27it [00:17,  1.49it/s]Extractor Predicting: 28it [00:18,  1.36it/s]Extractor Predicting: 29it [00:18,  1.40it/s]Extractor Predicting: 30it [00:19,  1.42it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:20,  1.46it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.54it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.59it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.60it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:26,  1.62it/s]Extractor Predicting: 42it [00:27,  1.50it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:29,  1.56it/s]Extractor Predicting: 46it [00:29,  1.59it/s]Extractor Predicting: 47it [00:30,  1.31it/s]Extractor Predicting: 48it [00:31,  1.39it/s]Extractor Predicting: 49it [00:31,  1.44it/s]Extractor Predicting: 50it [00:32,  1.47it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:33,  1.48it/s]Extractor Predicting: 53it [00:34,  1.52it/s]Extractor Predicting: 54it [00:35,  1.55it/s]Extractor Predicting: 55it [00:35,  1.53it/s]Extractor Predicting: 56it [00:36,  1.58it/s]Extractor Predicting: 57it [00:37,  1.44it/s]Extractor Predicting: 58it [00:37,  1.52it/s]Extractor Predicting: 59it [00:38,  1.55it/s]Extractor Predicting: 60it [00:39,  1.56it/s]Extractor Predicting: 61it [00:39,  1.59it/s]Extractor Predicting: 62it [00:40,  1.42it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:41,  1.49it/s]Extractor Predicting: 65it [00:42,  1.52it/s]Extractor Predicting: 66it [00:43,  1.54it/s]Extractor Predicting: 67it [00:43,  1.56it/s]Extractor Predicting: 68it [00:44,  1.63it/s]Extractor Predicting: 69it [00:44,  1.63it/s]Extractor Predicting: 70it [00:45,  1.63it/s]Extractor Predicting: 71it [00:46,  1.63it/s]Extractor Predicting: 72it [00:46,  1.44it/s]Extractor Predicting: 73it [00:47,  1.45it/s]Extractor Predicting: 74it [00:48,  1.50it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:49,  1.56it/s]Extractor Predicting: 77it [00:50,  1.39it/s]Extractor Predicting: 78it [00:50,  1.46it/s]Extractor Predicting: 79it [00:51,  1.40it/s]Extractor Predicting: 80it [00:52,  1.46it/s]Extractor Predicting: 81it [00:52,  1.51it/s]Extractor Predicting: 82it [00:53,  1.49it/s]Extractor Predicting: 83it [00:54,  1.51it/s]Extractor Predicting: 84it [00:55,  1.45it/s]Extractor Predicting: 85it [00:55,  1.50it/s]Extractor Predicting: 86it [00:56,  1.51it/s]Extractor Predicting: 87it [00:56,  1.53it/s]Extractor Predicting: 88it [00:57,  1.50it/s]Extractor Predicting: 89it [00:58,  1.46it/s]Extractor Predicting: 90it [00:58,  1.51it/s]Extractor Predicting: 91it [00:59,  1.50it/s]Extractor Predicting: 92it [01:00,  1.52it/s]Extractor Predicting: 93it [01:00,  1.51it/s]Extractor Predicting: 94it [01:02,  1.28it/s]Extractor Predicting: 95it [01:02,  1.35it/s]Extractor Predicting: 96it [01:03,  1.40it/s]Extractor Predicting: 97it [01:03,  1.45it/s]Extractor Predicting: 98it [01:04,  1.47it/s]Extractor Predicting: 99it [01:05,  1.33it/s]Extractor Predicting: 100it [01:06,  1.39it/s]Extractor Predicting: 101it [01:06,  1.44it/s]Extractor Predicting: 102it [01:07,  1.47it/s]Extractor Predicting: 103it [01:08,  1.51it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.52it/s]Extractor Predicting: 106it [01:10,  1.53it/s]Extractor Predicting: 107it [01:10,  1.52it/s]Extractor Predicting: 108it [01:11,  1.53it/s]Extractor Predicting: 109it [01:12,  1.45it/s]Extractor Predicting: 110it [01:12,  1.47it/s]Extractor Predicting: 111it [01:13,  1.49it/s]Extractor Predicting: 112it [01:14,  1.54it/s]Extractor Predicting: 113it [01:14,  1.58it/s]Extractor Predicting: 114it [01:15,  1.45it/s]Extractor Predicting: 115it [01:16,  1.49it/s]Extractor Predicting: 116it [01:16,  1.53it/s]Extractor Predicting: 117it [01:17,  1.56it/s]Extractor Predicting: 118it [01:17,  1.60it/s]Extractor Predicting: 119it [01:18,  1.42it/s]Extractor Predicting: 120it [01:19,  1.49it/s]Extractor Predicting: 121it [01:19,  1.53it/s]Extractor Predicting: 122it [01:20,  1.62it/s]Extractor Predicting: 123it [01:21,  1.62it/s]Extractor Predicting: 124it [01:21,  1.63it/s]Extractor Predicting: 125it [01:22,  1.52it/s]Extractor Predicting: 126it [01:23,  1.53it/s]Extractor Predicting: 127it [01:23,  1.56it/s]Extractor Predicting: 128it [01:24,  1.60it/s]Extractor Predicting: 129it [01:24,  1.57it/s]Extractor Predicting: 130it [01:25,  1.44it/s]Extractor Predicting: 131it [01:26,  1.48it/s]Extractor Predicting: 132it [01:27,  1.53it/s]Extractor Predicting: 133it [01:27,  1.55it/s]Extractor Predicting: 134it [01:28,  1.58it/s]Extractor Predicting: 135it [01:29,  1.51it/s]Extractor Predicting: 136it [01:29,  1.55it/s]Extractor Predicting: 137it [01:30,  1.57it/s]Extractor Predicting: 138it [01:30,  1.58it/s]Extractor Predicting: 139it [01:31,  1.65it/s]Extractor Predicting: 140it [01:32,  1.35it/s]Extractor Predicting: 141it [01:33,  1.42it/s]Extractor Predicting: 142it [01:33,  1.52it/s]Extractor Predicting: 143it [01:34,  1.55it/s]Extractor Predicting: 144it [01:34,  1.55it/s]Extractor Predicting: 145it [01:36,  1.10it/s]Extractor Predicting: 146it [01:37,  1.23it/s]Extractor Predicting: 147it [01:37,  1.31it/s]Extractor Predicting: 148it [01:38,  1.38it/s]Extractor Predicting: 149it [01:39,  1.36it/s]Extractor Predicting: 150it [01:39,  1.44it/s]Extractor Predicting: 151it [01:40,  1.53it/s]Extractor Predicting: 152it [01:40,  1.58it/s]Extractor Predicting: 153it [01:41,  1.60it/s]Extractor Predicting: 154it [01:42,  1.27it/s]Extractor Predicting: 155it [01:43,  1.35it/s]Extractor Predicting: 156it [01:43,  1.41it/s]Extractor Predicting: 157it [01:44,  1.45it/s]Extractor Predicting: 158it [01:45,  1.39it/s]Extractor Predicting: 159it [01:45,  1.44it/s]Extractor Predicting: 160it [01:46,  1.49it/s]Extractor Predicting: 161it [01:47,  1.56it/s]Extractor Predicting: 162it [01:47,  1.59it/s]Extractor Predicting: 163it [01:48,  1.23it/s]Extractor Predicting: 164it [01:49,  1.34it/s]Extractor Predicting: 165it [01:50,  1.40it/s]Extractor Predicting: 166it [01:50,  1.47it/s]Extractor Predicting: 167it [01:51,  1.53it/s]Extractor Predicting: 168it [01:51,  1.55it/s]Extractor Predicting: 169it [01:52,  1.57it/s]Extractor Predicting: 170it [01:53,  1.54it/s]Extractor Predicting: 171it [01:53,  1.51it/s]Extractor Predicting: 172it [01:54,  1.53it/s]Extractor Predicting: 173it [01:55,  1.54it/s]Extractor Predicting: 174it [01:55,  1.51it/s]Extractor Predicting: 175it [01:56,  1.31it/s]Extractor Predicting: 176it [01:57,  1.38it/s]Extractor Predicting: 177it [01:58,  1.42it/s]Extractor Predicting: 178it [01:58,  1.49it/s]Extractor Predicting: 179it [01:59,  1.51it/s]Extractor Predicting: 180it [02:00,  1.21it/s]Extractor Predicting: 181it [02:01,  1.19it/s]Extractor Predicting: 182it [02:02,  1.31it/s]Extractor Predicting: 183it [02:02,  1.37it/s]Extractor Predicting: 184it [02:03,  1.45it/s]Extractor Predicting: 185it [02:03,  1.51it/s]Extractor Predicting: 186it [02:04,  1.55it/s]Extractor Predicting: 187it [02:05,  1.59it/s]Extractor Predicting: 188it [02:05,  1.59it/s]Extractor Predicting: 189it [02:06,  1.52it/s]Extractor Predicting: 190it [02:07,  1.55it/s]Extractor Predicting: 191it [02:07,  1.52it/s]Extractor Predicting: 192it [02:08,  1.56it/s]Extractor Predicting: 193it [02:09,  1.60it/s]Extractor Predicting: 194it [02:09,  1.38it/s]Extractor Predicting: 195it [02:10,  1.44it/s]Extractor Predicting: 196it [02:11,  1.51it/s]Extractor Predicting: 197it [02:11,  1.55it/s]Extractor Predicting: 198it [02:12,  1.55it/s]Extractor Predicting: 199it [02:13,  1.36it/s]Extractor Predicting: 200it [02:14,  1.42it/s]Extractor Predicting: 201it [02:14,  1.49it/s]Extractor Predicting: 202it [02:15,  1.56it/s]Extractor Predicting: 203it [02:15,  1.59it/s]Extractor Predicting: 204it [02:16,  1.43it/s]Extractor Predicting: 205it [02:17,  1.45it/s]Extractor Predicting: 206it [02:17,  1.50it/s]Extractor Predicting: 207it [02:18,  1.54it/s]Extractor Predicting: 208it [02:19,  1.58it/s]Extractor Predicting: 209it [02:19,  1.48it/s]Extractor Predicting: 210it [02:20,  1.51it/s]Extractor Predicting: 211it [02:21,  1.53it/s]Extractor Predicting: 212it [02:21,  1.57it/s]Extractor Predicting: 213it [02:22,  1.59it/s]Extractor Predicting: 214it [02:23,  1.55it/s]Extractor Predicting: 215it [02:23,  1.54it/s]Extractor Predicting: 216it [02:24,  1.58it/s]Extractor Predicting: 217it [02:24,  1.59it/s]Extractor Predicting: 218it [02:25,  1.53it/s]Extractor Predicting: 219it [02:26,  1.55it/s]Extractor Predicting: 220it [02:27,  1.37it/s]Extractor Predicting: 221it [02:27,  1.40it/s]Extractor Predicting: 222it [02:28,  1.45it/s]Extractor Predicting: 223it [02:29,  1.48it/s]Extractor Predicting: 224it [02:29,  1.53it/s]Extractor Predicting: 225it [02:30,  1.44it/s]Extractor Predicting: 226it [02:31,  1.52it/s]Extractor Predicting: 227it [02:31,  1.58it/s]Extractor Predicting: 228it [02:32,  1.58it/s]Extractor Predicting: 229it [02:32,  1.60it/s]Extractor Predicting: 230it [02:33,  1.53it/s]Extractor Predicting: 231it [02:34,  1.55it/s]Extractor Predicting: 232it [02:34,  1.56it/s]Extractor Predicting: 233it [02:35,  1.62it/s]Extractor Predicting: 234it [02:36,  1.61it/s]Extractor Predicting: 235it [02:36,  1.51it/s]Extractor Predicting: 236it [02:37,  1.51it/s]Extractor Predicting: 237it [02:38,  1.53it/s]Extractor Predicting: 238it [02:38,  1.58it/s]Extractor Predicting: 239it [02:39,  1.60it/s]Extractor Predicting: 240it [02:40,  1.57it/s]Extractor Predicting: 241it [02:40,  1.58it/s]Extractor Predicting: 242it [02:41,  1.57it/s]Extractor Predicting: 243it [02:41,  1.55it/s]Extractor Predicting: 244it [02:42,  1.56it/s]Extractor Predicting: 245it [02:43,  1.53it/s]Extractor Predicting: 246it [02:43,  1.54it/s]Extractor Predicting: 247it [02:44,  1.58it/s]Extractor Predicting: 248it [02:45,  1.60it/s]Extractor Predicting: 249it [02:45,  1.60it/s]Extractor Predicting: 250it [02:46,  1.51it/s]Extractor Predicting: 251it [02:47,  1.51it/s]Extractor Predicting: 252it [02:47,  1.55it/s]Extractor Predicting: 253it [02:48,  1.58it/s]Extractor Predicting: 254it [02:48,  1.61it/s]Extractor Predicting: 255it [02:50,  1.32it/s]Extractor Predicting: 256it [02:50,  1.40it/s]Extractor Predicting: 257it [02:51,  1.14it/s]Extractor Predicting: 258it [02:52,  1.26it/s]Extractor Predicting: 259it [02:53,  1.31it/s]Extractor Predicting: 260it [02:53,  1.41it/s]Extractor Predicting: 261it [02:54,  1.26it/s]Extractor Predicting: 262it [02:55,  1.34it/s]Extractor Predicting: 263it [02:56,  1.40it/s]Extractor Predicting: 264it [02:56,  1.46it/s]Extractor Predicting: 265it [02:57,  1.48it/s]Extractor Predicting: 266it [02:58,  1.43it/s]Extractor Predicting: 267it [02:58,  1.45it/s]Extractor Predicting: 268it [02:59,  1.49it/s]Extractor Predicting: 269it [02:59,  1.51it/s]Extractor Predicting: 270it [03:00,  1.52it/s]Extractor Predicting: 271it [03:02,  1.12it/s]Extractor Predicting: 272it [03:02,  1.23it/s]Extractor Predicting: 273it [03:03,  1.19it/s]Extractor Predicting: 274it [03:04,  1.28it/s]Extractor Predicting: 275it [03:05,  1.19it/s]Extractor Predicting: 276it [03:05,  1.29it/s]Extractor Predicting: 277it [03:06,  1.35it/s]Extractor Predicting: 278it [03:07,  1.41it/s]Extractor Predicting: 279it [03:07,  1.46it/s]Extractor Predicting: 280it [03:08,  1.37it/s]Extractor Predicting: 281it [03:09,  1.41it/s]Extractor Predicting: 282it [03:09,  1.45it/s]Extractor Predicting: 283it [03:10,  1.43it/s]Extractor Predicting: 284it [03:11,  1.44it/s]Extractor Predicting: 285it [03:12,  1.43it/s]Extractor Predicting: 286it [03:12,  1.45it/s]Extractor Predicting: 287it [03:13,  1.48it/s]Extractor Predicting: 288it [03:13,  1.97it/s]Extractor Predicting: 288it [03:13,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:22,185 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:22,320 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:22,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:22,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:22,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:28:24,278 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:28:24,280 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:28:25,364 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:28:27,340 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:28:27,340 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:32,291 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:32,385 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:32,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:32,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:32,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:28:33,498 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:28:33,499 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:28:34,045 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:28:34,731 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:28:34,731 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.32677469135802467,
  "recall": 0.24589925968935986,
  "score": 0.2806261906734035,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:01,  1.92it/s]Extractor Predicting: 3it [00:01,  1.75it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:28:43,392 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:28:43,558 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:28:43,786 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:28:43,787 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:28:43,836 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:29:40,772 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:29:40,861 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:29:41,364 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:29:41,365 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:29:41,479 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:29:42,557 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:29:42,558 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:29:42,558 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:29:42,558 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:29:42,558 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:29:42,558 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2926829268292683,
  "recall": 0.10810810810810811,
  "score": 0.15789473684210528,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:29:43,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:43,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:45,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:45,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:46,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:46,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:47,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:48,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:48,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:49,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:49,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:50,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:50,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:51,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:52,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:52,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:53,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:53,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:54,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:54,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:55,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:12<03:00, 12.91s/it][WARNING|generation_utils.py:914] 2023-08-28 21:29:56,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:56,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:57,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:57,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:58,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:59,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:59,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:00,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:00,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:01,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:01,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:02,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:03,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:03,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:04,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:05,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:05,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:06,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:06,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:07,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:24<02:40, 12.33s/it][WARNING|generation_utils.py:914] 2023-08-28 21:30:08,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:08,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:09,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:10,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:10,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:11,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:12,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:13,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:13,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:14,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:15,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:15,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:16,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:16,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:17,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:18,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:18,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:19,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:20,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:20,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:21,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:38<02:37, 13.11s/it][WARNING|generation_utils.py:914] 2023-08-28 21:30:22,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:23,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:23,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:24,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:24,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:25,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:25,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:26,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:27,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:28,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:28,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:29,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:30,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:31,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:31,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:32,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:33,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:33,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:34,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:34,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:35,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:52<02:27, 13.43s/it][WARNING|generation_utils.py:914] 2023-08-28 21:30:36,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:36,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:37,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:37,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:38,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:38,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:39,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:40,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:40,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:41,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:41,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:42,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:42,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:43,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:43,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:44,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:44,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:45,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:45,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:46,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:47,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:04<02:07, 12.72s/it][WARNING|generation_utils.py:914] 2023-08-28 21:30:47,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:48,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:48,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:49,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:50,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:50,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:51,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:51,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:52,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:53,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:53,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:54,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:55,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:55,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:56,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:57,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:57,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:58,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:58,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:59,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:16<01:54, 12.70s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:00,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:00,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:01,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:02,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:03,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:03,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:04,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:05,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:05,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:06,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:07,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:07,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:08,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:09,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:09,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:10,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:10,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:11,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:12,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:12,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:29<01:42, 12.80s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:13,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:13,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:14,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:14,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:15,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:16,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:16,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:17,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:17,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:18,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:18,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:19,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:19,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:20,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:21,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:22,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:22,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:23,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:24,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:25,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:25,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:26,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:26,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:44<01:33, 13.36s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:27,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:28,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:28,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:29,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:29,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:30,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:31,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:31,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:32,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:32,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:33,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:33,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:34,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:35,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:35,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:36,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:36,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:37,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:37,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:38,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:55<01:15, 12.66s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:38,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:39,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:40,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:40,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:41,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:41,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:42,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:43,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:43,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:44,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:44,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:45,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:45,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:46,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:46,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:47,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:49,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:49,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:50,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:50,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:08<01:03, 12.72s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:51,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:52,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:52,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:53,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:53,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:54,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:54,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:55,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:56,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:56,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:57,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:57,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:58,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:58,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:59,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:59,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:00,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:00,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:01,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:02,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:19<00:48, 12.12s/it][WARNING|generation_utils.py:914] 2023-08-28 21:32:02,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:03,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:03,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:04,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:05,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:05,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:06,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:06,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:07,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:07,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:08,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:09,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:09,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:10,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:10,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:11,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:12,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:12,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:13,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:13,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:14,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:15,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:32<00:37, 12.41s/it][WARNING|generation_utils.py:914] 2023-08-28 21:32:15,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:16,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:16,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:17,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:18,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:18,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:19,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:20,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:20,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:21,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:21,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:22,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:23,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:24,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:24,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:25,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:26,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:27,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:28,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:28,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:29,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:46<00:25, 12.98s/it][WARNING|generation_utils.py:914] 2023-08-28 21:32:29,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:30,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:31,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:31,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:32,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:33,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:33,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:34,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:34,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:35,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:36,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:37,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:38,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:38,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:39,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:39,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:40,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:41,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:41,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:42,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:59<00:13, 13.01s/it][WARNING|generation_utils.py:914] 2023-08-28 21:32:43,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:43,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:44,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:44,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:45,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:45,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:46,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:46,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:47,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:47,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:48,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:48,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:49,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:49,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:50,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:51,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:51,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:52,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:52,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:53,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:53,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:54,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:54,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:55,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:55,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:56,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:13<00:00, 13.26s/it]Generating: 100%|██████████| 15/15 [03:13<00:00, 12.90s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:04,075 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:04,137 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:04,137 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:04,137 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:04,137 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:33:05,061 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:33:05,062 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:33:05,740 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:33:06,821 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:33:06,821 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:11,066 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:11,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:11,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:11,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:11,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:33:11,832 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:33:11,834 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:33:12,512 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:33:12,686 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:33:12,686 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.9300595238095238, 'errors': {''}}
['Relation : place served by transport hub . Context : Later in 2008 , the station became a hub for a fleet of buses that operate at different times around the city . Head Entity : station , Tail Entity : Krasnodar .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.8928571428571429, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : architect .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : developer .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.965625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 628, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8920454545454546, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : operator .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.95, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 383, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 531, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 575, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 623, 'raw': 832}
{'prompt': 'Relation : position held .', 'success_rate': 0.7487980769230769, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 8773
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8873, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.69it/s]Extractor Estimating: 2it [00:01,  1.55it/s]Extractor Estimating: 3it [00:02,  1.46it/s]Extractor Estimating: 4it [00:02,  1.57it/s]Extractor Estimating: 5it [00:03,  1.65it/s]Extractor Estimating: 6it [00:03,  1.66it/s]Extractor Estimating: 7it [00:04,  1.70it/s]Extractor Estimating: 8it [00:05,  1.52it/s]Extractor Estimating: 9it [00:05,  1.57it/s]Extractor Estimating: 10it [00:06,  1.59it/s]Extractor Estimating: 11it [00:06,  1.64it/s]Extractor Estimating: 12it [00:07,  1.63it/s]Extractor Estimating: 13it [00:08,  1.40it/s]Extractor Estimating: 14it [00:08,  1.49it/s]Extractor Estimating: 15it [00:09,  1.57it/s]Extractor Estimating: 16it [00:10,  1.59it/s]Extractor Estimating: 17it [00:10,  1.63it/s]Extractor Estimating: 18it [00:11,  1.53it/s]Extractor Estimating: 19it [00:11,  1.64it/s]Extractor Estimating: 20it [00:12,  1.72it/s]Extractor Estimating: 21it [00:13,  1.71it/s]Extractor Estimating: 22it [00:13,  1.76it/s]Extractor Estimating: 23it [00:14,  1.70it/s]Extractor Estimating: 24it [00:15,  1.45it/s]Extractor Estimating: 25it [00:15,  1.48it/s]Extractor Estimating: 26it [00:16,  1.55it/s]Extractor Estimating: 27it [00:16,  1.66it/s]Extractor Estimating: 28it [00:17,  1.72it/s]Extractor Estimating: 29it [00:18,  1.56it/s]Extractor Estimating: 30it [00:18,  1.60it/s]Extractor Estimating: 31it [00:19,  1.63it/s]Extractor Estimating: 32it [00:19,  1.70it/s]Extractor Estimating: 33it [00:20,  1.74it/s]Extractor Estimating: 34it [00:21,  1.66it/s]Extractor Estimating: 35it [00:21,  1.70it/s]Extractor Estimating: 36it [00:22,  1.62it/s]Extractor Estimating: 37it [00:22,  1.68it/s]Extractor Estimating: 38it [00:23,  1.73it/s]Extractor Estimating: 39it [00:23,  1.77it/s]Extractor Estimating: 40it [00:24,  1.71it/s]Extractor Estimating: 41it [00:25,  1.69it/s]Extractor Estimating: 42it [00:25,  1.69it/s]Extractor Estimating: 43it [00:26,  1.74it/s]Extractor Estimating: 44it [00:26,  1.77it/s]Extractor Estimating: 45it [00:27,  1.71it/s]Extractor Estimating: 46it [00:28,  1.65it/s]Extractor Estimating: 47it [00:28,  1.72it/s]Extractor Estimating: 48it [00:29,  1.77it/s]Extractor Estimating: 49it [00:29,  1.77it/s]Extractor Estimating: 50it [00:30,  1.76it/s]Extractor Estimating: 51it [00:31,  1.61it/s]Extractor Estimating: 52it [00:32,  1.41it/s]Extractor Estimating: 53it [00:32,  1.43it/s]Extractor Estimating: 54it [00:33,  1.38it/s]Extractor Estimating: 55it [00:34,  1.42it/s]Extractor Estimating: 56it [00:34,  1.42it/s]Extractor Estimating: 57it [00:35,  1.44it/s]Extractor Estimating: 58it [00:36,  1.48it/s]Extractor Estimating: 59it [00:36,  1.48it/s]Extractor Estimating: 60it [00:37,  1.50it/s]Extractor Estimating: 61it [00:38,  1.48it/s]Extractor Estimating: 62it [00:38,  1.47it/s]Extractor Estimating: 63it [00:39,  1.49it/s]Extractor Estimating: 64it [00:40,  1.53it/s]Extractor Estimating: 65it [00:40,  1.51it/s]Extractor Estimating: 66it [00:41,  1.51it/s]Extractor Estimating: 67it [00:42,  1.49it/s]Extractor Estimating: 68it [00:42,  1.49it/s]Extractor Estimating: 69it [00:43,  1.45it/s]Extractor Estimating: 70it [00:44,  1.53it/s]Extractor Estimating: 71it [00:44,  1.53it/s]Extractor Estimating: 72it [00:45,  1.50it/s]Extractor Estimating: 73it [00:46,  1.15it/s]Extractor Estimating: 74it [00:47,  1.26it/s]Extractor Estimating: 75it [00:48,  1.26it/s]Extractor Estimating: 76it [00:48,  1.41it/s]Extractor Estimating: 77it [00:49,  1.22it/s]Extractor Estimating: 78it [00:50,  1.37it/s]Extractor Estimating: 79it [00:50,  1.52it/s]Extractor Estimating: 80it [00:51,  1.62it/s]Extractor Estimating: 81it [00:51,  1.75it/s]Extractor Estimating: 82it [00:52,  1.66it/s]Extractor Estimating: 83it [00:53,  1.74it/s]Extractor Estimating: 84it [00:53,  1.76it/s]Extractor Estimating: 85it [00:54,  1.84it/s]Extractor Estimating: 86it [00:54,  1.88it/s]Extractor Estimating: 87it [00:55,  1.88it/s]Extractor Estimating: 88it [00:55,  1.73it/s]Extractor Estimating: 89it [00:56,  1.79it/s]Extractor Estimating: 90it [00:56,  1.84it/s]Extractor Estimating: 91it [00:57,  1.84it/s]Extractor Estimating: 92it [00:57,  1.87it/s]Extractor Estimating: 93it [00:58,  1.93it/s]Extractor Estimating: 94it [00:59,  1.61it/s]Extractor Estimating: 95it [00:59,  1.67it/s]Extractor Estimating: 96it [01:00,  1.73it/s]Extractor Estimating: 97it [01:00,  1.82it/s]Extractor Estimating: 98it [01:01,  1.82it/s]Extractor Estimating: 99it [01:01,  1.86it/s]Extractor Estimating: 100it [01:02,  1.78it/s]Extractor Estimating: 101it [01:03,  1.37it/s]Extractor Estimating: 102it [01:04,  1.52it/s]Extractor Estimating: 103it [01:04,  1.63it/s]Extractor Estimating: 104it [01:05,  1.68it/s]Extractor Estimating: 105it [01:05,  1.76it/s]Extractor Estimating: 106it [01:06,  1.31it/s]Extractor Estimating: 107it [01:07,  1.43it/s]Extractor Estimating: 108it [01:07,  1.58it/s]Extractor Estimating: 109it [01:08,  1.67it/s]Extractor Estimating: 110it [01:08,  1.75it/s]Extractor Estimating: 111it [01:09,  1.51it/s]Extractor Estimating: 112it [01:10,  1.62it/s]Extractor Estimating: 113it [01:10,  1.68it/s]Extractor Estimating: 114it [01:11,  1.78it/s]Extractor Estimating: 115it [01:11,  1.82it/s]Extractor Estimating: 116it [01:12,  1.86it/s]Extractor Estimating: 117it [01:13,  1.37it/s]Extractor Estimating: 118it [01:14,  1.47it/s]Extractor Estimating: 119it [01:14,  1.55it/s]Extractor Estimating: 120it [01:15,  1.67it/s]Extractor Estimating: 121it [01:15,  1.72it/s]Extractor Estimating: 122it [01:16,  1.65it/s]Extractor Estimating: 123it [01:16,  1.68it/s]Extractor Estimating: 124it [01:17,  1.78it/s]Extractor Estimating: 125it [01:17,  1.77it/s]Extractor Estimating: 126it [01:18,  1.77it/s]Extractor Estimating: 127it [01:19,  1.77it/s]Extractor Estimating: 128it [01:20,  1.35it/s]Extractor Estimating: 129it [01:20,  1.49it/s]Extractor Estimating: 130it [01:21,  1.53it/s]Extractor Estimating: 131it [01:21,  1.65it/s]Extractor Estimating: 132it [01:22,  1.66it/s]Extractor Estimating: 133it [01:23,  1.64it/s]Extractor Estimating: 134it [01:23,  1.73it/s]Extractor Estimating: 135it [01:24,  1.80it/s]Extractor Estimating: 136it [01:24,  1.79it/s]Extractor Estimating: 137it [01:25,  1.85it/s]Extractor Estimating: 138it [01:25,  1.81it/s]Extractor Estimating: 139it [01:26,  1.64it/s]Extractor Estimating: 140it [01:27,  1.69it/s]Extractor Estimating: 141it [01:27,  1.71it/s]Extractor Estimating: 142it [01:28,  1.72it/s]Extractor Estimating: 143it [01:28,  1.76it/s]Extractor Estimating: 144it [01:29,  1.76it/s]Extractor Estimating: 145it [01:29,  1.72it/s]Extractor Estimating: 146it [01:30,  1.75it/s]Extractor Estimating: 147it [01:30,  1.81it/s]Extractor Estimating: 148it [01:31,  1.83it/s]Extractor Estimating: 149it [01:32,  1.81it/s]Extractor Estimating: 150it [01:32,  1.84it/s]Extractor Estimating: 151it [01:33,  1.64it/s]Extractor Estimating: 152it [01:33,  1.74it/s]Extractor Estimating: 153it [01:34,  1.50it/s]Extractor Estimating: 154it [01:35,  1.62it/s]Extractor Estimating: 155it [01:35,  1.70it/s]Extractor Estimating: 156it [01:36,  1.77it/s]Extractor Estimating: 157it [01:36,  1.79it/s]Extractor Estimating: 158it [01:37,  1.67it/s]Extractor Estimating: 159it [01:37,  1.74it/s]Extractor Estimating: 160it [01:38,  1.79it/s]Extractor Estimating: 161it [01:39,  1.80it/s]Extractor Estimating: 162it [01:39,  1.83it/s]Extractor Estimating: 163it [01:40,  1.82it/s]Extractor Estimating: 164it [01:40,  1.79it/s]Extractor Estimating: 165it [01:41,  1.76it/s]Extractor Estimating: 166it [01:41,  1.81it/s]Extractor Estimating: 167it [01:42,  1.84it/s]Extractor Estimating: 168it [01:42,  1.90it/s]Extractor Estimating: 169it [01:43,  1.93it/s]Extractor Estimating: 170it [01:43,  1.78it/s]Extractor Estimating: 171it [01:44,  1.85it/s]Extractor Estimating: 172it [01:44,  1.88it/s]Extractor Estimating: 173it [01:45,  1.93it/s]Extractor Estimating: 174it [01:46,  1.90it/s]Extractor Estimating: 175it [01:46,  1.90it/s]Extractor Estimating: 176it [01:47,  1.69it/s]Extractor Estimating: 177it [01:47,  1.74it/s]Extractor Estimating: 178it [01:48,  1.75it/s]Extractor Estimating: 179it [01:48,  1.78it/s]Extractor Estimating: 180it [01:49,  1.82it/s]Extractor Estimating: 181it [01:50,  1.77it/s]Extractor Estimating: 182it [01:50,  1.76it/s]Extractor Estimating: 183it [01:51,  1.60it/s]Extractor Estimating: 184it [01:51,  1.62it/s]Extractor Estimating: 185it [01:52,  1.68it/s]Extractor Estimating: 186it [01:53,  1.70it/s]Extractor Estimating: 187it [01:53,  1.56it/s]Extractor Estimating: 188it [01:54,  1.61it/s]Extractor Estimating: 189it [01:55,  1.64it/s]Extractor Estimating: 190it [01:55,  1.69it/s]Extractor Estimating: 191it [01:56,  1.67it/s]Extractor Estimating: 192it [01:56,  1.63it/s]Extractor Estimating: 193it [01:57,  1.68it/s]Extractor Estimating: 194it [01:57,  1.71it/s]Extractor Estimating: 195it [01:58,  1.73it/s]Extractor Estimating: 196it [01:59,  1.78it/s]Extractor Estimating: 197it [01:59,  1.85it/s]Extractor Estimating: 198it [02:00,  1.59it/s]Extractor Estimating: 199it [02:00,  1.59it/s]Extractor Estimating: 200it [02:01,  1.66it/s]Extractor Estimating: 201it [02:02,  1.66it/s]Extractor Estimating: 202it [02:02,  1.50it/s]Extractor Estimating: 203it [02:03,  1.46it/s]Extractor Estimating: 204it [02:04,  1.52it/s]Extractor Estimating: 205it [02:04,  1.54it/s]Extractor Estimating: 206it [02:05,  1.31it/s]Extractor Estimating: 207it [02:06,  1.40it/s]Extractor Estimating: 208it [02:07,  1.49it/s]Extractor Estimating: 209it [02:07,  1.46it/s]Extractor Estimating: 210it [02:08,  1.38it/s]Extractor Estimating: 211it [02:09,  1.48it/s]Extractor Estimating: 212it [02:09,  1.51it/s]Extractor Estimating: 213it [02:10,  1.59it/s]Extractor Estimating: 214it [02:10,  1.64it/s]Extractor Estimating: 215it [02:11,  1.57it/s]Extractor Estimating: 216it [02:12,  1.56it/s]Extractor Estimating: 217it [02:12,  1.59it/s]Extractor Estimating: 218it [02:13,  1.62it/s]Extractor Estimating: 219it [02:14,  1.66it/s]Extractor Estimating: 220it [02:14,  1.66it/s]Extractor Estimating: 221it [02:15,  1.67it/s]Extractor Estimating: 222it [02:15,  1.65it/s]Extractor Estimating: 223it [02:16,  1.64it/s]Extractor Estimating: 224it [02:17,  1.65it/s]Extractor Estimating: 225it [02:17,  1.62it/s]Extractor Estimating: 226it [02:18,  1.57it/s]Extractor Estimating: 227it [02:19,  1.55it/s]Extractor Estimating: 228it [02:19,  1.56it/s]Extractor Estimating: 229it [02:20,  1.56it/s]Extractor Estimating: 230it [02:21,  1.43it/s]Extractor Estimating: 231it [02:21,  1.48it/s]Extractor Estimating: 232it [02:22,  1.53it/s]Extractor Estimating: 233it [02:22,  1.57it/s]Extractor Estimating: 234it [02:23,  1.58it/s]Extractor Estimating: 235it [02:24,  1.44it/s]Extractor Estimating: 236it [02:25,  1.47it/s]Extractor Estimating: 237it [02:25,  1.47it/s]Extractor Estimating: 238it [02:26,  1.50it/s]Extractor Estimating: 239it [02:27,  1.53it/s]Extractor Estimating: 240it [02:27,  1.47it/s]Extractor Estimating: 241it [02:28,  1.54it/s]Extractor Estimating: 242it [02:28,  1.56it/s]Extractor Estimating: 243it [02:29,  1.57it/s]Extractor Estimating: 244it [02:30,  1.54it/s]Extractor Estimating: 245it [02:31,  1.47it/s]Extractor Estimating: 246it [02:31,  1.49it/s]Extractor Estimating: 247it [02:32,  1.52it/s]Extractor Estimating: 248it [02:32,  1.51it/s]Extractor Estimating: 249it [02:33,  1.50it/s]Extractor Estimating: 250it [02:35,  1.14it/s]Extractor Estimating: 251it [02:35,  1.35it/s]Extractor Estimating: 252it [02:35,  1.57it/s]Extractor Estimating: 253it [02:36,  1.36it/s]Extractor Estimating: 254it [02:37,  1.54it/s]Extractor Estimating: 255it [02:37,  1.71it/s]Extractor Estimating: 256it [02:38,  1.84it/s]Extractor Estimating: 257it [02:38,  1.93it/s]Extractor Estimating: 258it [02:39,  1.96it/s]Extractor Estimating: 259it [02:39,  2.01it/s]Extractor Estimating: 260it [02:39,  2.10it/s]Extractor Estimating: 261it [02:40,  2.08it/s]Extractor Estimating: 262it [02:40,  2.19it/s]Extractor Estimating: 263it [02:41,  2.19it/s]Extractor Estimating: 264it [02:41,  2.21it/s]Extractor Estimating: 265it [02:42,  2.24it/s]Extractor Estimating: 266it [02:42,  2.17it/s]Extractor Estimating: 267it [02:43,  2.21it/s]Extractor Estimating: 268it [02:43,  2.31it/s]Extractor Estimating: 269it [02:43,  2.33it/s]Extractor Estimating: 270it [02:44,  2.40it/s]Extractor Estimating: 271it [02:44,  2.38it/s]Extractor Estimating: 272it [02:45,  2.41it/s]Extractor Estimating: 273it [02:45,  2.32it/s]Extractor Estimating: 274it [02:46,  2.06it/s]Extractor Estimating: 275it [02:46,  2.06it/s]Extractor Estimating: 276it [02:47,  1.92it/s]Extractor Estimating: 277it [02:47,  1.85it/s]Extractor Estimating: 278it [02:48,  1.78it/s]Extractor Estimating: 279it [02:49,  1.71it/s]Extractor Estimating: 280it [02:50,  1.47it/s]Extractor Estimating: 281it [02:50,  1.52it/s]Extractor Estimating: 282it [02:51,  1.55it/s]Extractor Estimating: 283it [02:51,  1.63it/s]Extractor Estimating: 284it [02:52,  1.62it/s]Extractor Estimating: 285it [02:53,  1.49it/s]Extractor Estimating: 286it [02:53,  1.56it/s]Extractor Estimating: 287it [02:54,  1.60it/s]Extractor Estimating: 288it [02:54,  1.65it/s]Extractor Estimating: 289it [02:55,  1.62it/s]Extractor Estimating: 290it [02:56,  1.44it/s]Extractor Estimating: 291it [02:57,  1.49it/s]Extractor Estimating: 292it [02:57,  1.50it/s]Extractor Estimating: 293it [02:58,  1.54it/s]Extractor Estimating: 294it [02:58,  1.59it/s]Extractor Estimating: 295it [02:59,  1.48it/s]Extractor Estimating: 296it [03:00,  1.54it/s]Extractor Estimating: 297it [03:00,  1.63it/s]Extractor Estimating: 298it [03:01,  1.62it/s]Extractor Estimating: 299it [03:02,  1.64it/s]Extractor Estimating: 300it [03:02,  1.56it/s]Extractor Estimating: 301it [03:03,  1.59it/s]Extractor Estimating: 302it [03:04,  1.58it/s]Extractor Estimating: 303it [03:04,  1.61it/s]Extractor Estimating: 304it [03:05,  1.65it/s]Extractor Estimating: 305it [03:05,  1.60it/s]Extractor Estimating: 306it [03:06,  1.66it/s]Extractor Estimating: 307it [03:06,  1.68it/s]Extractor Estimating: 308it [03:07,  1.69it/s]Extractor Estimating: 309it [03:08,  1.61it/s]Extractor Estimating: 310it [03:08,  1.68it/s]Extractor Estimating: 311it [03:09,  1.69it/s]Extractor Estimating: 312it [03:09,  1.69it/s]Extractor Estimating: 313it [03:10,  1.61it/s]Extractor Estimating: 314it [03:11,  1.63it/s]Extractor Estimating: 315it [03:11,  1.60it/s]Extractor Estimating: 316it [03:12,  1.65it/s]Extractor Estimating: 317it [03:13,  1.68it/s]Extractor Estimating: 318it [03:13,  1.46it/s]Extractor Estimating: 319it [03:14,  1.50it/s]Extractor Estimating: 320it [03:15,  1.55it/s]Extractor Estimating: 321it [03:15,  1.57it/s]Extractor Estimating: 322it [03:16,  1.55it/s]Extractor Estimating: 323it [03:17,  1.56it/s]Extractor Estimating: 324it [03:17,  1.59it/s]Extractor Estimating: 325it [03:18,  1.58it/s]Extractor Estimating: 326it [03:18,  1.58it/s]Extractor Estimating: 327it [03:19,  1.59it/s]Extractor Estimating: 328it [03:20,  1.41it/s]Extractor Estimating: 329it [03:21,  1.48it/s]Extractor Estimating: 330it [03:21,  1.49it/s]Extractor Estimating: 331it [03:22,  1.53it/s]Extractor Estimating: 332it [03:22,  1.59it/s]Extractor Estimating: 333it [03:23,  1.45it/s]Extractor Estimating: 334it [03:24,  1.45it/s]Extractor Estimating: 335it [03:25,  1.49it/s]Extractor Estimating: 336it [03:25,  1.53it/s]Extractor Estimating: 337it [03:26,  1.59it/s]Extractor Estimating: 338it [03:26,  1.53it/s]Extractor Estimating: 339it [03:27,  1.49it/s]Extractor Estimating: 340it [03:28,  1.53it/s]Extractor Estimating: 341it [03:28,  1.53it/s]Extractor Estimating: 342it [03:29,  1.60it/s]Extractor Estimating: 343it [03:30,  1.53it/s]Extractor Estimating: 344it [03:30,  1.59it/s]Extractor Estimating: 345it [03:31,  1.46it/s]Extractor Estimating: 346it [03:32,  1.51it/s]Extractor Estimating: 347it [03:32,  1.56it/s]Extractor Estimating: 348it [03:33,  1.53it/s]Extractor Estimating: 349it [03:34,  1.58it/s]Extractor Estimating: 350it [03:34,  1.59it/s]Extractor Estimating: 351it [03:35,  1.65it/s]Extractor Estimating: 352it [03:35,  1.68it/s]Extractor Estimating: 353it [03:36,  1.62it/s]Extractor Estimating: 354it [03:37,  1.64it/s]Extractor Estimating: 355it [03:37,  1.71it/s]Extractor Estimating: 356it [03:38,  1.75it/s]Extractor Estimating: 357it [03:38,  1.80it/s]Extractor Estimating: 358it [03:39,  1.73it/s]Extractor Estimating: 359it [03:39,  1.76it/s]Extractor Estimating: 360it [03:40,  1.81it/s]Extractor Estimating: 361it [03:40,  1.79it/s]Extractor Estimating: 362it [03:41,  1.82it/s]Extractor Estimating: 363it [03:42,  1.74it/s]Extractor Estimating: 364it [03:42,  1.74it/s]Extractor Estimating: 365it [03:43,  1.73it/s]Extractor Estimating: 366it [03:43,  1.74it/s]Extractor Estimating: 367it [03:44,  1.74it/s]Extractor Estimating: 368it [03:44,  1.77it/s]Extractor Estimating: 369it [03:45,  1.78it/s]Extractor Estimating: 370it [03:46,  1.79it/s]Extractor Estimating: 371it [03:46,  1.79it/s]Extractor Estimating: 372it [03:47,  1.77it/s]Extractor Estimating: 373it [03:47,  1.77it/s]Extractor Estimating: 374it [03:48,  1.77it/s]Extractor Estimating: 375it [03:48,  1.74it/s]Extractor Estimating: 375it [03:48,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:30,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:30,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:30,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:30,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:30,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:37:33,514 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:37:33,516 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:37:34,255 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:37:36,048 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:37:36,048 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:40,241 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:40,408 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:40,408 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:40,408 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:40,408 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:37:42,310 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:37:42,311 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:37:43,279 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:37:44,424 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:37:44,424 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 23:18:48,074 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 23:18:48,132 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 6000 mean pseudo reward: 0.984586912053835
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 15755
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15855, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15855, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.987, loss:264.6471
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.945, loss:238.7919
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 0.949, loss:222.9228
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 150, avg_time 0.949, loss:204.2180
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 250, avg_time 0.961, loss:207.8878
>> valid entity prec:0.5145, rec:0.5208, f1:0.5176
>> valid relation prec:0.1767, rec:0.1248, f1:0.1463
>> valid relation with NER prec:0.1767, rec:0.1248, f1:0.1463
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 0.955, loss:191.5822
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 200, avg_time 0.973, loss:195.4781
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 0.971, loss:193.3948
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 150, avg_time 0.960, loss:190.9668
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 250, avg_time 0.944, loss:220.6698
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5149, rec:0.4888, f1:0.5015
>> valid relation prec:0.1683, rec:0.0919, f1:0.1189
>> valid relation with NER prec:0.1683, rec:0.0919, f1:0.1189
g_step 1100, step 100, avg_time 0.952, loss:177.9949
g_step 1200, step 200, avg_time 0.963, loss:196.3049
g_step 1300, step 50, avg_time 0.959, loss:178.8263
g_step 1400, step 150, avg_time 0.964, loss:173.3095
g_step 1500, step 250, avg_time 0.974, loss:167.1612
>> valid entity prec:0.5444, rec:0.4702, f1:0.5046
>> valid relation prec:0.1763, rec:0.1011, f1:0.1285
>> valid relation with NER prec:0.1763, rec:0.1011, f1:0.1285
g_step 1600, step 100, avg_time 0.954, loss:160.1456
g_step 1700, step 200, avg_time 0.972, loss:184.5262
g_step 1800, step 50, avg_time 0.963, loss:158.6457
g_step 1900, step 150, avg_time 0.963, loss:157.2927
g_step 2000, step 250, avg_time 0.962, loss:157.4399
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5165, rec:0.5430, f1:0.5294
>> valid relation prec:0.1541, rec:0.1191, f1:0.1344
>> valid relation with NER prec:0.1541, rec:0.1191, f1:0.1344
new max entity f1 on valid!
g_step 2100, step 100, avg_time 0.946, loss:122.1131
g_step 2200, step 200, avg_time 0.965, loss:144.3342
g_step 2300, step 50, avg_time 0.971, loss:137.5261
g_step 2400, step 150, avg_time 0.967, loss:133.2324
g_step 2500, step 250, avg_time 0.969, loss:134.2201
>> valid entity prec:0.5446, rec:0.5235, f1:0.5339
>> valid relation prec:0.1883, rec:0.1366, f1:0.1583
>> valid relation with NER prec:0.1883, rec:0.1366, f1:0.1583
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 100, avg_time 0.974, loss:111.2081
g_step 2700, step 200, avg_time 0.955, loss:130.1118
g_step 2800, step 50, avg_time 0.943, loss:131.0611
g_step 2900, step 150, avg_time 0.982, loss:119.1435
g_step 3000, step 250, avg_time 0.956, loss:133.8188
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5320, rec:0.4975, f1:0.5141
>> valid relation prec:0.1650, rec:0.1117, f1:0.1332
>> valid relation with NER prec:0.1650, rec:0.1117, f1:0.1332
g_step 3100, step 100, avg_time 0.985, loss:106.9665
g_step 3200, step 200, avg_time 0.951, loss:116.1313
g_step 3300, step 50, avg_time 0.968, loss:110.1365
g_step 3400, step 150, avg_time 0.966, loss:104.7751
g_step 3500, step 250, avg_time 0.973, loss:125.3111
>> valid entity prec:0.5373, rec:0.5044, f1:0.5203
>> valid relation prec:0.1770, rec:0.1200, f1:0.1430
>> valid relation with NER prec:0.1770, rec:0.1200, f1:0.1430
g_step 3600, step 100, avg_time 0.967, loss:97.2678
g_step 3700, step 200, avg_time 0.969, loss:94.8631
g_step 3800, step 50, avg_time 0.972, loss:102.3503
g_step 3900, step 150, avg_time 0.973, loss:92.3912
g_step 4000, step 250, avg_time 0.972, loss:109.6994
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5436, rec:0.5144, f1:0.5286
>> valid relation prec:0.1737, rec:0.1151, f1:0.1384
>> valid relation with NER prec:0.1737, rec:0.1151, f1:0.1384
g_step 4100, step 100, avg_time 0.974, loss:85.4343
g_step 4200, step 200, avg_time 0.973, loss:96.6069
g_step 4300, step 50, avg_time 1.004, loss:82.9105
g_step 4400, step 150, avg_time 0.967, loss:90.1487
g_step 4500, step 250, avg_time 0.965, loss:95.8343
>> valid entity prec:0.5219, rec:0.5009, f1:0.5112
>> valid relation prec:0.1470, rec:0.0976, f1:0.1173
>> valid relation with NER prec:0.1470, rec:0.0976, f1:0.1173
g_step 4600, step 100, avg_time 0.981, loss:82.7393
g_step 4700, step 200, avg_time 0.972, loss:88.4494
g_step 4800, step 50, avg_time 0.967, loss:84.1222
g_step 4900, step 150, avg_time 0.983, loss:74.3094
g_step 5000, step 250, avg_time 0.970, loss:82.4180
learning rate was adjusted to 0.0008
>> valid entity prec:0.5573, rec:0.4641, f1:0.5065
>> valid relation prec:0.1601, rec:0.0950, f1:0.1193
>> valid relation with NER prec:0.1601, rec:0.0950, f1:0.1193
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 23:18:48 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 23:18:48 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_23-18-48_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 23:18:50 - WARNING - datasets.builder -   Using custom data configuration default-04e1ab0080081023
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-04e1ab0080081023/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]1 tables [00:00,  3.04 tables/s]                                [INFO|configuration_utils.py:515] 2023-08-28 23:18:58,158 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:18:58,215 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:18:58,216 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:18:58,217 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:18:58,451 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:18:58,561 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:18:58,561 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:18:58,562 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:18:58,562 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:18:58,562 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:18:58,562 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 23:18:59,071 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:19:02,381 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 23:19:02,400 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-04e1ab0080081023/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.03ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.13ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.81ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.23ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.50ba/s]100%|██████████| 6/6 [00:01<00:00,  4.67ba/s]100%|██████████| 6/6 [00:01<00:00,  4.06ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.22ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.13ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.61ba/s]100%|██████████| 4/4 [00:01<00:00,  4.69ba/s]100%|██████████| 4/4 [00:01<00:00,  3.92ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.63ba/s] 50%|█████     | 3/6 [00:00<00:00,  5.96ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  7.81ba/s]100%|██████████| 6/6 [00:00<00:00,  7.14ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.41ba/s] 50%|█████     | 2/4 [00:00<00:00,  5.27ba/s]100%|██████████| 4/4 [00:00<00:00,  8.63ba/s]100%|██████████| 4/4 [00:00<00:00,  7.21ba/s]
[INFO|trainer.py:414] 2023-08-28 23:19:09,323 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 23:19:09,437 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 23:19:09,437 >>   Num examples = 6000
[INFO|trainer.py:1149] 2023-08-28 23:19:09,437 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 23:19:09,437 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 23:19:09,437 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 23:19:09,437 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 23:19:09,437 >>   Total optimization steps = 470
  0%|          | 0/470 [00:00<?, ?it/s]  0%|          | 1/470 [00:01<11:02,  1.41s/it]  0%|          | 2/470 [00:01<06:59,  1.12it/s]  1%|          | 3/470 [00:02<05:36,  1.39it/s]  1%|          | 4/470 [00:03<05:08,  1.51it/s]  1%|          | 5/470 [00:03<04:12,  1.84it/s]  1%|▏         | 6/470 [00:03<03:37,  2.14it/s]  1%|▏         | 7/470 [00:04<03:30,  2.20it/s]  2%|▏         | 8/470 [00:04<03:13,  2.39it/s]  2%|▏         | 9/470 [00:04<02:54,  2.64it/s]  2%|▏         | 10/470 [00:05<02:42,  2.83it/s]  2%|▏         | 11/470 [00:05<02:33,  2.98it/s]  3%|▎         | 12/470 [00:05<02:27,  3.10it/s]  3%|▎         | 13/470 [00:06<02:36,  2.92it/s]  3%|▎         | 14/470 [00:06<02:29,  3.05it/s]  3%|▎         | 15/470 [00:06<02:36,  2.91it/s]  3%|▎         | 16/470 [00:06<02:29,  3.04it/s]  4%|▎         | 17/470 [00:07<02:24,  3.14it/s]  4%|▍         | 18/470 [00:07<02:20,  3.21it/s]  4%|▍         | 19/470 [00:07<02:17,  3.27it/s]  4%|▍         | 20/470 [00:08<02:15,  3.31it/s]  4%|▍         | 21/470 [00:08<02:14,  3.33it/s]  5%|▍         | 22/470 [00:08<02:13,  3.35it/s]  5%|▍         | 23/470 [00:09<02:12,  3.36it/s]  5%|▌         | 24/470 [00:09<02:12,  3.37it/s]  5%|▌         | 25/470 [00:09<02:18,  3.22it/s]  6%|▌         | 26/470 [00:09<02:15,  3.27it/s]  6%|▌         | 27/470 [00:10<02:13,  3.31it/s]  6%|▌         | 28/470 [00:10<02:12,  3.34it/s]  6%|▌         | 29/470 [00:10<02:11,  3.35it/s]  6%|▋         | 30/470 [00:11<02:10,  3.36it/s]  7%|▋         | 31/470 [00:11<02:10,  3.38it/s]  7%|▋         | 32/470 [00:11<02:24,  3.04it/s]  7%|▋         | 33/470 [00:12<02:19,  3.13it/s]  7%|▋         | 34/470 [00:12<02:15,  3.21it/s]  7%|▋         | 35/470 [00:12<02:13,  3.26it/s]  8%|▊         | 36/470 [00:13<02:11,  3.30it/s]  8%|▊         | 37/470 [00:13<02:10,  3.33it/s]  8%|▊         | 38/470 [00:13<02:08,  3.35it/s]  8%|▊         | 39/470 [00:13<02:08,  3.36it/s]  9%|▊         | 40/470 [00:14<02:07,  3.37it/s]  9%|▊         | 41/470 [00:14<02:06,  3.38it/s]  9%|▉         | 42/470 [00:14<02:06,  3.39it/s]  9%|▉         | 43/470 [00:15<02:06,  3.39it/s]  9%|▉         | 44/470 [00:15<02:05,  3.39it/s] 10%|▉         | 45/470 [00:15<02:13,  3.18it/s] 10%|▉         | 46/470 [00:16<02:10,  3.24it/s] 10%|█         | 47/470 [00:16<02:08,  3.28it/s] 10%|█         | 48/470 [00:16<02:07,  3.31it/s] 10%|█         | 49/470 [00:16<02:06,  3.34it/s] 11%|█         | 50/470 [00:17<02:05,  3.35it/s] 11%|█         | 51/470 [00:17<02:04,  3.36it/s] 11%|█         | 52/470 [00:17<02:04,  3.37it/s] 11%|█▏        | 53/470 [00:18<02:03,  3.37it/s] 11%|█▏        | 54/470 [00:18<02:03,  3.38it/s] 12%|█▏        | 55/470 [00:18<02:12,  3.13it/s] 12%|█▏        | 56/470 [00:19<02:09,  3.19it/s] 12%|█▏        | 57/470 [00:19<02:07,  3.24it/s] 12%|█▏        | 58/470 [00:19<02:05,  3.28it/s] 13%|█▎        | 59/470 [00:19<02:04,  3.30it/s] 13%|█▎        | 60/470 [00:20<02:03,  3.32it/s] 13%|█▎        | 61/470 [00:20<02:02,  3.35it/s] 13%|█▎        | 62/470 [00:20<02:01,  3.36it/s] 13%|█▎        | 63/470 [00:21<02:01,  3.36it/s] 14%|█▎        | 64/470 [00:21<02:00,  3.38it/s] 14%|█▍        | 65/470 [00:21<02:05,  3.24it/s] 14%|█▍        | 66/470 [00:22<02:02,  3.29it/s] 14%|█▍        | 67/470 [00:22<02:01,  3.33it/s] 14%|█▍        | 68/470 [00:22<01:59,  3.36it/s] 15%|█▍        | 69/470 [00:22<01:58,  3.38it/s] 15%|█▍        | 70/470 [00:23<01:57,  3.40it/s] 15%|█▌        | 71/470 [00:23<02:07,  3.13it/s] 15%|█▌        | 72/470 [00:23<02:04,  3.21it/s] 16%|█▌        | 73/470 [00:24<02:01,  3.27it/s] 16%|█▌        | 74/470 [00:24<01:59,  3.32it/s] 16%|█▌        | 75/470 [00:24<02:03,  3.21it/s] 16%|█▌        | 76/470 [00:25<02:00,  3.27it/s] 16%|█▋        | 77/470 [00:25<01:58,  3.32it/s] 17%|█▋        | 78/470 [00:25<01:56,  3.35it/s] 17%|█▋        | 79/470 [00:26<01:55,  3.38it/s] 17%|█▋        | 80/470 [00:26<01:54,  3.39it/s] 17%|█▋        | 81/470 [00:26<01:54,  3.41it/s] 17%|█▋        | 82/470 [00:26<01:53,  3.41it/s] 18%|█▊        | 83/470 [00:27<01:53,  3.42it/s] 18%|█▊        | 84/470 [00:27<01:52,  3.43it/s] 18%|█▊        | 85/470 [00:27<01:52,  3.43it/s] 18%|█▊        | 86/470 [00:28<01:57,  3.28it/s] 19%|█▊        | 87/470 [00:28<01:55,  3.32it/s] 19%|█▊        | 88/470 [00:28<01:53,  3.35it/s] 19%|█▉        | 89/470 [00:28<01:53,  3.37it/s] 19%|█▉        | 90/470 [00:29<01:52,  3.38it/s] 19%|█▉        | 91/470 [00:29<01:51,  3.40it/s] 20%|█▉        | 92/470 [00:29<01:55,  3.27it/s] 20%|█▉        | 93/470 [00:30<01:53,  3.31it/s] 20%|██        | 94/470 [00:30<01:45,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 23:19:39,841 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:19:39,841 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:19:39,841 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.45it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.63it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.82it/s][A
  5%|▌         | 22/437 [00:00<00:13, 30.66it/s][A
  6%|▌         | 27/437 [00:00<00:11, 34.28it/s][A
  7%|▋         | 32/437 [00:00<00:10, 36.94it/s][A
  8%|▊         | 37/437 [00:00<00:10, 39.13it/s][A
 10%|▉         | 42/437 [00:01<00:09, 40.78it/s][A
 11%|█         | 47/437 [00:01<00:09, 41.82it/s][A
 12%|█▏        | 52/437 [00:01<00:09, 42.53it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.02it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 42.97it/s][A
 15%|█▌        | 67/437 [00:01<00:09, 38.80it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 40.71it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 41.86it/s][A
 19%|█▉        | 82/437 [00:02<00:08, 42.58it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 43.35it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.60it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.90it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.94it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.48it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.23it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.46it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.73it/s][A
 29%|██▉       | 127/437 [00:03<00:07, 43.83it/s][A
 30%|███       | 132/437 [00:03<00:06, 43.94it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.13it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.26it/s][A
 34%|███▎      | 147/437 [00:03<00:08, 35.79it/s][A
 35%|███▍      | 152/437 [00:03<00:07, 38.09it/s][A
 36%|███▌      | 157/437 [00:03<00:07, 39.75it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 41.08it/s][A
 38%|███▊      | 167/437 [00:04<00:06, 41.97it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 42.65it/s][A
 41%|████      | 177/437 [00:04<00:06, 43.18it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.24it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.17it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.05it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.33it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.60it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.81it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 44.00it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.22it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.09it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.08it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.71it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.68it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.77it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.88it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.96it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 44.05it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.05it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.08it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.98it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 42.19it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 42.75it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 42.97it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.18it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 43.47it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 43.88it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.91it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.99it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.72it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.69it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.89it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.87it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.92it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 43.99it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 43.99it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.98it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.16it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.92it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.79it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.83it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.84it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.10it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 44.12it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.17it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.94it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.78it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.82it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 30.17it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 33.28it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 35.92it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 38.26it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 39.97it/s][A
100%|██████████| 437/437 [00:10<00:00, 41.38it/s][A                                                
                                                 [A 20%|██        | 94/470 [00:40<01:45,  3.57it/s]
100%|██████████| 437/437 [00:10<00:00, 41.38it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:19:50,635 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 23:19:51,066 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:19:57,589 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:19:58,387 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:19:58,672 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-94/special_tokens_map.json
 20%|██        | 95/470 [01:05<1:07:27, 10.79s/it] 20%|██        | 96/470 [01:06<47:51,  7.68s/it]   21%|██        | 97/470 [01:06<33:57,  5.46s/it] 21%|██        | 98/470 [01:06<24:15,  3.91s/it] 21%|██        | 99/470 [01:07<17:28,  2.83s/it] 21%|██▏       | 100/470 [01:07<12:44,  2.07s/it] 21%|██▏       | 101/470 [01:07<09:26,  1.54s/it] 22%|██▏       | 102/470 [01:07<07:07,  1.16s/it] 22%|██▏       | 103/470 [01:08<05:31,  1.11it/s] 22%|██▏       | 104/470 [01:08<04:23,  1.39it/s] 22%|██▏       | 105/470 [01:08<03:36,  1.69it/s] 23%|██▎       | 106/470 [01:09<03:21,  1.81it/s] 23%|██▎       | 107/470 [01:09<02:52,  2.10it/s] 23%|██▎       | 108/470 [01:09<02:32,  2.38it/s] 23%|██▎       | 109/470 [01:10<02:18,  2.61it/s] 23%|██▎       | 110/470 [01:10<02:08,  2.80it/s] 24%|██▎       | 111/470 [01:10<02:01,  2.96it/s] 24%|██▍       | 112/470 [01:11<01:56,  3.08it/s] 24%|██▍       | 113/470 [01:11<01:52,  3.16it/s] 24%|██▍       | 114/470 [01:11<01:50,  3.23it/s] 24%|██▍       | 115/470 [01:11<01:48,  3.27it/s] 25%|██▍       | 116/470 [01:12<01:51,  3.18it/s] 25%|██▍       | 117/470 [01:12<01:48,  3.24it/s] 25%|██▌       | 118/470 [01:12<01:47,  3.28it/s] 25%|██▌       | 119/470 [01:13<01:45,  3.31it/s] 26%|██▌       | 120/470 [01:13<01:44,  3.34it/s] 26%|██▌       | 121/470 [01:13<01:44,  3.35it/s] 26%|██▌       | 122/470 [01:14<01:53,  3.07it/s] 26%|██▌       | 123/470 [01:14<01:49,  3.15it/s] 26%|██▋       | 124/470 [01:14<01:47,  3.22it/s] 27%|██▋       | 125/470 [01:14<01:45,  3.27it/s] 27%|██▋       | 126/470 [01:15<01:44,  3.30it/s] 27%|██▋       | 127/470 [01:15<01:43,  3.33it/s] 27%|██▋       | 128/470 [01:15<01:41,  3.35it/s] 27%|██▋       | 129/470 [01:16<01:41,  3.37it/s] 28%|██▊       | 130/470 [01:16<01:40,  3.39it/s] 28%|██▊       | 131/470 [01:16<01:39,  3.40it/s] 28%|██▊       | 132/470 [01:17<01:42,  3.30it/s] 28%|██▊       | 133/470 [01:17<01:41,  3.33it/s] 29%|██▊       | 134/470 [01:17<01:39,  3.37it/s] 29%|██▊       | 135/470 [01:17<01:38,  3.39it/s] 29%|██▉       | 136/470 [01:18<01:38,  3.40it/s] 29%|██▉       | 137/470 [01:18<01:37,  3.41it/s] 29%|██▉       | 138/470 [01:18<01:37,  3.41it/s] 30%|██▉       | 139/470 [01:19<01:36,  3.42it/s] 30%|██▉       | 140/470 [01:19<01:36,  3.42it/s] 30%|███       | 141/470 [01:19<01:36,  3.43it/s] 30%|███       | 142/470 [01:19<01:35,  3.43it/s] 30%|███       | 143/470 [01:20<01:41,  3.23it/s] 31%|███       | 144/470 [01:20<01:39,  3.28it/s] 31%|███       | 145/470 [01:20<01:37,  3.32it/s] 31%|███       | 146/470 [01:21<01:36,  3.35it/s] 31%|███▏      | 147/470 [01:21<01:35,  3.37it/s] 31%|███▏      | 148/470 [01:21<01:35,  3.38it/s] 32%|███▏      | 149/470 [01:22<01:34,  3.40it/s] 32%|███▏      | 150/470 [01:22<01:33,  3.41it/s] 32%|███▏      | 151/470 [01:22<01:33,  3.42it/s] 32%|███▏      | 152/470 [01:22<01:33,  3.42it/s] 33%|███▎      | 153/470 [01:23<01:32,  3.42it/s] 33%|███▎      | 154/470 [01:23<02:14,  2.35it/s] 33%|███▎      | 155/470 [01:24<02:01,  2.59it/s] 33%|███▎      | 156/470 [01:24<01:52,  2.79it/s] 33%|███▎      | 157/470 [01:24<01:45,  2.95it/s] 34%|███▎      | 158/470 [01:25<01:41,  3.08it/s] 34%|███▍      | 159/470 [01:25<01:37,  3.17it/s] 34%|███▍      | 160/470 [01:25<01:35,  3.24it/s] 34%|███▍      | 161/470 [01:26<01:33,  3.29it/s] 34%|███▍      | 162/470 [01:26<01:32,  3.33it/s] 35%|███▍      | 163/470 [01:26<01:45,  2.92it/s] 35%|███▍      | 164/470 [01:27<01:40,  3.05it/s] 35%|███▌      | 165/470 [01:27<01:36,  3.15it/s] 35%|███▌      | 166/470 [01:27<01:34,  3.23it/s] 36%|███▌      | 167/470 [01:27<01:32,  3.28it/s] 36%|███▌      | 168/470 [01:28<01:31,  3.32it/s] 36%|███▌      | 169/470 [01:28<01:29,  3.34it/s] 36%|███▌      | 170/470 [01:28<01:29,  3.37it/s] 36%|███▋      | 171/470 [01:29<01:28,  3.38it/s] 37%|███▋      | 172/470 [01:29<01:27,  3.39it/s] 37%|███▋      | 173/470 [01:29<01:31,  3.26it/s] 37%|███▋      | 174/470 [01:30<01:29,  3.29it/s] 37%|███▋      | 175/470 [01:30<01:28,  3.32it/s] 37%|███▋      | 176/470 [01:30<01:28,  3.33it/s] 38%|███▊      | 177/470 [01:30<01:27,  3.34it/s] 38%|███▊      | 178/470 [01:31<01:27,  3.35it/s] 38%|███▊      | 179/470 [01:31<01:26,  3.36it/s] 38%|███▊      | 180/470 [01:31<01:26,  3.37it/s] 39%|███▊      | 181/470 [01:32<01:32,  3.11it/s] 39%|███▊      | 182/470 [01:32<01:30,  3.18it/s] 39%|███▉      | 183/470 [01:32<01:41,  2.83it/s] 39%|███▉      | 184/470 [01:33<01:36,  2.97it/s] 39%|███▉      | 185/470 [01:33<01:32,  3.09it/s] 40%|███▉      | 186/470 [01:33<01:29,  3.17it/s] 40%|███▉      | 187/470 [01:34<01:27,  3.24it/s] 40%|████      | 188/470 [01:34<01:20,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 23:20:43,799 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:20:43,799 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:20:43,799 >>   Batch size = 8
{'eval_loss': 1.0908325910568237, 'eval_runtime': 10.3683, 'eval_samples_per_second': 336.893, 'eval_steps_per_second': 42.148, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.92it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.48it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.11it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.37it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.92it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.36it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.08it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.93it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.11it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.12it/s][A
 13%|█▎        | 57/437 [00:01<00:10, 35.68it/s][A
 14%|█▍        | 62/437 [00:01<00:09, 37.93it/s][A
 15%|█▌        | 67/437 [00:01<00:09, 39.78it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 41.13it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 42.14it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 42.70it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 43.14it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.35it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.14it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.21it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.38it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.62it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.75it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.98it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.17it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.20it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.91it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.58it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.54it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.60it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.73it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.01it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.17it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.12it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.12it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.78it/s][A
 43%|████▎     | 187/437 [00:04<00:07, 32.93it/s][A
 44%|████▍     | 192/437 [00:04<00:06, 35.56it/s][A
 45%|████▌     | 197/437 [00:04<00:06, 37.80it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 39.60it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 40.97it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 41.85it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 42.69it/s][A
 51%|█████     | 222/437 [00:05<00:05, 42.97it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 42.94it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 42.95it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.24it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.56it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.84it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.03it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 44.18it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.24it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.97it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.67it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.59it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.69it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.87it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.09it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.25it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.26it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.10it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.85it/s][A
 73%|███████▎  | 317/437 [00:07<00:03, 37.52it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 39.60it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 41.02it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 42.07it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 42.83it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 43.31it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 43.46it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.46it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.11it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 42.83it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.25it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.60it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.93it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.05it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 44.14it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.33it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.97it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.67it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.45it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.76it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.79it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.07it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.93it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.11it/s][A
100%|██████████| 437/437 [00:10<00:00, 40.30it/s][A                                                 
                                                 [A 40%|████      | 188/470 [01:44<01:20,  3.49it/s]
100%|██████████| 437/437 [00:10<00:00, 40.30it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:20:55,044 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 23:20:56,256 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:21:08,539 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:21:09,103 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:21:09,283 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-188/special_tokens_map.json
 40%|████      | 189/470 [02:17<1:01:26, 13.12s/it] 40%|████      | 190/470 [02:17<43:15,  9.27s/it]   41%|████      | 191/470 [02:18<30:35,  6.58s/it] 41%|████      | 192/470 [02:18<21:44,  4.69s/it] 41%|████      | 193/470 [02:18<15:38,  3.39s/it] 41%|████▏     | 194/470 [02:18<11:18,  2.46s/it] 41%|████▏     | 195/470 [02:19<08:17,  1.81s/it] 42%|████▏     | 196/470 [02:19<06:11,  1.36s/it] 42%|████▏     | 197/470 [02:19<04:43,  1.04s/it] 42%|████▏     | 198/470 [02:20<03:41,  1.23it/s] 42%|████▏     | 199/470 [02:20<02:58,  1.52it/s] 43%|████▎     | 200/470 [02:20<02:28,  1.82it/s] 43%|████▎     | 201/470 [02:20<02:07,  2.12it/s] 43%|████▎     | 202/470 [02:21<01:52,  2.39it/s] 43%|████▎     | 203/470 [02:21<01:41,  2.62it/s] 43%|████▎     | 204/470 [02:21<01:41,  2.62it/s] 44%|████▎     | 205/470 [02:22<01:34,  2.81it/s] 44%|████▍     | 206/470 [02:22<01:29,  2.96it/s] 44%|████▍     | 207/470 [02:22<01:25,  3.08it/s] 44%|████▍     | 208/470 [02:23<01:22,  3.17it/s] 44%|████▍     | 209/470 [02:23<01:20,  3.23it/s] 45%|████▍     | 210/470 [02:23<01:23,  3.10it/s] 45%|████▍     | 211/470 [02:24<01:21,  3.18it/s] 45%|████▌     | 212/470 [02:24<01:19,  3.24it/s] 45%|████▌     | 213/470 [02:24<01:18,  3.28it/s] 46%|████▌     | 214/470 [02:25<01:21,  3.15it/s] 46%|████▌     | 215/470 [02:25<01:19,  3.21it/s] 46%|████▌     | 216/470 [02:25<01:17,  3.26it/s] 46%|████▌     | 217/470 [02:25<01:16,  3.31it/s] 46%|████▋     | 218/470 [02:26<01:15,  3.33it/s] 47%|████▋     | 219/470 [02:26<01:15,  3.34it/s] 47%|████▋     | 220/470 [02:26<01:14,  3.36it/s] 47%|████▋     | 221/470 [02:27<01:13,  3.37it/s] 47%|████▋     | 222/470 [02:27<01:13,  3.37it/s] 47%|████▋     | 223/470 [02:27<01:13,  3.38it/s] 48%|████▊     | 224/470 [02:28<01:27,  2.81it/s] 48%|████▊     | 225/470 [02:28<01:22,  2.96it/s] 48%|████▊     | 226/470 [02:28<01:19,  3.08it/s] 48%|████▊     | 227/470 [02:29<01:16,  3.18it/s] 49%|████▊     | 228/470 [02:29<01:14,  3.25it/s] 49%|████▊     | 229/470 [02:29<01:12,  3.31it/s] 49%|████▉     | 230/470 [02:30<01:19,  3.01it/s] 49%|████▉     | 231/470 [02:30<01:16,  3.12it/s] 49%|████▉     | 232/470 [02:30<01:14,  3.21it/s] 50%|████▉     | 233/470 [02:30<01:12,  3.26it/s] 50%|████▉     | 234/470 [02:31<01:20,  2.95it/s] 50%|█████     | 235/470 [02:31<01:16,  3.08it/s] 50%|█████     | 236/470 [02:31<01:13,  3.17it/s] 50%|█████     | 237/470 [02:32<01:14,  3.13it/s] 51%|█████     | 238/470 [02:32<01:12,  3.21it/s] 51%|█████     | 239/470 [02:32<01:10,  3.27it/s] 51%|█████     | 240/470 [02:33<01:09,  3.32it/s] 51%|█████▏    | 241/470 [02:33<01:08,  3.35it/s] 51%|█████▏    | 242/470 [02:33<01:07,  3.37it/s] 52%|█████▏    | 243/470 [02:34<01:06,  3.39it/s] 52%|█████▏    | 244/470 [02:34<01:20,  2.82it/s] 52%|█████▏    | 245/470 [02:34<01:15,  2.97it/s] 52%|█████▏    | 246/470 [02:35<01:12,  3.10it/s] 53%|█████▎    | 247/470 [02:35<01:09,  3.19it/s] 53%|█████▎    | 248/470 [02:35<01:08,  3.26it/s] 53%|█████▎    | 249/470 [02:35<01:06,  3.31it/s] 53%|█████▎    | 250/470 [02:36<01:05,  3.34it/s] 53%|█████▎    | 251/470 [02:36<01:04,  3.37it/s] 54%|█████▎    | 252/470 [02:36<01:04,  3.39it/s] 54%|█████▍    | 253/470 [02:37<01:03,  3.41it/s] 54%|█████▍    | 254/470 [02:37<01:09,  3.12it/s] 54%|█████▍    | 255/470 [02:37<01:07,  3.21it/s] 54%|█████▍    | 256/470 [02:38<01:05,  3.27it/s] 55%|█████▍    | 257/470 [02:38<01:04,  3.32it/s] 55%|█████▍    | 258/470 [02:38<01:03,  3.35it/s] 55%|█████▌    | 259/470 [02:38<01:02,  3.38it/s] 55%|█████▌    | 260/470 [02:39<01:01,  3.39it/s] 56%|█████▌    | 261/470 [02:39<01:01,  3.41it/s] 56%|█████▌    | 262/470 [02:39<01:00,  3.41it/s] 56%|█████▌    | 263/470 [02:40<01:00,  3.42it/s] 56%|█████▌    | 264/470 [02:40<01:07,  3.05it/s] 56%|█████▋    | 265/470 [02:40<01:04,  3.15it/s] 57%|█████▋    | 266/470 [02:41<01:03,  3.23it/s] 57%|█████▋    | 267/470 [02:41<01:01,  3.28it/s] 57%|█████▋    | 268/470 [02:41<01:00,  3.33it/s] 57%|█████▋    | 269/470 [02:41<00:59,  3.36it/s] 57%|█████▋    | 270/470 [02:42<00:59,  3.39it/s] 58%|█████▊    | 271/470 [02:42<00:58,  3.40it/s] 58%|█████▊    | 272/470 [02:42<00:57,  3.42it/s] 58%|█████▊    | 273/470 [02:43<00:57,  3.42it/s] 58%|█████▊    | 274/470 [02:43<01:21,  2.42it/s] 59%|█████▊    | 275/470 [02:44<01:13,  2.65it/s] 59%|█████▊    | 276/470 [02:44<01:08,  2.84it/s] 59%|█████▉    | 277/470 [02:44<01:04,  2.99it/s] 59%|█████▉    | 278/470 [02:45<01:01,  3.11it/s] 59%|█████▉    | 279/470 [02:45<00:59,  3.20it/s] 60%|█████▉    | 280/470 [02:45<00:58,  3.27it/s] 60%|█████▉    | 281/470 [02:45<00:57,  3.31it/s] 60%|██████    | 282/470 [02:46<01:00,  3.13it/s][INFO|trainer.py:2140] 2023-08-28 23:21:55,700 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:21:55,700 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:21:55,700 >>   Batch size = 8
{'eval_loss': 1.1069632768630981, 'eval_runtime': 10.2407, 'eval_samples_per_second': 341.09, 'eval_steps_per_second': 42.673, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.31it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.20it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.62it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.73it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.31it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.67it/s][A
  8%|▊         | 37/437 [00:00<00:09, 43.92it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.59it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.77it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 43.83it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.29it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.42it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.47it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.35it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.05it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.64it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.52it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.72it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.87it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.04it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.26it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.35it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.14it/s][A
 28%|██▊       | 122/437 [00:02<00:08, 35.49it/s][A
 29%|██▉       | 127/437 [00:02<00:08, 37.84it/s][A
 30%|███       | 132/437 [00:03<00:07, 39.69it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 40.95it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 41.90it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 42.67it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.26it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.43it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.10it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.02it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.13it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.42it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.72it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.81it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.16it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.26it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.33it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.00it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.61it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.70it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.73it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.92it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.00it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.11it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.20it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.20it/s][A
 58%|█████▊    | 252/437 [00:05<00:05, 31.14it/s][A
 59%|█████▉    | 257/437 [00:06<00:05, 34.09it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 36.56it/s][A
 61%|██████    | 267/437 [00:06<00:04, 38.78it/s][A
 62%|██████▏   | 272/437 [00:06<00:04, 40.40it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 41.44it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 42.55it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 42.96it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 42.67it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 42.60it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 42.73it/s][A
 70%|███████   | 307/437 [00:07<00:03, 43.03it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.48it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.80it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.04it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.14it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.25it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.92it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.66it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 43.36it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.65it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.88it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.85it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.09it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.28it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 35.47it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 37.76it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 39.50it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 40.75it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 41.98it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 42.74it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.19it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.30it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 42.88it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.14it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 43.49it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 43.50it/s][A
100%|██████████| 437/437 [00:10<00:00, 43.84it/s][A                                                 
                                                 [A 60%|██████    | 282/470 [02:56<01:00,  3.13it/s]
100%|██████████| 437/437 [00:10<00:00, 43.84it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:22:06,419 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-282
[INFO|configuration_utils.py:351] 2023-08-28 23:22:07,023 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-282/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:22:13,583 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-282/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:22:13,867 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-282/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:22:14,062 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-282/special_tokens_map.json
 60%|██████    | 283/470 [03:15<28:21,  9.10s/it] 60%|██████    | 284/470 [03:16<20:03,  6.47s/it] 61%|██████    | 285/470 [03:16<14:14,  4.62s/it] 61%|██████    | 286/470 [03:16<10:10,  3.32s/it] 61%|██████    | 287/470 [03:17<07:21,  2.41s/it] 61%|██████▏   | 288/470 [03:17<05:23,  1.78s/it] 61%|██████▏   | 289/470 [03:17<04:01,  1.33s/it] 62%|██████▏   | 290/470 [03:17<03:03,  1.02s/it] 62%|██████▏   | 291/470 [03:18<02:23,  1.25it/s] 62%|██████▏   | 292/470 [03:18<01:55,  1.54it/s] 62%|██████▏   | 293/470 [03:18<01:36,  1.84it/s] 63%|██████▎   | 294/470 [03:19<01:22,  2.13it/s] 63%|██████▎   | 295/470 [03:19<01:12,  2.40it/s] 63%|██████▎   | 296/470 [03:19<01:06,  2.63it/s] 63%|██████▎   | 297/470 [03:20<01:02,  2.77it/s] 63%|██████▎   | 298/470 [03:20<00:58,  2.93it/s] 64%|██████▎   | 299/470 [03:20<00:55,  3.06it/s] 64%|██████▍   | 300/470 [03:20<00:53,  3.15it/s] 64%|██████▍   | 301/470 [03:21<00:52,  3.22it/s] 64%|██████▍   | 302/470 [03:21<00:51,  3.27it/s] 64%|██████▍   | 303/470 [03:21<00:50,  3.30it/s] 65%|██████▍   | 304/470 [03:22<00:49,  3.33it/s] 65%|██████▍   | 305/470 [03:22<00:49,  3.35it/s] 65%|██████▌   | 306/470 [03:22<00:48,  3.38it/s] 65%|██████▌   | 307/470 [03:22<00:48,  3.39it/s] 66%|██████▌   | 308/470 [03:23<00:48,  3.32it/s] 66%|██████▌   | 309/470 [03:23<00:48,  3.35it/s] 66%|██████▌   | 310/470 [03:24<00:56,  2.85it/s] 66%|██████▌   | 311/470 [03:24<00:53,  3.00it/s] 66%|██████▋   | 312/470 [03:24<00:50,  3.12it/s] 67%|██████▋   | 313/470 [03:24<00:49,  3.20it/s] 67%|██████▋   | 314/470 [03:25<00:47,  3.26it/s] 67%|██████▋   | 315/470 [03:25<00:46,  3.31it/s] 67%|██████▋   | 316/470 [03:25<00:46,  3.34it/s] 67%|██████▋   | 317/470 [03:26<00:45,  3.36it/s] 68%|██████▊   | 318/470 [03:26<00:46,  3.27it/s] 68%|██████▊   | 319/470 [03:26<00:45,  3.32it/s] 68%|██████▊   | 320/470 [03:27<00:44,  3.35it/s] 68%|██████▊   | 321/470 [03:27<00:44,  3.37it/s] 69%|██████▊   | 322/470 [03:27<00:43,  3.39it/s] 69%|██████▊   | 323/470 [03:27<00:43,  3.39it/s] 69%|██████▉   | 324/470 [03:28<00:42,  3.41it/s] 69%|██████▉   | 325/470 [03:28<00:42,  3.41it/s] 69%|██████▉   | 326/470 [03:28<00:42,  3.42it/s] 70%|██████▉   | 327/470 [03:29<00:41,  3.42it/s] 70%|██████▉   | 328/470 [03:29<00:41,  3.42it/s] 70%|███████   | 329/470 [03:29<00:42,  3.30it/s] 70%|███████   | 330/470 [03:29<00:41,  3.34it/s] 70%|███████   | 331/470 [03:30<00:41,  3.37it/s] 71%|███████   | 332/470 [03:30<00:40,  3.38it/s] 71%|███████   | 333/470 [03:30<00:40,  3.40it/s] 71%|███████   | 334/470 [03:31<00:39,  3.41it/s] 71%|███████▏  | 335/470 [03:31<00:39,  3.42it/s] 71%|███████▏  | 336/470 [03:31<00:39,  3.43it/s] 72%|███████▏  | 337/470 [03:31<00:38,  3.43it/s] 72%|███████▏  | 338/470 [03:32<00:38,  3.44it/s] 72%|███████▏  | 339/470 [03:32<00:38,  3.43it/s] 72%|███████▏  | 340/470 [03:32<00:39,  3.27it/s] 73%|███████▎  | 341/470 [03:33<00:38,  3.32it/s] 73%|███████▎  | 342/470 [03:33<00:38,  3.35it/s] 73%|███████▎  | 343/470 [03:33<00:37,  3.37it/s] 73%|███████▎  | 344/470 [03:34<00:37,  3.39it/s] 73%|███████▎  | 345/470 [03:34<00:36,  3.40it/s] 74%|███████▎  | 346/470 [03:34<00:36,  3.41it/s] 74%|███████▍  | 347/470 [03:34<00:36,  3.41it/s] 74%|███████▍  | 348/470 [03:35<00:35,  3.42it/s] 74%|███████▍  | 349/470 [03:35<00:35,  3.43it/s] 74%|███████▍  | 350/470 [03:35<00:34,  3.44it/s] 75%|███████▍  | 351/470 [03:36<00:37,  3.21it/s] 75%|███████▍  | 352/470 [03:36<00:36,  3.27it/s] 75%|███████▌  | 353/470 [03:36<00:35,  3.31it/s] 75%|███████▌  | 354/470 [03:37<00:34,  3.34it/s] 76%|███████▌  | 355/470 [03:37<00:34,  3.37it/s] 76%|███████▌  | 356/470 [03:37<00:33,  3.39it/s] 76%|███████▌  | 357/470 [03:37<00:33,  3.40it/s] 76%|███████▌  | 358/470 [03:38<00:32,  3.41it/s] 76%|███████▋  | 359/470 [03:38<00:32,  3.42it/s] 77%|███████▋  | 360/470 [03:38<00:32,  3.42it/s] 77%|███████▋  | 361/470 [03:39<00:31,  3.43it/s] 77%|███████▋  | 362/470 [03:39<00:32,  3.34it/s] 77%|███████▋  | 363/470 [03:39<00:31,  3.37it/s] 77%|███████▋  | 364/470 [03:40<00:31,  3.39it/s] 78%|███████▊  | 365/470 [03:40<00:30,  3.40it/s] 78%|███████▊  | 366/470 [03:40<00:30,  3.41it/s] 78%|███████▊  | 367/470 [03:40<00:30,  3.42it/s] 78%|███████▊  | 368/470 [03:41<00:29,  3.42it/s] 79%|███████▊  | 369/470 [03:41<00:29,  3.43it/s] 79%|███████▊  | 370/470 [03:41<00:29,  3.43it/s] 79%|███████▉  | 371/470 [03:42<00:28,  3.44it/s] 79%|███████▉  | 372/470 [03:42<00:28,  3.44it/s] 79%|███████▉  | 373/470 [03:42<00:29,  3.24it/s] 80%|███████▉  | 374/470 [03:42<00:29,  3.30it/s] 80%|███████▉  | 375/470 [03:43<00:28,  3.34it/s] 80%|████████  | 376/470 [03:43<00:26,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 23:22:52,931 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:22:52,931 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:22:52,931 >>   Batch size = 8
{'eval_loss': 1.1302461624145508, 'eval_runtime': 10.2609, 'eval_samples_per_second': 340.419, 'eval_steps_per_second': 42.589, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.84it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.38it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.37it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.08it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.59it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.28it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.05it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.90it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.99it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.18it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.22it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.31it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.19it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.00it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.75it/s][A
 19%|█▉        | 82/437 [00:02<00:08, 43.58it/s][A
 20%|█▉        | 87/437 [00:02<00:12, 28.16it/s][A
 21%|██        | 92/437 [00:02<00:10, 31.60it/s][A
 22%|██▏       | 97/437 [00:02<00:09, 34.57it/s][A
 23%|██▎       | 102/437 [00:02<00:09, 37.13it/s][A
 24%|██▍       | 107/437 [00:02<00:08, 39.12it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 40.65it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 41.88it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 42.43it/s][A
 29%|██▉       | 127/437 [00:03<00:07, 42.44it/s][A
 30%|███       | 132/437 [00:03<00:07, 42.53it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 42.53it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.02it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.38it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.72it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.96it/s][A
 37%|███▋      | 162/437 [00:04<00:06, 44.14it/s][A
 38%|███▊      | 167/437 [00:04<00:10, 26.13it/s][A
 39%|███▉      | 172/437 [00:04<00:08, 29.81it/s][A
 41%|████      | 177/437 [00:04<00:07, 33.09it/s][A
 42%|████▏     | 182/437 [00:04<00:07, 35.91it/s][A
 43%|████▎     | 187/437 [00:04<00:06, 38.15it/s][A
 44%|████▍     | 192/437 [00:04<00:06, 39.92it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 41.33it/s][A
 46%|████▌     | 202/437 [00:05<00:05, 42.00it/s][A
 47%|████▋     | 207/437 [00:05<00:05, 42.16it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 42.26it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 42.40it/s][A
 51%|█████     | 222/437 [00:05<00:05, 42.88it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.33it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.51it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.64it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.94it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 44.06it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 43.94it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 43.62it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 43.53it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.70it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.67it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.94it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.11it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 42.76it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 43.28it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 43.15it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 43.31it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.46it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.51it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.70it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.87it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.57it/s][A
 76%|███████▌  | 332/437 [00:08<00:02, 43.90it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 44.00it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 43.92it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 43.80it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.93it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.86it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.95it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.91it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.95it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 44.05it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 43.98it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 43.82it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.75it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.82it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.93it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.85it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.97it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.05it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 41.87it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 42.47it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 42.81it/s][A
100%|██████████| 437/437 [00:10<00:00, 43.17it/s][A                                                 
                                                 [A 80%|████████  | 376/470 [03:53<00:26,  3.60it/s]
100%|██████████| 437/437 [00:10<00:00, 43.17it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:23:03,445 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-376
[INFO|configuration_utils.py:351] 2023-08-28 23:23:03,575 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-376/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:23:08,096 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-376/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:23:08,342 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-376/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:23:08,472 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-376/special_tokens_map.json
 80%|████████  | 377/470 [04:07<11:37,  7.50s/it] 80%|████████  | 378/470 [04:08<08:12,  5.35s/it] 81%|████████  | 379/470 [04:08<05:48,  3.83s/it] 81%|████████  | 380/470 [04:08<04:09,  2.77s/it] 81%|████████  | 381/470 [04:09<03:00,  2.03s/it] 81%|████████▏ | 382/470 [04:09<02:12,  1.51s/it] 81%|████████▏ | 383/470 [04:09<01:39,  1.14s/it] 82%|████████▏ | 384/470 [04:09<01:16,  1.12it/s] 82%|████████▏ | 385/470 [04:10<01:00,  1.41it/s] 82%|████████▏ | 386/470 [04:10<00:49,  1.71it/s] 82%|████████▏ | 387/470 [04:10<00:41,  2.00it/s] 83%|████████▎ | 388/470 [04:11<00:35,  2.28it/s] 83%|████████▎ | 389/470 [04:11<00:32,  2.51it/s] 83%|████████▎ | 390/470 [04:11<00:29,  2.72it/s] 83%|████████▎ | 391/470 [04:12<00:27,  2.89it/s] 83%|████████▎ | 392/470 [04:12<00:25,  3.02it/s] 84%|████████▎ | 393/470 [04:12<00:24,  3.12it/s] 84%|████████▍ | 394/470 [04:12<00:23,  3.19it/s] 84%|████████▍ | 395/470 [04:13<00:23,  3.25it/s] 84%|████████▍ | 396/470 [04:13<00:22,  3.30it/s] 84%|████████▍ | 397/470 [04:13<00:21,  3.34it/s] 85%|████████▍ | 398/470 [04:14<00:21,  3.37it/s] 85%|████████▍ | 399/470 [04:14<00:20,  3.39it/s] 85%|████████▌ | 400/470 [04:14<00:20,  3.39it/s] 85%|████████▌ | 401/470 [04:14<00:20,  3.40it/s] 86%|████████▌ | 402/470 [04:15<00:19,  3.41it/s] 86%|████████▌ | 403/470 [04:15<00:19,  3.42it/s] 86%|████████▌ | 404/470 [04:15<00:19,  3.42it/s] 86%|████████▌ | 405/470 [04:16<00:18,  3.42it/s] 86%|████████▋ | 406/470 [04:16<00:18,  3.43it/s] 87%|████████▋ | 407/470 [04:16<00:18,  3.43it/s] 87%|████████▋ | 408/470 [04:16<00:18,  3.44it/s] 87%|████████▋ | 409/470 [04:17<00:17,  3.44it/s] 87%|████████▋ | 410/470 [04:17<00:17,  3.43it/s] 87%|████████▋ | 411/470 [04:17<00:17,  3.31it/s] 88%|████████▊ | 412/470 [04:18<00:17,  3.35it/s] 88%|████████▊ | 413/470 [04:18<00:16,  3.37it/s] 88%|████████▊ | 414/470 [04:18<00:16,  3.38it/s] 88%|████████▊ | 415/470 [04:19<00:16,  3.40it/s] 89%|████████▊ | 416/470 [04:19<00:15,  3.40it/s] 89%|████████▊ | 417/470 [04:19<00:15,  3.41it/s] 89%|████████▉ | 418/470 [04:19<00:15,  3.41it/s] 89%|████████▉ | 419/470 [04:20<00:14,  3.42it/s] 89%|████████▉ | 420/470 [04:20<00:14,  3.42it/s] 90%|████████▉ | 421/470 [04:20<00:14,  3.42it/s] 90%|████████▉ | 422/470 [04:21<00:13,  3.43it/s] 90%|█████████ | 423/470 [04:21<00:13,  3.39it/s] 90%|█████████ | 424/470 [04:21<00:13,  3.40it/s] 90%|█████████ | 425/470 [04:22<00:13,  3.41it/s] 91%|█████████ | 426/470 [04:22<00:12,  3.42it/s] 91%|█████████ | 427/470 [04:22<00:12,  3.42it/s] 91%|█████████ | 428/470 [04:22<00:12,  3.43it/s] 91%|█████████▏| 429/470 [04:23<00:11,  3.43it/s] 91%|█████████▏| 430/470 [04:23<00:11,  3.44it/s] 92%|█████████▏| 431/470 [04:23<00:11,  3.43it/s] 92%|█████████▏| 432/470 [04:24<00:11,  3.39it/s] 92%|█████████▏| 433/470 [04:24<00:10,  3.40it/s] 92%|█████████▏| 434/470 [04:24<00:10,  3.38it/s] 93%|█████████▎| 435/470 [04:24<00:10,  3.39it/s] 93%|█████████▎| 436/470 [04:25<00:09,  3.41it/s] 93%|█████████▎| 437/470 [04:25<00:09,  3.42it/s] 93%|█████████▎| 438/470 [04:25<00:09,  3.42it/s] 93%|█████████▎| 439/470 [04:26<00:09,  3.43it/s] 94%|█████████▎| 440/470 [04:26<00:08,  3.43it/s] 94%|█████████▍| 441/470 [04:26<00:08,  3.44it/s] 94%|█████████▍| 442/470 [04:26<00:08,  3.44it/s] 94%|█████████▍| 443/470 [04:27<00:07,  3.44it/s] 94%|█████████▍| 444/470 [04:27<00:07,  3.45it/s] 95%|█████████▍| 445/470 [04:27<00:07,  3.43it/s] 95%|█████████▍| 446/470 [04:28<00:06,  3.44it/s] 95%|█████████▌| 447/470 [04:28<00:06,  3.44it/s] 95%|█████████▌| 448/470 [04:28<00:06,  3.44it/s] 96%|█████████▌| 449/470 [04:29<00:06,  3.45it/s] 96%|█████████▌| 450/470 [04:29<00:05,  3.45it/s] 96%|█████████▌| 451/470 [04:29<00:05,  3.45it/s] 96%|█████████▌| 452/470 [04:29<00:05,  3.45it/s] 96%|█████████▋| 453/470 [04:30<00:04,  3.44it/s] 97%|█████████▋| 454/470 [04:30<00:04,  3.44it/s] 97%|█████████▋| 455/470 [04:30<00:04,  3.44it/s] 97%|█████████▋| 456/470 [04:31<00:04,  3.41it/s] 97%|█████████▋| 457/470 [04:31<00:03,  3.42it/s] 97%|█████████▋| 458/470 [04:31<00:03,  3.43it/s] 98%|█████████▊| 459/470 [04:31<00:03,  3.43it/s] 98%|█████████▊| 460/470 [04:32<00:02,  3.43it/s] 98%|█████████▊| 461/470 [04:32<00:02,  3.44it/s] 98%|█████████▊| 462/470 [04:32<00:02,  3.43it/s] 99%|█████████▊| 463/470 [04:33<00:02,  3.43it/s] 99%|█████████▊| 464/470 [04:33<00:01,  3.44it/s] 99%|█████████▉| 465/470 [04:33<00:01,  3.44it/s] 99%|█████████▉| 466/470 [04:33<00:01,  3.44it/s] 99%|█████████▉| 467/470 [04:34<00:00,  3.43it/s]100%|█████████▉| 468/470 [04:34<00:00,  3.43it/s]100%|█████████▉| 469/470 [04:34<00:00,  3.44it/s]100%|██████████| 470/470 [04:35<00:00,  3.67it/s][INFO|trainer.py:2140] 2023-08-28 23:23:44,495 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:23:44,496 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:23:44,496 >>   Batch size = 8
{'eval_loss': 1.1431677341461182, 'eval_runtime': 10.4367, 'eval_samples_per_second': 334.685, 'eval_steps_per_second': 41.872, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.23it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.67it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.85it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.00it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.61it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.42it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.20it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.04it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.09it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.19it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.89it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.02it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.92it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.97it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.91it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.79it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.84it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.93it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.03it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.87it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.93it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.88it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.87it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.81it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.92it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.84it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.05it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.99it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.84it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.24it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.76it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.82it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.72it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.75it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.86it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.77it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.74it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.94it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.91it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.87it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.87it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.86it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.92it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.87it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.60it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.73it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.85it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.75it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.78it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.85it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.86it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.78it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.83it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.89it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.95it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.89it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.89it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.88it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.80it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.77it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.70it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.90it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.71it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.86it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.72it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.70it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.90it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.92it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.89it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.76it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.66it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 38.71it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 40.27it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 41.45it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 42.35it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 42.98it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.32it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.61it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.64it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.22it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.17it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.32it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.78it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.01it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.94it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.05it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.19it/s][A                                                 
                                                 [A100%|██████████| 470/470 [04:45<00:00,  3.67it/s]
100%|██████████| 437/437 [00:09<00:00, 44.19it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:23:54,770 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-470
[INFO|configuration_utils.py:351] 2023-08-28 23:23:55,537 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-470/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:24:05,485 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:24:06,158 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:24:06,653 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-470/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 23:24:22,791 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 23:24:22,866 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-94 (score: 1.0908325910568237).
                                                 100%|██████████| 470/470 [05:32<00:00,  3.67it/s]100%|██████████| 470/470 [05:32<00:00,  1.41it/s]
[INFO|trainer.py:1894] 2023-08-28 23:24:42,205 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 23:24:42,438 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:24:48,094 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:24:48,254 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:24:48,396 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:24:49,046 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:49,047 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:49,047 >>   train_loss               =     0.3951
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:49,047 >>   train_runtime            = 0:05:32.61
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:49,047 >>   train_samples            =       6000
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:49,047 >>   train_samples_per_second =     90.195
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:49,047 >>   train_steps_per_second   =      1.413
{'eval_loss': 1.1459565162658691, 'eval_runtime': 10.0026, 'eval_samples_per_second': 349.211, 'eval_steps_per_second': 43.689, 'epoch': 5.0}
{'train_runtime': 332.6116, 'train_samples_per_second': 90.195, 'train_steps_per_second': 1.413, 'train_loss': 0.3950906307139295, 'epoch': 5.0}
08/28/2023 23:24:49 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 23:24:49,298 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:24:49,298 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:24:49,298 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.03it/s]  3%|▎         | 12/437 [00:00<00:08, 48.42it/s]  4%|▍         | 17/437 [00:00<00:09, 46.65it/s]  5%|▌         | 22/437 [00:00<00:09, 45.94it/s]  6%|▌         | 27/437 [00:00<00:09, 45.48it/s]  7%|▋         | 32/437 [00:00<00:08, 45.13it/s]  8%|▊         | 37/437 [00:00<00:08, 44.91it/s] 10%|▉         | 42/437 [00:00<00:08, 44.56it/s] 11%|█         | 47/437 [00:01<00:08, 44.05it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.05it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.21it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.52it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.66it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.63it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.43it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.44it/s] 20%|█▉        | 87/437 [00:01<00:08, 41.89it/s] 21%|██        | 92/437 [00:02<00:08, 42.52it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.11it/s] 23%|██▎       | 102/437 [00:02<00:07, 43.46it/s] 24%|██▍       | 107/437 [00:02<00:07, 43.75it/s] 26%|██▌       | 112/437 [00:02<00:07, 43.93it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.16it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.33it/s] 29%|██▉       | 127/437 [00:02<00:07, 44.05it/s] 30%|███       | 132/437 [00:02<00:06, 44.03it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.29it/s] 32%|███▏      | 142/437 [00:03<00:06, 44.15it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.30it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.45it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.55it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.62it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.27it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.26it/s] 41%|████      | 177/437 [00:03<00:05, 44.05it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.10it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.10it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.26it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.45it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.36it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.50it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.45it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.44it/s] 51%|█████     | 222/437 [00:05<00:05, 36.37it/s] 52%|█████▏    | 227/437 [00:05<00:05, 38.58it/s] 53%|█████▎    | 232/437 [00:05<00:05, 40.35it/s] 54%|█████▍    | 237/437 [00:05<00:04, 41.55it/s] 55%|█████▌    | 242/437 [00:05<00:04, 42.57it/s] 57%|█████▋    | 247/437 [00:05<00:04, 43.10it/s] 58%|█████▊    | 252/437 [00:05<00:04, 43.53it/s] 59%|█████▉    | 257/437 [00:05<00:04, 43.83it/s] 60%|█████▉    | 262/437 [00:05<00:04, 43.65it/s] 61%|██████    | 267/437 [00:06<00:03, 43.62it/s] 62%|██████▏   | 272/437 [00:06<00:03, 43.80it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.17it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.37it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.45it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.45it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.38it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.24it/s] 70%|███████   | 307/437 [00:06<00:02, 43.97it/s] 71%|███████▏  | 312/437 [00:07<00:02, 43.77it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.02it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.19it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.30it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.58it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.60it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.49it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.35it/s] 81%|████████  | 352/437 [00:08<00:02, 36.04it/s] 82%|████████▏ | 357/437 [00:08<00:02, 38.33it/s] 83%|████████▎ | 362/437 [00:08<00:01, 40.04it/s] 84%|████████▍ | 367/437 [00:08<00:01, 41.34it/s] 85%|████████▌ | 372/437 [00:08<00:01, 42.16it/s] 86%|████████▋ | 377/437 [00:08<00:01, 42.91it/s] 87%|████████▋ | 382/437 [00:08<00:01, 43.36it/s] 89%|████████▊ | 387/437 [00:08<00:01, 43.74it/s] 90%|████████▉ | 392/437 [00:08<00:01, 43.47it/s] 91%|█████████ | 397/437 [00:09<00:00, 43.52it/s] 92%|█████████▏| 402/437 [00:09<00:00, 43.84it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.04it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.19it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.46it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.33it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.54it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.26it/s]100%|██████████| 437/437 [00:10<00:00, 43.95it/s]100%|██████████| 437/437 [00:10<00:00, 43.60it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:24:59,340 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:59,340 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:59,340 >>   eval_loss               =     1.0908
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:59,340 >>   eval_runtime            = 0:00:10.04
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:59,340 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:59,340 >>   eval_samples_per_second =    347.858
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:59,340 >>   eval_steps_per_second   =      43.52
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:24:59,340 >>   perplexity              =     2.9768
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:25:21,459 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:25:21,480 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:25:21,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:25:21,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:25:21,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:25:21,933 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:25:21,935 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:25:22,225 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:25:23,262 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:25:23,262 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:25:25,206 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:25:25,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:25:25,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:25:25,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:25:25,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:25:26,066 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:25:26,067 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:25:26,440 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:25:26,605 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:25:26,605 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-282
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-94
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-376
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/checkpoint-470
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.74it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.65it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:04,  1.59it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:06,  1.52it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.68it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:17,  1.72it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:19,  1.64it/s]Extractor Predicting: 32it [00:19,  1.59it/s]Extractor Predicting: 33it [00:20,  1.56it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:21,  1.56it/s]Extractor Predicting: 36it [00:22,  1.55it/s]Extractor Predicting: 37it [00:23,  1.51it/s]Extractor Predicting: 38it [00:23,  1.52it/s]Extractor Predicting: 39it [00:24,  1.54it/s]Extractor Predicting: 40it [00:24,  1.56it/s]Extractor Predicting: 41it [00:25,  1.55it/s]Extractor Predicting: 42it [00:26,  1.51it/s]Extractor Predicting: 43it [00:26,  1.52it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:28,  1.59it/s]Extractor Predicting: 46it [00:28,  1.55it/s]Extractor Predicting: 47it [00:29,  1.46it/s]Extractor Predicting: 48it [00:30,  1.49it/s]Extractor Predicting: 49it [00:31,  1.45it/s]Extractor Predicting: 50it [00:31,  1.48it/s]Extractor Predicting: 51it [00:32,  1.51it/s]Extractor Predicting: 52it [00:32,  1.51it/s]Extractor Predicting: 53it [00:33,  1.57it/s]Extractor Predicting: 54it [00:34,  1.49it/s]Extractor Predicting: 55it [00:34,  1.48it/s]Extractor Predicting: 56it [00:35,  1.49it/s]Extractor Predicting: 57it [00:36,  1.48it/s]Extractor Predicting: 58it [00:36,  1.49it/s]Extractor Predicting: 59it [00:37,  1.46it/s]Extractor Predicting: 60it [00:38,  1.47it/s]Extractor Predicting: 61it [00:39,  1.50it/s]Extractor Predicting: 62it [00:39,  1.51it/s]Extractor Predicting: 63it [00:40,  1.54it/s]Extractor Predicting: 64it [00:40,  1.53it/s]Extractor Predicting: 65it [00:41,  1.53it/s]Extractor Predicting: 66it [00:42,  1.57it/s]Extractor Predicting: 67it [00:42,  1.57it/s]Extractor Predicting: 68it [00:43,  1.57it/s]Extractor Predicting: 69it [00:44,  1.41it/s]Extractor Predicting: 70it [00:44,  1.44it/s]Extractor Predicting: 71it [00:45,  1.48it/s]Extractor Predicting: 72it [00:46,  1.51it/s]Extractor Predicting: 73it [00:46,  1.52it/s]Extractor Predicting: 74it [00:47,  1.54it/s]Extractor Predicting: 75it [00:48,  1.55it/s]Extractor Predicting: 76it [00:48,  1.54it/s]Extractor Predicting: 77it [00:49,  1.54it/s]Extractor Predicting: 78it [00:50,  1.54it/s]Extractor Predicting: 79it [00:50,  1.44it/s]Extractor Predicting: 80it [00:51,  1.46it/s]Extractor Predicting: 81it [00:52,  1.52it/s]Extractor Predicting: 82it [00:52,  1.53it/s]Extractor Predicting: 83it [00:53,  1.50it/s]Extractor Predicting: 84it [00:54,  1.51it/s]Extractor Predicting: 85it [00:54,  1.55it/s]Extractor Predicting: 86it [00:55,  1.61it/s]Extractor Predicting: 87it [00:55,  1.66it/s]Extractor Predicting: 88it [00:56,  1.66it/s]Extractor Predicting: 89it [00:57,  1.66it/s]Extractor Predicting: 90it [00:57,  1.65it/s]Extractor Predicting: 91it [00:58,  1.64it/s]Extractor Predicting: 92it [00:58,  1.61it/s]Extractor Predicting: 93it [00:59,  1.66it/s]Extractor Predicting: 94it [01:00,  1.69it/s]Extractor Predicting: 95it [01:00,  1.63it/s]Extractor Predicting: 96it [01:01,  1.65it/s]Extractor Predicting: 97it [01:01,  1.64it/s]Extractor Predicting: 98it [01:02,  1.61it/s]Extractor Predicting: 99it [01:03,  1.58it/s]Extractor Predicting: 100it [01:03,  1.62it/s]Extractor Predicting: 101it [01:04,  1.65it/s]Extractor Predicting: 102it [01:05,  1.59it/s]Extractor Predicting: 103it [01:05,  1.61it/s]Extractor Predicting: 104it [01:06,  1.62it/s]Extractor Predicting: 105it [01:06,  1.65it/s]Extractor Predicting: 106it [01:07,  1.62it/s]Extractor Predicting: 107it [01:08,  1.63it/s]Extractor Predicting: 108it [01:08,  1.64it/s]Extractor Predicting: 109it [01:09,  1.68it/s]Extractor Predicting: 110it [01:09,  1.68it/s]Extractor Predicting: 111it [01:10,  1.66it/s]Extractor Predicting: 112it [01:11,  1.64it/s]Extractor Predicting: 113it [01:11,  1.59it/s]Extractor Predicting: 114it [01:12,  1.56it/s]Extractor Predicting: 115it [01:13,  1.58it/s]Extractor Predicting: 116it [01:13,  1.58it/s]Extractor Predicting: 117it [01:14,  1.56it/s]Extractor Predicting: 118it [01:15,  1.61it/s]Extractor Predicting: 119it [01:15,  1.59it/s]Extractor Predicting: 120it [01:16,  1.63it/s]Extractor Predicting: 121it [01:16,  1.62it/s]Extractor Predicting: 122it [01:17,  1.59it/s]Extractor Predicting: 123it [01:18,  1.55it/s]Extractor Predicting: 124it [01:18,  1.53it/s]Extractor Predicting: 125it [01:19,  1.54it/s]Extractor Predicting: 126it [01:20,  1.55it/s]Extractor Predicting: 127it [01:20,  1.54it/s]Extractor Predicting: 128it [01:21,  1.49it/s]Extractor Predicting: 129it [01:22,  1.46it/s]Extractor Predicting: 130it [01:22,  1.50it/s]Extractor Predicting: 131it [01:23,  1.52it/s]Extractor Predicting: 132it [01:24,  1.57it/s]Extractor Predicting: 133it [01:24,  1.55it/s]Extractor Predicting: 134it [01:25,  1.56it/s]Extractor Predicting: 135it [01:26,  1.57it/s]Extractor Predicting: 136it [01:26,  1.57it/s]Extractor Predicting: 137it [01:27,  1.56it/s]Extractor Predicting: 138it [01:28,  1.48it/s]Extractor Predicting: 139it [01:28,  1.49it/s]Extractor Predicting: 140it [01:29,  1.55it/s]Extractor Predicting: 141it [01:30,  1.40it/s]Extractor Predicting: 142it [01:30,  1.51it/s]Extractor Predicting: 142it [01:30,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:11,862 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:11,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:11,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:11,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:11,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:27:12,551 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:27:12,553 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:27:13,342 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:27:14,486 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:27:14,486 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:17,020 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:17,072 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:17,072 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:17,072 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:17,072 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:27:17,899 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:27:17,900 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:27:18,341 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:27:18,628 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:27:18,628 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.22114574557708508,
  "recall": 0.15030060120240482,
  "score": 0.17896710414181013,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.59it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:13,  1.62it/s]Extractor Predicting: 22it [00:13,  1.63it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.57it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:18,  1.53it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:20,  1.53it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:23,  1.51it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:24,  1.53it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:25,  1.57it/s]Extractor Predicting: 42it [00:26,  1.55it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:27,  1.55it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.57it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:30,  1.52it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:31,  1.53it/s]Extractor Predicting: 51it [00:32,  1.57it/s]Extractor Predicting: 52it [00:33,  1.58it/s]Extractor Predicting: 53it [00:33,  1.52it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:35,  1.52it/s]Extractor Predicting: 56it [00:35,  1.55it/s]Extractor Predicting: 57it [00:36,  1.59it/s]Extractor Predicting: 58it [00:36,  1.57it/s]Extractor Predicting: 59it [00:37,  1.59it/s]Extractor Predicting: 60it [00:38,  1.60it/s]Extractor Predicting: 61it [00:38,  1.58it/s]Extractor Predicting: 62it [00:39,  1.60it/s]Extractor Predicting: 63it [00:40,  1.61it/s]Extractor Predicting: 64it [00:40,  1.57it/s]Extractor Predicting: 65it [00:41,  1.58it/s]Extractor Predicting: 66it [00:42,  1.48it/s]Extractor Predicting: 67it [00:42,  1.53it/s]Extractor Predicting: 68it [00:43,  1.60it/s]Extractor Predicting: 69it [00:43,  1.59it/s]Extractor Predicting: 70it [00:44,  1.60it/s]Extractor Predicting: 71it [00:45,  1.53it/s]Extractor Predicting: 72it [00:45,  1.57it/s]Extractor Predicting: 73it [00:46,  1.53it/s]Extractor Predicting: 74it [00:47,  1.56it/s]Extractor Predicting: 75it [00:47,  1.57it/s]Extractor Predicting: 76it [00:48,  1.55it/s]Extractor Predicting: 77it [00:49,  1.53it/s]Extractor Predicting: 78it [00:49,  1.55it/s]Extractor Predicting: 79it [00:50,  1.55it/s]Extractor Predicting: 80it [00:50,  1.57it/s]Extractor Predicting: 81it [00:51,  1.57it/s]Extractor Predicting: 82it [00:52,  1.53it/s]Extractor Predicting: 83it [00:52,  1.55it/s]Extractor Predicting: 84it [00:53,  1.56it/s]Extractor Predicting: 85it [00:54,  1.58it/s]Extractor Predicting: 86it [00:54,  1.51it/s]Extractor Predicting: 87it [00:55,  1.53it/s]Extractor Predicting: 88it [00:56,  1.50it/s]Extractor Predicting: 89it [00:56,  1.53it/s]Extractor Predicting: 90it [00:57,  1.55it/s]Extractor Predicting: 91it [00:58,  1.53it/s]Extractor Predicting: 92it [00:58,  1.53it/s]Extractor Predicting: 93it [00:59,  1.51it/s]Extractor Predicting: 94it [01:00,  1.49it/s]Extractor Predicting: 95it [01:00,  1.51it/s]Extractor Predicting: 96it [01:01,  1.50it/s]Extractor Predicting: 97it [01:02,  1.51it/s]Extractor Predicting: 98it [01:02,  1.51it/s]Extractor Predicting: 99it [01:03,  1.52it/s]Extractor Predicting: 100it [01:04,  1.54it/s]Extractor Predicting: 101it [01:04,  1.48it/s]Extractor Predicting: 102it [01:05,  1.49it/s]Extractor Predicting: 103it [01:06,  1.51it/s]Extractor Predicting: 104it [01:06,  1.52it/s]Extractor Predicting: 105it [01:07,  1.52it/s]Extractor Predicting: 106it [01:08,  1.51it/s]Extractor Predicting: 107it [01:08,  1.50it/s]Extractor Predicting: 108it [01:09,  1.49it/s]Extractor Predicting: 109it [01:10,  1.51it/s]Extractor Predicting: 110it [01:10,  1.49it/s]Extractor Predicting: 111it [01:11,  1.49it/s]Extractor Predicting: 112it [01:12,  1.54it/s]Extractor Predicting: 113it [01:12,  1.49it/s]Extractor Predicting: 114it [01:13,  1.53it/s]Extractor Predicting: 115it [01:14,  1.54it/s]Extractor Predicting: 116it [01:14,  1.55it/s]Extractor Predicting: 117it [01:15,  1.58it/s]Extractor Predicting: 118it [01:15,  1.58it/s]Extractor Predicting: 119it [01:16,  1.57it/s]Extractor Predicting: 120it [01:17,  1.59it/s]Extractor Predicting: 121it [01:17,  1.59it/s]Extractor Predicting: 122it [01:18,  1.64it/s]Extractor Predicting: 123it [01:19,  1.61it/s]Extractor Predicting: 124it [01:19,  1.61it/s]Extractor Predicting: 125it [01:20,  1.60it/s]Extractor Predicting: 126it [01:20,  1.59it/s]Extractor Predicting: 127it [01:21,  1.60it/s]Extractor Predicting: 128it [01:22,  1.56it/s]Extractor Predicting: 129it [01:22,  1.54it/s]Extractor Predicting: 130it [01:23,  1.58it/s]Extractor Predicting: 131it [01:24,  1.56it/s]Extractor Predicting: 132it [01:24,  1.58it/s]Extractor Predicting: 133it [01:25,  1.56it/s]Extractor Predicting: 134it [01:26,  1.58it/s]Extractor Predicting: 135it [01:26,  1.61it/s]Extractor Predicting: 136it [01:27,  1.62it/s]Extractor Predicting: 137it [01:27,  1.60it/s]Extractor Predicting: 138it [01:28,  1.51it/s]Extractor Predicting: 139it [01:29,  1.58it/s]Extractor Predicting: 140it [01:29,  1.57it/s]Extractor Predicting: 141it [01:30,  1.56it/s]Extractor Predicting: 142it [01:31,  1.62it/s]Extractor Predicting: 143it [01:31,  1.60it/s]Extractor Predicting: 144it [01:32,  1.59it/s]Extractor Predicting: 145it [01:32,  1.61it/s]Extractor Predicting: 146it [01:33,  1.60it/s]Extractor Predicting: 147it [01:34,  1.58it/s]Extractor Predicting: 148it [01:34,  1.53it/s]Extractor Predicting: 149it [01:35,  1.39it/s]Extractor Predicting: 150it [01:36,  1.46it/s]Extractor Predicting: 151it [01:36,  1.51it/s]Extractor Predicting: 152it [01:37,  1.55it/s]Extractor Predicting: 153it [01:38,  1.48it/s]Extractor Predicting: 154it [01:38,  1.50it/s]Extractor Predicting: 155it [01:39,  1.52it/s]Extractor Predicting: 156it [01:40,  1.46it/s]Extractor Predicting: 157it [01:41,  1.47it/s]Extractor Predicting: 158it [01:41,  1.49it/s]Extractor Predicting: 159it [01:42,  1.51it/s]Extractor Predicting: 160it [01:42,  1.53it/s]Extractor Predicting: 161it [01:43,  1.51it/s]Extractor Predicting: 162it [01:44,  1.54it/s]Extractor Predicting: 163it [01:44,  1.55it/s]Extractor Predicting: 164it [01:45,  1.57it/s]Extractor Predicting: 165it [01:46,  1.55it/s]Extractor Predicting: 166it [01:46,  1.55it/s]Extractor Predicting: 167it [01:47,  1.58it/s]Extractor Predicting: 168it [01:48,  1.58it/s]Extractor Predicting: 169it [01:48,  1.58it/s]Extractor Predicting: 170it [01:49,  1.56it/s]Extractor Predicting: 171it [01:50,  1.48it/s]Extractor Predicting: 172it [01:50,  1.51it/s]Extractor Predicting: 173it [01:51,  1.52it/s]Extractor Predicting: 174it [01:52,  1.49it/s]Extractor Predicting: 175it [01:52,  1.46it/s]Extractor Predicting: 176it [01:53,  1.46it/s]Extractor Predicting: 177it [01:54,  1.47it/s]Extractor Predicting: 178it [01:54,  1.52it/s]Extractor Predicting: 179it [01:55,  1.53it/s]Extractor Predicting: 180it [01:55,  1.58it/s]Extractor Predicting: 181it [01:56,  1.55it/s]Extractor Predicting: 182it [01:57,  1.57it/s]Extractor Predicting: 183it [01:57,  1.57it/s]Extractor Predicting: 184it [01:58,  1.61it/s]Extractor Predicting: 185it [01:59,  1.63it/s]Extractor Predicting: 186it [01:59,  1.61it/s]Extractor Predicting: 187it [02:00,  1.62it/s]Extractor Predicting: 188it [02:00,  1.61it/s]Extractor Predicting: 189it [02:01,  1.61it/s]Extractor Predicting: 190it [02:02,  1.59it/s]Extractor Predicting: 191it [02:03,  1.47it/s]Extractor Predicting: 192it [02:03,  1.51it/s]Extractor Predicting: 193it [02:04,  1.56it/s]Extractor Predicting: 194it [02:04,  1.57it/s]Extractor Predicting: 195it [02:05,  1.58it/s]Extractor Predicting: 196it [02:06,  1.57it/s]Extractor Predicting: 197it [02:06,  1.60it/s]Extractor Predicting: 198it [02:07,  1.56it/s]Extractor Predicting: 199it [02:08,  1.58it/s]Extractor Predicting: 200it [02:08,  1.58it/s]Extractor Predicting: 201it [02:09,  1.58it/s]Extractor Predicting: 202it [02:09,  1.62it/s]Extractor Predicting: 203it [02:10,  1.64it/s]Extractor Predicting: 204it [02:11,  1.63it/s]Extractor Predicting: 205it [02:11,  1.58it/s]Extractor Predicting: 206it [02:12,  1.59it/s]Extractor Predicting: 207it [02:12,  1.62it/s]Extractor Predicting: 208it [02:13,  1.63it/s]Extractor Predicting: 209it [02:14,  1.53it/s]Extractor Predicting: 210it [02:15,  1.52it/s]Extractor Predicting: 211it [02:15,  1.54it/s]Extractor Predicting: 212it [02:16,  1.57it/s]Extractor Predicting: 213it [02:16,  1.59it/s]Extractor Predicting: 214it [02:17,  1.51it/s]Extractor Predicting: 215it [02:18,  1.53it/s]Extractor Predicting: 216it [02:18,  1.56it/s]Extractor Predicting: 217it [02:19,  1.58it/s]Extractor Predicting: 218it [02:20,  1.52it/s]Extractor Predicting: 219it [02:20,  1.54it/s]Extractor Predicting: 220it [02:21,  1.56it/s]Extractor Predicting: 221it [02:22,  1.53it/s]Extractor Predicting: 222it [02:22,  1.54it/s]Extractor Predicting: 223it [02:23,  1.54it/s]Extractor Predicting: 224it [02:24,  1.56it/s]Extractor Predicting: 225it [02:24,  1.56it/s]Extractor Predicting: 226it [02:25,  1.61it/s]Extractor Predicting: 227it [02:25,  1.63it/s]Extractor Predicting: 228it [02:26,  1.61it/s]Extractor Predicting: 229it [02:27,  1.59it/s]Extractor Predicting: 230it [02:27,  1.57it/s]Extractor Predicting: 231it [02:28,  1.56it/s]Extractor Predicting: 232it [02:29,  1.40it/s]Extractor Predicting: 233it [02:29,  1.48it/s]Extractor Predicting: 234it [02:30,  1.46it/s]Extractor Predicting: 235it [02:31,  1.50it/s]Extractor Predicting: 236it [02:31,  1.50it/s]Extractor Predicting: 237it [02:32,  1.51it/s]Extractor Predicting: 238it [02:33,  1.55it/s]Extractor Predicting: 239it [02:33,  1.53it/s]Extractor Predicting: 240it [02:34,  1.54it/s]Extractor Predicting: 241it [02:35,  1.54it/s]Extractor Predicting: 242it [02:35,  1.53it/s]Extractor Predicting: 243it [02:36,  1.51it/s]Extractor Predicting: 244it [02:37,  1.49it/s]Extractor Predicting: 245it [02:37,  1.55it/s]Extractor Predicting: 246it [02:38,  1.55it/s]Extractor Predicting: 247it [02:39,  1.57it/s]Extractor Predicting: 248it [02:39,  1.59it/s]Extractor Predicting: 249it [02:40,  1.53it/s]Extractor Predicting: 250it [02:40,  1.55it/s]Extractor Predicting: 251it [02:41,  1.49it/s]Extractor Predicting: 252it [02:42,  1.52it/s]Extractor Predicting: 253it [02:42,  1.54it/s]Extractor Predicting: 254it [02:43,  1.57it/s]Extractor Predicting: 255it [02:44,  1.57it/s]Extractor Predicting: 256it [02:45,  1.44it/s]Extractor Predicting: 257it [02:45,  1.50it/s]Extractor Predicting: 258it [02:46,  1.52it/s]Extractor Predicting: 259it [02:46,  1.51it/s]Extractor Predicting: 260it [02:47,  1.54it/s]Extractor Predicting: 261it [02:48,  1.48it/s]Extractor Predicting: 262it [02:48,  1.49it/s]Extractor Predicting: 263it [02:49,  1.51it/s]Extractor Predicting: 264it [02:50,  1.52it/s]Extractor Predicting: 265it [02:50,  1.52it/s]Extractor Predicting: 266it [02:51,  1.48it/s]Extractor Predicting: 267it [02:52,  1.48it/s]Extractor Predicting: 268it [02:52,  1.50it/s]Extractor Predicting: 269it [02:53,  1.50it/s]Extractor Predicting: 270it [02:54,  1.49it/s]Extractor Predicting: 271it [02:54,  1.49it/s]Extractor Predicting: 272it [02:55,  1.51it/s]Extractor Predicting: 273it [02:56,  1.49it/s]Extractor Predicting: 274it [02:56,  1.50it/s]Extractor Predicting: 275it [02:57,  1.55it/s]Extractor Predicting: 276it [02:58,  1.48it/s]Extractor Predicting: 277it [02:58,  1.48it/s]Extractor Predicting: 278it [02:59,  1.50it/s]Extractor Predicting: 279it [03:00,  1.51it/s]Extractor Predicting: 280it [03:00,  1.50it/s]Extractor Predicting: 281it [03:01,  1.47it/s]Extractor Predicting: 282it [03:02,  1.49it/s]Extractor Predicting: 283it [03:03,  1.46it/s]Extractor Predicting: 284it [03:03,  1.45it/s]Extractor Predicting: 285it [03:04,  1.46it/s]Extractor Predicting: 286it [03:05,  1.45it/s]Extractor Predicting: 287it [03:05,  1.47it/s]Extractor Predicting: 288it [03:05,  1.95it/s]Extractor Predicting: 288it [03:05,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:30:39,863 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:30:39,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:30:39,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:30:39,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:30:39,867 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:30:40,725 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:30:40,777 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:30:41,544 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:30:42,744 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:30:42,744 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:30:45,824 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:30:45,931 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:30:45,931 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:30:45,931 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:30:45,931 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:30:46,523 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:30:46,524 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:30:46,889 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:30:47,164 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:30:47,165 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.33249542961608775,
  "recall": 0.21120627086659893,
  "score": 0.2583222370173103,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:01,  1.84it/s]Extractor Predicting: 3it [00:01,  1.69it/s]
[INFO|configuration_utils.py:515] 2023-08-28 23:30:50,924 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:30:50,925 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:30:51,075 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:30:51,076 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 23:30:51,162 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:31:09,194 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 23:31:09,195 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 23:31:09,696 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:31:09,697 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:31:09,931 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:31:10,221 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:31:10,221 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:31:10,221 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:31:10,221 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:31:10,221 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:31:10,221 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4117647058823529,
  "recall": 0.12612612612612611,
  "score": 0.19310344827586207,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 23:31:10,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:11,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:12,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:12,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:13,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:13,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:14,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:15,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:16,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:16,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:17,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:17,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:18,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:19,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:20,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:20,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:21,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:21,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:22,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:23,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:23,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:10, 13.58s/it][WARNING|generation_utils.py:914] 2023-08-28 23:31:24,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:25,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:25,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:26,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:26,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:27,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:28,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:28,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:29,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:29,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:30,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:31,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:31,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:32,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:32,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:33,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:33,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:34,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:35,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:35,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:25<02:46, 12.79s/it][WARNING|generation_utils.py:914] 2023-08-28 23:31:36,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:37,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:37,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:38,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:39,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:39,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:40,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:41,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:41,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:42,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:43,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:43,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:44,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:44,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:45,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:46,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:47,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:47,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:48,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:49,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:49,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:39<02:39, 13.25s/it][WARNING|generation_utils.py:914] 2023-08-28 23:31:50,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:50,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:51,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:52,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:53,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:53,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:54,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:54,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:55,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:56,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:56,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:57,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:58,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:59,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:31:59,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:00,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:01,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:01,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:02,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:03,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:53<02:28, 13.54s/it][WARNING|generation_utils.py:914] 2023-08-28 23:32:04,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:04,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:05,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:05,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:06,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:07,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:07,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:08,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:08,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:09,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:09,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:10,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:10,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:11,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:11,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:12,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:12,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:13,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:14,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:14,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:15,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:04<02:07, 12.71s/it][WARNING|generation_utils.py:914] 2023-08-28 23:32:15,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:16,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:17,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:17,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:18,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:18,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:19,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:20,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:21,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:21,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:22,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:23,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:23,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:24,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:25,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:25,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:26,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:27,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:28,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:28,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:18<01:57, 13.05s/it][WARNING|generation_utils.py:914] 2023-08-28 23:32:29,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:29,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:30,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:31,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:32,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:33,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:33,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:34,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:35,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:36,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:36,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:37,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:38,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:38,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:39,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:40,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:41,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:42,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:43,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:43,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:33<01:50, 13.77s/it][WARNING|generation_utils.py:914] 2023-08-28 23:32:44,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:45,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:46,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:46,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:47,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:48,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:49,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:49,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:50,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:51,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:51,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:52,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:52,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:53,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:54,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:54,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:55,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:55,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:56,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:57,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:58,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:59,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:32:59,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:49<01:40, 14.42s/it][WARNING|generation_utils.py:914] 2023-08-28 23:33:00,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:01,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:01,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:02,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:02,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:03,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:04,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:04,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:05,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:06,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:06,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:07,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:07,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:08,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:09,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:09,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:10,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:10,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:11,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:11,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:01<01:21, 13.66s/it][WARNING|generation_utils.py:914] 2023-08-28 23:33:12,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:13,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:13,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:14,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:14,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:15,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:16,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:16,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:17,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:18,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:18,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:19,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:20,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:20,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:21,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:22,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:22,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:23,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:24,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:24,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:14<01:07, 13.42s/it][WARNING|generation_utils.py:914] 2023-08-28 23:33:25,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:25,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:26,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:26,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:27,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:27,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:28,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:29,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:29,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:30,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:30,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:31,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:31,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:32,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:33,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:33,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:34,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:35,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:35,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:36,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:26<00:51, 12.87s/it][WARNING|generation_utils.py:914] 2023-08-28 23:33:36,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:37,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:38,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:38,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:39,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:40,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:40,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:41,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:42,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:42,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:43,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:44,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:44,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:45,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:46,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:46,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:47,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:48,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:49,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:49,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:50,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:40<00:39, 13.30s/it][WARNING|generation_utils.py:914] 2023-08-28 23:33:51,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:51,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:52,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:53,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:53,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:54,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:55,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:55,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:56,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:57,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:58,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:59,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:59,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:00,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:00,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:01,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:02,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:02,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:03,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:04,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:53<00:26, 13.34s/it][WARNING|generation_utils.py:914] 2023-08-28 23:34:04,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:05,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:06,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:06,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:07,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:08,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:08,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:09,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:10,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:10,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:11,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:11,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:12,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:13,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:14,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:14,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:15,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:15,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:16,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:17,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:06<00:13, 13.29s/it][WARNING|generation_utils.py:914] 2023-08-28 23:34:17,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:18,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:18,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:19,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:20,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:20,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:21,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:22,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:22,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:23,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:23,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:24,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:24,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:25,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:26,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:26,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:27,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:27,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:28,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:28,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:29,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:30,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:30,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:20<00:00, 13.32s/it]Generating: 100%|██████████| 15/15 [03:20<00:00, 13.36s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:34:40,365 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:34:40,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:34:40,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:34:40,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:34:40,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:34:41,469 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:34:41,470 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:34:41,844 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:34:43,031 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:34:43,031 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:34:44,528 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:34:44,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:34:44,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:34:44,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:34:44,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:34:44,992 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:34:44,993 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:34:45,299 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:34:45,526 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:34:45,526 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9047619047619048, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9546875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9390625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : sports season of league or competition .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : architect .', 'success_rate': 0.9515625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : developer .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 498, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 346, 'raw': 352}
{'target': 600, 'success': 378, 'raw': 384}
{'target': 600, 'success': 410, 'raw': 416}
{'target': 600, 'success': 441, 'raw': 448}
{'target': 600, 'success': 471, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 595, 'raw': 608}
{'target': 600, 'success': 626, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.978125, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : operator .', 'success_rate': 0.9546875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9640625, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 575, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : position held .', 'success_rate': 0.8179347826086957, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 7915
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8015, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.73it/s]Extractor Estimating: 2it [00:01,  1.60it/s]Extractor Estimating: 3it [00:01,  1.60it/s]Extractor Estimating: 4it [00:02,  1.71it/s]Extractor Estimating: 5it [00:02,  1.74it/s]Extractor Estimating: 6it [00:03,  1.66it/s]Extractor Estimating: 7it [00:04,  1.56it/s]Extractor Estimating: 8it [00:04,  1.59it/s]Extractor Estimating: 9it [00:05,  1.65it/s]Extractor Estimating: 10it [00:06,  1.67it/s]Extractor Estimating: 11it [00:06,  1.65it/s]Extractor Estimating: 12it [00:07,  1.69it/s]Extractor Estimating: 13it [00:07,  1.64it/s]Extractor Estimating: 14it [00:08,  1.64it/s]Extractor Estimating: 15it [00:09,  1.67it/s]Extractor Estimating: 16it [00:09,  1.66it/s]Extractor Estimating: 17it [00:10,  1.68it/s]Extractor Estimating: 18it [00:10,  1.69it/s]Extractor Estimating: 19it [00:11,  1.72it/s]Extractor Estimating: 20it [00:11,  1.75it/s]Extractor Estimating: 21it [00:12,  1.73it/s]Extractor Estimating: 22it [00:13,  1.65it/s]Extractor Estimating: 23it [00:13,  1.68it/s]Extractor Estimating: 24it [00:14,  1.70it/s]Extractor Estimating: 25it [00:15,  1.66it/s]Extractor Estimating: 26it [00:15,  1.74it/s]Extractor Estimating: 27it [00:16,  1.78it/s]Extractor Estimating: 28it [00:16,  1.74it/s]Extractor Estimating: 29it [00:17,  1.78it/s]Extractor Estimating: 30it [00:17,  1.77it/s]Extractor Estimating: 31it [00:18,  1.79it/s]Extractor Estimating: 32it [00:18,  1.84it/s]Extractor Estimating: 33it [00:19,  1.90it/s]Extractor Estimating: 34it [00:19,  1.77it/s]Extractor Estimating: 35it [00:20,  1.83it/s]Extractor Estimating: 36it [00:20,  1.82it/s]Extractor Estimating: 37it [00:21,  1.82it/s]Extractor Estimating: 38it [00:22,  1.79it/s]Extractor Estimating: 39it [00:22,  1.77it/s]Extractor Estimating: 40it [00:23,  1.63it/s]Extractor Estimating: 41it [00:23,  1.72it/s]Extractor Estimating: 42it [00:24,  1.84it/s]Extractor Estimating: 43it [00:24,  1.87it/s]Extractor Estimating: 44it [00:25,  1.88it/s]Extractor Estimating: 45it [00:25,  1.87it/s]Extractor Estimating: 46it [00:26,  1.87it/s]Extractor Estimating: 47it [00:27,  1.86it/s]Extractor Estimating: 48it [00:27,  1.79it/s]Extractor Estimating: 49it [00:28,  1.84it/s]Extractor Estimating: 50it [00:28,  1.61it/s]Extractor Estimating: 51it [00:29,  1.57it/s]Extractor Estimating: 52it [00:30,  1.55it/s]Extractor Estimating: 53it [00:31,  1.48it/s]Extractor Estimating: 54it [00:31,  1.46it/s]Extractor Estimating: 55it [00:32,  1.48it/s]Extractor Estimating: 56it [00:33,  1.43it/s]Extractor Estimating: 57it [00:33,  1.45it/s]Extractor Estimating: 58it [00:34,  1.45it/s]Extractor Estimating: 59it [00:35,  1.45it/s]Extractor Estimating: 60it [00:36,  1.33it/s]Extractor Estimating: 61it [00:36,  1.34it/s]Extractor Estimating: 62it [00:37,  1.37it/s]Extractor Estimating: 63it [00:38,  1.39it/s]Extractor Estimating: 64it [00:38,  1.38it/s]Extractor Estimating: 65it [00:39,  1.42it/s]Extractor Estimating: 66it [00:40,  1.41it/s]Extractor Estimating: 67it [00:41,  1.40it/s]Extractor Estimating: 68it [00:41,  1.38it/s]Extractor Estimating: 69it [00:42,  1.33it/s]Extractor Estimating: 70it [00:43,  1.35it/s]Extractor Estimating: 71it [00:44,  1.36it/s]Extractor Estimating: 72it [00:44,  1.41it/s]Extractor Estimating: 73it [00:45,  1.36it/s]Extractor Estimating: 74it [00:46,  1.37it/s]Extractor Estimating: 75it [00:47,  1.35it/s]Extractor Estimating: 76it [00:47,  1.50it/s]Extractor Estimating: 77it [00:48,  1.62it/s]Extractor Estimating: 78it [00:48,  1.69it/s]Extractor Estimating: 79it [00:49,  1.69it/s]Extractor Estimating: 80it [00:49,  1.70it/s]Extractor Estimating: 81it [00:50,  1.75it/s]Extractor Estimating: 82it [00:50,  1.81it/s]Extractor Estimating: 83it [00:51,  1.82it/s]Extractor Estimating: 84it [00:51,  1.83it/s]Extractor Estimating: 85it [00:52,  1.82it/s]Extractor Estimating: 86it [00:52,  1.85it/s]Extractor Estimating: 87it [00:53,  1.83it/s]Extractor Estimating: 88it [00:53,  1.86it/s]Extractor Estimating: 89it [00:54,  1.85it/s]Extractor Estimating: 90it [00:55,  1.88it/s]Extractor Estimating: 91it [00:55,  1.75it/s]Extractor Estimating: 92it [00:56,  1.81it/s]Extractor Estimating: 93it [00:56,  1.73it/s]Extractor Estimating: 94it [00:57,  1.77it/s]Extractor Estimating: 95it [00:57,  1.82it/s]Extractor Estimating: 96it [00:58,  1.82it/s]Extractor Estimating: 97it [00:58,  1.86it/s]Extractor Estimating: 98it [00:59,  1.86it/s]Extractor Estimating: 99it [01:00,  1.85it/s]Extractor Estimating: 100it [01:00,  1.88it/s]Extractor Estimating: 101it [01:01,  1.84it/s]Extractor Estimating: 102it [01:01,  1.83it/s]Extractor Estimating: 103it [01:02,  1.83it/s]Extractor Estimating: 104it [01:02,  1.78it/s]Extractor Estimating: 105it [01:03,  1.80it/s]Extractor Estimating: 106it [01:03,  1.84it/s]Extractor Estimating: 107it [01:04,  1.82it/s]Extractor Estimating: 108it [01:04,  1.89it/s]Extractor Estimating: 109it [01:05,  1.93it/s]Extractor Estimating: 110it [01:06,  1.83it/s]Extractor Estimating: 111it [01:06,  1.91it/s]Extractor Estimating: 112it [01:07,  1.93it/s]Extractor Estimating: 113it [01:07,  1.88it/s]Extractor Estimating: 114it [01:08,  1.84it/s]Extractor Estimating: 115it [01:08,  1.85it/s]Extractor Estimating: 116it [01:09,  1.78it/s]Extractor Estimating: 117it [01:09,  1.82it/s]Extractor Estimating: 118it [01:10,  1.87it/s]Extractor Estimating: 119it [01:10,  1.86it/s]Extractor Estimating: 120it [01:11,  1.87it/s]Extractor Estimating: 121it [01:11,  1.86it/s]Extractor Estimating: 122it [01:12,  1.78it/s]Extractor Estimating: 123it [01:13,  1.76it/s]Extractor Estimating: 124it [01:13,  1.79it/s]Extractor Estimating: 125it [01:14,  1.81it/s]Extractor Estimating: 126it [01:14,  1.74it/s]Extractor Estimating: 127it [01:15,  1.73it/s]Extractor Estimating: 128it [01:16,  1.66it/s]Extractor Estimating: 129it [01:16,  1.68it/s]Extractor Estimating: 130it [01:17,  1.69it/s]Extractor Estimating: 131it [01:17,  1.68it/s]Extractor Estimating: 132it [01:18,  1.75it/s]Extractor Estimating: 133it [01:18,  1.77it/s]Extractor Estimating: 134it [01:19,  1.70it/s]Extractor Estimating: 135it [01:20,  1.71it/s]Extractor Estimating: 136it [01:20,  1.73it/s]Extractor Estimating: 137it [01:21,  1.72it/s]Extractor Estimating: 138it [01:21,  1.72it/s]Extractor Estimating: 139it [01:22,  1.69it/s]Extractor Estimating: 140it [01:23,  1.61it/s]Extractor Estimating: 141it [01:23,  1.64it/s]Extractor Estimating: 142it [01:24,  1.50it/s]Extractor Estimating: 143it [01:25,  1.59it/s]Extractor Estimating: 144it [01:25,  1.64it/s]Extractor Estimating: 145it [01:26,  1.60it/s]Extractor Estimating: 146it [01:26,  1.63it/s]Extractor Estimating: 147it [01:27,  1.68it/s]Extractor Estimating: 148it [01:28,  1.69it/s]Extractor Estimating: 149it [01:28,  1.75it/s]Extractor Estimating: 150it [01:29,  1.79it/s]Extractor Estimating: 151it [01:29,  1.85it/s]Extractor Estimating: 152it [01:30,  1.80it/s]Extractor Estimating: 153it [01:30,  1.82it/s]Extractor Estimating: 154it [01:31,  1.86it/s]Extractor Estimating: 155it [01:31,  1.89it/s]Extractor Estimating: 156it [01:32,  1.93it/s]Extractor Estimating: 157it [01:32,  1.97it/s]Extractor Estimating: 158it [01:33,  1.91it/s]Extractor Estimating: 159it [01:33,  1.86it/s]Extractor Estimating: 160it [01:34,  1.84it/s]Extractor Estimating: 161it [01:34,  1.84it/s]Extractor Estimating: 162it [01:35,  1.84it/s]Extractor Estimating: 163it [01:36,  1.83it/s]Extractor Estimating: 164it [01:36,  1.75it/s]Extractor Estimating: 165it [01:37,  1.81it/s]Extractor Estimating: 166it [01:37,  1.85it/s]Extractor Estimating: 167it [01:38,  1.88it/s]Extractor Estimating: 168it [01:38,  1.82it/s]Extractor Estimating: 169it [01:39,  1.86it/s]Extractor Estimating: 170it [01:39,  1.89it/s]Extractor Estimating: 171it [01:40,  1.87it/s]Extractor Estimating: 172it [01:40,  1.82it/s]Extractor Estimating: 173it [01:41,  1.78it/s]Extractor Estimating: 174it [01:42,  1.80it/s]Extractor Estimating: 175it [01:42,  1.74it/s]Extractor Estimating: 176it [01:43,  1.65it/s]Extractor Estimating: 177it [01:43,  1.68it/s]Extractor Estimating: 178it [01:44,  1.70it/s]Extractor Estimating: 179it [01:45,  1.73it/s]Extractor Estimating: 180it [01:45,  1.72it/s]Extractor Estimating: 181it [01:46,  1.71it/s]Extractor Estimating: 182it [01:46,  1.69it/s]Extractor Estimating: 183it [01:47,  1.80it/s]Extractor Estimating: 184it [01:47,  1.74it/s]Extractor Estimating: 185it [01:48,  1.76it/s]Extractor Estimating: 186it [01:49,  1.77it/s]Extractor Estimating: 187it [01:49,  1.73it/s]Extractor Estimating: 188it [01:50,  1.64it/s]Extractor Estimating: 189it [01:50,  1.67it/s]Extractor Estimating: 190it [01:51,  1.69it/s]Extractor Estimating: 191it [01:52,  1.70it/s]Extractor Estimating: 192it [01:52,  1.74it/s]Extractor Estimating: 193it [01:53,  1.81it/s]Extractor Estimating: 194it [01:53,  1.67it/s]Extractor Estimating: 195it [01:54,  1.71it/s]Extractor Estimating: 196it [01:54,  1.72it/s]Extractor Estimating: 197it [01:55,  1.72it/s]Extractor Estimating: 198it [01:56,  1.68it/s]Extractor Estimating: 199it [01:56,  1.67it/s]Extractor Estimating: 200it [01:57,  1.67it/s]Extractor Estimating: 201it [01:58,  1.60it/s]Extractor Estimating: 202it [01:58,  1.53it/s]Extractor Estimating: 203it [01:59,  1.55it/s]Extractor Estimating: 204it [01:59,  1.60it/s]Extractor Estimating: 205it [02:00,  1.60it/s]Extractor Estimating: 206it [02:01,  1.55it/s]Extractor Estimating: 207it [02:02,  1.51it/s]Extractor Estimating: 208it [02:02,  1.53it/s]Extractor Estimating: 209it [02:03,  1.56it/s]Extractor Estimating: 210it [02:03,  1.52it/s]Extractor Estimating: 211it [02:04,  1.48it/s]Extractor Estimating: 212it [02:05,  1.54it/s]Extractor Estimating: 213it [02:05,  1.57it/s]Extractor Estimating: 214it [02:06,  1.58it/s]Extractor Estimating: 215it [02:07,  1.53it/s]Extractor Estimating: 216it [02:07,  1.52it/s]Extractor Estimating: 217it [02:08,  1.57it/s]Extractor Estimating: 218it [02:09,  1.62it/s]Extractor Estimating: 219it [02:09,  1.64it/s]Extractor Estimating: 220it [02:10,  1.63it/s]Extractor Estimating: 221it [02:10,  1.64it/s]Extractor Estimating: 222it [02:11,  1.63it/s]Extractor Estimating: 223it [02:12,  1.59it/s]Extractor Estimating: 224it [02:12,  1.61it/s]Extractor Estimating: 225it [02:13,  1.60it/s]Extractor Estimating: 226it [02:14,  1.50it/s]Extractor Estimating: 227it [02:14,  1.48it/s]Extractor Estimating: 228it [02:15,  1.46it/s]Extractor Estimating: 229it [02:16,  1.48it/s]Extractor Estimating: 230it [02:16,  1.48it/s]Extractor Estimating: 231it [02:17,  1.49it/s]Extractor Estimating: 232it [02:18,  1.47it/s]Extractor Estimating: 233it [02:18,  1.48it/s]Extractor Estimating: 234it [02:19,  1.35it/s]Extractor Estimating: 235it [02:20,  1.35it/s]Extractor Estimating: 236it [02:21,  1.34it/s]Extractor Estimating: 237it [02:21,  1.40it/s]Extractor Estimating: 238it [02:22,  1.43it/s]Extractor Estimating: 239it [02:23,  1.46it/s]Extractor Estimating: 240it [02:23,  1.44it/s]Extractor Estimating: 241it [02:24,  1.43it/s]Extractor Estimating: 242it [02:25,  1.37it/s]Extractor Estimating: 243it [02:26,  1.42it/s]Extractor Estimating: 244it [02:26,  1.41it/s]Extractor Estimating: 245it [02:27,  1.40it/s]Extractor Estimating: 246it [02:28,  1.36it/s]Extractor Estimating: 247it [02:29,  1.39it/s]Extractor Estimating: 248it [02:29,  1.43it/s]Extractor Estimating: 249it [02:30,  1.45it/s]Extractor Estimating: 250it [02:30,  1.48it/s]Extractor Estimating: 251it [02:31,  1.57it/s]Extractor Estimating: 252it [02:31,  1.77it/s]Extractor Estimating: 253it [02:32,  1.93it/s]Extractor Estimating: 254it [02:32,  1.99it/s]Extractor Estimating: 255it [02:33,  2.12it/s]Extractor Estimating: 256it [02:33,  2.26it/s]Extractor Estimating: 257it [02:33,  2.32it/s]Extractor Estimating: 258it [02:34,  2.34it/s]Extractor Estimating: 259it [02:34,  2.16it/s]Extractor Estimating: 260it [02:35,  2.19it/s]Extractor Estimating: 261it [02:35,  2.30it/s]Extractor Estimating: 262it [02:36,  2.34it/s]Extractor Estimating: 263it [02:36,  2.38it/s]Extractor Estimating: 264it [02:36,  2.38it/s]Extractor Estimating: 265it [02:37,  2.41it/s]Extractor Estimating: 266it [02:38,  2.10it/s]Extractor Estimating: 267it [02:38,  2.22it/s]Extractor Estimating: 268it [02:38,  2.28it/s]Extractor Estimating: 269it [02:39,  2.28it/s]Extractor Estimating: 270it [02:39,  2.25it/s]Extractor Estimating: 271it [02:40,  2.30it/s]Extractor Estimating: 272it [02:40,  2.32it/s]Extractor Estimating: 273it [02:40,  2.34it/s]Extractor Estimating: 274it [02:41,  2.32it/s]Extractor Estimating: 275it [02:41,  2.23it/s]Extractor Estimating: 276it [02:42,  2.04it/s]Extractor Estimating: 277it [02:43,  1.84it/s]Extractor Estimating: 278it [02:43,  1.76it/s]Extractor Estimating: 279it [02:44,  1.65it/s]Extractor Estimating: 280it [02:45,  1.59it/s]Extractor Estimating: 281it [02:45,  1.62it/s]Extractor Estimating: 282it [02:46,  1.67it/s]Extractor Estimating: 283it [02:46,  1.62it/s]Extractor Estimating: 284it [02:47,  1.58it/s]Extractor Estimating: 285it [02:48,  1.60it/s]Extractor Estimating: 286it [02:48,  1.57it/s]Extractor Estimating: 287it [02:49,  1.54it/s]Extractor Estimating: 288it [02:50,  1.58it/s]Extractor Estimating: 289it [02:50,  1.59it/s]Extractor Estimating: 290it [02:51,  1.63it/s]Extractor Estimating: 291it [02:52,  1.60it/s]Extractor Estimating: 292it [02:52,  1.64it/s]Extractor Estimating: 293it [02:53,  1.63it/s]Extractor Estimating: 294it [02:53,  1.59it/s]Extractor Estimating: 295it [02:54,  1.57it/s]Extractor Estimating: 296it [02:55,  1.58it/s]Extractor Estimating: 297it [02:55,  1.59it/s]Extractor Estimating: 298it [02:56,  1.65it/s]Extractor Estimating: 299it [02:56,  1.60it/s]Extractor Estimating: 300it [02:57,  1.63it/s]Extractor Estimating: 301it [02:58,  1.58it/s]Extractor Estimating: 302it [02:58,  1.61it/s]Extractor Estimating: 303it [02:59,  1.68it/s]Extractor Estimating: 304it [02:59,  1.68it/s]Extractor Estimating: 305it [03:00,  1.62it/s]Extractor Estimating: 306it [03:01,  1.59it/s]Extractor Estimating: 307it [03:01,  1.65it/s]Extractor Estimating: 308it [03:02,  1.65it/s]Extractor Estimating: 309it [03:03,  1.69it/s]Extractor Estimating: 310it [03:03,  1.75it/s]Extractor Estimating: 311it [03:04,  1.71it/s]Extractor Estimating: 312it [03:04,  1.67it/s]Extractor Estimating: 313it [03:05,  1.68it/s]Extractor Estimating: 314it [03:06,  1.64it/s]Extractor Estimating: 315it [03:06,  1.67it/s]Extractor Estimating: 316it [03:07,  1.67it/s]Extractor Estimating: 317it [03:07,  1.70it/s]Extractor Estimating: 318it [03:08,  1.79it/s]Extractor Estimating: 319it [03:08,  1.75it/s]Extractor Estimating: 320it [03:09,  1.66it/s]Extractor Estimating: 321it [03:10,  1.68it/s]Extractor Estimating: 322it [03:10,  1.65it/s]Extractor Estimating: 323it [03:11,  1.66it/s]Extractor Estimating: 324it [03:11,  1.68it/s]Extractor Estimating: 325it [03:12,  1.67it/s]Extractor Estimating: 326it [03:13,  1.70it/s]Extractor Estimating: 327it [03:13,  1.65it/s]Extractor Estimating: 328it [03:14,  1.64it/s]Extractor Estimating: 329it [03:15,  1.47it/s]Extractor Estimating: 330it [03:15,  1.51it/s]Extractor Estimating: 331it [03:16,  1.52it/s]Extractor Estimating: 332it [03:17,  1.51it/s]Extractor Estimating: 333it [03:17,  1.53it/s]Extractor Estimating: 334it [03:18,  1.55it/s]Extractor Estimating: 335it [03:19,  1.56it/s]Extractor Estimating: 336it [03:19,  1.60it/s]Extractor Estimating: 337it [03:20,  1.61it/s]Extractor Estimating: 338it [03:20,  1.59it/s]Extractor Estimating: 339it [03:21,  1.61it/s]Extractor Estimating: 340it [03:22,  1.51it/s]Extractor Estimating: 341it [03:22,  1.53it/s]Extractor Estimating: 342it [03:23,  1.55it/s]Extractor Estimating: 343it [03:24,  1.52it/s]Extractor Estimating: 344it [03:24,  1.54it/s]Extractor Estimating: 345it [03:25,  1.56it/s]Extractor Estimating: 346it [03:25,  1.63it/s]Extractor Estimating: 347it [03:26,  1.62it/s]Extractor Estimating: 348it [03:27,  1.67it/s]Extractor Estimating: 349it [03:27,  1.63it/s]Extractor Estimating: 350it [03:28,  1.56it/s]Extractor Estimating: 351it [03:29,  1.65it/s]Extractor Estimating: 352it [03:29,  1.73it/s]Extractor Estimating: 353it [03:30,  1.73it/s]Extractor Estimating: 354it [03:30,  1.78it/s]Extractor Estimating: 355it [03:31,  1.81it/s]Extractor Estimating: 356it [03:31,  1.83it/s]Extractor Estimating: 357it [03:32,  1.82it/s]Extractor Estimating: 358it [03:32,  1.75it/s]Extractor Estimating: 359it [03:33,  1.66it/s]Extractor Estimating: 360it [03:34,  1.71it/s]Extractor Estimating: 361it [03:34,  1.80it/s]Extractor Estimating: 362it [03:35,  1.84it/s]Extractor Estimating: 363it [03:35,  1.76it/s]Extractor Estimating: 364it [03:36,  1.75it/s]Extractor Estimating: 365it [03:36,  1.68it/s]Extractor Estimating: 366it [03:37,  1.72it/s]Extractor Estimating: 367it [03:38,  1.75it/s]Extractor Estimating: 368it [03:38,  1.84it/s]Extractor Estimating: 369it [03:39,  1.83it/s]Extractor Estimating: 370it [03:39,  1.84it/s]Extractor Estimating: 371it [03:40,  1.82it/s]Extractor Estimating: 372it [03:40,  1.78it/s]Extractor Estimating: 373it [03:41,  1.81it/s]Extractor Estimating: 374it [03:41,  1.82it/s]Extractor Estimating: 375it [03:42,  1.73it/s]Extractor Estimating: 375it [03:42,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:48,641 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:48,685 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:48,685 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:48,685 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:48,685 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:38:49,481 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:38:49,482 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:38:50,129 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:38:51,290 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:38:51,290 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:54,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:54,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:54,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:54,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:54,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:38:55,551 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:38:55,552 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:38:56,257 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:38:56,512 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:38:56,512 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 01:51:06,013 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 01:51:06,362 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7499 mean pseudo reward: 0.9724643443921884
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl'}
train vocab size: 15966
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16066, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16066, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.009, loss:325.1961
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.000, loss:294.4011
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.023, loss:302.2368
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.013, loss:245.6234
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.019, loss:261.2060
>> valid entity prec:0.5270, rec:0.5154, f1:0.5212
>> valid relation prec:0.1690, rec:0.0979, f1:0.1240
>> valid relation with NER prec:0.1690, rec:0.0979, f1:0.1240
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.288, loss:254.4657
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.024, loss:260.3338
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.001, loss:238.8034
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.032, loss:267.9269
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.017, loss:230.4054
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5310, rec:0.5147, f1:0.5227
>> valid relation prec:0.2018, rec:0.1162, f1:0.1475
>> valid relation with NER prec:0.2018, rec:0.1162, f1:0.1475
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.289, loss:249.9863
g_step 1200, step 261, avg_time 1.020, loss:249.5999
g_step 1300, step 48, avg_time 1.026, loss:243.1530
g_step 1400, step 148, avg_time 1.020, loss:226.7948
g_step 1500, step 248, avg_time 1.005, loss:235.6667
>> valid entity prec:0.4950, rec:0.5402, f1:0.5166
>> valid relation prec:0.1681, rec:0.0948, f1:0.1212
>> valid relation with NER prec:0.1681, rec:0.0948, f1:0.1212
g_step 1600, step 35, avg_time 2.265, loss:240.0286
g_step 1700, step 135, avg_time 1.021, loss:210.0930
g_step 1800, step 235, avg_time 1.017, loss:212.6090
g_step 1900, step 22, avg_time 1.024, loss:221.1771
g_step 2000, step 122, avg_time 1.026, loss:200.1727
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5158, rec:0.5169, f1:0.5163
>> valid relation prec:0.1611, rec:0.0936, f1:0.1184
>> valid relation with NER prec:0.1611, rec:0.0936, f1:0.1184
g_step 2100, step 222, avg_time 2.272, loss:212.9598
g_step 2200, step 9, avg_time 1.026, loss:206.4950
g_step 2300, step 109, avg_time 1.019, loss:183.9925
g_step 2400, step 209, avg_time 1.018, loss:194.7390
g_step 2500, step 309, avg_time 1.023, loss:201.2789
>> valid entity prec:0.5331, rec:0.4810, f1:0.5057
>> valid relation prec:0.1804, rec:0.1039, f1:0.1319
>> valid relation with NER prec:0.1804, rec:0.1039, f1:0.1319
g_step 2600, step 96, avg_time 2.280, loss:166.3392
g_step 2700, step 196, avg_time 1.030, loss:188.7847
g_step 2800, step 296, avg_time 1.020, loss:196.7134
g_step 2900, step 83, avg_time 1.023, loss:166.3138
g_step 3000, step 183, avg_time 1.045, loss:171.7635
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5327, rec:0.5176, f1:0.5251
>> valid relation prec:0.2005, rec:0.1231, f1:0.1525
>> valid relation with NER prec:0.2005, rec:0.1231, f1:0.1525
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 283, avg_time 2.289, loss:168.7505
g_step 3200, step 70, avg_time 1.018, loss:152.6376
g_step 3300, step 170, avg_time 1.022, loss:173.7093
g_step 3400, step 270, avg_time 1.032, loss:165.2894
g_step 3500, step 57, avg_time 1.026, loss:158.6332
>> valid entity prec:0.5464, rec:0.5002, f1:0.5223
>> valid relation prec:0.1619, rec:0.0916, f1:0.1170
>> valid relation with NER prec:0.1619, rec:0.0916, f1:0.1170
g_step 3600, step 157, avg_time 2.289, loss:150.8777
g_step 3700, step 257, avg_time 1.021, loss:166.2433
g_step 3800, step 44, avg_time 1.024, loss:141.9307
g_step 3900, step 144, avg_time 1.037, loss:141.3209
g_step 4000, step 244, avg_time 1.013, loss:154.8749
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5351, rec:0.5043, f1:0.5192
>> valid relation prec:0.1494, rec:0.0925, f1:0.1142
>> valid relation with NER prec:0.1494, rec:0.0925, f1:0.1142
g_step 4100, step 31, avg_time 2.274, loss:147.0472
g_step 4200, step 131, avg_time 1.038, loss:140.2161
g_step 4300, step 231, avg_time 1.026, loss:154.4617
g_step 4400, step 18, avg_time 1.020, loss:143.6221
g_step 4500, step 118, avg_time 1.027, loss:130.0217
>> valid entity prec:0.5440, rec:0.4986, f1:0.5203
>> valid relation prec:0.1525, rec:0.0959, f1:0.1178
>> valid relation with NER prec:0.1525, rec:0.0959, f1:0.1178
g_step 4600, step 218, avg_time 2.276, loss:140.4792
g_step 4700, step 5, avg_time 1.031, loss:145.1980
g_step 4800, step 105, avg_time 1.033, loss:122.4243
g_step 4900, step 205, avg_time 1.027, loss:137.1576
g_step 5000, step 305, avg_time 1.030, loss:131.3740
learning rate was adjusted to 0.0008
>> valid entity prec:0.5244, rec:0.5035, f1:0.5137
>> valid relation prec:0.1696, rec:0.1068, f1:0.1311
>> valid relation with NER prec:0.1696, rec:0.1068, f1:0.1311
g_step 5100, step 92, avg_time 2.301, loss:119.4385
g_step 5200, step 192, avg_time 1.035, loss:123.0739
g_step 5300, step 292, avg_time 1.033, loss:116.7189
g_step 5400, step 79, avg_time 1.028, loss:119.4040
g_step 5500, step 179, avg_time 1.043, loss:117.0455
>> valid entity prec:0.5309, rec:0.4762, f1:0.5021
>> valid relation prec:0.1678, rec:0.0982, f1:0.1239
>> valid relation with NER prec:0.1678, rec:0.0982, f1:0.1239
g_step 5600, step 279, avg_time 2.297, loss:114.4737
g_step 5700, step 66, avg_time 1.022, loss:121.5670
g_step 5800, step 166, avg_time 1.046, loss:118.4094
g_step 5900, step 266, avg_time 1.020, loss:122.0412
g_step 6000, step 53, avg_time 1.031, loss:119.0851
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5404, rec:0.4527, f1:0.4927
>> valid relation prec:0.1497, rec:0.0827, f1:0.1066
>> valid relation with NER prec:0.1497, rec:0.0827, f1:0.1066
g_step 6100, step 153, avg_time 2.289, loss:116.4625
g_step 6200, step 253, avg_time 1.032, loss:112.2877
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 01:51:06 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 01:51:06 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_01-51-06_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 01:51:07 - WARNING - datasets.builder -   Using custom data configuration default-8751c80d4b0eef02
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8751c80d4b0eef02/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 01:51:09,750 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:51:09,751 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:51:09,752 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:51:09,753 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:51:09,821 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:51:09,856 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:51:09,856 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:51:09,856 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:51:09,856 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:51:09,856 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:51:09,856 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 01:51:10,147 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:51:13,344 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 01:51:13,369 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8751c80d4b0eef02/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  1.94ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.02ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.67ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.11ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.41ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.60ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.74ba/s]100%|██████████| 8/8 [00:01<00:00,  5.68ba/s]100%|██████████| 8/8 [00:01<00:00,  4.41ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.94ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.65ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.94ba/s]100%|██████████| 4/4 [00:00<00:00,  5.00ba/s]100%|██████████| 4/4 [00:00<00:00,  4.22ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  5.15ba/s] 38%|███▊      | 3/8 [00:00<00:00,  8.35ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.37ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.79ba/s]100%|██████████| 8/8 [00:00<00:00,  9.86ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.91ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.77ba/s]100%|██████████| 4/4 [00:00<00:00,  9.80ba/s]
[INFO|trainer.py:414] 2023-08-29 01:51:18,444 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 01:51:18,527 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 01:51:18,527 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 01:51:18,527 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 01:51:18,527 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 01:51:18,527 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 01:51:18,527 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 01:51:18,527 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:54,  3.34it/s]  0%|          | 2/585 [00:00<02:51,  3.39it/s]  1%|          | 3/585 [00:00<02:50,  3.41it/s]  1%|          | 4/585 [00:01<02:54,  3.33it/s]  1%|          | 5/585 [00:01<02:52,  3.36it/s]  1%|          | 6/585 [00:01<02:50,  3.39it/s]  1%|          | 7/585 [00:02<02:49,  3.40it/s]  1%|▏         | 8/585 [00:02<02:49,  3.41it/s]  2%|▏         | 9/585 [00:02<02:48,  3.42it/s]  2%|▏         | 10/585 [00:02<02:48,  3.42it/s]  2%|▏         | 11/585 [00:03<02:47,  3.43it/s]  2%|▏         | 12/585 [00:03<02:47,  3.43it/s]  2%|▏         | 13/585 [00:03<02:46,  3.43it/s]  2%|▏         | 14/585 [00:04<02:46,  3.43it/s]  3%|▎         | 15/585 [00:04<02:54,  3.27it/s]  3%|▎         | 16/585 [00:04<02:51,  3.32it/s]  3%|▎         | 17/585 [00:05<02:49,  3.35it/s]  3%|▎         | 18/585 [00:05<02:48,  3.37it/s]  3%|▎         | 19/585 [00:05<02:46,  3.39it/s]  3%|▎         | 20/585 [00:05<02:45,  3.41it/s]  4%|▎         | 21/585 [00:06<02:45,  3.41it/s]  4%|▍         | 22/585 [00:06<02:44,  3.42it/s]  4%|▍         | 23/585 [00:06<02:44,  3.42it/s]  4%|▍         | 24/585 [00:07<02:43,  3.42it/s]  4%|▍         | 25/585 [00:07<02:43,  3.43it/s]  4%|▍         | 26/585 [00:07<02:53,  3.23it/s]  5%|▍         | 27/585 [00:08<02:49,  3.28it/s]  5%|▍         | 28/585 [00:08<02:47,  3.33it/s]  5%|▍         | 29/585 [00:08<02:45,  3.35it/s]  5%|▌         | 30/585 [00:08<02:44,  3.37it/s]  5%|▌         | 31/585 [00:09<02:43,  3.39it/s]  5%|▌         | 32/585 [00:09<02:42,  3.39it/s]  6%|▌         | 33/585 [00:09<02:42,  3.40it/s]  6%|▌         | 34/585 [00:10<02:41,  3.41it/s]  6%|▌         | 35/585 [00:10<02:41,  3.41it/s]  6%|▌         | 36/585 [00:10<02:40,  3.42it/s]  6%|▋         | 37/585 [00:11<02:58,  3.08it/s]  6%|▋         | 38/585 [00:11<02:52,  3.17it/s]  7%|▋         | 39/585 [00:11<02:47,  3.25it/s]  7%|▋         | 40/585 [00:11<02:44,  3.31it/s]  7%|▋         | 41/585 [00:12<02:42,  3.34it/s]  7%|▋         | 42/585 [00:12<02:41,  3.37it/s]  7%|▋         | 43/585 [00:12<02:39,  3.39it/s]  8%|▊         | 44/585 [00:13<02:38,  3.41it/s]  8%|▊         | 45/585 [00:13<02:38,  3.41it/s]  8%|▊         | 46/585 [00:13<02:37,  3.42it/s]  8%|▊         | 47/585 [00:14<02:46,  3.23it/s]  8%|▊         | 48/585 [00:14<02:43,  3.29it/s]  8%|▊         | 49/585 [00:14<02:41,  3.32it/s]  9%|▊         | 50/585 [00:14<02:39,  3.35it/s]  9%|▊         | 51/585 [00:15<02:38,  3.37it/s]  9%|▉         | 52/585 [00:15<02:37,  3.38it/s]  9%|▉         | 53/585 [00:15<02:37,  3.39it/s]  9%|▉         | 54/585 [00:16<02:36,  3.40it/s]  9%|▉         | 55/585 [00:16<02:35,  3.40it/s] 10%|▉         | 56/585 [00:16<02:35,  3.40it/s] 10%|▉         | 57/585 [00:16<02:34,  3.41it/s] 10%|▉         | 58/585 [00:17<02:40,  3.29it/s] 10%|█         | 59/585 [00:17<02:38,  3.33it/s] 10%|█         | 60/585 [00:17<02:36,  3.36it/s] 10%|█         | 61/585 [00:18<02:35,  3.38it/s] 11%|█         | 62/585 [00:18<02:34,  3.39it/s] 11%|█         | 63/585 [00:18<02:33,  3.40it/s] 11%|█         | 64/585 [00:19<02:32,  3.41it/s] 11%|█         | 65/585 [00:19<02:32,  3.41it/s] 11%|█▏        | 66/585 [00:19<02:31,  3.42it/s] 11%|█▏        | 67/585 [00:19<02:31,  3.42it/s] 12%|█▏        | 68/585 [00:20<02:31,  3.42it/s] 12%|█▏        | 69/585 [00:20<02:30,  3.42it/s] 12%|█▏        | 70/585 [00:20<02:30,  3.42it/s] 12%|█▏        | 71/585 [00:21<02:30,  3.42it/s] 12%|█▏        | 72/585 [00:21<02:29,  3.42it/s] 12%|█▏        | 73/585 [00:21<02:29,  3.42it/s] 13%|█▎        | 74/585 [00:21<02:33,  3.32it/s] 13%|█▎        | 75/585 [00:22<02:32,  3.35it/s] 13%|█▎        | 76/585 [00:22<02:30,  3.37it/s] 13%|█▎        | 77/585 [00:22<02:30,  3.39it/s] 13%|█▎        | 78/585 [00:23<02:29,  3.40it/s] 14%|█▎        | 79/585 [00:23<02:28,  3.40it/s] 14%|█▎        | 80/585 [00:23<02:28,  3.41it/s] 14%|█▍        | 81/585 [00:24<02:27,  3.42it/s] 14%|█▍        | 82/585 [00:24<02:27,  3.42it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.42it/s] 14%|█▍        | 84/585 [00:24<02:26,  3.42it/s] 15%|█▍        | 85/585 [00:25<02:28,  3.36it/s] 15%|█▍        | 86/585 [00:25<02:27,  3.37it/s] 15%|█▍        | 87/585 [00:25<02:27,  3.39it/s] 15%|█▌        | 88/585 [00:26<02:26,  3.40it/s] 15%|█▌        | 89/585 [00:26<02:25,  3.40it/s] 15%|█▌        | 90/585 [00:26<02:25,  3.41it/s] 16%|█▌        | 91/585 [00:26<02:24,  3.41it/s] 16%|█▌        | 92/585 [00:27<02:24,  3.42it/s] 16%|█▌        | 93/585 [00:27<02:24,  3.42it/s] 16%|█▌        | 94/585 [00:27<02:23,  3.42it/s] 16%|█▌        | 95/585 [00:28<02:23,  3.42it/s] 16%|█▋        | 96/585 [00:28<02:26,  3.35it/s] 17%|█▋        | 97/585 [00:28<02:24,  3.37it/s] 17%|█▋        | 98/585 [00:29<02:24,  3.38it/s] 17%|█▋        | 99/585 [00:29<02:23,  3.39it/s] 17%|█▋        | 100/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 101/585 [00:29<02:21,  3.41it/s] 17%|█▋        | 102/585 [00:30<02:21,  3.41it/s] 18%|█▊        | 103/585 [00:30<02:21,  3.42it/s] 18%|█▊        | 104/585 [00:30<02:20,  3.42it/s] 18%|█▊        | 105/585 [00:31<02:20,  3.42it/s] 18%|█▊        | 106/585 [00:31<02:20,  3.42it/s] 18%|█▊        | 107/585 [00:31<02:23,  3.34it/s] 18%|█▊        | 108/585 [00:31<02:21,  3.36it/s] 19%|█▊        | 109/585 [00:32<02:20,  3.38it/s] 19%|█▉        | 110/585 [00:32<02:20,  3.39it/s] 19%|█▉        | 111/585 [00:32<02:19,  3.40it/s] 19%|█▉        | 112/585 [00:33<02:18,  3.41it/s] 19%|█▉        | 113/585 [00:33<02:18,  3.41it/s] 19%|█▉        | 114/585 [00:33<02:17,  3.41it/s] 20%|█▉        | 115/585 [00:34<02:17,  3.42it/s] 20%|█▉        | 116/585 [00:34<02:17,  3.42it/s] 20%|██        | 117/585 [00:34<02:16,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 01:51:53,185 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:51:53,185 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 01:51:53,185 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.93it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.66it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.29it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.40it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.74it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.30it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.21it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.65it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.86it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.00it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.07it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.99it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.03it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.98it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.78it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.52it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.48it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.75it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.76it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.91it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.98it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.92it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.88it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.84it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.66it/s][A
 30%|███       | 132/437 [00:03<00:07, 43.56it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.03it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.45it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.51it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.74it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.83it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.86it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.52it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.39it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.43it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.67it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.81it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.87it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.88it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.12it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.85it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.81it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.70it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.65it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.59it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.68it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.72it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.99it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.85it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.87it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.74it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 43.70it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.64it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.70it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.77it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.88it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.82it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.69it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.69it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.73it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.71it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.78it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.79it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.93it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.89it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.84it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.88it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.77it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.51it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.51it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.85it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.90it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.84it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.91it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.84it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.78it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.68it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.64it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.78it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.61it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 42.42it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 42.94it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.40it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.51it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.60it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.69it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.78it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:44<02:16,  3.42it/s]
100%|██████████| 437/437 [00:09<00:00, 43.78it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:52:03,305 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 01:52:03,528 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:52:06,554 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:52:07,035 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:52:07,130 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:55<50:38,  6.51s/it] 20%|██        | 119/585 [00:55<36:06,  4.65s/it] 21%|██        | 120/585 [00:56<25:54,  3.34s/it] 21%|██        | 121/585 [00:56<18:47,  2.43s/it] 21%|██        | 122/585 [00:56<13:48,  1.79s/it] 21%|██        | 123/585 [00:57<10:19,  1.34s/it] 21%|██        | 124/585 [00:57<07:53,  1.03s/it] 21%|██▏       | 125/585 [00:57<06:11,  1.24it/s] 22%|██▏       | 126/585 [00:57<05:00,  1.53it/s] 22%|██▏       | 127/585 [00:58<04:10,  1.83it/s] 22%|██▏       | 128/585 [00:58<03:35,  2.12it/s] 22%|██▏       | 129/585 [00:58<03:11,  2.39it/s] 22%|██▏       | 130/585 [00:59<02:56,  2.58it/s] 22%|██▏       | 131/585 [00:59<02:43,  2.77it/s] 23%|██▎       | 132/585 [00:59<02:34,  2.93it/s] 23%|██▎       | 133/585 [01:00<02:28,  3.05it/s] 23%|██▎       | 134/585 [01:00<02:23,  3.15it/s] 23%|██▎       | 135/585 [01:00<02:20,  3.21it/s] 23%|██▎       | 136/585 [01:00<02:17,  3.26it/s] 23%|██▎       | 137/585 [01:01<02:16,  3.29it/s] 24%|██▎       | 138/585 [01:01<02:14,  3.32it/s] 24%|██▍       | 139/585 [01:01<02:13,  3.34it/s] 24%|██▍       | 140/585 [01:02<02:12,  3.35it/s] 24%|██▍       | 141/585 [01:02<02:13,  3.32it/s] 24%|██▍       | 142/585 [01:02<02:12,  3.34it/s] 24%|██▍       | 143/585 [01:03<02:11,  3.35it/s] 25%|██▍       | 144/585 [01:03<02:11,  3.36it/s] 25%|██▍       | 145/585 [01:03<02:10,  3.36it/s] 25%|██▍       | 146/585 [01:03<02:10,  3.37it/s] 25%|██▌       | 147/585 [01:04<02:09,  3.37it/s] 25%|██▌       | 148/585 [01:04<02:09,  3.38it/s] 25%|██▌       | 149/585 [01:04<02:09,  3.38it/s] 26%|██▌       | 150/585 [01:05<02:08,  3.38it/s] 26%|██▌       | 151/585 [01:05<02:08,  3.38it/s] 26%|██▌       | 152/585 [01:05<02:13,  3.26it/s] 26%|██▌       | 153/585 [01:06<02:11,  3.29it/s] 26%|██▋       | 154/585 [01:06<02:10,  3.31it/s] 26%|██▋       | 155/585 [01:06<02:09,  3.33it/s] 27%|██▋       | 156/585 [01:06<02:08,  3.35it/s] 27%|██▋       | 157/585 [01:07<02:07,  3.36it/s] 27%|██▋       | 158/585 [01:07<02:06,  3.37it/s] 27%|██▋       | 159/585 [01:07<02:06,  3.37it/s] 27%|██▋       | 160/585 [01:08<02:06,  3.37it/s] 28%|██▊       | 161/585 [01:08<02:05,  3.38it/s] 28%|██▊       | 162/585 [01:08<02:05,  3.38it/s] 28%|██▊       | 163/585 [01:09<02:12,  3.19it/s] 28%|██▊       | 164/585 [01:09<02:09,  3.24it/s] 28%|██▊       | 165/585 [01:09<02:08,  3.27it/s] 28%|██▊       | 166/585 [01:09<02:06,  3.30it/s] 29%|██▊       | 167/585 [01:10<02:05,  3.32it/s] 29%|██▊       | 168/585 [01:10<02:04,  3.34it/s] 29%|██▉       | 169/585 [01:10<02:04,  3.35it/s] 29%|██▉       | 170/585 [01:11<02:03,  3.36it/s] 29%|██▉       | 171/585 [01:11<02:02,  3.37it/s] 29%|██▉       | 172/585 [01:11<02:02,  3.37it/s] 30%|██▉       | 173/585 [01:12<02:09,  3.18it/s] 30%|██▉       | 174/585 [01:12<02:07,  3.23it/s] 30%|██▉       | 175/585 [01:12<02:05,  3.27it/s] 30%|███       | 176/585 [01:12<02:04,  3.30it/s] 30%|███       | 177/585 [01:13<02:02,  3.32it/s] 30%|███       | 178/585 [01:13<02:01,  3.34it/s] 31%|███       | 179/585 [01:13<02:01,  3.35it/s] 31%|███       | 180/585 [01:14<02:00,  3.36it/s] 31%|███       | 181/585 [01:14<02:00,  3.36it/s] 31%|███       | 182/585 [01:14<01:59,  3.36it/s] 31%|███▏      | 183/585 [01:15<02:04,  3.23it/s] 31%|███▏      | 184/585 [01:15<02:02,  3.27it/s] 32%|███▏      | 185/585 [01:15<02:01,  3.31it/s] 32%|███▏      | 186/585 [01:15<01:59,  3.33it/s] 32%|███▏      | 187/585 [01:16<01:59,  3.34it/s] 32%|███▏      | 188/585 [01:16<01:58,  3.34it/s] 32%|███▏      | 189/585 [01:16<01:58,  3.35it/s] 32%|███▏      | 190/585 [01:17<01:57,  3.36it/s] 33%|███▎      | 191/585 [01:17<01:57,  3.37it/s] 33%|███▎      | 192/585 [01:17<01:56,  3.37it/s] 33%|███▎      | 193/585 [01:18<02:03,  3.17it/s] 33%|███▎      | 194/585 [01:18<02:01,  3.23it/s] 33%|███▎      | 195/585 [01:18<01:59,  3.27it/s] 34%|███▎      | 196/585 [01:19<01:57,  3.30it/s] 34%|███▎      | 197/585 [01:19<01:56,  3.32it/s] 34%|███▍      | 198/585 [01:19<01:55,  3.34it/s] 34%|███▍      | 199/585 [01:19<01:55,  3.35it/s] 34%|███▍      | 200/585 [01:20<01:54,  3.36it/s] 34%|███▍      | 201/585 [01:20<01:54,  3.37it/s] 35%|███▍      | 202/585 [01:20<01:53,  3.37it/s] 35%|███▍      | 203/585 [01:21<01:53,  3.37it/s] 35%|███▍      | 204/585 [01:21<01:53,  3.37it/s] 35%|███▌      | 205/585 [01:21<01:52,  3.38it/s] 35%|███▌      | 206/585 [01:21<01:51,  3.39it/s] 35%|███▌      | 207/585 [01:22<01:51,  3.40it/s] 36%|███▌      | 208/585 [01:22<01:50,  3.41it/s] 36%|███▌      | 209/585 [01:22<01:50,  3.41it/s] 36%|███▌      | 210/585 [01:23<01:49,  3.42it/s] 36%|███▌      | 211/585 [01:23<01:51,  3.35it/s] 36%|███▌      | 212/585 [01:23<01:50,  3.37it/s] 36%|███▋      | 213/585 [01:24<01:49,  3.39it/s] 37%|███▋      | 214/585 [01:24<01:49,  3.40it/s] 37%|███▋      | 215/585 [01:24<01:48,  3.40it/s] 37%|███▋      | 216/585 [01:24<01:48,  3.41it/s] 37%|███▋      | 217/585 [01:25<01:47,  3.42it/s] 37%|███▋      | 218/585 [01:25<01:47,  3.42it/s] 37%|███▋      | 219/585 [01:25<01:46,  3.42it/s] 38%|███▊      | 220/585 [01:26<01:46,  3.42it/s] 38%|███▊      | 221/585 [01:26<01:46,  3.42it/s] 38%|███▊      | 222/585 [01:26<01:48,  3.34it/s] 38%|███▊      | 223/585 [01:26<01:47,  3.37it/s] 38%|███▊      | 224/585 [01:27<01:46,  3.39it/s] 38%|███▊      | 225/585 [01:27<01:46,  3.40it/s] 39%|███▊      | 226/585 [01:27<01:45,  3.40it/s] 39%|███▉      | 227/585 [01:28<01:44,  3.41it/s] 39%|███▉      | 228/585 [01:28<01:44,  3.41it/s] 39%|███▉      | 229/585 [01:28<01:44,  3.42it/s] 39%|███▉      | 230/585 [01:29<01:43,  3.42it/s] 39%|███▉      | 231/585 [01:29<01:43,  3.42it/s] 40%|███▉      | 232/585 [01:29<01:43,  3.43it/s] 40%|███▉      | 233/585 [01:29<01:45,  3.35it/s] 40%|████      | 234/585 [01:30<01:44,  3.37it/s][INFO|trainer.py:2140] 2023-08-29 01:52:48,785 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:52:48,785 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 01:52:48,785 >>   Batch size = 8
{'eval_loss': 1.1175851821899414, 'eval_runtime': 10.0039, 'eval_samples_per_second': 349.165, 'eval_steps_per_second': 43.683, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 53.90it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.33it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.45it/s][A
  5%|▌         | 22/437 [00:00<00:09, 44.33it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.21it/s][A
  7%|▋         | 32/437 [00:00<00:09, 43.99it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.10it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.08it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.05it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.17it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.13it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.84it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.68it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.70it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.61it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.70it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.60it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.94it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.84it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.81it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.68it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 42.54it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.03it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.27it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.49it/s][A
 30%|███       | 132/437 [00:03<00:07, 43.56it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.76it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.68it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.59it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.23it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.25it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.52it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.70it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.84it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.78it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.93it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.90it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.82it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.68it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.62it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.83it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.70it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.84it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.77it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.95it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.73it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.46it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.54it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 41.79it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 42.59it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.10it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 43.42it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.71it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.65it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.70it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.59it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.30it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.35it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.69it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.04it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.07it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.95it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.83it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.86it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.44it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.33it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.44it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.82it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.94it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.06it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.06it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.94it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.68it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.44it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.52it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 42.47it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.02it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.50it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.80it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.86it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.59it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.38it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.28it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.30it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.50it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.71it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.85it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:40<01:44,  3.37it/s]
100%|██████████| 437/437 [00:10<00:00, 43.85it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:52:58,914 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 01:52:59,097 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:53:02,529 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:53:02,695 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:53:02,772 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:52<39:34,  6.78s/it] 40%|████      | 236/585 [01:52<28:08,  4.84s/it] 41%|████      | 237/585 [01:52<20:09,  3.48s/it] 41%|████      | 238/585 [01:53<14:34,  2.52s/it] 41%|████      | 239/585 [01:53<10:41,  1.85s/it] 41%|████      | 240/585 [01:53<07:58,  1.39s/it] 41%|████      | 241/585 [01:53<06:04,  1.06s/it] 41%|████▏     | 242/585 [01:54<04:47,  1.19it/s] 42%|████▏     | 243/585 [01:54<03:51,  1.48it/s] 42%|████▏     | 244/585 [01:54<03:11,  1.78it/s] 42%|████▏     | 245/585 [01:55<02:43,  2.08it/s] 42%|████▏     | 246/585 [01:55<02:24,  2.35it/s] 42%|████▏     | 247/585 [01:55<02:10,  2.58it/s] 42%|████▏     | 248/585 [01:56<02:01,  2.78it/s] 43%|████▎     | 249/585 [01:56<01:54,  2.94it/s] 43%|████▎     | 250/585 [01:56<01:49,  3.06it/s] 43%|████▎     | 251/585 [01:56<01:46,  3.15it/s] 43%|████▎     | 252/585 [01:57<01:43,  3.21it/s] 43%|████▎     | 253/585 [01:57<01:43,  3.20it/s] 43%|████▎     | 254/585 [01:57<01:41,  3.25it/s] 44%|████▎     | 255/585 [01:58<01:40,  3.29it/s] 44%|████▍     | 256/585 [01:58<01:39,  3.32it/s] 44%|████▍     | 257/585 [01:58<01:38,  3.33it/s] 44%|████▍     | 258/585 [01:58<01:37,  3.34it/s] 44%|████▍     | 259/585 [01:59<01:37,  3.36it/s] 44%|████▍     | 260/585 [01:59<01:36,  3.37it/s] 45%|████▍     | 261/585 [01:59<01:36,  3.37it/s] 45%|████▍     | 262/585 [02:00<01:35,  3.37it/s] 45%|████▍     | 263/585 [02:00<01:35,  3.37it/s] 45%|████▌     | 264/585 [02:00<01:37,  3.30it/s] 45%|████▌     | 265/585 [02:01<01:36,  3.32it/s] 45%|████▌     | 266/585 [02:01<01:35,  3.34it/s] 46%|████▌     | 267/585 [02:01<01:34,  3.35it/s] 46%|████▌     | 268/585 [02:01<01:34,  3.36it/s] 46%|████▌     | 269/585 [02:02<01:33,  3.36it/s] 46%|████▌     | 270/585 [02:02<01:33,  3.37it/s] 46%|████▋     | 271/585 [02:02<01:33,  3.37it/s] 46%|████▋     | 272/585 [02:03<01:32,  3.37it/s] 47%|████▋     | 273/585 [02:03<01:32,  3.37it/s] 47%|████▋     | 274/585 [02:03<01:32,  3.37it/s] 47%|████▋     | 275/585 [02:04<01:34,  3.28it/s] 47%|████▋     | 276/585 [02:04<01:33,  3.31it/s] 47%|████▋     | 277/585 [02:04<01:32,  3.33it/s] 48%|████▊     | 278/585 [02:04<01:31,  3.35it/s] 48%|████▊     | 279/585 [02:05<01:31,  3.36it/s] 48%|████▊     | 280/585 [02:05<01:30,  3.36it/s] 48%|████▊     | 281/585 [02:05<01:30,  3.36it/s] 48%|████▊     | 282/585 [02:06<01:29,  3.37it/s] 48%|████▊     | 283/585 [02:06<01:29,  3.37it/s] 49%|████▊     | 284/585 [02:06<01:29,  3.37it/s] 49%|████▊     | 285/585 [02:07<01:28,  3.38it/s] 49%|████▉     | 286/585 [02:07<01:33,  3.20it/s] 49%|████▉     | 287/585 [02:07<01:31,  3.25it/s] 49%|████▉     | 288/585 [02:07<01:30,  3.29it/s] 49%|████▉     | 289/585 [02:08<01:29,  3.32it/s] 50%|████▉     | 290/585 [02:08<01:28,  3.34it/s] 50%|████▉     | 291/585 [02:08<01:27,  3.35it/s] 50%|████▉     | 292/585 [02:09<01:27,  3.36it/s] 50%|█████     | 293/585 [02:09<01:26,  3.36it/s] 50%|█████     | 294/585 [02:09<01:26,  3.36it/s] 50%|█████     | 295/585 [02:10<01:26,  3.37it/s] 51%|█████     | 296/585 [02:10<01:29,  3.24it/s] 51%|█████     | 297/585 [02:10<01:27,  3.28it/s] 51%|█████     | 298/585 [02:10<01:26,  3.31it/s] 51%|█████     | 299/585 [02:11<01:25,  3.33it/s] 51%|█████▏    | 300/585 [02:11<01:25,  3.35it/s] 51%|█████▏    | 301/585 [02:11<01:24,  3.36it/s] 52%|█████▏    | 302/585 [02:12<01:23,  3.37it/s] 52%|█████▏    | 303/585 [02:12<01:23,  3.37it/s] 52%|█████▏    | 304/585 [02:12<01:23,  3.37it/s] 52%|█████▏    | 305/585 [02:13<01:22,  3.38it/s] 52%|█████▏    | 306/585 [02:13<01:22,  3.38it/s] 52%|█████▏    | 307/585 [02:13<01:28,  3.15it/s] 53%|█████▎    | 308/585 [02:13<01:25,  3.23it/s] 53%|█████▎    | 309/585 [02:14<01:24,  3.28it/s] 53%|█████▎    | 310/585 [02:14<01:22,  3.33it/s] 53%|█████▎    | 311/585 [02:14<01:21,  3.36it/s] 53%|█████▎    | 312/585 [02:15<01:20,  3.38it/s] 54%|█████▎    | 313/585 [02:15<01:20,  3.40it/s] 54%|█████▎    | 314/585 [02:15<01:19,  3.41it/s] 54%|█████▍    | 315/585 [02:16<01:18,  3.42it/s] 54%|█████▍    | 316/585 [02:16<01:18,  3.42it/s] 54%|█████▍    | 317/585 [02:16<01:18,  3.42it/s] 54%|█████▍    | 318/585 [02:17<01:27,  3.05it/s] 55%|█████▍    | 319/585 [02:17<01:24,  3.16it/s] 55%|█████▍    | 320/585 [02:17<01:21,  3.23it/s] 55%|█████▍    | 321/585 [02:17<01:20,  3.29it/s] 55%|█████▌    | 322/585 [02:18<01:19,  3.33it/s] 55%|█████▌    | 323/585 [02:18<01:18,  3.36it/s] 55%|█████▌    | 324/585 [02:18<01:17,  3.38it/s] 56%|█████▌    | 325/585 [02:19<01:16,  3.40it/s] 56%|█████▌    | 326/585 [02:19<01:16,  3.40it/s] 56%|█████▌    | 327/585 [02:19<01:15,  3.41it/s] 56%|█████▌    | 328/585 [02:19<01:18,  3.29it/s] 56%|█████▌    | 329/585 [02:20<01:16,  3.33it/s] 56%|█████▋    | 330/585 [02:20<01:15,  3.36it/s] 57%|█████▋    | 331/585 [02:20<01:15,  3.38it/s] 57%|█████▋    | 332/585 [02:21<01:14,  3.39it/s] 57%|█████▋    | 333/585 [02:21<01:14,  3.40it/s] 57%|█████▋    | 334/585 [02:21<01:13,  3.40it/s] 57%|█████▋    | 335/585 [02:22<01:13,  3.41it/s] 57%|█████▋    | 336/585 [02:22<01:12,  3.41it/s] 58%|█████▊    | 337/585 [02:22<01:12,  3.42it/s] 58%|█████▊    | 338/585 [02:22<01:12,  3.42it/s] 58%|█████▊    | 339/585 [02:23<01:11,  3.42it/s] 58%|█████▊    | 340/585 [02:23<01:11,  3.42it/s] 58%|█████▊    | 341/585 [02:23<01:11,  3.42it/s] 58%|█████▊    | 342/585 [02:24<01:11,  3.42it/s] 59%|█████▊    | 343/585 [02:24<01:10,  3.42it/s] 59%|█████▉    | 344/585 [02:24<01:10,  3.42it/s] 59%|█████▉    | 345/585 [02:24<01:11,  3.34it/s] 59%|█████▉    | 346/585 [02:25<01:11,  3.36it/s] 59%|█████▉    | 347/585 [02:25<01:10,  3.38it/s] 59%|█████▉    | 348/585 [02:25<01:09,  3.39it/s] 60%|█████▉    | 349/585 [02:26<01:09,  3.40it/s] 60%|█████▉    | 350/585 [02:26<01:08,  3.41it/s] 60%|██████    | 351/585 [02:26<01:08,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 01:53:45,301 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:53:45,301 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 01:53:45,301 >>   Batch size = 8
{'eval_loss': 1.136726975440979, 'eval_runtime': 10.03, 'eval_samples_per_second': 348.255, 'eval_steps_per_second': 43.569, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.47it/s][A
  3%|▎         | 12/437 [00:00<00:09, 47.13it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.58it/s][A
  5%|▌         | 22/437 [00:00<00:09, 44.92it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.26it/s][A
  7%|▋         | 32/437 [00:00<00:09, 43.88it/s][A
  8%|▊         | 37/437 [00:00<00:09, 43.88it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.88it/s][A
 11%|█         | 47/437 [00:01<00:09, 43.26it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 43.71it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.60it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.72it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.63it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.69it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.76it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.73it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.66it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.79it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.79it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.83it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.80it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.74it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.77it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.67it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.77it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.72it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.85it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.77it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.75it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.78it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.53it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.70it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.74it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.65it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.77it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 42.65it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.16it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.24it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.45it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.59it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.56it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.67it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.53it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.54it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.72it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.74it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.90it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.89it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.79it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.78it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.65it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.79it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.77it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.85it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.72it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.81it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.86it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.80it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.87it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.77it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.75it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.68it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 42.73it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.19it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.16it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.40it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.53it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.55it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.64it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.44it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.63it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.79it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.91it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.88it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.82it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.75it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.66it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.69it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.53it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.52it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.82it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.92it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.86it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.74it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.69it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.50it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.70it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:36<01:08,  3.41it/s]
100%|██████████| 437/437 [00:09<00:00, 43.70it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:53:55,380 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 01:53:55,525 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:53:58,573 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:53:58,695 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:53:58,782 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:46<23:44,  6.12s/it] 60%|██████    | 353/585 [02:46<16:54,  4.37s/it] 61%|██████    | 354/585 [02:47<12:07,  3.15s/it] 61%|██████    | 355/585 [02:47<08:47,  2.29s/it] 61%|██████    | 356/585 [02:47<06:28,  1.69s/it] 61%|██████    | 357/585 [02:47<04:50,  1.28s/it] 61%|██████    | 358/585 [02:48<03:42,  1.02it/s] 61%|██████▏   | 359/585 [02:48<02:55,  1.29it/s] 62%|██████▏   | 360/585 [02:48<02:22,  1.58it/s] 62%|██████▏   | 361/585 [02:49<01:59,  1.88it/s] 62%|██████▏   | 362/585 [02:49<01:42,  2.17it/s] 62%|██████▏   | 363/585 [02:49<01:31,  2.43it/s] 62%|██████▏   | 364/585 [02:50<01:24,  2.61it/s] 62%|██████▏   | 365/585 [02:50<01:18,  2.80it/s] 63%|██████▎   | 366/585 [02:50<01:14,  2.95it/s] 63%|██████▎   | 367/585 [02:50<01:11,  3.07it/s] 63%|██████▎   | 368/585 [02:51<01:09,  3.11it/s] 63%|██████▎   | 369/585 [02:51<01:19,  2.72it/s] 63%|██████▎   | 370/585 [02:51<01:14,  2.89it/s] 63%|██████▎   | 371/585 [02:52<01:10,  3.02it/s] 64%|██████▎   | 372/585 [02:52<01:08,  3.12it/s] 64%|██████▍   | 373/585 [02:52<01:06,  3.19it/s] 64%|██████▍   | 374/585 [02:53<01:05,  3.25it/s] 64%|██████▍   | 375/585 [02:53<01:03,  3.28it/s] 64%|██████▍   | 376/585 [02:53<01:03,  3.31it/s] 64%|██████▍   | 377/585 [02:54<01:02,  3.33it/s] 65%|██████▍   | 378/585 [02:54<01:01,  3.35it/s] 65%|██████▍   | 379/585 [02:54<01:01,  3.36it/s] 65%|██████▍   | 380/585 [02:54<01:01,  3.36it/s] 65%|██████▌   | 381/585 [02:55<01:00,  3.36it/s] 65%|██████▌   | 382/585 [02:55<01:01,  3.30it/s] 65%|██████▌   | 383/585 [02:55<01:00,  3.32it/s] 66%|██████▌   | 384/585 [02:56<01:00,  3.34it/s] 66%|██████▌   | 385/585 [02:56<00:59,  3.35it/s] 66%|██████▌   | 386/585 [02:56<00:59,  3.36it/s] 66%|██████▌   | 387/585 [02:57<00:58,  3.36it/s] 66%|██████▋   | 388/585 [02:57<00:58,  3.37it/s] 66%|██████▋   | 389/585 [02:57<00:58,  3.37it/s] 67%|██████▋   | 390/585 [02:57<00:57,  3.37it/s] 67%|██████▋   | 391/585 [02:58<00:57,  3.37it/s] 67%|██████▋   | 392/585 [02:58<00:57,  3.37it/s] 67%|██████▋   | 393/585 [02:58<00:57,  3.34it/s] 67%|██████▋   | 394/585 [02:59<00:57,  3.35it/s] 68%|██████▊   | 395/585 [02:59<00:56,  3.36it/s] 68%|██████▊   | 396/585 [02:59<00:56,  3.37it/s] 68%|██████▊   | 397/585 [03:00<00:55,  3.37it/s] 68%|██████▊   | 398/585 [03:00<00:55,  3.37it/s] 68%|██████▊   | 399/585 [03:00<00:55,  3.36it/s] 68%|██████▊   | 400/585 [03:00<00:55,  3.36it/s] 69%|██████▊   | 401/585 [03:01<00:54,  3.37it/s] 69%|██████▊   | 402/585 [03:01<00:54,  3.37it/s] 69%|██████▉   | 403/585 [03:01<00:53,  3.39it/s] 69%|██████▉   | 404/585 [03:02<00:53,  3.35it/s] 69%|██████▉   | 405/585 [03:02<00:53,  3.38it/s] 69%|██████▉   | 406/585 [03:02<00:52,  3.39it/s] 70%|██████▉   | 407/585 [03:02<00:52,  3.40it/s] 70%|██████▉   | 408/585 [03:03<00:51,  3.41it/s] 70%|██████▉   | 409/585 [03:03<00:51,  3.41it/s] 70%|███████   | 410/585 [03:03<00:51,  3.41it/s] 70%|███████   | 411/585 [03:04<00:51,  3.41it/s] 70%|███████   | 412/585 [03:04<00:50,  3.41it/s] 71%|███████   | 413/585 [03:04<00:50,  3.42it/s] 71%|███████   | 414/585 [03:05<00:50,  3.42it/s] 71%|███████   | 415/585 [03:05<00:50,  3.37it/s] 71%|███████   | 416/585 [03:05<00:49,  3.38it/s] 71%|███████▏  | 417/585 [03:05<00:49,  3.40it/s] 71%|███████▏  | 418/585 [03:06<00:49,  3.41it/s] 72%|███████▏  | 419/585 [03:06<00:48,  3.41it/s] 72%|███████▏  | 420/585 [03:06<00:48,  3.42it/s] 72%|███████▏  | 421/585 [03:07<00:47,  3.42it/s] 72%|███████▏  | 422/585 [03:07<00:47,  3.42it/s] 72%|███████▏  | 423/585 [03:07<00:47,  3.42it/s] 72%|███████▏  | 424/585 [03:08<00:49,  3.28it/s] 73%|███████▎  | 425/585 [03:08<00:48,  3.32it/s] 73%|███████▎  | 426/585 [03:08<00:48,  3.25it/s] 73%|███████▎  | 427/585 [03:08<00:47,  3.29it/s] 73%|███████▎  | 428/585 [03:09<00:47,  3.33it/s] 73%|███████▎  | 429/585 [03:09<00:46,  3.36it/s] 74%|███████▎  | 430/585 [03:09<00:45,  3.37it/s] 74%|███████▎  | 431/585 [03:10<00:45,  3.39it/s] 74%|███████▍  | 432/585 [03:10<00:45,  3.40it/s] 74%|███████▍  | 433/585 [03:10<00:44,  3.41it/s] 74%|███████▍  | 434/585 [03:10<00:44,  3.42it/s] 74%|███████▍  | 435/585 [03:11<00:43,  3.41it/s] 75%|███████▍  | 436/585 [03:11<00:43,  3.42it/s] 75%|███████▍  | 437/585 [03:11<00:46,  3.21it/s] 75%|███████▍  | 438/585 [03:12<00:44,  3.27it/s] 75%|███████▌  | 439/585 [03:12<00:44,  3.31it/s] 75%|███████▌  | 440/585 [03:12<00:43,  3.35it/s] 75%|███████▌  | 441/585 [03:13<00:42,  3.37it/s] 76%|███████▌  | 442/585 [03:13<00:42,  3.39it/s] 76%|███████▌  | 443/585 [03:13<00:41,  3.40it/s] 76%|███████▌  | 444/585 [03:13<00:41,  3.41it/s] 76%|███████▌  | 445/585 [03:14<00:40,  3.42it/s] 76%|███████▌  | 446/585 [03:14<00:40,  3.42it/s] 76%|███████▋  | 447/585 [03:14<00:40,  3.42it/s] 77%|███████▋  | 448/585 [03:15<00:42,  3.22it/s] 77%|███████▋  | 449/585 [03:15<00:41,  3.28it/s] 77%|███████▋  | 450/585 [03:15<00:40,  3.33it/s] 77%|███████▋  | 451/585 [03:16<00:39,  3.36it/s] 77%|███████▋  | 452/585 [03:16<00:39,  3.38it/s] 77%|███████▋  | 453/585 [03:16<00:38,  3.39it/s] 78%|███████▊  | 454/585 [03:16<00:38,  3.40it/s] 78%|███████▊  | 455/585 [03:17<00:38,  3.41it/s] 78%|███████▊  | 456/585 [03:17<00:37,  3.41it/s] 78%|███████▊  | 457/585 [03:17<00:37,  3.42it/s] 78%|███████▊  | 458/585 [03:18<00:37,  3.42it/s] 78%|███████▊  | 459/585 [03:18<00:37,  3.36it/s] 79%|███████▊  | 460/585 [03:18<00:36,  3.38it/s] 79%|███████▉  | 461/585 [03:18<00:36,  3.39it/s] 79%|███████▉  | 462/585 [03:19<00:36,  3.40it/s] 79%|███████▉  | 463/585 [03:19<00:35,  3.40it/s] 79%|███████▉  | 464/585 [03:19<00:35,  3.41it/s] 79%|███████▉  | 465/585 [03:20<00:35,  3.41it/s] 80%|███████▉  | 466/585 [03:20<00:34,  3.41it/s] 80%|███████▉  | 467/585 [03:20<00:34,  3.42it/s] 80%|████████  | 468/585 [03:21<00:34,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 01:54:39,601 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:54:39,601 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 01:54:39,601 >>   Batch size = 8
{'eval_loss': 1.1529080867767334, 'eval_runtime': 10.0233, 'eval_samples_per_second': 348.489, 'eval_steps_per_second': 43.598, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 59.16it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.49it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.98it/s][A
  5%|▌         | 22/437 [00:00<00:09, 44.93it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.53it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.36it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.00it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.88it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.81it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 43.78it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.68it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.73it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.77it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.69it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.78it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.63it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.67it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.66it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.83it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.71it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.68it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.86it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.70it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.62it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.75it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.89it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.82it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.66it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.83it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.77it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.59it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.51it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.78it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.84it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.83it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.82it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.70it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.71it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.86it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.83it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.61it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.14it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.24it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.44it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.60it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.54it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.54it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.65it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.83it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.67it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.65it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.97it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.92it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.90it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.82it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.69it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.82it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.75it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.52it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.81it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.78it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.85it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.79it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.76it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.76it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.75it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.67it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.62it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 42.89it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.42it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.43it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.36it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.46it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.71it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.69it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.60it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.48it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.60it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.63it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.71it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.61it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.60it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.71it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.71it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.41it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.46it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.67it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:31<00:34,  3.42it/s]
100%|██████████| 437/437 [00:09<00:00, 43.67it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:54:49,734 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 01:54:49,825 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:54:52,747 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:54:52,915 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:54:52,999 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:40<11:46,  6.09s/it] 80%|████████  | 470/585 [03:40<08:21,  4.36s/it] 81%|████████  | 471/585 [03:41<05:57,  3.14s/it] 81%|████████  | 472/585 [03:41<04:18,  2.29s/it] 81%|████████  | 473/585 [03:41<03:09,  1.69s/it] 81%|████████  | 474/585 [03:42<02:21,  1.27s/it] 81%|████████  | 475/585 [03:42<01:47,  1.02it/s] 81%|████████▏ | 476/585 [03:42<01:24,  1.29it/s] 82%|████████▏ | 477/585 [03:43<01:08,  1.59it/s] 82%|████████▏ | 478/585 [03:43<00:56,  1.89it/s] 82%|████████▏ | 479/585 [03:43<00:48,  2.17it/s] 82%|████████▏ | 480/585 [03:43<00:43,  2.43it/s] 82%|████████▏ | 481/585 [03:44<00:39,  2.61it/s] 82%|████████▏ | 482/585 [03:44<00:36,  2.80it/s] 83%|████████▎ | 483/585 [03:44<00:34,  2.95it/s] 83%|████████▎ | 484/585 [03:45<00:32,  3.07it/s] 83%|████████▎ | 485/585 [03:45<00:31,  3.16it/s] 83%|████████▎ | 486/585 [03:45<00:30,  3.22it/s] 83%|████████▎ | 487/585 [03:46<00:30,  3.26it/s] 83%|████████▎ | 488/585 [03:46<00:29,  3.29it/s] 84%|████████▎ | 489/585 [03:46<00:28,  3.32it/s] 84%|████████▍ | 490/585 [03:46<00:28,  3.34it/s] 84%|████████▍ | 491/585 [03:47<00:28,  3.35it/s] 84%|████████▍ | 492/585 [03:47<00:28,  3.28it/s] 84%|████████▍ | 493/585 [03:47<00:27,  3.31it/s] 84%|████████▍ | 494/585 [03:48<00:27,  3.33it/s] 85%|████████▍ | 495/585 [03:48<00:26,  3.34it/s] 85%|████████▍ | 496/585 [03:48<00:26,  3.35it/s] 85%|████████▍ | 497/585 [03:49<00:26,  3.36it/s] 85%|████████▌ | 498/585 [03:49<00:25,  3.37it/s] 85%|████████▌ | 499/585 [03:49<00:25,  3.36it/s] 85%|████████▌ | 500/585 [03:49<00:25,  3.37it/s]                                                  85%|████████▌ | 500/585 [03:49<00:25,  3.37it/s] 86%|████████▌ | 501/585 [03:50<00:24,  3.37it/s] 86%|████████▌ | 502/585 [03:50<00:24,  3.37it/s] 86%|████████▌ | 503/585 [03:50<00:24,  3.32it/s] 86%|████████▌ | 504/585 [03:51<00:24,  3.34it/s] 86%|████████▋ | 505/585 [03:51<00:23,  3.35it/s] 86%|████████▋ | 506/585 [03:51<00:29,  2.63it/s] 87%|████████▋ | 507/585 [03:52<00:27,  2.82it/s] 87%|████████▋ | 508/585 [03:52<00:25,  2.96it/s] 87%|████████▋ | 509/585 [03:52<00:24,  3.08it/s] 87%|████████▋ | 510/585 [03:53<00:23,  3.16it/s] 87%|████████▋ | 511/585 [03:53<00:22,  3.22it/s] 88%|████████▊ | 512/585 [03:53<00:22,  3.26it/s] 88%|████████▊ | 513/585 [03:54<00:21,  3.29it/s] 88%|████████▊ | 514/585 [03:54<00:21,  3.31it/s] 88%|████████▊ | 515/585 [03:54<00:21,  3.33it/s] 88%|████████▊ | 516/585 [03:54<00:20,  3.34it/s] 88%|████████▊ | 517/585 [03:55<00:20,  3.35it/s] 89%|████████▊ | 518/585 [03:55<00:19,  3.36it/s] 89%|████████▊ | 519/585 [03:55<00:19,  3.36it/s] 89%|████████▉ | 520/585 [03:56<00:19,  3.36it/s] 89%|████████▉ | 521/585 [03:56<00:19,  3.36it/s] 89%|████████▉ | 522/585 [03:56<00:18,  3.36it/s] 89%|████████▉ | 523/585 [03:57<00:18,  3.26it/s] 90%|████████▉ | 524/585 [03:57<00:18,  3.29it/s] 90%|████████▉ | 525/585 [03:57<00:18,  3.31it/s] 90%|████████▉ | 526/585 [03:57<00:17,  3.33it/s] 90%|█████████ | 527/585 [03:58<00:17,  3.34it/s] 90%|█████████ | 528/585 [03:58<00:16,  3.35it/s] 90%|█████████ | 529/585 [03:58<00:16,  3.36it/s] 91%|█████████ | 530/585 [03:59<00:16,  3.36it/s] 91%|█████████ | 531/585 [03:59<00:16,  3.36it/s] 91%|█████████ | 532/585 [03:59<00:15,  3.36it/s] 91%|█████████ | 533/585 [04:00<00:15,  3.30it/s] 91%|█████████▏| 534/585 [04:00<00:15,  3.32it/s] 91%|█████████▏| 535/585 [04:00<00:14,  3.34it/s] 92%|█████████▏| 536/585 [04:00<00:14,  3.35it/s] 92%|█████████▏| 537/585 [04:01<00:14,  3.35it/s] 92%|█████████▏| 538/585 [04:01<00:14,  3.36it/s] 92%|█████████▏| 539/585 [04:01<00:13,  3.36it/s] 92%|█████████▏| 540/585 [04:02<00:13,  3.36it/s] 92%|█████████▏| 541/585 [04:02<00:13,  3.37it/s] 93%|█████████▎| 542/585 [04:02<00:12,  3.37it/s] 93%|█████████▎| 543/585 [04:02<00:12,  3.37it/s] 93%|█████████▎| 544/585 [04:03<00:12,  3.22it/s] 93%|█████████▎| 545/585 [04:03<00:12,  3.26it/s] 93%|█████████▎| 546/585 [04:03<00:11,  3.29it/s] 94%|█████████▎| 547/585 [04:04<00:11,  3.31it/s] 94%|█████████▎| 548/585 [04:04<00:11,  3.33it/s] 94%|█████████▍| 549/585 [04:04<00:10,  3.36it/s] 94%|█████████▍| 550/585 [04:05<00:10,  3.37it/s] 94%|█████████▍| 551/585 [04:05<00:10,  3.38it/s] 94%|█████████▍| 552/585 [04:05<00:09,  3.40it/s] 95%|█████████▍| 553/585 [04:05<00:09,  3.40it/s] 95%|█████████▍| 554/585 [04:06<00:09,  3.41it/s] 95%|█████████▍| 555/585 [04:06<00:09,  3.31it/s] 95%|█████████▌| 556/585 [04:06<00:08,  3.35it/s] 95%|█████████▌| 557/585 [04:07<00:08,  3.37it/s] 95%|█████████▌| 558/585 [04:07<00:07,  3.39it/s] 96%|█████████▌| 559/585 [04:07<00:07,  3.40it/s] 96%|█████████▌| 560/585 [04:08<00:07,  3.41it/s] 96%|█████████▌| 561/585 [04:08<00:07,  3.42it/s] 96%|█████████▌| 562/585 [04:08<00:06,  3.42it/s] 96%|█████████▌| 563/585 [04:08<00:06,  3.42it/s] 96%|█████████▋| 564/585 [04:09<00:06,  3.42it/s] 97%|█████████▋| 565/585 [04:09<00:05,  3.43it/s] 97%|█████████▋| 566/585 [04:09<00:05,  3.30it/s] 97%|█████████▋| 567/585 [04:10<00:05,  3.34it/s] 97%|█████████▋| 568/585 [04:10<00:05,  3.37it/s] 97%|█████████▋| 569/585 [04:10<00:04,  3.39it/s] 97%|█████████▋| 570/585 [04:11<00:04,  3.40it/s] 98%|█████████▊| 571/585 [04:11<00:04,  3.41it/s] 98%|█████████▊| 572/585 [04:11<00:03,  3.42it/s] 98%|█████████▊| 573/585 [04:11<00:03,  3.42it/s] 98%|█████████▊| 574/585 [04:12<00:03,  3.42it/s] 98%|█████████▊| 575/585 [04:12<00:02,  3.42it/s] 98%|█████████▊| 576/585 [04:12<00:02,  3.42it/s] 99%|█████████▊| 577/585 [04:13<00:02,  3.25it/s] 99%|█████████▉| 578/585 [04:13<00:02,  3.30it/s] 99%|█████████▉| 579/585 [04:13<00:01,  3.34it/s] 99%|█████████▉| 580/585 [04:13<00:01,  3.36it/s] 99%|█████████▉| 581/585 [04:14<00:01,  3.38it/s] 99%|█████████▉| 582/585 [04:14<00:00,  3.39it/s]100%|█████████▉| 583/585 [04:14<00:00,  3.40it/s]100%|█████████▉| 584/585 [04:15<00:00,  3.40it/s]100%|██████████| 585/585 [04:15<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 01:55:33,979 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:55:33,979 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 01:55:33,979 >>   Batch size = 8
{'eval_loss': 1.1650124788284302, 'eval_runtime': 10.028, 'eval_samples_per_second': 348.323, 'eval_steps_per_second': 43.578, 'epoch': 4.0}
{'loss': 0.3847, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.31it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.18it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.00it/s][A
  5%|▌         | 22/437 [00:00<00:10, 41.12it/s][A
  6%|▌         | 27/437 [00:00<00:09, 42.04it/s][A
  7%|▋         | 32/437 [00:00<00:09, 42.79it/s][A
  8%|▊         | 37/437 [00:00<00:09, 43.05it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.35it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.65it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 43.82it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.93it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.64it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.54it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.64it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.64it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.86it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.90it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.13it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.01it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.70it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.73it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.51it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.84it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.83it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.95it/s][A
 30%|███       | 132/437 [00:03<00:06, 43.96it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.02it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.84it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.87it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.68it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 42.35it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 42.84it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.14it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.32it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.48it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.66it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.61it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.59it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.64it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.63it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.77it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.91it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.87it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.85it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.89it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.75it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.72it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.60it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.77it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.87it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.87it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.96it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.97it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.95it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.80it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.79it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.58it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 40.37it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 41.45it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 42.24it/s][A
 70%|███████   | 307/437 [00:07<00:03, 42.78it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.22it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.30it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.36it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.33it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.09it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.30it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.46it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.69it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.71it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.88it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.80it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.57it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.44it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.43it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.35it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.60it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.78it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.81it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.07it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.84it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.75it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.51it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.41it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.54it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.78it/s][A
100%|██████████| 437/437 [00:10<00:00, 43.97it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:25<00:00,  3.41it/s]
100%|██████████| 437/437 [00:10<00:00, 43.97it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:55:44,088 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 01:55:44,190 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:55:46,627 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:55:46,735 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:55:46,788 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 01:55:54,319 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 01:55:54,344 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117 (score: 1.1175851821899414).
                                                 100%|██████████| 585/585 [04:44<00:00,  3.41it/s]100%|██████████| 585/585 [04:44<00:00,  2.06it/s]
[INFO|trainer.py:1894] 2023-08-29 01:56:02,573 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 01:56:02,663 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:56:05,663 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:56:05,742 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:56:05,769 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:56:06,174 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:06,175 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:06,175 >>   train_loss               =     0.3817
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:06,175 >>   train_runtime            = 0:04:44.01
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:06,175 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:06,175 >>   train_samples_per_second =    132.036
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:06,175 >>   train_steps_per_second   =       2.06
{'eval_loss': 1.1731042861938477, 'eval_runtime': 10.0409, 'eval_samples_per_second': 347.876, 'eval_steps_per_second': 43.522, 'epoch': 5.0}
{'train_runtime': 284.0125, 'train_samples_per_second': 132.036, 'train_steps_per_second': 2.06, 'train_loss': 0.38170397505800946, 'epoch': 5.0}
08/29/2023 01:56:06 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 01:56:06,349 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:56:06,350 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 01:56:06,350 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 53.92it/s]  3%|▎         | 12/437 [00:00<00:08, 48.14it/s]  4%|▍         | 17/437 [00:00<00:09, 46.53it/s]  5%|▌         | 22/437 [00:00<00:09, 45.90it/s]  6%|▌         | 27/437 [00:00<00:08, 45.58it/s]  7%|▋         | 32/437 [00:00<00:08, 45.02it/s]  8%|▊         | 37/437 [00:00<00:08, 45.09it/s] 10%|▉         | 42/437 [00:00<00:08, 44.51it/s] 11%|█         | 47/437 [00:01<00:08, 43.92it/s] 12%|█▏        | 52/437 [00:01<00:08, 43.69it/s] 13%|█▎        | 57/437 [00:01<00:08, 43.90it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.11it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.04it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.22it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.36it/s] 19%|█▉        | 82/437 [00:01<00:08, 44.14it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.05it/s] 21%|██        | 92/437 [00:02<00:07, 43.92it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.93it/s] 23%|██▎       | 102/437 [00:02<00:07, 43.95it/s] 24%|██▍       | 107/437 [00:02<00:07, 43.99it/s] 26%|██▌       | 112/437 [00:02<00:07, 43.76it/s] 27%|██▋       | 117/437 [00:02<00:07, 43.98it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.08it/s] 29%|██▉       | 127/437 [00:02<00:07, 44.05it/s] 30%|███       | 132/437 [00:02<00:06, 43.72it/s] 31%|███▏      | 137/437 [00:03<00:06, 43.79it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.73it/s] 34%|███▎      | 147/437 [00:03<00:06, 43.80it/s] 35%|███▍      | 152/437 [00:03<00:06, 43.79it/s] 36%|███▌      | 157/437 [00:03<00:06, 43.36it/s] 37%|███▋      | 162/437 [00:03<00:06, 43.64it/s] 38%|███▊      | 167/437 [00:03<00:06, 43.71it/s] 39%|███▉      | 172/437 [00:04<00:07, 33.51it/s] 40%|████      | 176/437 [00:04<00:08, 30.06it/s] 41%|████▏     | 181/437 [00:04<00:07, 33.43it/s] 43%|████▎     | 186/437 [00:04<00:06, 36.18it/s] 44%|████▎     | 191/437 [00:04<00:06, 38.40it/s] 45%|████▍     | 196/437 [00:04<00:05, 40.31it/s] 46%|████▌     | 201/437 [00:04<00:05, 41.32it/s] 47%|████▋     | 206/437 [00:04<00:05, 42.32it/s] 48%|████▊     | 211/437 [00:04<00:05, 43.07it/s] 49%|████▉     | 216/437 [00:05<00:05, 42.90it/s] 51%|█████     | 221/437 [00:05<00:05, 42.80it/s] 52%|█████▏    | 226/437 [00:05<00:04, 42.90it/s] 53%|█████▎    | 231/437 [00:05<00:04, 43.16it/s] 54%|█████▍    | 236/437 [00:05<00:04, 42.76it/s] 55%|█████▌    | 241/437 [00:05<00:04, 43.38it/s] 56%|█████▋    | 246/437 [00:05<00:04, 43.62it/s] 57%|█████▋    | 251/437 [00:05<00:04, 43.97it/s] 59%|█████▊    | 256/437 [00:05<00:04, 43.99it/s] 60%|█████▉    | 261/437 [00:06<00:04, 43.83it/s] 61%|██████    | 266/437 [00:06<00:03, 43.75it/s] 62%|██████▏   | 271/437 [00:06<00:03, 43.76it/s] 63%|██████▎   | 276/437 [00:06<00:03, 43.81it/s] 64%|██████▍   | 281/437 [00:06<00:03, 43.72it/s] 65%|██████▌   | 286/437 [00:06<00:03, 44.06it/s] 67%|██████▋   | 291/437 [00:06<00:03, 44.25it/s] 68%|██████▊   | 296/437 [00:06<00:03, 44.39it/s] 69%|██████▉   | 301/437 [00:07<00:03, 44.26it/s] 70%|███████   | 306/437 [00:07<00:02, 43.93it/s] 71%|███████   | 311/437 [00:07<00:03, 40.64it/s] 72%|███████▏  | 316/437 [00:07<00:02, 41.79it/s] 73%|███████▎  | 321/437 [00:07<00:02, 42.51it/s] 75%|███████▍  | 326/437 [00:07<00:02, 43.05it/s] 76%|███████▌  | 331/437 [00:07<00:02, 43.50it/s] 77%|███████▋  | 336/437 [00:07<00:02, 43.89it/s] 78%|███████▊  | 341/437 [00:07<00:02, 44.09it/s] 79%|███████▉  | 346/437 [00:08<00:02, 43.62it/s] 80%|████████  | 351/437 [00:08<00:01, 43.40it/s] 81%|████████▏ | 356/437 [00:08<00:01, 43.46it/s] 83%|████████▎ | 361/437 [00:08<00:01, 43.60it/s] 84%|████████▍ | 366/437 [00:08<00:01, 43.70it/s] 85%|████████▍ | 371/437 [00:08<00:01, 44.04it/s] 86%|████████▌ | 376/437 [00:08<00:01, 44.11it/s] 87%|████████▋ | 381/437 [00:08<00:01, 44.28it/s] 88%|████████▊ | 386/437 [00:08<00:01, 44.17it/s] 89%|████████▉ | 391/437 [00:09<00:01, 43.70it/s] 91%|█████████ | 396/437 [00:09<00:00, 43.48it/s] 92%|█████████▏| 401/437 [00:09<00:00, 43.58it/s] 93%|█████████▎| 406/437 [00:09<00:00, 43.86it/s] 94%|█████████▍| 411/437 [00:09<00:00, 43.86it/s] 95%|█████████▌| 416/437 [00:09<00:00, 44.02it/s] 96%|█████████▋| 421/437 [00:09<00:00, 44.15it/s] 97%|█████████▋| 426/437 [00:09<00:00, 44.19it/s] 99%|█████████▊| 431/437 [00:10<00:00, 44.14it/s]100%|█████████▉| 436/437 [00:10<00:00, 44.02it/s]100%|██████████| 437/437 [00:10<00:00, 43.07it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:56:16,535 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:16,535 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:16,535 >>   eval_loss               =     1.1176
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:16,535 >>   eval_runtime            = 0:00:10.16
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:16,535 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:16,535 >>   eval_samples_per_second =    343.626
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:16,535 >>   eval_steps_per_second   =      42.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:56:16,535 >>   perplexity              =     3.0575
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:26,351 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:26,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:26,380 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:26,380 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:26,380 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:56:27,185 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:56:27,186 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:56:27,853 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:56:29,060 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:56:29,060 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:32,321 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:32,382 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:32,382 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:32,382 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:32,382 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:56:33,265 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:56:33,266 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:56:33,910 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:56:34,185 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:56:34,186 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/generator/iter5/model/checkpoint-117
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/dev.jsonl', 'labels': ['field of work', 'manufacturer', 'nominated for', 'place served by transport hub', 'sports season of league or competition'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:06,  1.54it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:09,  1.61it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.53it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.66it/s]Extractor Predicting: 27it [00:16,  1.64it/s]Extractor Predicting: 28it [00:17,  1.68it/s]Extractor Predicting: 29it [00:18,  1.65it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:19,  1.60it/s]Extractor Predicting: 32it [00:19,  1.57it/s]Extractor Predicting: 33it [00:20,  1.54it/s]Extractor Predicting: 34it [00:21,  1.52it/s]Extractor Predicting: 35it [00:21,  1.53it/s]Extractor Predicting: 36it [00:22,  1.52it/s]Extractor Predicting: 37it [00:23,  1.50it/s]Extractor Predicting: 38it [00:24,  1.49it/s]Extractor Predicting: 39it [00:24,  1.52it/s]Extractor Predicting: 40it [00:25,  1.53it/s]Extractor Predicting: 41it [00:25,  1.53it/s]Extractor Predicting: 42it [00:26,  1.53it/s]Extractor Predicting: 43it [00:27,  1.53it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:28,  1.58it/s]Extractor Predicting: 46it [00:29,  1.54it/s]Extractor Predicting: 47it [00:30,  1.40it/s]Extractor Predicting: 48it [00:30,  1.44it/s]Extractor Predicting: 49it [00:31,  1.45it/s]Extractor Predicting: 50it [00:32,  1.47it/s]Extractor Predicting: 51it [00:32,  1.50it/s]Extractor Predicting: 52it [00:33,  1.47it/s]Extractor Predicting: 53it [00:33,  1.52it/s]Extractor Predicting: 54it [00:34,  1.51it/s]Extractor Predicting: 55it [00:35,  1.49it/s]Extractor Predicting: 56it [00:35,  1.49it/s]Extractor Predicting: 57it [00:36,  1.47it/s]Extractor Predicting: 58it [00:37,  1.47it/s]Extractor Predicting: 59it [00:38,  1.45it/s]Extractor Predicting: 60it [00:38,  1.45it/s]Extractor Predicting: 61it [00:39,  1.48it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:40,  1.51it/s]Extractor Predicting: 64it [00:41,  1.53it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:42,  1.54it/s]Extractor Predicting: 67it [00:43,  1.54it/s]Extractor Predicting: 68it [00:43,  1.53it/s]Extractor Predicting: 69it [00:44,  1.50it/s]Extractor Predicting: 70it [00:45,  1.50it/s]Extractor Predicting: 71it [00:45,  1.50it/s]Extractor Predicting: 72it [00:46,  1.52it/s]Extractor Predicting: 73it [00:47,  1.52it/s]Extractor Predicting: 74it [00:47,  1.53it/s]Extractor Predicting: 75it [00:48,  1.55it/s]Extractor Predicting: 76it [00:49,  1.54it/s]Extractor Predicting: 77it [00:49,  1.54it/s]Extractor Predicting: 78it [00:50,  1.53it/s]Extractor Predicting: 79it [00:51,  1.51it/s]Extractor Predicting: 80it [00:51,  1.51it/s]Extractor Predicting: 81it [00:52,  1.56it/s]Extractor Predicting: 82it [00:53,  1.55it/s]Extractor Predicting: 83it [00:53,  1.52it/s]Extractor Predicting: 84it [00:54,  1.51it/s]Extractor Predicting: 85it [00:55,  1.55it/s]Extractor Predicting: 86it [00:55,  1.61it/s]Extractor Predicting: 87it [00:56,  1.66it/s]Extractor Predicting: 88it [00:56,  1.67it/s]Extractor Predicting: 89it [00:57,  1.68it/s]Extractor Predicting: 90it [00:58,  1.63it/s]Extractor Predicting: 91it [00:58,  1.63it/s]Extractor Predicting: 92it [00:59,  1.61it/s]Extractor Predicting: 93it [00:59,  1.67it/s]Extractor Predicting: 94it [01:00,  1.69it/s]Extractor Predicting: 95it [01:01,  1.65it/s]Extractor Predicting: 96it [01:01,  1.67it/s]Extractor Predicting: 97it [01:02,  1.65it/s]Extractor Predicting: 98it [01:02,  1.62it/s]Extractor Predicting: 99it [01:03,  1.60it/s]Extractor Predicting: 100it [01:04,  1.60it/s]Extractor Predicting: 101it [01:04,  1.64it/s]Extractor Predicting: 102it [01:05,  1.63it/s]Extractor Predicting: 103it [01:05,  1.63it/s]Extractor Predicting: 104it [01:06,  1.63it/s]Extractor Predicting: 105it [01:07,  1.67it/s]Extractor Predicting: 106it [01:07,  1.63it/s]Extractor Predicting: 107it [01:08,  1.65it/s]Extractor Predicting: 108it [01:09,  1.62it/s]Extractor Predicting: 109it [01:09,  1.66it/s]Extractor Predicting: 110it [01:10,  1.65it/s]Extractor Predicting: 111it [01:10,  1.64it/s]Extractor Predicting: 112it [01:11,  1.62it/s]Extractor Predicting: 113it [01:12,  1.60it/s]Extractor Predicting: 114it [01:12,  1.56it/s]Extractor Predicting: 115it [01:13,  1.58it/s]Extractor Predicting: 116it [01:14,  1.57it/s]Extractor Predicting: 117it [01:14,  1.57it/s]Extractor Predicting: 118it [01:15,  1.61it/s]Extractor Predicting: 119it [01:15,  1.59it/s]Extractor Predicting: 120it [01:16,  1.61it/s]Extractor Predicting: 121it [01:17,  1.60it/s]Extractor Predicting: 122it [01:17,  1.58it/s]Extractor Predicting: 123it [01:18,  1.57it/s]Extractor Predicting: 124it [01:19,  1.54it/s]Extractor Predicting: 125it [01:19,  1.54it/s]Extractor Predicting: 126it [01:20,  1.42it/s]Extractor Predicting: 127it [01:21,  1.43it/s]Extractor Predicting: 128it [01:22,  1.43it/s]Extractor Predicting: 129it [01:22,  1.41it/s]Extractor Predicting: 130it [01:23,  1.45it/s]Extractor Predicting: 131it [01:24,  1.48it/s]Extractor Predicting: 132it [01:24,  1.53it/s]Extractor Predicting: 133it [01:25,  1.52it/s]Extractor Predicting: 134it [01:25,  1.52it/s]Extractor Predicting: 135it [01:26,  1.52it/s]Extractor Predicting: 136it [01:27,  1.52it/s]Extractor Predicting: 137it [01:27,  1.52it/s]Extractor Predicting: 138it [01:28,  1.50it/s]Extractor Predicting: 139it [01:29,  1.50it/s]Extractor Predicting: 140it [01:29,  1.54it/s]Extractor Predicting: 141it [01:30,  1.52it/s]Extractor Predicting: 142it [01:31,  1.58it/s]Extractor Predicting: 142it [01:31,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:58:20,777 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:58:20,816 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:58:20,816 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:58:20,816 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:58:20,816 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:58:21,438 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:58:21,439 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:58:22,021 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:58:23,073 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:58:23,073 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:58:26,017 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:58:26,041 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:58:26,041 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:58:26,041 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:58:26,041 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:58:26,684 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:58:26,685 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:58:27,296 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:58:27,463 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:58:27,463 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.22418879056047197,
  "recall": 0.13054680790151732,
  "score": 0.1650081418491044,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20744
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20844, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.50it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.60it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:19,  1.49it/s]Extractor Predicting: 32it [00:20,  1.50it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:21,  1.53it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.59it/s]Extractor Predicting: 38it [00:24,  1.60it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:26,  1.58it/s]Extractor Predicting: 42it [00:26,  1.57it/s]Extractor Predicting: 43it [00:27,  1.58it/s]Extractor Predicting: 44it [00:28,  1.57it/s]Extractor Predicting: 45it [00:28,  1.56it/s]Extractor Predicting: 46it [00:29,  1.55it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:30,  1.57it/s]Extractor Predicting: 49it [00:31,  1.58it/s]Extractor Predicting: 50it [00:31,  1.55it/s]Extractor Predicting: 51it [00:32,  1.54it/s]Extractor Predicting: 52it [00:33,  1.55it/s]Extractor Predicting: 53it [00:33,  1.55it/s]Extractor Predicting: 54it [00:34,  1.56it/s]Extractor Predicting: 55it [00:35,  1.53it/s]Extractor Predicting: 56it [00:35,  1.53it/s]Extractor Predicting: 57it [00:36,  1.58it/s]Extractor Predicting: 58it [00:36,  1.61it/s]Extractor Predicting: 59it [00:37,  1.61it/s]Extractor Predicting: 60it [00:38,  1.61it/s]Extractor Predicting: 61it [00:38,  1.56it/s]Extractor Predicting: 62it [00:39,  1.57it/s]Extractor Predicting: 63it [00:40,  1.58it/s]Extractor Predicting: 64it [00:40,  1.55it/s]Extractor Predicting: 65it [00:41,  1.55it/s]Extractor Predicting: 66it [00:42,  1.53it/s]Extractor Predicting: 67it [00:42,  1.56it/s]Extractor Predicting: 68it [00:43,  1.61it/s]Extractor Predicting: 69it [00:43,  1.60it/s]Extractor Predicting: 70it [00:44,  1.59it/s]Extractor Predicting: 71it [00:45,  1.56it/s]Extractor Predicting: 72it [00:45,  1.58it/s]Extractor Predicting: 73it [00:46,  1.53it/s]Extractor Predicting: 74it [00:47,  1.52it/s]Extractor Predicting: 75it [00:47,  1.53it/s]Extractor Predicting: 76it [00:48,  1.54it/s]Extractor Predicting: 77it [00:49,  1.53it/s]Extractor Predicting: 78it [00:49,  1.55it/s]Extractor Predicting: 79it [00:50,  1.52it/s]Extractor Predicting: 80it [00:51,  1.54it/s]Extractor Predicting: 81it [00:51,  1.56it/s]Extractor Predicting: 82it [00:52,  1.52it/s]Extractor Predicting: 83it [00:53,  1.53it/s]Extractor Predicting: 84it [00:53,  1.53it/s]Extractor Predicting: 85it [00:54,  1.57it/s]Extractor Predicting: 86it [00:55,  1.54it/s]Extractor Predicting: 87it [00:55,  1.55it/s]Extractor Predicting: 88it [00:56,  1.51it/s]Extractor Predicting: 89it [00:57,  1.52it/s]Extractor Predicting: 90it [00:57,  1.55it/s]Extractor Predicting: 91it [00:58,  1.53it/s]Extractor Predicting: 92it [00:59,  1.37it/s]Extractor Predicting: 93it [00:59,  1.39it/s]Extractor Predicting: 94it [01:00,  1.39it/s]Extractor Predicting: 95it [01:01,  1.43it/s]Extractor Predicting: 96it [01:01,  1.45it/s]Extractor Predicting: 97it [01:02,  1.48it/s]Extractor Predicting: 98it [01:03,  1.48it/s]Extractor Predicting: 99it [01:03,  1.48it/s]Extractor Predicting: 100it [01:04,  1.49it/s]Extractor Predicting: 101it [01:05,  1.50it/s]Extractor Predicting: 102it [01:05,  1.50it/s]Extractor Predicting: 103it [01:06,  1.52it/s]Extractor Predicting: 104it [01:07,  1.49it/s]Extractor Predicting: 105it [01:07,  1.48it/s]Extractor Predicting: 106it [01:08,  1.50it/s]Extractor Predicting: 107it [01:09,  1.49it/s]Extractor Predicting: 108it [01:09,  1.49it/s]Extractor Predicting: 109it [01:10,  1.47it/s]Extractor Predicting: 110it [01:11,  1.47it/s]Extractor Predicting: 111it [01:11,  1.49it/s]Extractor Predicting: 112it [01:12,  1.54it/s]Extractor Predicting: 113it [01:13,  1.56it/s]Extractor Predicting: 114it [01:13,  1.55it/s]Extractor Predicting: 115it [01:14,  1.56it/s]Extractor Predicting: 116it [01:15,  1.57it/s]Extractor Predicting: 117it [01:15,  1.59it/s]Extractor Predicting: 118it [01:16,  1.60it/s]Extractor Predicting: 119it [01:17,  1.54it/s]Extractor Predicting: 120it [01:17,  1.57it/s]Extractor Predicting: 121it [01:18,  1.57it/s]Extractor Predicting: 122it [01:18,  1.62it/s]Extractor Predicting: 123it [01:19,  1.61it/s]Extractor Predicting: 124it [01:20,  1.61it/s]Extractor Predicting: 125it [01:20,  1.60it/s]Extractor Predicting: 126it [01:21,  1.57it/s]Extractor Predicting: 127it [01:22,  1.57it/s]Extractor Predicting: 128it [01:22,  1.59it/s]Extractor Predicting: 129it [01:23,  1.56it/s]Extractor Predicting: 130it [01:23,  1.57it/s]Extractor Predicting: 131it [01:24,  1.54it/s]Extractor Predicting: 132it [01:25,  1.56it/s]Extractor Predicting: 133it [01:25,  1.55it/s]Extractor Predicting: 134it [01:26,  1.57it/s]Extractor Predicting: 135it [01:27,  1.59it/s]Extractor Predicting: 136it [01:27,  1.58it/s]Extractor Predicting: 137it [01:28,  1.57it/s]Extractor Predicting: 138it [01:29,  1.58it/s]Extractor Predicting: 139it [01:29,  1.62it/s]Extractor Predicting: 140it [01:30,  1.59it/s]Extractor Predicting: 141it [01:30,  1.55it/s]Extractor Predicting: 142it [01:31,  1.61it/s]Extractor Predicting: 143it [01:32,  1.60it/s]Extractor Predicting: 144it [01:32,  1.58it/s]Extractor Predicting: 145it [01:33,  1.59it/s]Extractor Predicting: 146it [01:34,  1.57it/s]Extractor Predicting: 147it [01:34,  1.55it/s]Extractor Predicting: 148it [01:35,  1.55it/s]Extractor Predicting: 149it [01:36,  1.54it/s]Extractor Predicting: 150it [01:36,  1.57it/s]Extractor Predicting: 151it [01:37,  1.57it/s]Extractor Predicting: 152it [01:37,  1.59it/s]Extractor Predicting: 153it [01:38,  1.58it/s]Extractor Predicting: 154it [01:39,  1.57it/s]Extractor Predicting: 155it [01:39,  1.55it/s]Extractor Predicting: 156it [01:40,  1.54it/s]Extractor Predicting: 157it [01:41,  1.52it/s]Extractor Predicting: 158it [01:41,  1.53it/s]Extractor Predicting: 159it [01:42,  1.53it/s]Extractor Predicting: 160it [01:43,  1.54it/s]Extractor Predicting: 161it [01:43,  1.56it/s]Extractor Predicting: 162it [01:44,  1.58it/s]Extractor Predicting: 163it [01:45,  1.58it/s]Extractor Predicting: 164it [01:45,  1.59it/s]Extractor Predicting: 165it [01:46,  1.55it/s]Extractor Predicting: 166it [01:46,  1.56it/s]Extractor Predicting: 167it [01:47,  1.59it/s]Extractor Predicting: 168it [01:48,  1.58it/s]Extractor Predicting: 169it [01:48,  1.57it/s]Extractor Predicting: 170it [01:49,  1.55it/s]Extractor Predicting: 171it [01:50,  1.51it/s]Extractor Predicting: 172it [01:50,  1.52it/s]Extractor Predicting: 173it [01:51,  1.52it/s]Extractor Predicting: 174it [01:52,  1.46it/s]Extractor Predicting: 175it [01:53,  1.43it/s]Extractor Predicting: 176it [01:53,  1.46it/s]Extractor Predicting: 177it [01:54,  1.46it/s]Extractor Predicting: 178it [01:54,  1.51it/s]Extractor Predicting: 179it [01:55,  1.48it/s]Extractor Predicting: 180it [01:56,  1.53it/s]Extractor Predicting: 181it [01:56,  1.53it/s]Extractor Predicting: 182it [01:57,  1.55it/s]Extractor Predicting: 183it [01:58,  1.55it/s]Extractor Predicting: 184it [01:58,  1.57it/s]Extractor Predicting: 185it [01:59,  1.60it/s]Extractor Predicting: 186it [02:00,  1.61it/s]Extractor Predicting: 187it [02:00,  1.62it/s]Extractor Predicting: 188it [02:01,  1.61it/s]Extractor Predicting: 189it [02:01,  1.58it/s]Extractor Predicting: 190it [02:02,  1.56it/s]Extractor Predicting: 191it [02:03,  1.52it/s]Extractor Predicting: 192it [02:03,  1.55it/s]Extractor Predicting: 193it [02:04,  1.59it/s]Extractor Predicting: 194it [02:05,  1.56it/s]Extractor Predicting: 195it [02:05,  1.58it/s]Extractor Predicting: 196it [02:06,  1.44it/s]Extractor Predicting: 197it [02:07,  1.49it/s]Extractor Predicting: 198it [02:07,  1.50it/s]Extractor Predicting: 199it [02:08,  1.50it/s]Extractor Predicting: 200it [02:09,  1.52it/s]Extractor Predicting: 201it [02:09,  1.55it/s]Extractor Predicting: 202it [02:10,  1.59it/s]Extractor Predicting: 203it [02:10,  1.61it/s]Extractor Predicting: 204it [02:11,  1.56it/s]Extractor Predicting: 205it [02:12,  1.53it/s]Extractor Predicting: 206it [02:12,  1.55it/s]Extractor Predicting: 207it [02:13,  1.58it/s]Extractor Predicting: 208it [02:14,  1.59it/s]Extractor Predicting: 209it [02:14,  1.51it/s]Extractor Predicting: 210it [02:15,  1.50it/s]Extractor Predicting: 211it [02:16,  1.51it/s]Extractor Predicting: 212it [02:16,  1.54it/s]Extractor Predicting: 213it [02:17,  1.56it/s]Extractor Predicting: 214it [02:18,  1.51it/s]Extractor Predicting: 215it [02:18,  1.53it/s]Extractor Predicting: 216it [02:19,  1.56it/s]Extractor Predicting: 217it [02:20,  1.57it/s]Extractor Predicting: 218it [02:20,  1.51it/s]Extractor Predicting: 219it [02:21,  1.52it/s]Extractor Predicting: 220it [02:22,  1.53it/s]Extractor Predicting: 221it [02:22,  1.49it/s]Extractor Predicting: 222it [02:23,  1.49it/s]Extractor Predicting: 223it [02:24,  1.50it/s]Extractor Predicting: 224it [02:24,  1.54it/s]Extractor Predicting: 225it [02:25,  1.53it/s]Extractor Predicting: 226it [02:26,  1.56it/s]Extractor Predicting: 227it [02:26,  1.59it/s]Extractor Predicting: 228it [02:27,  1.57it/s]Extractor Predicting: 229it [02:27,  1.57it/s]Extractor Predicting: 230it [02:28,  1.55it/s]Extractor Predicting: 231it [02:29,  1.52it/s]Extractor Predicting: 232it [02:29,  1.53it/s]Extractor Predicting: 233it [02:30,  1.58it/s]Extractor Predicting: 234it [02:31,  1.56it/s]Extractor Predicting: 235it [02:31,  1.57it/s]Extractor Predicting: 236it [02:32,  1.52it/s]Extractor Predicting: 237it [02:33,  1.53it/s]Extractor Predicting: 238it [02:33,  1.56it/s]Extractor Predicting: 239it [02:34,  1.57it/s]Extractor Predicting: 240it [02:35,  1.56it/s]Extractor Predicting: 241it [02:35,  1.54it/s]Extractor Predicting: 242it [02:36,  1.53it/s]Extractor Predicting: 243it [02:37,  1.50it/s]Extractor Predicting: 244it [02:37,  1.52it/s]Extractor Predicting: 245it [02:38,  1.57it/s]Extractor Predicting: 246it [02:38,  1.54it/s]Extractor Predicting: 247it [02:39,  1.56it/s]Extractor Predicting: 248it [02:40,  1.57it/s]Extractor Predicting: 249it [02:40,  1.57it/s]Extractor Predicting: 250it [02:41,  1.55it/s]Extractor Predicting: 251it [02:42,  1.51it/s]Extractor Predicting: 252it [02:42,  1.52it/s]Extractor Predicting: 253it [02:43,  1.53it/s]Extractor Predicting: 254it [02:44,  1.55it/s]Extractor Predicting: 255it [02:44,  1.55it/s]Extractor Predicting: 256it [02:45,  1.52it/s]Extractor Predicting: 257it [02:46,  1.54it/s]Extractor Predicting: 258it [02:46,  1.54it/s]Extractor Predicting: 259it [02:47,  1.51it/s]Extractor Predicting: 260it [02:48,  1.53it/s]Extractor Predicting: 261it [02:48,  1.54it/s]Extractor Predicting: 262it [02:49,  1.53it/s]Extractor Predicting: 263it [02:50,  1.52it/s]Extractor Predicting: 264it [02:50,  1.52it/s]Extractor Predicting: 265it [02:51,  1.51it/s]Extractor Predicting: 266it [02:52,  1.49it/s]Extractor Predicting: 267it [02:52,  1.47it/s]Extractor Predicting: 268it [02:53,  1.48it/s]Extractor Predicting: 269it [02:54,  1.49it/s]Extractor Predicting: 270it [02:54,  1.47it/s]Extractor Predicting: 271it [02:55,  1.48it/s]Extractor Predicting: 272it [02:56,  1.50it/s]Extractor Predicting: 273it [02:56,  1.47it/s]Extractor Predicting: 274it [02:57,  1.48it/s]Extractor Predicting: 275it [02:58,  1.52it/s]Extractor Predicting: 276it [02:58,  1.52it/s]Extractor Predicting: 277it [02:59,  1.51it/s]Extractor Predicting: 278it [03:00,  1.49it/s]Extractor Predicting: 279it [03:00,  1.50it/s]Extractor Predicting: 280it [03:01,  1.49it/s]Extractor Predicting: 281it [03:02,  1.49it/s]Extractor Predicting: 282it [03:02,  1.52it/s]Extractor Predicting: 283it [03:03,  1.30it/s]Extractor Predicting: 284it [03:04,  1.33it/s]Extractor Predicting: 285it [03:05,  1.37it/s]Extractor Predicting: 286it [03:05,  1.38it/s]Extractor Predicting: 287it [03:06,  1.39it/s]Extractor Predicting: 288it [03:06,  1.84it/s]Extractor Predicting: 288it [03:06,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:01:47,085 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:01:47,141 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:01:47,141 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:01:47,141 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:01:47,141 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:01:47,808 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:01:47,809 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:01:48,151 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:01:49,339 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:01:49,340 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:01:52,496 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:01:52,498 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:01:52,498 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:01:52,498 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:01:52,498 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:01:53,306 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:01:53,308 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:01:53,926 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:01:54,154 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:01:54,154 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.36140069500133654,
  "recall": 0.19625489911453042,
  "score": 0.2543744120413923,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 704
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 804, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.37it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:01,  1.84it/s]Extractor Predicting: 3it [00:01,  1.69it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3548387096774194,
  "recall": 0.0990990990990991,
  "score": 0.15492957746478872,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/', 'labels': ['architect', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'located in or next to body of water', 'member of political party', 'operator', 'original broadcaster', 'position held'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
