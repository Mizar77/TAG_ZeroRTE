/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_5_seed_1', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 4986 mean pseudo reward: 0.9247330587143847
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl'}
train vocab size: 23560
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23660, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_5_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23660, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.587, loss:3124.1525
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.333, loss:2101.0592
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.270, loss:1799.6378
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.278, loss:1693.7213
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.273, loss:1597.7593
>> valid entity prec:0.5119, rec:0.5876, f1:0.5471
>> valid relation prec:0.5066, rec:0.0984, f1:0.1648
>> valid relation with NER prec:0.5066, rec:0.0984, f1:0.1648
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 3.110, loss:1487.8193
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.279, loss:1335.2723
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.287, loss:1309.0578
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.283, loss:1208.3657
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.275, loss:1172.6367
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5813, rec:0.4107, f1:0.4813
>> valid relation prec:0.3248, rec:0.0511, f1:0.0882
>> valid relation with NER prec:0.3248, rec:0.0511, f1:0.0882
g_step 1100, step 60, avg_time 3.040, loss:1131.1159
g_step 1200, step 160, avg_time 1.276, loss:1077.5594
g_step 1300, step 52, avg_time 1.290, loss:1011.2713
g_step 1400, step 152, avg_time 1.280, loss:998.0419
g_step 1500, step 44, avg_time 1.286, loss:993.9898
>> valid entity prec:0.5424, rec:0.5173, f1:0.5296
>> valid relation prec:0.2404, rec:0.0861, f1:0.1267
>> valid relation with NER prec:0.2404, rec:0.0861, f1:0.1267
g_step 1600, step 144, avg_time 3.041, loss:917.5785
g_step 1700, step 36, avg_time 1.287, loss:910.7453
g_step 1800, step 136, avg_time 1.293, loss:870.1208
g_step 1900, step 28, avg_time 1.271, loss:898.5880
g_step 2000, step 128, avg_time 1.287, loss:850.7323
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5531, rec:0.4348, f1:0.4869
>> valid relation prec:0.1957, rec:0.0542, f1:0.0849
>> valid relation with NER prec:0.1957, rec:0.0542, f1:0.0849
g_step 2100, step 20, avg_time 3.039, loss:826.2222
g_step 2200, step 120, avg_time 1.281, loss:773.7155
g_step 2300, step 12, avg_time 1.278, loss:779.7816
g_step 2400, step 112, avg_time 1.275, loss:724.3045
g_step 2500, step 4, avg_time 1.287, loss:768.7777
>> valid entity prec:0.5267, rec:0.5346, f1:0.5307
>> valid relation prec:0.2892, rec:0.1308, f1:0.1801
>> valid relation with NER prec:0.2892, rec:0.1308, f1:0.1801
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 104, avg_time 3.038, loss:697.1589
g_step 2700, step 204, avg_time 1.290, loss:733.8496
g_step 2800, step 96, avg_time 1.276, loss:672.5207
g_step 2900, step 196, avg_time 1.287, loss:694.4918
g_step 3000, step 88, avg_time 1.283, loss:634.1962
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5192, rec:0.4970, f1:0.5079
>> valid relation prec:0.1427, rec:0.0631, f1:0.0875
>> valid relation with NER prec:0.1427, rec:0.0631, f1:0.0875
g_step 3100, step 188, avg_time 3.037, loss:681.8069
g_step 3200, step 80, avg_time 1.279, loss:600.6542
g_step 3300, step 180, avg_time 1.277, loss:658.6097
g_step 3400, step 72, avg_time 1.283, loss:601.0017
g_step 3500, step 172, avg_time 1.276, loss:581.6386
>> valid entity prec:0.5210, rec:0.4788, f1:0.4990
>> valid relation prec:0.2268, rec:0.1024, f1:0.1411
>> valid relation with NER prec:0.2268, rec:0.1024, f1:0.1411
g_step 3600, step 64, avg_time 3.039, loss:569.1139
g_step 3700, step 164, avg_time 1.283, loss:580.3399
g_step 3800, step 56, avg_time 1.277, loss:557.2227
g_step 3900, step 156, avg_time 1.285, loss:550.3352
g_step 4000, step 48, avg_time 1.290, loss:513.6511
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5211, rec:0.4838, f1:0.5018
>> valid relation prec:0.2090, rec:0.1104, f1:0.1445
>> valid relation with NER prec:0.2090, rec:0.1104, f1:0.1445
g_step 4100, step 148, avg_time 3.029, loss:524.7758
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 22:43:44 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 22:43:44 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_22-43-44_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 22:43:45 - WARNING - datasets.builder -   Using custom data configuration default-86769d20cca90def
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-86769d20cca90def/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 22:43:45,474 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:43:45,475 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 22:43:45,476 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:43:45,478 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 22:43:45,488 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:43:45,492 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:43:45,492 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:43:45,492 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:43:45,493 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:43:45,493 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:43:45,493 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 22:43:45,648 >> loading weights file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 22:43:50,902 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 22:43:50,902 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_5_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-86769d20cca90def/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 22:43:50 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14c4a3600710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.99ba/s] 40%|████      | 2/5 [00:00<00:00,  3.79ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.09ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.57ba/s]100%|██████████| 5/5 [00:01<00:00,  3.86ba/s]100%|██████████| 5/5 [00:01<00:00,  3.75ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.08ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.33ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.42ba/s]100%|██████████| 4/4 [00:00<00:00,  5.55ba/s]100%|██████████| 4/4 [00:00<00:00,  5.02ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  7.41ba/s] 40%|████      | 2/5 [00:00<00:00,  8.30ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.59ba/s] 80%|████████  | 4/5 [00:00<00:00,  8.77ba/s]100%|██████████| 5/5 [00:00<00:00,  8.89ba/s]100%|██████████| 5/5 [00:00<00:00,  8.66ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.49ba/s] 50%|█████     | 2/4 [00:00<00:00,  8.49ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.82ba/s]100%|██████████| 4/4 [00:00<00:00,  9.97ba/s]
[INFO|trainer.py:414] 2023-08-27 22:43:54,382 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 22:43:54,394 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 22:43:54,394 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-27 22:43:54,394 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 22:43:54,394 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 22:43:54,394 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 22:43:54,394 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 22:43:54,394 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:58,  3.29it/s]  1%|          | 2/390 [00:00<01:55,  3.36it/s]  1%|          | 3/390 [00:00<01:54,  3.39it/s]  1%|          | 4/390 [00:01<01:53,  3.40it/s]  1%|▏         | 5/390 [00:01<01:53,  3.40it/s]  2%|▏         | 6/390 [00:01<01:52,  3.41it/s]  2%|▏         | 7/390 [00:02<01:52,  3.42it/s]  2%|▏         | 8/390 [00:02<01:51,  3.42it/s]  2%|▏         | 9/390 [00:02<01:51,  3.42it/s]  3%|▎         | 10/390 [00:02<01:51,  3.42it/s]  3%|▎         | 11/390 [00:03<01:51,  3.41it/s]  3%|▎         | 12/390 [00:03<01:50,  3.42it/s]  3%|▎         | 13/390 [00:03<01:50,  3.42it/s]  4%|▎         | 14/390 [00:04<01:49,  3.42it/s]  4%|▍         | 15/390 [00:04<01:49,  3.42it/s]  4%|▍         | 16/390 [00:04<01:49,  3.42it/s]  4%|▍         | 17/390 [00:04<01:48,  3.43it/s]  5%|▍         | 18/390 [00:05<01:48,  3.42it/s]  5%|▍         | 19/390 [00:05<01:48,  3.42it/s]  5%|▌         | 20/390 [00:05<01:48,  3.42it/s]  5%|▌         | 21/390 [00:06<01:47,  3.42it/s]  6%|▌         | 22/390 [00:06<01:47,  3.42it/s]  6%|▌         | 23/390 [00:06<01:47,  3.42it/s]  6%|▌         | 24/390 [00:07<01:46,  3.42it/s]  6%|▋         | 25/390 [00:07<01:46,  3.42it/s]  7%|▋         | 26/390 [00:07<01:46,  3.43it/s]  7%|▋         | 27/390 [00:07<01:45,  3.43it/s]  7%|▋         | 28/390 [00:08<01:45,  3.42it/s]  7%|▋         | 29/390 [00:08<01:45,  3.42it/s]  8%|▊         | 30/390 [00:08<01:45,  3.42it/s]  8%|▊         | 31/390 [00:09<01:44,  3.42it/s]  8%|▊         | 32/390 [00:09<01:44,  3.42it/s]  8%|▊         | 33/390 [00:09<01:44,  3.42it/s]  9%|▊         | 34/390 [00:09<01:44,  3.42it/s]  9%|▉         | 35/390 [00:10<01:43,  3.42it/s]  9%|▉         | 36/390 [00:10<01:43,  3.42it/s]  9%|▉         | 37/390 [00:10<01:43,  3.42it/s] 10%|▉         | 38/390 [00:11<01:42,  3.42it/s] 10%|█         | 39/390 [00:11<01:42,  3.42it/s] 10%|█         | 40/390 [00:11<01:42,  3.42it/s] 11%|█         | 41/390 [00:11<01:41,  3.42it/s] 11%|█         | 42/390 [00:12<01:41,  3.42it/s] 11%|█         | 43/390 [00:12<01:41,  3.42it/s] 11%|█▏        | 44/390 [00:12<01:41,  3.42it/s] 12%|█▏        | 45/390 [00:13<01:40,  3.42it/s] 12%|█▏        | 46/390 [00:13<01:41,  3.40it/s] 12%|█▏        | 47/390 [00:13<01:40,  3.41it/s] 12%|█▏        | 48/390 [00:14<01:40,  3.41it/s] 13%|█▎        | 49/390 [00:14<01:40,  3.41it/s] 13%|█▎        | 50/390 [00:14<01:40,  3.38it/s] 13%|█▎        | 51/390 [00:14<01:39,  3.39it/s] 13%|█▎        | 52/390 [00:15<01:39,  3.40it/s] 14%|█▎        | 53/390 [00:15<01:38,  3.41it/s] 14%|█▍        | 54/390 [00:15<01:38,  3.41it/s] 14%|█▍        | 55/390 [00:16<01:38,  3.41it/s] 14%|█▍        | 56/390 [00:16<01:37,  3.42it/s] 15%|█▍        | 57/390 [00:16<01:37,  3.41it/s] 15%|█▍        | 58/390 [00:16<01:37,  3.42it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.42it/s] 15%|█▌        | 60/390 [00:17<01:36,  3.41it/s] 16%|█▌        | 61/390 [00:17<01:36,  3.41it/s] 16%|█▌        | 62/390 [00:18<01:36,  3.41it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.41it/s] 16%|█▋        | 64/390 [00:18<01:35,  3.41it/s] 17%|█▋        | 65/390 [00:19<01:35,  3.41it/s] 17%|█▋        | 66/390 [00:19<01:35,  3.41it/s] 17%|█▋        | 67/390 [00:19<01:34,  3.41it/s] 17%|█▋        | 68/390 [00:19<01:34,  3.41it/s] 18%|█▊        | 69/390 [00:20<01:34,  3.41it/s] 18%|█▊        | 70/390 [00:20<01:33,  3.41it/s] 18%|█▊        | 71/390 [00:20<01:33,  3.41it/s] 18%|█▊        | 72/390 [00:21<01:33,  3.41it/s] 19%|█▊        | 73/390 [00:21<01:32,  3.41it/s] 19%|█▉        | 74/390 [00:21<01:32,  3.41it/s] 19%|█▉        | 75/390 [00:21<01:32,  3.41it/s] 19%|█▉        | 76/390 [00:22<01:32,  3.41it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.41it/s] 20%|██        | 78/390 [00:22<01:31,  3.41it/s][INFO|trainer.py:2140] 2023-08-27 22:44:17,299 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:44:17,299 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-27 22:44:17,299 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.45it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.05it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.29it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.58it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.16it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.80it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.60it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.19it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.22it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.26it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.21it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.24it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.34it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.28it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.34it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.24it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.02it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.10it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.08it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.13it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.23it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.14it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.24it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.23it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.08it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.16it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.17it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.06it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.09it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.17it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.16it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.15it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.23it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.15it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.14it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.16it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.11it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.12it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.13it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.12it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.18it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.21it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.11it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.16it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.11it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.10it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.10it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.14it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.18it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.07it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.20it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.08it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.05it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.04it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.03it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.13it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.98it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.14it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.16it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.11it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.10it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.05it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.10it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.06it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.13it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.17it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.13it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.19it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.14it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.09it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.08it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.08it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.03it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.07it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.19it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.14it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.22it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.14it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.16it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.73it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.95it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.00it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.94it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.07it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.17it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.20it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:32<01:31,  3.41it/s]
100%|██████████| 436/436 [00:09<00:00, 46.20it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:44:26,767 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-27 22:44:26,788 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:44:29,105 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:44:29,121 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:44:29,126 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:39<26:51,  5.18s/it] 21%|██        | 80/390 [00:39<19:11,  3.72s/it] 21%|██        | 81/390 [00:40<13:50,  2.69s/it] 21%|██        | 82/390 [00:40<10:06,  1.97s/it] 21%|██▏       | 83/390 [00:40<07:30,  1.47s/it] 22%|██▏       | 84/390 [00:40<05:40,  1.11s/it] 22%|██▏       | 85/390 [00:41<04:24,  1.15it/s] 22%|██▏       | 86/390 [00:41<03:31,  1.44it/s] 22%|██▏       | 87/390 [00:41<02:54,  1.74it/s] 23%|██▎       | 88/390 [00:42<02:28,  2.04it/s] 23%|██▎       | 89/390 [00:42<02:09,  2.32it/s] 23%|██▎       | 90/390 [00:42<01:57,  2.56it/s] 23%|██▎       | 91/390 [00:42<01:48,  2.75it/s] 24%|██▎       | 92/390 [00:43<01:41,  2.92it/s] 24%|██▍       | 93/390 [00:43<01:37,  3.05it/s] 24%|██▍       | 94/390 [00:43<01:33,  3.15it/s] 24%|██▍       | 95/390 [00:44<01:31,  3.22it/s] 25%|██▍       | 96/390 [00:44<01:29,  3.28it/s] 25%|██▍       | 97/390 [00:44<01:28,  3.32it/s] 25%|██▌       | 98/390 [00:45<01:27,  3.34it/s] 25%|██▌       | 99/390 [00:45<01:26,  3.36it/s] 26%|██▌       | 100/390 [00:45<01:25,  3.38it/s] 26%|██▌       | 101/390 [00:45<01:25,  3.38it/s] 26%|██▌       | 102/390 [00:46<01:25,  3.38it/s] 26%|██▋       | 103/390 [00:46<01:24,  3.39it/s] 27%|██▋       | 104/390 [00:46<01:24,  3.40it/s] 27%|██▋       | 105/390 [00:47<01:23,  3.40it/s] 27%|██▋       | 106/390 [00:47<01:23,  3.40it/s] 27%|██▋       | 107/390 [00:47<01:23,  3.40it/s] 28%|██▊       | 108/390 [00:47<01:22,  3.40it/s] 28%|██▊       | 109/390 [00:48<01:22,  3.40it/s] 28%|██▊       | 110/390 [00:48<01:22,  3.40it/s] 28%|██▊       | 111/390 [00:48<01:21,  3.41it/s] 29%|██▊       | 112/390 [00:49<01:21,  3.41it/s] 29%|██▉       | 113/390 [00:49<01:21,  3.40it/s] 29%|██▉       | 114/390 [00:49<01:21,  3.40it/s] 29%|██▉       | 115/390 [00:50<01:21,  3.36it/s] 30%|██▉       | 116/390 [00:50<01:21,  3.38it/s] 30%|███       | 117/390 [00:50<01:20,  3.39it/s] 30%|███       | 118/390 [00:50<01:20,  3.39it/s] 31%|███       | 119/390 [00:51<01:19,  3.40it/s] 31%|███       | 120/390 [00:51<01:19,  3.40it/s] 31%|███       | 121/390 [00:51<01:19,  3.40it/s] 31%|███▏      | 122/390 [00:52<01:18,  3.40it/s] 32%|███▏      | 123/390 [00:52<01:18,  3.40it/s] 32%|███▏      | 124/390 [00:52<01:18,  3.39it/s] 32%|███▏      | 125/390 [00:52<01:18,  3.40it/s] 32%|███▏      | 126/390 [00:53<01:17,  3.40it/s] 33%|███▎      | 127/390 [00:53<01:17,  3.40it/s] 33%|███▎      | 128/390 [00:53<01:17,  3.40it/s] 33%|███▎      | 129/390 [00:54<01:16,  3.40it/s] 33%|███▎      | 130/390 [00:54<01:16,  3.40it/s] 34%|███▎      | 131/390 [00:54<01:16,  3.40it/s] 34%|███▍      | 132/390 [00:55<01:15,  3.40it/s] 34%|███▍      | 133/390 [00:55<01:15,  3.40it/s] 34%|███▍      | 134/390 [00:55<01:15,  3.40it/s] 35%|███▍      | 135/390 [00:55<01:15,  3.39it/s] 35%|███▍      | 136/390 [00:56<01:14,  3.40it/s] 35%|███▌      | 137/390 [00:56<01:14,  3.40it/s] 35%|███▌      | 138/390 [00:56<01:14,  3.40it/s] 36%|███▌      | 139/390 [00:57<01:13,  3.40it/s] 36%|███▌      | 140/390 [00:57<01:13,  3.40it/s] 36%|███▌      | 141/390 [00:57<01:13,  3.40it/s] 36%|███▋      | 142/390 [00:57<01:12,  3.40it/s] 37%|███▋      | 143/390 [00:58<01:12,  3.41it/s] 37%|███▋      | 144/390 [00:58<01:12,  3.40it/s] 37%|███▋      | 145/390 [00:58<01:12,  3.40it/s] 37%|███▋      | 146/390 [00:59<01:11,  3.40it/s] 38%|███▊      | 147/390 [00:59<01:11,  3.40it/s] 38%|███▊      | 148/390 [00:59<01:11,  3.40it/s] 38%|███▊      | 149/390 [01:00<01:10,  3.40it/s] 38%|███▊      | 150/390 [01:00<01:10,  3.40it/s] 39%|███▊      | 151/390 [01:00<01:10,  3.39it/s] 39%|███▉      | 152/390 [01:00<01:10,  3.39it/s] 39%|███▉      | 153/390 [01:01<01:09,  3.39it/s] 39%|███▉      | 154/390 [01:01<01:09,  3.40it/s] 40%|███▉      | 155/390 [01:01<01:09,  3.40it/s] 40%|████      | 156/390 [01:02<01:08,  3.40it/s][INFO|trainer.py:2140] 2023-08-27 22:44:56,525 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:44:56,526 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-27 22:44:56,526 >>   Batch size = 8
{'eval_loss': 0.9702504277229309, 'eval_runtime': 9.4495, 'eval_samples_per_second': 368.91, 'eval_steps_per_second': 46.14, 'epoch': 0.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.86it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.79it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.13it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.50it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.00it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.71it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.48it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.05it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.97it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.95it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.00it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.09it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.12it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.14it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.17it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.05it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.89it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.89it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.92it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.98it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.96it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.97it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.03it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.09it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.99it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.95it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.86it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.92it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.01it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.95it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.04it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.01it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.07it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.98it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.04it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.03it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.82it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.90it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.97it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.11it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.04it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.06it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.00it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.01it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.94it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.86it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.95it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.06it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.01it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.98it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.86it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.89it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.89it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.86it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.89it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.95it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.95it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.04it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.05it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.90it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.89it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.82it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.92it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.88it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.90it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.88it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.00it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.98it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.04it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.04it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.98it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.91it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.91it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.03it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.94it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.00it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.00it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.99it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.04it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.94it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.97it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.87it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.96it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.95it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.97it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.97it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:11<01:08,  3.40it/s]
100%|██████████| 436/436 [00:09<00:00, 45.97it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:45:06,033 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-27 22:45:06,049 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:45:08,227 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:45:08,250 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:45:08,258 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:19<20:54,  5.38s/it] 41%|████      | 158/390 [01:19<14:54,  3.86s/it] 41%|████      | 159/390 [01:19<10:43,  2.79s/it] 41%|████      | 160/390 [01:20<07:48,  2.04s/it] 41%|████▏     | 161/390 [01:20<05:46,  1.52s/it] 42%|████▏     | 162/390 [01:20<04:21,  1.15s/it] 42%|████▏     | 163/390 [01:21<03:22,  1.12it/s] 42%|████▏     | 164/390 [01:21<02:41,  1.40it/s] 42%|████▏     | 165/390 [01:21<02:12,  1.70it/s] 43%|████▎     | 166/390 [01:21<01:51,  2.00it/s] 43%|████▎     | 167/390 [01:22<01:37,  2.29it/s] 43%|████▎     | 168/390 [01:22<01:27,  2.54it/s] 43%|████▎     | 169/390 [01:22<01:20,  2.74it/s] 44%|████▎     | 170/390 [01:23<01:15,  2.91it/s] 44%|████▍     | 171/390 [01:23<01:11,  3.05it/s] 44%|████▍     | 172/390 [01:23<01:09,  3.15it/s] 44%|████▍     | 173/390 [01:24<01:07,  3.22it/s] 45%|████▍     | 174/390 [01:24<01:05,  3.27it/s] 45%|████▍     | 175/390 [01:24<01:04,  3.32it/s] 45%|████▌     | 176/390 [01:24<01:04,  3.34it/s] 45%|████▌     | 177/390 [01:25<01:03,  3.36it/s] 46%|████▌     | 178/390 [01:25<01:02,  3.38it/s] 46%|████▌     | 179/390 [01:25<01:02,  3.39it/s] 46%|████▌     | 180/390 [01:26<01:02,  3.38it/s] 46%|████▋     | 181/390 [01:26<01:01,  3.39it/s] 47%|████▋     | 182/390 [01:26<01:01,  3.40it/s] 47%|████▋     | 183/390 [01:26<01:00,  3.40it/s] 47%|████▋     | 184/390 [01:27<01:00,  3.40it/s] 47%|████▋     | 185/390 [01:27<01:00,  3.40it/s] 48%|████▊     | 186/390 [01:27<00:59,  3.40it/s] 48%|████▊     | 187/390 [01:28<00:59,  3.40it/s] 48%|████▊     | 188/390 [01:28<00:59,  3.40it/s] 48%|████▊     | 189/390 [01:28<00:59,  3.40it/s] 49%|████▊     | 190/390 [01:29<00:58,  3.40it/s] 49%|████▉     | 191/390 [01:29<00:58,  3.40it/s] 49%|████▉     | 192/390 [01:29<00:58,  3.40it/s] 49%|████▉     | 193/390 [01:29<00:57,  3.40it/s] 50%|████▉     | 194/390 [01:30<00:57,  3.40it/s] 50%|█████     | 195/390 [01:30<00:57,  3.40it/s] 50%|█████     | 196/390 [01:30<00:56,  3.41it/s] 51%|█████     | 197/390 [01:31<00:56,  3.41it/s] 51%|█████     | 198/390 [01:31<00:56,  3.41it/s] 51%|█████     | 199/390 [01:31<00:56,  3.40it/s] 51%|█████▏    | 200/390 [01:31<00:55,  3.40it/s] 52%|█████▏    | 201/390 [01:32<00:55,  3.41it/s] 52%|█████▏    | 202/390 [01:32<00:55,  3.41it/s] 52%|█████▏    | 203/390 [01:32<00:54,  3.41it/s] 52%|█████▏    | 204/390 [01:33<00:54,  3.41it/s] 53%|█████▎    | 205/390 [01:33<00:54,  3.41it/s] 53%|█████▎    | 206/390 [01:33<00:54,  3.40it/s] 53%|█████▎    | 207/390 [01:34<00:53,  3.40it/s] 53%|█████▎    | 208/390 [01:34<00:53,  3.40it/s] 54%|█████▎    | 209/390 [01:34<00:53,  3.40it/s] 54%|█████▍    | 210/390 [01:34<00:52,  3.40it/s] 54%|█████▍    | 211/390 [01:35<00:52,  3.40it/s] 54%|█████▍    | 212/390 [01:35<00:52,  3.39it/s] 55%|█████▍    | 213/390 [01:35<00:52,  3.40it/s] 55%|█████▍    | 214/390 [01:36<00:51,  3.40it/s] 55%|█████▌    | 215/390 [01:36<00:51,  3.40it/s] 55%|█████▌    | 216/390 [01:36<00:51,  3.40it/s] 56%|█████▌    | 217/390 [01:36<00:50,  3.40it/s] 56%|█████▌    | 218/390 [01:37<00:50,  3.41it/s] 56%|█████▌    | 219/390 [01:37<00:50,  3.40it/s] 56%|█████▋    | 220/390 [01:37<00:49,  3.40it/s] 57%|█████▋    | 221/390 [01:38<00:49,  3.40it/s] 57%|█████▋    | 222/390 [01:38<00:49,  3.40it/s] 57%|█████▋    | 223/390 [01:38<00:49,  3.39it/s] 57%|█████▋    | 224/390 [01:39<00:48,  3.40it/s] 58%|█████▊    | 225/390 [01:39<00:48,  3.40it/s] 58%|█████▊    | 226/390 [01:39<00:48,  3.40it/s] 58%|█████▊    | 227/390 [01:39<00:47,  3.40it/s] 58%|█████▊    | 228/390 [01:40<00:47,  3.40it/s] 59%|█████▊    | 229/390 [01:40<00:47,  3.40it/s] 59%|█████▉    | 230/390 [01:40<00:47,  3.40it/s] 59%|█████▉    | 231/390 [01:41<00:46,  3.40it/s] 59%|█████▉    | 232/390 [01:41<00:46,  3.40it/s] 60%|█████▉    | 233/390 [01:41<00:46,  3.40it/s] 60%|██████    | 234/390 [01:41<00:46,  3.39it/s][INFO|trainer.py:2140] 2023-08-27 22:45:36,408 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:45:36,408 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-27 22:45:36,408 >>   Batch size = 8
{'eval_loss': 0.9639974236488342, 'eval_runtime': 9.4819, 'eval_samples_per_second': 367.646, 'eval_steps_per_second': 45.982, 'epoch': 1.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.33it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.90it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.14it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.48it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.00it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.63it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.40it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.95it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.89it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.95it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.01it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.13it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.16it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.13it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.12it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.00it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.85it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.84it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.86it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.03it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.03it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.08it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.13it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.03it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.97it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.84it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.89it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.78it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.99it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.02it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.98it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.06it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.93it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.89it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.70it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.82it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.88it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.96it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.97it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.06it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.00it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.97it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.89it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.84it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.85it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.87it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.01it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.98it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.96it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.93it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.91it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.91it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.83it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.88it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.89it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.97it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.93it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.02it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.03it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.90it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.95it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.85it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.89it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.91it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.05it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.97it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.99it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.02it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.89it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.94it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.88it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.84it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 44.65it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.41it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.62it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.84it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.43it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.78it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.82it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.88it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.87it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.82it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.79it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.91it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.02it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.95it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [01:51<00:46,  3.39it/s]
100%|██████████| 436/436 [00:09<00:00, 45.95it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:45:45,921 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-27 22:45:45,941 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:45:48,118 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:45:48,133 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:45:48,142 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [01:59<13:56,  5.40s/it] 61%|██████    | 236/390 [01:59<09:55,  3.87s/it] 61%|██████    | 237/390 [01:59<07:07,  2.79s/it] 61%|██████    | 238/390 [02:00<05:10,  2.04s/it] 61%|██████▏   | 239/390 [02:00<03:49,  1.52s/it] 62%|██████▏   | 240/390 [02:00<02:52,  1.15s/it] 62%|██████▏   | 241/390 [02:01<02:13,  1.12it/s] 62%|██████▏   | 242/390 [02:01<01:45,  1.40it/s] 62%|██████▏   | 243/390 [02:01<01:26,  1.70it/s] 63%|██████▎   | 244/390 [02:01<01:12,  2.00it/s] 63%|██████▎   | 245/390 [02:02<01:03,  2.28it/s] 63%|██████▎   | 246/390 [02:02<00:56,  2.54it/s] 63%|██████▎   | 247/390 [02:02<00:52,  2.74it/s] 64%|██████▎   | 248/390 [02:03<00:48,  2.91it/s] 64%|██████▍   | 249/390 [02:03<00:46,  3.05it/s] 64%|██████▍   | 250/390 [02:03<00:44,  3.15it/s] 64%|██████▍   | 251/390 [02:03<00:43,  3.22it/s] 65%|██████▍   | 252/390 [02:04<00:42,  3.27it/s] 65%|██████▍   | 253/390 [02:04<00:41,  3.31it/s] 65%|██████▌   | 254/390 [02:04<00:40,  3.34it/s] 65%|██████▌   | 255/390 [02:05<00:40,  3.36it/s] 66%|██████▌   | 256/390 [02:05<00:39,  3.37it/s] 66%|██████▌   | 257/390 [02:05<00:39,  3.38it/s] 66%|██████▌   | 258/390 [02:06<00:38,  3.39it/s] 66%|██████▋   | 259/390 [02:06<00:38,  3.39it/s] 67%|██████▋   | 260/390 [02:06<00:38,  3.39it/s] 67%|██████▋   | 261/390 [02:06<00:37,  3.39it/s] 67%|██████▋   | 262/390 [02:07<00:37,  3.40it/s] 67%|██████▋   | 263/390 [02:07<00:37,  3.40it/s] 68%|██████▊   | 264/390 [02:07<00:37,  3.40it/s] 68%|██████▊   | 265/390 [02:08<00:36,  3.41it/s] 68%|██████▊   | 266/390 [02:08<00:36,  3.41it/s] 68%|██████▊   | 267/390 [02:08<00:36,  3.41it/s] 69%|██████▊   | 268/390 [02:08<00:35,  3.41it/s] 69%|██████▉   | 269/390 [02:09<00:35,  3.41it/s] 69%|██████▉   | 270/390 [02:09<00:35,  3.40it/s] 69%|██████▉   | 271/390 [02:09<00:34,  3.40it/s] 70%|██████▉   | 272/390 [02:10<00:34,  3.41it/s] 70%|███████   | 273/390 [02:10<00:34,  3.40it/s] 70%|███████   | 274/390 [02:10<00:34,  3.41it/s] 71%|███████   | 275/390 [02:11<00:33,  3.41it/s] 71%|███████   | 276/390 [02:11<00:33,  3.40it/s] 71%|███████   | 277/390 [02:11<00:33,  3.40it/s] 71%|███████▏  | 278/390 [02:11<00:32,  3.41it/s] 72%|███████▏  | 279/390 [02:12<00:32,  3.41it/s] 72%|███████▏  | 280/390 [02:12<00:32,  3.41it/s] 72%|███████▏  | 281/390 [02:12<00:32,  3.40it/s] 72%|███████▏  | 282/390 [02:13<00:31,  3.40it/s] 73%|███████▎  | 283/390 [02:13<00:31,  3.40it/s] 73%|███████▎  | 284/390 [02:13<00:31,  3.40it/s] 73%|███████▎  | 285/390 [02:13<00:30,  3.41it/s] 73%|███████▎  | 286/390 [02:14<00:30,  3.40it/s] 74%|███████▎  | 287/390 [02:14<00:30,  3.40it/s] 74%|███████▍  | 288/390 [02:14<00:29,  3.41it/s] 74%|███████▍  | 289/390 [02:15<00:29,  3.40it/s] 74%|███████▍  | 290/390 [02:15<00:29,  3.41it/s] 75%|███████▍  | 291/390 [02:15<00:29,  3.41it/s] 75%|███████▍  | 292/390 [02:16<00:28,  3.39it/s] 75%|███████▌  | 293/390 [02:16<00:28,  3.39it/s] 75%|███████▌  | 294/390 [02:16<00:28,  3.39it/s] 76%|███████▌  | 295/390 [02:16<00:27,  3.40it/s] 76%|███████▌  | 296/390 [02:17<00:27,  3.40it/s] 76%|███████▌  | 297/390 [02:17<00:27,  3.40it/s] 76%|███████▋  | 298/390 [02:17<00:27,  3.40it/s] 77%|███████▋  | 299/390 [02:18<00:26,  3.40it/s] 77%|███████▋  | 300/390 [02:18<00:26,  3.40it/s] 77%|███████▋  | 301/390 [02:18<00:26,  3.40it/s] 77%|███████▋  | 302/390 [02:18<00:25,  3.40it/s] 78%|███████▊  | 303/390 [02:19<00:25,  3.36it/s] 78%|███████▊  | 304/390 [02:19<00:25,  3.37it/s] 78%|███████▊  | 305/390 [02:19<00:25,  3.38it/s] 78%|███████▊  | 306/390 [02:20<00:24,  3.39it/s] 79%|███████▊  | 307/390 [02:20<00:24,  3.40it/s] 79%|███████▉  | 308/390 [02:20<00:24,  3.40it/s] 79%|███████▉  | 309/390 [02:21<00:23,  3.40it/s] 79%|███████▉  | 310/390 [02:21<00:23,  3.40it/s] 80%|███████▉  | 311/390 [02:21<00:23,  3.40it/s] 80%|████████  | 312/390 [02:21<00:22,  3.40it/s][INFO|trainer.py:2140] 2023-08-27 22:46:16,344 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:46:16,346 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-27 22:46:16,346 >>   Batch size = 8
{'eval_loss': 0.9722856283187866, 'eval_runtime': 9.4944, 'eval_samples_per_second': 367.162, 'eval_steps_per_second': 45.922, 'epoch': 2.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.79it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.16it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.24it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.47it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.93it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.66it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.38it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.93it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.96it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.03it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.08it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.16it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.18it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.14it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.96it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.89it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.82it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.76it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.94it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.02it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.03it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.11it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.06it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.09it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.91it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.85it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.77it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.96it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.82it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.94it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.02it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.00it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.06it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.88it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.90it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.79it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.93it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.95it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.01it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.05it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.90it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.97it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.89it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.92it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.86it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.96it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.90it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.99it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.08it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.97it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.96it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.92it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.91it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.85it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.89it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.91it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.95it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.99it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.94it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.98it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.81it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.90it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.90it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.91it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.91it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.93it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.94it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.89it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.87it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.84it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.82it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.76it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.88it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.95it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.96it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.05it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.00it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.98it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.92it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.92it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.82it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.81it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.85it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.97it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.00it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.98it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:31<00:22,  3.40it/s]
100%|██████████| 436/436 [00:09<00:00, 45.98it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:46:25,858 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-27 22:46:25,874 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:46:28,152 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:46:28,168 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:46:28,178 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:39<06:55,  5.39s/it] 81%|████████  | 314/390 [02:39<04:53,  3.86s/it] 81%|████████  | 315/390 [02:39<03:29,  2.79s/it] 81%|████████  | 316/390 [02:40<02:31,  2.04s/it] 81%|████████▏ | 317/390 [02:40<01:50,  1.52s/it] 82%|████████▏ | 318/390 [02:40<01:22,  1.15s/it] 82%|████████▏ | 319/390 [02:40<01:03,  1.12it/s] 82%|████████▏ | 320/390 [02:41<00:49,  1.40it/s] 82%|████████▏ | 321/390 [02:41<00:40,  1.70it/s] 83%|████████▎ | 322/390 [02:41<00:33,  2.00it/s] 83%|████████▎ | 323/390 [02:42<00:29,  2.29it/s] 83%|████████▎ | 324/390 [02:42<00:26,  2.54it/s] 83%|████████▎ | 325/390 [02:42<00:23,  2.74it/s] 84%|████████▎ | 326/390 [02:43<00:21,  2.91it/s] 84%|████████▍ | 327/390 [02:43<00:20,  3.04it/s] 84%|████████▍ | 328/390 [02:43<00:19,  3.14it/s] 84%|████████▍ | 329/390 [02:43<00:18,  3.22it/s] 85%|████████▍ | 330/390 [02:44<00:18,  3.27it/s] 85%|████████▍ | 331/390 [02:44<00:17,  3.31it/s] 85%|████████▌ | 332/390 [02:44<00:17,  3.34it/s] 85%|████████▌ | 333/390 [02:45<00:16,  3.36it/s] 86%|████████▌ | 334/390 [02:45<00:16,  3.37it/s] 86%|████████▌ | 335/390 [02:45<00:16,  3.38it/s] 86%|████████▌ | 336/390 [02:45<00:15,  3.38it/s] 86%|████████▋ | 337/390 [02:46<00:15,  3.39it/s] 87%|████████▋ | 338/390 [02:46<00:15,  3.39it/s] 87%|████████▋ | 339/390 [02:46<00:15,  3.40it/s] 87%|████████▋ | 340/390 [02:47<00:14,  3.40it/s] 87%|████████▋ | 341/390 [02:47<00:14,  3.40it/s] 88%|████████▊ | 342/390 [02:47<00:14,  3.40it/s] 88%|████████▊ | 343/390 [02:47<00:13,  3.41it/s] 88%|████████▊ | 344/390 [02:48<00:13,  3.41it/s] 88%|████████▊ | 345/390 [02:48<00:13,  3.41it/s] 89%|████████▊ | 346/390 [02:48<00:12,  3.40it/s] 89%|████████▉ | 347/390 [02:49<00:12,  3.38it/s] 89%|████████▉ | 348/390 [02:49<00:12,  3.39it/s] 89%|████████▉ | 349/390 [02:49<00:12,  3.39it/s] 90%|████████▉ | 350/390 [02:50<00:12,  3.32it/s] 90%|█████████ | 351/390 [02:50<00:11,  3.35it/s] 90%|█████████ | 352/390 [02:50<00:11,  3.37it/s] 91%|█████████ | 353/390 [02:50<00:10,  3.37it/s] 91%|█████████ | 354/390 [02:51<00:10,  3.38it/s] 91%|█████████ | 355/390 [02:51<00:10,  3.39it/s] 91%|█████████▏| 356/390 [02:51<00:10,  3.39it/s] 92%|█████████▏| 357/390 [02:52<00:09,  3.39it/s] 92%|█████████▏| 358/390 [02:52<00:09,  3.38it/s] 92%|█████████▏| 359/390 [02:52<00:09,  3.39it/s] 92%|█████████▏| 360/390 [02:53<00:08,  3.39it/s] 93%|█████████▎| 361/390 [02:53<00:08,  3.39it/s] 93%|█████████▎| 362/390 [02:53<00:08,  3.40it/s] 93%|█████████▎| 363/390 [02:53<00:07,  3.40it/s] 93%|█████████▎| 364/390 [02:54<00:07,  3.40it/s] 94%|█████████▎| 365/390 [02:54<00:07,  3.40it/s] 94%|█████████▍| 366/390 [02:54<00:07,  3.40it/s] 94%|█████████▍| 367/390 [02:55<00:06,  3.40it/s] 94%|█████████▍| 368/390 [02:55<00:06,  3.40it/s] 95%|█████████▍| 369/390 [02:55<00:06,  3.38it/s] 95%|█████████▍| 370/390 [02:55<00:05,  3.39it/s] 95%|█████████▌| 371/390 [02:56<00:05,  3.39it/s] 95%|█████████▌| 372/390 [02:56<00:05,  3.39it/s] 96%|█████████▌| 373/390 [02:56<00:05,  3.40it/s] 96%|█████████▌| 374/390 [02:57<00:04,  3.40it/s] 96%|█████████▌| 375/390 [02:57<00:04,  3.40it/s] 96%|█████████▋| 376/390 [02:57<00:04,  3.40it/s] 97%|█████████▋| 377/390 [02:58<00:03,  3.40it/s] 97%|█████████▋| 378/390 [02:58<00:03,  3.40it/s] 97%|█████████▋| 379/390 [02:58<00:03,  3.40it/s] 97%|█████████▋| 380/390 [02:58<00:02,  3.39it/s] 98%|█████████▊| 381/390 [02:59<00:02,  3.39it/s] 98%|█████████▊| 382/390 [02:59<00:02,  3.40it/s] 98%|█████████▊| 383/390 [02:59<00:02,  3.40it/s] 98%|█████████▊| 384/390 [03:00<00:01,  3.40it/s] 99%|█████████▊| 385/390 [03:00<00:01,  3.40it/s] 99%|█████████▉| 386/390 [03:00<00:01,  3.40it/s] 99%|█████████▉| 387/390 [03:00<00:00,  3.40it/s] 99%|█████████▉| 388/390 [03:01<00:00,  3.40it/s]100%|█████████▉| 389/390 [03:01<00:00,  3.40it/s]100%|██████████| 390/390 [03:01<00:00,  3.40it/s][INFO|trainer.py:2140] 2023-08-27 22:46:56,257 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:46:56,257 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-27 22:46:56,257 >>   Batch size = 8
{'eval_loss': 0.9749689102172852, 'eval_runtime': 9.4892, 'eval_samples_per_second': 367.365, 'eval_steps_per_second': 45.947, 'epoch': 3.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.48it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.85it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.18it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.37it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.03it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.58it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.29it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.00it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.01it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.04it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.99it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.09it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.10it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.06it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.03it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.90it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.81it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.82it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.93it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.91it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.98it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.04it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.09it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.00it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.89it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.89it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.81it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.92it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.88it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.92it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.91it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.00it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.87it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.86it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.83it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.84it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.78it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.86it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.00it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.95it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.03it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.01it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.83it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.84it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.89it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.84it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.60it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.94it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.95it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.98it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.95it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.92it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.89it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.81it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.88it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.86it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.97it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.00it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.96it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.96it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.98it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.99it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.82it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.83it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.88it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.97it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.97it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.03it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.94it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.86it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.92it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.91it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.85it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.83it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.70it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.80it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.94it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.18it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.35it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.52it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.66it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.67it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.79it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.83it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.85it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.85it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:11<00:00,  3.40it/s]
100%|██████████| 436/436 [00:09<00:00, 45.85it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:47:05,771 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-27 22:47:05,793 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:47:08,937 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:47:08,958 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:47:08,964 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-27 22:47:13,605 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-27 22:47:13,607 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156 (score: 0.9639974236488342).
                                                 100%|██████████| 390/390 [03:20<00:00,  3.40it/s]100%|██████████| 390/390 [03:20<00:00,  1.94it/s]
[INFO|trainer.py:1894] 2023-08-27 22:47:15,269 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-27 22:47:15,281 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:47:17,478 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:47:17,493 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:47:17,499 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:47:17,678 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:17,678 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:17,678 >>   train_loss               =      0.802
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:17,678 >>   train_runtime            = 0:03:20.86
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:17,678 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:17,678 >>   train_samples_per_second =    124.461
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:17,678 >>   train_steps_per_second   =      1.942
{'eval_loss': 0.9785000681877136, 'eval_runtime': 9.499, 'eval_samples_per_second': 366.986, 'eval_steps_per_second': 45.9, 'epoch': 4.99}
{'train_runtime': 200.8669, 'train_samples_per_second': 124.461, 'train_steps_per_second': 1.942, 'train_loss': 0.8019622802734375, 'epoch': 4.99}
08/27/2023 22:47:17 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-27 22:47:17,718 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:47:17,718 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-27 22:47:17,718 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 57.41it/s]  3%|▎         | 12/436 [00:00<00:08, 50.40it/s]  4%|▍         | 18/436 [00:00<00:08, 48.61it/s]  5%|▌         | 23/436 [00:00<00:08, 47.82it/s]  6%|▋         | 28/436 [00:00<00:08, 47.37it/s]  8%|▊         | 33/436 [00:00<00:08, 47.12it/s]  9%|▊         | 38/436 [00:00<00:08, 46.97it/s] 10%|▉         | 43/436 [00:00<00:08, 46.58it/s] 11%|█         | 48/436 [00:01<00:08, 46.33it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.25it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.30it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.31it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.36it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.32it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.43it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.43it/s] 20%|██        | 88/436 [00:01<00:07, 46.32it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.15it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.10it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.09it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.25it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.28it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.28it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.39it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.40it/s] 31%|███       | 133/436 [00:02<00:06, 46.26it/s] 32%|███▏      | 138/436 [00:02<00:06, 46.20it/s] 33%|███▎      | 143/436 [00:03<00:06, 46.09it/s] 34%|███▍      | 148/436 [00:03<00:06, 46.05it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.09it/s] 36%|███▌      | 158/436 [00:03<00:06, 46.19it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.22it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.33it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.34it/s] 41%|████      | 178/436 [00:03<00:05, 46.30it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.26it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.14it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.11it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.18it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.08it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.16it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.20it/s] 50%|█████     | 218/436 [00:04<00:04, 46.25it/s] 51%|█████     | 223/436 [00:04<00:04, 46.27it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.21it/s] 53%|█████▎    | 233/436 [00:05<00:04, 46.10it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.07it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.18it/s] 57%|█████▋    | 248/436 [00:05<00:04, 45.99it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.20it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.17it/s] 60%|██████    | 263/436 [00:05<00:03, 46.17it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.22it/s] 63%|██████▎   | 273/436 [00:05<00:03, 45.99it/s] 64%|██████▍   | 278/436 [00:05<00:03, 46.01it/s] 65%|██████▍   | 283/436 [00:06<00:03, 45.94it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.07it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.06it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.11it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.13it/s] 71%|███████   | 308/436 [00:06<00:02, 46.12it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.13it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.12it/s] 74%|███████▍  | 323/436 [00:06<00:02, 46.15it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.08it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.01it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.06it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.06it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.09it/s] 81%|████████  | 353/436 [00:07<00:01, 46.15it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.14it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.16it/s] 84%|████████▍ | 368/436 [00:07<00:01, 46.11it/s] 86%|████████▌ | 373/436 [00:08<00:01, 46.15it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.04it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.05it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.05it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.14it/s] 91%|█████████▏| 398/436 [00:08<00:00, 46.18it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.11it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.14it/s] 95%|█████████▍| 413/436 [00:08<00:00, 46.11it/s] 96%|█████████▌| 418/436 [00:09<00:00, 46.20it/s] 97%|█████████▋| 423/436 [00:09<00:00, 46.12it/s] 98%|█████████▊| 428/436 [00:09<00:00, 46.06it/s] 99%|█████████▉| 433/436 [00:09<00:00, 46.07it/s]100%|██████████| 436/436 [00:09<00:00, 46.28it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:47:27,164 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:27,165 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:27,165 >>   eval_loss               =      0.964
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:27,165 >>   eval_runtime            = 0:00:09.44
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:27,165 >>   eval_samples            =       3486
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:27,165 >>   eval_samples_per_second =    369.039
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:27,165 >>   eval_steps_per_second   =     46.156
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:47:27,165 >>   perplexity              =     2.6222
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_1/generator/iter1/model/checkpoint-312
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_5_seed_1', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 12301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_5_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.75s/it]Extractor Estimating: 2it [00:18,  7.76s/it]Extractor Estimating: 3it [00:20,  5.39s/it]Extractor Estimating: 4it [00:21,  3.57s/it]Extractor Estimating: 5it [00:22,  2.57s/it]Extractor Estimating: 6it [00:23,  1.98s/it]Extractor Estimating: 7it [00:24,  1.61s/it]Extractor Estimating: 8it [00:24,  1.34s/it]Extractor Estimating: 9it [00:25,  1.16s/it]Extractor Estimating: 10it [00:26,  1.05s/it]Extractor Estimating: 11it [00:27,  1.02it/s]Extractor Estimating: 12it [00:28,  1.07it/s]Extractor Estimating: 13it [00:28,  1.15it/s]Extractor Estimating: 14it [00:29,  1.21it/s]Extractor Estimating: 15it [00:30,  1.20it/s]Extractor Estimating: 16it [00:31,  1.21it/s]Extractor Estimating: 17it [00:33,  1.19s/it]Extractor Estimating: 18it [00:33,  1.08s/it]Extractor Estimating: 19it [00:34,  1.01s/it]Extractor Estimating: 20it [00:35,  1.06it/s]Extractor Estimating: 21it [00:36,  1.12it/s]Extractor Estimating: 22it [00:37,  1.15it/s]Extractor Estimating: 23it [00:37,  1.19it/s]Extractor Estimating: 24it [00:38,  1.22it/s]Extractor Estimating: 25it [00:39,  1.24it/s]Extractor Estimating: 26it [00:40,  1.20it/s]Extractor Estimating: 27it [00:41,  1.21it/s]Extractor Estimating: 28it [00:42,  1.22it/s]Extractor Estimating: 29it [00:42,  1.23it/s]Extractor Estimating: 30it [00:43,  1.22it/s]Extractor Estimating: 31it [00:44,  1.23it/s]Extractor Estimating: 32it [00:45,  1.25it/s]Extractor Estimating: 33it [00:46,  1.25it/s]Extractor Estimating: 34it [00:46,  1.22it/s]Extractor Estimating: 35it [00:47,  1.21it/s]Extractor Estimating: 36it [00:48,  1.23it/s]Extractor Estimating: 37it [00:49,  1.19it/s]Extractor Estimating: 38it [00:50,  1.16it/s]Extractor Estimating: 39it [00:51,  1.17it/s]Extractor Estimating: 40it [00:51,  1.20it/s]Extractor Estimating: 41it [00:52,  1.16it/s]Extractor Estimating: 42it [00:53,  1.19it/s]Extractor Estimating: 43it [00:54,  1.21it/s]Extractor Estimating: 44it [00:55,  1.24it/s]Extractor Estimating: 45it [00:56,  1.24it/s]Extractor Estimating: 46it [00:56,  1.22it/s]Extractor Estimating: 47it [00:57,  1.26it/s]Extractor Estimating: 48it [00:58,  1.25it/s]Extractor Estimating: 49it [00:59,  1.24it/s]Extractor Estimating: 50it [01:00,  1.26it/s]Extractor Estimating: 51it [01:00,  1.27it/s]Extractor Estimating: 52it [01:01,  1.25it/s]Extractor Estimating: 53it [01:02,  1.26it/s]Extractor Estimating: 54it [01:05,  1.59s/it]Extractor Estimating: 55it [01:06,  1.35s/it]Extractor Estimating: 56it [01:07,  1.22s/it]Extractor Estimating: 57it [01:08,  1.13s/it]Extractor Estimating: 58it [01:09,  1.01s/it]Extractor Estimating: 59it [01:10,  1.05it/s]Extractor Estimating: 60it [01:10,  1.09it/s]Extractor Estimating: 61it [01:11,  1.06it/s]Extractor Estimating: 62it [01:12,  1.14it/s]Extractor Estimating: 63it [01:13,  1.15it/s]Extractor Estimating: 64it [01:14,  1.14it/s]Extractor Estimating: 65it [01:15,  1.15it/s]Extractor Estimating: 66it [01:16,  1.15it/s]Extractor Estimating: 67it [01:16,  1.19it/s]Extractor Estimating: 68it [01:17,  1.21it/s]Extractor Estimating: 69it [01:18,  1.22it/s]Extractor Estimating: 70it [01:19,  1.22it/s]Extractor Estimating: 71it [01:20,  1.23it/s]Extractor Estimating: 72it [01:20,  1.20it/s]Extractor Estimating: 73it [01:21,  1.20it/s]Extractor Estimating: 74it [01:22,  1.22it/s]Extractor Estimating: 75it [01:23,  1.23it/s]Extractor Estimating: 76it [01:24,  1.26it/s]Extractor Estimating: 77it [01:25,  1.17it/s]Extractor Estimating: 78it [01:25,  1.17it/s]Extractor Estimating: 79it [01:26,  1.20it/s]Extractor Estimating: 80it [01:27,  1.24it/s]Extractor Estimating: 81it [01:28,  1.24it/s]Extractor Estimating: 82it [01:29,  1.27it/s]Extractor Estimating: 83it [01:29,  1.31it/s]Extractor Estimating: 84it [01:30,  1.29it/s]Extractor Estimating: 85it [01:31,  1.29it/s]Extractor Estimating: 86it [01:32,  1.24it/s]Extractor Estimating: 87it [01:32,  1.28it/s]Extractor Estimating: 88it [01:33,  1.27it/s]Extractor Estimating: 89it [01:34,  1.29it/s]Extractor Estimating: 90it [01:35,  1.22it/s]Extractor Estimating: 91it [01:36,  1.20it/s]Extractor Estimating: 92it [01:37,  1.22it/s]Extractor Estimating: 93it [01:37,  1.21it/s]Extractor Estimating: 94it [01:38,  1.20it/s]Extractor Estimating: 95it [01:39,  1.20it/s]Extractor Estimating: 96it [01:40,  1.25it/s]Extractor Estimating: 97it [01:41,  1.20it/s]Extractor Estimating: 98it [01:41,  1.23it/s]Extractor Estimating: 99it [01:42,  1.25it/s]Extractor Estimating: 100it [01:43,  1.28it/s]Extractor Estimating: 101it [01:44,  1.24it/s]Extractor Estimating: 102it [01:45,  1.23it/s]Extractor Estimating: 103it [01:45,  1.23it/s]Extractor Estimating: 104it [01:46,  1.22it/s]Extractor Estimating: 105it [01:47,  1.27it/s]Extractor Estimating: 106it [01:48,  1.31it/s]Extractor Estimating: 107it [01:49,  1.29it/s]Extractor Estimating: 108it [01:49,  1.28it/s]Extractor Estimating: 109it [01:50,  1.30it/s]Extractor Estimating: 110it [01:51,  1.25it/s]Extractor Estimating: 111it [01:52,  1.28it/s]Extractor Estimating: 112it [01:52,  1.28it/s]Extractor Estimating: 113it [01:53,  1.31it/s]Extractor Estimating: 114it [01:54,  1.31it/s]Extractor Estimating: 115it [01:55,  1.29it/s]Extractor Estimating: 116it [01:56,  1.22it/s]Extractor Estimating: 117it [01:57,  1.20it/s]Extractor Estimating: 118it [01:57,  1.19it/s]Extractor Estimating: 119it [01:58,  1.22it/s]Extractor Estimating: 120it [01:59,  1.19it/s]Extractor Estimating: 121it [02:00,  1.20it/s]Extractor Estimating: 122it [02:01,  1.19it/s]Extractor Estimating: 123it [02:01,  1.23it/s]Extractor Estimating: 124it [02:02,  1.25it/s]Extractor Estimating: 125it [02:03,  1.22it/s]Extractor Estimating: 126it [02:04,  1.21it/s]Extractor Estimating: 127it [02:05,  1.21it/s]Extractor Estimating: 128it [02:06,  1.24it/s]Extractor Estimating: 129it [02:06,  1.26it/s]Extractor Estimating: 130it [02:07,  1.28it/s]Extractor Estimating: 131it [02:08,  1.27it/s]Extractor Estimating: 132it [02:09,  1.26it/s]Extractor Estimating: 133it [02:09,  1.29it/s]Extractor Estimating: 134it [02:10,  1.28it/s]Extractor Estimating: 135it [02:11,  1.28it/s]Extractor Estimating: 136it [02:12,  1.30it/s]Extractor Estimating: 137it [02:13,  1.25it/s]Extractor Estimating: 138it [02:13,  1.29it/s]Extractor Estimating: 139it [02:14,  1.25it/s]Extractor Estimating: 140it [02:15,  1.28it/s]Extractor Estimating: 141it [02:16,  1.26it/s]Extractor Estimating: 142it [02:16,  1.27it/s]Extractor Estimating: 143it [02:17,  1.26it/s]Extractor Estimating: 144it [02:18,  1.29it/s]Extractor Estimating: 145it [02:19,  1.30it/s]Extractor Estimating: 146it [02:20,  1.28it/s]Extractor Estimating: 147it [02:20,  1.27it/s]Extractor Estimating: 148it [02:21,  1.27it/s]Extractor Estimating: 149it [02:22,  1.26it/s]Extractor Estimating: 150it [02:23,  1.25it/s]Extractor Estimating: 151it [02:24,  1.14it/s]Extractor Estimating: 152it [02:25,  1.19it/s]Extractor Estimating: 153it [02:25,  1.21it/s]Extractor Estimating: 154it [02:26,  1.21it/s]Extractor Estimating: 155it [02:27,  1.14it/s]Extractor Estimating: 156it [02:28,  1.16it/s]Extractor Estimating: 157it [02:29,  1.14it/s]Extractor Estimating: 158it [02:30,  1.21it/s]Extractor Estimating: 159it [02:30,  1.22it/s]Extractor Estimating: 160it [02:31,  1.22it/s]Extractor Estimating: 161it [02:32,  1.25it/s]Extractor Estimating: 162it [02:33,  1.22it/s]Extractor Estimating: 163it [02:34,  1.23it/s]Extractor Estimating: 164it [02:35,  1.21it/s]Extractor Estimating: 165it [02:35,  1.17it/s]Extractor Estimating: 166it [02:36,  1.18it/s]Extractor Estimating: 167it [02:37,  1.18it/s]Extractor Estimating: 168it [02:38,  1.18it/s]Extractor Estimating: 169it [02:39,  1.13it/s]Extractor Estimating: 170it [02:40,  1.17it/s]Extractor Estimating: 171it [02:41,  1.17it/s]Extractor Estimating: 172it [02:41,  1.17it/s]Extractor Estimating: 173it [02:42,  1.16it/s]Extractor Estimating: 174it [02:43,  1.13it/s]Extractor Estimating: 175it [02:44,  1.14it/s]Extractor Estimating: 176it [02:45,  1.12it/s]Extractor Estimating: 177it [02:46,  1.13it/s]Extractor Estimating: 178it [02:47,  1.19it/s]Extractor Estimating: 179it [02:47,  1.23it/s]Extractor Estimating: 180it [02:48,  1.25it/s]Extractor Estimating: 181it [02:49,  1.19it/s]Extractor Estimating: 182it [02:50,  1.19it/s]Extractor Estimating: 183it [02:51,  1.19it/s]Extractor Estimating: 184it [02:52,  1.20it/s]Extractor Estimating: 185it [02:52,  1.23it/s]Extractor Estimating: 186it [02:53,  1.26it/s]Extractor Estimating: 187it [02:54,  1.23it/s]Extractor Estimating: 188it [02:55,  1.19it/s]Extractor Estimating: 189it [02:56,  1.19it/s]Extractor Estimating: 190it [02:57,  1.22it/s]Extractor Estimating: 191it [02:57,  1.21it/s]Extractor Estimating: 192it [02:58,  1.21it/s]Extractor Estimating: 193it [02:59,  1.21it/s]Extractor Estimating: 194it [03:00,  1.20it/s]Extractor Estimating: 195it [03:01,  1.23it/s]Extractor Estimating: 196it [03:01,  1.24it/s]Extractor Estimating: 197it [03:02,  1.23it/s]Extractor Estimating: 198it [03:03,  1.24it/s]Extractor Estimating: 199it [03:04,  1.24it/s]Extractor Estimating: 200it [03:05,  1.23it/s]Extractor Estimating: 201it [03:06,  1.23it/s]Extractor Estimating: 202it [03:06,  1.21it/s]Extractor Estimating: 203it [03:07,  1.25it/s]Extractor Estimating: 204it [03:08,  1.26it/s]Extractor Estimating: 205it [03:09,  1.26it/s]Extractor Estimating: 206it [03:09,  1.28it/s]Extractor Estimating: 207it [03:10,  1.31it/s]Extractor Estimating: 208it [03:11,  1.28it/s]Extractor Estimating: 209it [03:12,  1.27it/s]Extractor Estimating: 210it [03:13,  1.25it/s]Extractor Estimating: 211it [03:13,  1.26it/s]Extractor Estimating: 212it [03:14,  1.26it/s]Extractor Estimating: 213it [03:15,  1.24it/s]Extractor Estimating: 214it [03:16,  1.22it/s]Extractor Estimating: 215it [03:17,  1.22it/s]Extractor Estimating: 216it [03:17,  1.27it/s]Extractor Estimating: 217it [03:18,  1.25it/s]Extractor Estimating: 218it [03:19,  1.21it/s]Extractor Estimating: 219it [03:20,  1.19it/s]Extractor Estimating: 220it [03:21,  1.19it/s]Extractor Estimating: 221it [03:22,  1.20it/s]Extractor Estimating: 222it [03:23,  1.13it/s]Extractor Estimating: 223it [03:23,  1.19it/s]Extractor Estimating: 224it [03:24,  1.22it/s]Extractor Estimating: 225it [03:25,  1.19it/s]Extractor Estimating: 226it [03:26,  1.24it/s]Extractor Estimating: 227it [03:27,  1.25it/s]Extractor Estimating: 228it [03:27,  1.23it/s]Extractor Estimating: 229it [03:28,  1.27it/s]Extractor Estimating: 230it [03:29,  1.25it/s]Extractor Estimating: 231it [03:30,  1.27it/s]Extractor Estimating: 232it [03:31,  1.27it/s]Extractor Estimating: 233it [03:31,  1.20it/s]Extractor Estimating: 234it [03:32,  1.24it/s]Extractor Estimating: 235it [03:33,  1.22it/s]Extractor Estimating: 236it [03:34,  1.26it/s]Extractor Estimating: 237it [03:35,  1.26it/s]Extractor Estimating: 238it [03:35,  1.28it/s]Extractor Estimating: 239it [03:36,  1.28it/s]Extractor Estimating: 240it [03:37,  1.32it/s]Extractor Estimating: 241it [03:38,  1.27it/s]Extractor Estimating: 242it [03:38,  1.28it/s]Extractor Estimating: 243it [03:39,  1.31it/s]Extractor Estimating: 244it [03:40,  1.21it/s]Extractor Estimating: 245it [03:41,  1.28it/s]Extractor Estimating: 246it [03:42,  1.28it/s]Extractor Estimating: 247it [03:42,  1.28it/s]Extractor Estimating: 248it [03:43,  1.27it/s]Extractor Estimating: 249it [03:44,  1.28it/s]Extractor Estimating: 250it [03:45,  1.24it/s]Extractor Estimating: 250it [03:45,  1.11it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5147 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl'}
train vocab size: 23547
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23647, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_train_large/unseen_5_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23647, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.679, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.356, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 85, avg_time 1.367, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 185, avg_time 1.345, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 70, avg_time 1.353, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 170, avg_time 2.797, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 55, avg_time 1.345, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 155, avg_time 1.352, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 40, avg_time 1.366, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 140, avg_time 1.348, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 25, avg_time 2.793, loss:nan
g_step 1200, step 125, avg_time 1.362, loss:nan
g_step 1300, step 10, avg_time 1.350, loss:nan
g_step 1400, step 110, avg_time 1.362, loss:nan
g_step 1500, step 210, avg_time 1.368, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 95, avg_time 2.782, loss:nan
g_step 1700, step 195, avg_time 1.362, loss:nan
g_step 1800, step 80, avg_time 1.376, loss:nan
g_step 1900, step 180, avg_time 1.340, loss:nan
g_step 2000, step 65, avg_time 1.349, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 165, avg_time 2.786, loss:nan
g_step 2200, step 50, avg_time 1.371, loss:nan
g_step 2300, step 150, avg_time 1.360, loss:nan
g_step 2400, step 35, avg_time 1.349, loss:nan
g_step 2500, step 135, avg_time 1.356, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 20, avg_time 2.773, loss:nan
g_step 2700, step 120, avg_time 1.362, loss:nan
g_step 2800, step 5, avg_time 1.353, loss:nan
g_step 2900, step 105, avg_time 1.374, loss:nan
g_step 3000, step 205, avg_time 1.347, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 90, avg_time 2.776, loss:nan
g_step 3200, step 190, avg_time 1.364, loss:nan
g_step 3300, step 75, avg_time 1.356, loss:nan
g_step 3400, step 175, avg_time 1.357, loss:nan
g_step 3500, step 60, avg_time 1.351, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 160, avg_time 2.790, loss:nan
g_step 3700, step 45, avg_time 1.376, loss:nan
g_step 3800, step 145, avg_time 1.350, loss:nan
g_step 3900, step 30, avg_time 1.343, loss:nan
g_step 4000, step 130, avg_time 1.364, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 15, avg_time 2.780, loss:nan
g_step 4200, step 115, avg_time 1.358, loss:nan
g_step 4300, step 215, avg_time 1.361, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 00:49:06 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 00:49:06 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_00-49-06_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 00:49:07 - WARNING - datasets.builder -   Using custom data configuration default-1000ea08a5725a85
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-1000ea08a5725a85/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 00:49:08,092 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:49:08,093 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:49:08,094 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:49:08,095 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:49:08,107 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:49:08,110 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:49:08,110 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:49:08,110 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:49:08,110 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:49:08,110 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:49:08,110 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 00:49:08,225 >> loading weights file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:49:11,330 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 00:49:11,334 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_5_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-1000ea08a5725a85/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 00:49:11 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1489163ec290> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.06ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.85ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.13ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.25ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.70ba/s]100%|██████████| 6/6 [00:01<00:00,  4.41ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.02ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.30ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.41ba/s]100%|██████████| 4/4 [00:00<00:00,  5.53ba/s]100%|██████████| 4/4 [00:00<00:00,  5.00ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  7.69ba/s] 33%|███▎      | 2/6 [00:00<00:00,  8.69ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.98ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.12ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.12ba/s]100%|██████████| 6/6 [00:00<00:00, 10.39ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.16ba/s] 50%|█████     | 2/4 [00:00<00:00,  8.54ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.09ba/s]100%|██████████| 4/4 [00:00<00:00, 10.24ba/s]
[INFO|trainer.py:414] 2023-08-28 00:49:14,850 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 00:49:14,878 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 00:49:14,878 >>   Num examples = 5160
[INFO|trainer.py:1149] 2023-08-28 00:49:14,879 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 00:49:14,879 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 00:49:14,879 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 00:49:14,879 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 00:49:14,879 >>   Total optimization steps = 405
  0%|          | 0/405 [00:00<?, ?it/s]  0%|          | 1/405 [00:00<01:59,  3.37it/s]  0%|          | 2/405 [00:00<01:55,  3.50it/s]  1%|          | 3/405 [00:00<01:53,  3.54it/s]  1%|          | 4/405 [00:01<01:52,  3.56it/s]  1%|          | 5/405 [00:01<01:51,  3.58it/s]  1%|▏         | 6/405 [00:01<01:51,  3.58it/s]  2%|▏         | 7/405 [00:01<01:50,  3.59it/s]  2%|▏         | 8/405 [00:02<01:50,  3.59it/s]  2%|▏         | 9/405 [00:02<01:50,  3.59it/s]  2%|▏         | 10/405 [00:02<01:50,  3.57it/s]  3%|▎         | 11/405 [00:03<01:50,  3.58it/s]  3%|▎         | 12/405 [00:03<01:49,  3.58it/s]  3%|▎         | 13/405 [00:03<01:49,  3.59it/s]  3%|▎         | 14/405 [00:03<01:48,  3.59it/s]  4%|▎         | 15/405 [00:04<01:48,  3.59it/s]  4%|▍         | 16/405 [00:04<01:48,  3.59it/s]  4%|▍         | 17/405 [00:04<01:48,  3.59it/s]  4%|▍         | 18/405 [00:05<01:47,  3.59it/s]  5%|▍         | 19/405 [00:05<01:47,  3.59it/s]  5%|▍         | 20/405 [00:05<01:47,  3.59it/s]  5%|▌         | 21/405 [00:05<01:47,  3.59it/s]  5%|▌         | 22/405 [00:06<01:46,  3.59it/s]  6%|▌         | 23/405 [00:06<01:46,  3.59it/s]  6%|▌         | 24/405 [00:06<01:46,  3.59it/s]  6%|▌         | 25/405 [00:06<01:45,  3.59it/s]  6%|▋         | 26/405 [00:07<01:45,  3.59it/s]  7%|▋         | 27/405 [00:07<01:45,  3.59it/s]  7%|▋         | 28/405 [00:07<01:45,  3.59it/s]  7%|▋         | 29/405 [00:08<01:44,  3.59it/s]  7%|▋         | 30/405 [00:08<01:44,  3.59it/s]  8%|▊         | 31/405 [00:08<01:44,  3.59it/s]  8%|▊         | 32/405 [00:08<01:43,  3.59it/s]  8%|▊         | 33/405 [00:09<01:43,  3.59it/s]  8%|▊         | 34/405 [00:09<01:43,  3.59it/s]  9%|▊         | 35/405 [00:09<01:43,  3.59it/s]  9%|▉         | 36/405 [00:10<01:42,  3.59it/s]  9%|▉         | 37/405 [00:10<01:42,  3.59it/s]  9%|▉         | 38/405 [00:10<01:42,  3.58it/s] 10%|▉         | 39/405 [00:10<01:42,  3.58it/s] 10%|▉         | 40/405 [00:11<01:41,  3.58it/s] 10%|█         | 41/405 [00:11<01:41,  3.58it/s] 10%|█         | 42/405 [00:11<01:41,  3.58it/s] 11%|█         | 43/405 [00:12<01:41,  3.58it/s] 11%|█         | 44/405 [00:12<01:40,  3.58it/s] 11%|█         | 45/405 [00:12<01:40,  3.59it/s] 11%|█▏        | 46/405 [00:12<01:40,  3.59it/s] 12%|█▏        | 47/405 [00:13<01:39,  3.59it/s] 12%|█▏        | 48/405 [00:13<01:39,  3.59it/s] 12%|█▏        | 49/405 [00:13<01:39,  3.59it/s] 12%|█▏        | 50/405 [00:13<01:38,  3.59it/s] 13%|█▎        | 51/405 [00:14<01:38,  3.59it/s] 13%|█▎        | 52/405 [00:14<01:38,  3.59it/s] 13%|█▎        | 53/405 [00:14<01:38,  3.59it/s] 13%|█▎        | 54/405 [00:15<01:39,  3.54it/s] 14%|█▎        | 55/405 [00:15<01:38,  3.55it/s] 14%|█▍        | 56/405 [00:15<01:37,  3.56it/s] 14%|█▍        | 57/405 [00:15<01:37,  3.57it/s] 14%|█▍        | 58/405 [00:16<01:37,  3.58it/s] 15%|█▍        | 59/405 [00:16<01:36,  3.58it/s] 15%|█▍        | 60/405 [00:16<01:36,  3.58it/s] 15%|█▌        | 61/405 [00:17<01:36,  3.58it/s] 15%|█▌        | 62/405 [00:17<01:35,  3.58it/s] 16%|█▌        | 63/405 [00:17<01:35,  3.59it/s] 16%|█▌        | 64/405 [00:17<01:35,  3.58it/s] 16%|█▌        | 65/405 [00:18<01:34,  3.58it/s] 16%|█▋        | 66/405 [00:18<01:34,  3.58it/s] 17%|█▋        | 67/405 [00:18<01:34,  3.58it/s] 17%|█▋        | 68/405 [00:18<01:34,  3.58it/s] 17%|█▋        | 69/405 [00:19<01:33,  3.58it/s] 17%|█▋        | 70/405 [00:19<01:33,  3.58it/s] 18%|█▊        | 71/405 [00:19<01:33,  3.58it/s] 18%|█▊        | 72/405 [00:20<01:33,  3.58it/s] 18%|█▊        | 73/405 [00:20<01:32,  3.58it/s] 18%|█▊        | 74/405 [00:20<01:32,  3.58it/s] 19%|█▊        | 75/405 [00:20<01:32,  3.58it/s] 19%|█▉        | 76/405 [00:21<01:32,  3.58it/s] 19%|█▉        | 77/405 [00:21<01:31,  3.57it/s] 19%|█▉        | 78/405 [00:21<01:31,  3.58it/s] 20%|█▉        | 79/405 [00:22<01:31,  3.58it/s] 20%|█▉        | 80/405 [00:22<01:30,  3.58it/s] 20%|██        | 81/405 [00:22<01:22,  3.93it/s][INFO|trainer.py:2140] 2023-08-28 00:49:37,420 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:49:37,420 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 00:49:37,420 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.55it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.15it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.42it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.76it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.40it/s][A
  8%|▊         | 33/436 [00:00<00:08, 47.10it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.77it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.29it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.14it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.29it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.32it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.26it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.39it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.40it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.39it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.30it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.17it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.06it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.02it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.23it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.18it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.26it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.33it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.30it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.30it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.21it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.12it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.06it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.12it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.21it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.20it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.26it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.19it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.25it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.15it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.10it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.10it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.07it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.11it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.13it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.21it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.27it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.17it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.21it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.11it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.09it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.01it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.08it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.12it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.16it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.25it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.20it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.13it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.16it/s][A
 64%|██████▍   | 278/436 [00:05<00:03, 46.07it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.09it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.14it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.11it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.21it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.24it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.21it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.11it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.11it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.05it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.05it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.06it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.02it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.14it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.19it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.18it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.10it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.10it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.12it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.02it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.08it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.05it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.06it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.16it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.22it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.14it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.08it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.08it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.00it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.07it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.98it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.06it/s][A                                                
                                                 [A 20%|██        | 81/405 [00:31<01:22,  3.93it/s]
100%|██████████| 436/436 [00:09<00:00, 46.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:49:46,887 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81
[INFO|configuration_utils.py:351] 2023-08-28 00:49:46,914 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:49:49,385 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:49:49,397 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:49:49,408 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81/special_tokens_map.json
 20%|██        | 82/405 [00:35<21:10,  3.93s/it] 20%|██        | 83/405 [00:35<15:13,  2.84s/it] 21%|██        | 84/405 [00:35<11:04,  2.07s/it] 21%|██        | 85/405 [00:35<08:10,  1.53s/it] 21%|██        | 86/405 [00:36<06:09,  1.16s/it] 21%|██▏       | 87/405 [00:36<04:44,  1.12it/s] 22%|██▏       | 88/405 [00:36<03:44,  1.41it/s] 22%|██▏       | 89/405 [00:37<03:03,  1.72it/s] 22%|██▏       | 90/405 [00:37<02:34,  2.04it/s] 22%|██▏       | 91/405 [00:37<02:14,  2.34it/s] 23%|██▎       | 92/405 [00:37<01:59,  2.61it/s] 23%|██▎       | 93/405 [00:38<01:49,  2.84it/s] 23%|██▎       | 94/405 [00:38<01:42,  3.03it/s] 23%|██▎       | 95/405 [00:38<01:37,  3.17it/s] 24%|██▎       | 96/405 [00:38<01:34,  3.28it/s] 24%|██▍       | 97/405 [00:39<01:31,  3.37it/s] 24%|██▍       | 98/405 [00:39<01:29,  3.43it/s] 24%|██▍       | 99/405 [00:39<01:28,  3.47it/s] 25%|██▍       | 100/405 [00:40<01:27,  3.50it/s] 25%|██▍       | 101/405 [00:40<01:26,  3.52it/s] 25%|██▌       | 102/405 [00:40<01:25,  3.54it/s] 25%|██▌       | 103/405 [00:40<01:25,  3.55it/s] 26%|██▌       | 104/405 [00:41<01:24,  3.56it/s] 26%|██▌       | 105/405 [00:41<01:24,  3.56it/s] 26%|██▌       | 106/405 [00:41<01:23,  3.57it/s] 26%|██▋       | 107/405 [00:42<01:23,  3.57it/s] 27%|██▋       | 108/405 [00:42<01:23,  3.57it/s] 27%|██▋       | 109/405 [00:42<01:22,  3.57it/s] 27%|██▋       | 110/405 [00:42<01:22,  3.57it/s] 27%|██▋       | 111/405 [00:43<01:22,  3.58it/s] 28%|██▊       | 112/405 [00:43<01:21,  3.58it/s] 28%|██▊       | 113/405 [00:43<01:21,  3.58it/s] 28%|██▊       | 114/405 [00:44<01:21,  3.58it/s] 28%|██▊       | 115/405 [00:44<01:20,  3.58it/s] 29%|██▊       | 116/405 [00:44<01:20,  3.58it/s] 29%|██▉       | 117/405 [00:44<01:20,  3.58it/s] 29%|██▉       | 118/405 [00:45<01:20,  3.58it/s] 29%|██▉       | 119/405 [00:45<01:19,  3.58it/s] 30%|██▉       | 120/405 [00:45<01:19,  3.58it/s] 30%|██▉       | 121/405 [00:45<01:19,  3.58it/s] 30%|███       | 122/405 [00:46<01:19,  3.58it/s] 30%|███       | 123/405 [00:46<01:18,  3.58it/s] 31%|███       | 124/405 [00:46<01:18,  3.57it/s] 31%|███       | 125/405 [00:47<01:18,  3.57it/s] 31%|███       | 126/405 [00:47<01:18,  3.57it/s] 31%|███▏      | 127/405 [00:47<01:17,  3.57it/s] 32%|███▏      | 128/405 [00:47<01:17,  3.58it/s] 32%|███▏      | 129/405 [00:48<01:17,  3.57it/s] 32%|███▏      | 130/405 [00:48<01:16,  3.57it/s] 32%|███▏      | 131/405 [00:48<01:16,  3.58it/s] 33%|███▎      | 132/405 [00:49<01:16,  3.57it/s] 33%|███▎      | 133/405 [00:49<01:16,  3.57it/s] 33%|███▎      | 134/405 [00:49<01:15,  3.57it/s] 33%|███▎      | 135/405 [00:49<01:15,  3.56it/s] 34%|███▎      | 136/405 [00:50<01:15,  3.57it/s] 34%|███▍      | 137/405 [00:50<01:15,  3.57it/s] 34%|███▍      | 138/405 [00:50<01:14,  3.57it/s] 34%|███▍      | 139/405 [00:51<01:14,  3.56it/s] 35%|███▍      | 140/405 [00:51<01:14,  3.56it/s] 35%|███▍      | 141/405 [00:51<01:14,  3.56it/s] 35%|███▌      | 142/405 [00:51<01:13,  3.57it/s] 35%|███▌      | 143/405 [00:52<01:16,  3.45it/s] 36%|███▌      | 144/405 [00:52<01:15,  3.46it/s] 36%|███▌      | 145/405 [00:52<01:14,  3.50it/s] 36%|███▌      | 146/405 [00:53<01:13,  3.50it/s] 36%|███▋      | 147/405 [00:53<01:13,  3.52it/s] 37%|███▋      | 148/405 [00:53<01:12,  3.54it/s] 37%|███▋      | 149/405 [00:53<01:12,  3.55it/s] 37%|███▋      | 150/405 [00:54<01:11,  3.56it/s] 37%|███▋      | 151/405 [00:54<01:11,  3.56it/s] 38%|███▊      | 152/405 [00:54<01:10,  3.57it/s] 38%|███▊      | 153/405 [00:54<01:10,  3.57it/s] 38%|███▊      | 154/405 [00:55<01:10,  3.57it/s] 38%|███▊      | 155/405 [00:55<01:09,  3.57it/s] 39%|███▊      | 156/405 [00:55<01:09,  3.58it/s] 39%|███▉      | 157/405 [00:56<01:09,  3.58it/s] 39%|███▉      | 158/405 [00:56<01:09,  3.57it/s] 39%|███▉      | 159/405 [00:56<01:09,  3.56it/s] 40%|███▉      | 160/405 [00:56<01:08,  3.57it/s] 40%|███▉      | 161/405 [00:57<01:08,  3.57it/s] 40%|████      | 162/405 [00:57<01:02,  3.92it/s][INFO|trainer.py:2140] 2023-08-28 00:50:12,286 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:50:12,286 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 00:50:12,286 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4419, 'eval_samples_per_second': 369.207, 'eval_steps_per_second': 46.177, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.65it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.08it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.10it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.46it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.12it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.76it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.51it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.02it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.95it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.94it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.15it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.14it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.12it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.15it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.16it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.07it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.94it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.85it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.91it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.98it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.00it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.09it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.11it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.18it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.05it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.88it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.96it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.01it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.02it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.97it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.04it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.07it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.11it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.01it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.98it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.93it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.96it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.01it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.01it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.07it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.98it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.04it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.99it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.96it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.85it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.86it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.00it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.92it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.05it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.03it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.01it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.00it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.98it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.98it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.95it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.05it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.93it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.04it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.97it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.95it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.96it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.84it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.91it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 45.89it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.92it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.95it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.00it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.97it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.05it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.06it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.86it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.95it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.94it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.88it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.70it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.92it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.91it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.94it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.01it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.91it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.92it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.90it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.93it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.96it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.04it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.02it/s][A                                                 
                                                 [A 40%|████      | 162/405 [01:06<01:02,  3.92it/s]
100%|██████████| 436/436 [00:09<00:00, 46.02it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:50:21,785 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-162
[INFO|configuration_utils.py:351] 2023-08-28 00:50:21,799 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-162/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:50:24,393 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-162/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:50:24,406 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-162/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:50:24,414 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-162/special_tokens_map.json
 40%|████      | 163/405 [01:10<16:02,  3.98s/it] 40%|████      | 164/405 [01:10<11:31,  2.87s/it] 41%|████      | 165/405 [01:10<08:22,  2.09s/it] 41%|████      | 166/405 [01:10<06:10,  1.55s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/405 [01:11<04:39,  1.17s/it] 41%|████▏     | 168/405 [01:11<03:34,  1.10it/s] 42%|████▏     | 169/405 [01:11<02:49,  1.39it/s] 42%|████▏     | 170/405 [01:12<02:17,  1.70it/s] 42%|████▏     | 171/405 [01:12<01:55,  2.02it/s] 42%|████▏     | 172/405 [01:12<01:40,  2.32it/s] 43%|████▎     | 173/405 [01:12<01:29,  2.59it/s] 43%|████▎     | 174/405 [01:13<01:21,  2.82it/s] 43%|████▎     | 175/405 [01:13<01:16,  3.01it/s] 43%|████▎     | 176/405 [01:13<01:12,  3.16it/s] 44%|████▎     | 177/405 [01:14<01:09,  3.27it/s] 44%|████▍     | 178/405 [01:14<01:07,  3.36it/s] 44%|████▍     | 179/405 [01:14<01:06,  3.42it/s] 44%|████▍     | 180/405 [01:14<01:04,  3.46it/s] 45%|████▍     | 181/405 [01:15<01:04,  3.50it/s] 45%|████▍     | 182/405 [01:15<01:03,  3.52it/s] 45%|████▌     | 183/405 [01:15<01:02,  3.54it/s] 45%|████▌     | 184/405 [01:15<01:02,  3.54it/s] 46%|████▌     | 185/405 [01:16<01:01,  3.55it/s] 46%|████▌     | 186/405 [01:16<01:01,  3.56it/s] 46%|████▌     | 187/405 [01:16<01:01,  3.56it/s] 46%|████▋     | 188/405 [01:17<01:00,  3.57it/s] 47%|████▋     | 189/405 [01:17<01:00,  3.57it/s] 47%|████▋     | 190/405 [01:17<01:00,  3.57it/s] 47%|████▋     | 191/405 [01:17<00:59,  3.57it/s] 47%|████▋     | 192/405 [01:18<00:59,  3.57it/s] 48%|████▊     | 193/405 [01:18<00:59,  3.57it/s] 48%|████▊     | 194/405 [01:18<00:59,  3.57it/s] 48%|████▊     | 195/405 [01:19<01:03,  3.31it/s] 48%|████▊     | 196/405 [01:19<01:01,  3.39it/s] 49%|████▊     | 197/405 [01:19<01:00,  3.44it/s] 49%|████▉     | 198/405 [01:19<00:59,  3.48it/s] 49%|████▉     | 199/405 [01:20<00:58,  3.50it/s] 49%|████▉     | 200/405 [01:20<00:58,  3.52it/s] 50%|████▉     | 201/405 [01:20<00:57,  3.54it/s] 50%|████▉     | 202/405 [01:21<00:57,  3.55it/s] 50%|█████     | 203/405 [01:21<00:56,  3.55it/s] 50%|█████     | 204/405 [01:21<00:56,  3.56it/s] 51%|█████     | 205/405 [01:21<00:56,  3.56it/s] 51%|█████     | 206/405 [01:22<00:55,  3.56it/s] 51%|█████     | 207/405 [01:22<00:55,  3.56it/s] 51%|█████▏    | 208/405 [01:22<00:55,  3.56it/s] 52%|█████▏    | 209/405 [01:23<00:54,  3.56it/s] 52%|█████▏    | 210/405 [01:23<00:54,  3.57it/s] 52%|█████▏    | 211/405 [01:23<00:54,  3.57it/s] 52%|█████▏    | 212/405 [01:23<00:54,  3.57it/s] 53%|█████▎    | 213/405 [01:24<00:53,  3.57it/s] 53%|█████▎    | 214/405 [01:24<00:53,  3.57it/s] 53%|█████▎    | 215/405 [01:24<00:53,  3.57it/s] 53%|█████▎    | 216/405 [01:25<00:52,  3.57it/s] 54%|█████▎    | 217/405 [01:25<00:52,  3.57it/s] 54%|█████▍    | 218/405 [01:25<00:52,  3.57it/s] 54%|█████▍    | 219/405 [01:25<00:52,  3.57it/s] 54%|█████▍    | 220/405 [01:26<00:51,  3.57it/s] 55%|█████▍    | 221/405 [01:26<00:51,  3.57it/s] 55%|█████▍    | 222/405 [01:26<00:51,  3.57it/s] 55%|█████▌    | 223/405 [01:26<00:50,  3.57it/s] 55%|█████▌    | 224/405 [01:27<00:50,  3.56it/s] 56%|█████▌    | 225/405 [01:27<00:50,  3.56it/s] 56%|█████▌    | 226/405 [01:27<00:50,  3.56it/s] 56%|█████▌    | 227/405 [01:28<00:49,  3.57it/s] 56%|█████▋    | 228/405 [01:28<00:49,  3.57it/s] 57%|█████▋    | 229/405 [01:28<00:49,  3.57it/s] 57%|█████▋    | 230/405 [01:28<00:49,  3.57it/s] 57%|█████▋    | 231/405 [01:29<00:48,  3.57it/s] 57%|█████▋    | 232/405 [01:29<00:48,  3.57it/s] 58%|█████▊    | 233/405 [01:29<00:48,  3.57it/s] 58%|█████▊    | 234/405 [01:30<00:47,  3.57it/s] 58%|█████▊    | 235/405 [01:30<00:47,  3.57it/s] 58%|█████▊    | 236/405 [01:30<00:47,  3.57it/s] 59%|█████▊    | 237/405 [01:30<00:47,  3.57it/s] 59%|█████▉    | 238/405 [01:31<00:46,  3.57it/s] 59%|█████▉    | 239/405 [01:31<00:46,  3.58it/s] 59%|█████▉    | 240/405 [01:31<00:46,  3.58it/s] 60%|█████▉    | 241/405 [01:32<00:45,  3.57it/s] 60%|█████▉    | 242/405 [01:32<00:45,  3.57it/s] 60%|██████    | 243/405 [01:32<00:41,  3.92it/s][INFO|trainer.py:2140] 2023-08-28 00:50:47,372 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:50:47,372 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 00:50:47,372 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.478, 'eval_samples_per_second': 367.798, 'eval_steps_per_second': 46.001, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.35it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.99it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.17it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.52it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.01it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.75it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.52it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.02it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.91it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.02it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.08it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.13it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.20it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.14it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.12it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.02it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.93it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.85it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.87it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.98it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.98it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.06it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.05it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.11it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.97it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.90it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.89it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.91it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.99it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.03it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.09it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.08it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.07it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.02it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.83it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.80it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.90it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.96it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.90it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.03it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.06it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.03it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.98it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.93it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.91it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.85it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.96it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.95it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.99it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.04it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.96it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.94it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.94it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.97it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.93it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.91it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.94it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.04it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.02it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.99it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.91it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.95it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.96it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.96it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.00it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.94it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.01it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.00it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.94it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.98it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.98it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.95it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.98it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.94it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.87it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.97it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.01it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.96it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.01it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.94it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.89it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.92it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.96it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.98it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.99it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.05it/s][A                                                 
                                                 [A 60%|██████    | 243/405 [01:41<00:41,  3.92it/s]
100%|██████████| 436/436 [00:09<00:00, 46.05it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:50:56,866 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-243
[INFO|configuration_utils.py:351] 2023-08-28 00:50:56,885 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-243/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:50:59,516 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-243/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:50:59,530 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-243/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:50:59,539 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-243/special_tokens_map.json
 60%|██████    | 244/405 [01:45<10:43,  4.00s/it] 60%|██████    | 245/405 [01:45<07:40,  2.88s/it] 61%|██████    | 246/405 [01:45<05:33,  2.10s/it] 61%|██████    | 247/405 [01:46<04:05,  1.55s/it] 61%|██████    | 248/405 [01:46<03:03,  1.17s/it] 61%|██████▏   | 249/405 [01:46<02:21,  1.11it/s] 62%|██████▏   | 250/405 [01:46<01:51,  1.40it/s] 62%|██████▏   | 251/405 [01:47<01:30,  1.71it/s] 62%|██████▏   | 252/405 [01:47<01:15,  2.03it/s] 62%|██████▏   | 253/405 [01:47<01:05,  2.33it/s] 63%|██████▎   | 254/405 [01:48<00:58,  2.59it/s] 63%|██████▎   | 255/405 [01:48<00:53,  2.83it/s] 63%|██████▎   | 256/405 [01:48<00:49,  3.02it/s] 63%|██████▎   | 257/405 [01:48<00:46,  3.17it/s] 64%|██████▎   | 258/405 [01:49<00:44,  3.28it/s] 64%|██████▍   | 259/405 [01:49<00:43,  3.36it/s] 64%|██████▍   | 260/405 [01:49<00:42,  3.43it/s] 64%|██████▍   | 261/405 [01:49<00:41,  3.47it/s] 65%|██████▍   | 262/405 [01:50<00:40,  3.50it/s] 65%|██████▍   | 263/405 [01:50<00:40,  3.52it/s] 65%|██████▌   | 264/405 [01:50<00:39,  3.54it/s] 65%|██████▌   | 265/405 [01:51<00:39,  3.53it/s] 66%|██████▌   | 266/405 [01:51<00:39,  3.54it/s] 66%|██████▌   | 267/405 [01:51<00:38,  3.55it/s] 66%|██████▌   | 268/405 [01:51<00:38,  3.56it/s] 66%|██████▋   | 269/405 [01:52<00:38,  3.50it/s] 67%|██████▋   | 270/405 [01:52<00:38,  3.50it/s] 67%|██████▋   | 271/405 [01:52<00:38,  3.52it/s] 67%|██████▋   | 272/405 [01:53<00:37,  3.54it/s] 67%|██████▋   | 273/405 [01:53<00:37,  3.55it/s] 68%|██████▊   | 274/405 [01:53<00:36,  3.56it/s] 68%|██████▊   | 275/405 [01:53<00:36,  3.56it/s] 68%|██████▊   | 276/405 [01:54<00:36,  3.56it/s] 68%|██████▊   | 277/405 [01:54<00:35,  3.56it/s] 69%|██████▊   | 278/405 [01:54<00:35,  3.57it/s] 69%|██████▉   | 279/405 [01:55<00:35,  3.57it/s] 69%|██████▉   | 280/405 [01:55<00:35,  3.57it/s] 69%|██████▉   | 281/405 [01:55<00:34,  3.57it/s] 70%|██████▉   | 282/405 [01:55<00:34,  3.57it/s] 70%|██████▉   | 283/405 [01:56<00:34,  3.57it/s] 70%|███████   | 284/405 [01:56<00:33,  3.57it/s] 70%|███████   | 285/405 [01:56<00:33,  3.58it/s] 71%|███████   | 286/405 [01:56<00:33,  3.57it/s] 71%|███████   | 287/405 [01:57<00:33,  3.57it/s] 71%|███████   | 288/405 [01:57<00:32,  3.57it/s] 71%|███████▏  | 289/405 [01:57<00:32,  3.57it/s] 72%|███████▏  | 290/405 [01:58<00:32,  3.54it/s] 72%|███████▏  | 291/405 [01:58<00:32,  3.54it/s] 72%|███████▏  | 292/405 [01:58<00:31,  3.55it/s] 72%|███████▏  | 293/405 [01:58<00:31,  3.56it/s] 73%|███████▎  | 294/405 [01:59<00:31,  3.56it/s] 73%|███████▎  | 295/405 [01:59<00:30,  3.56it/s] 73%|███████▎  | 296/405 [01:59<00:30,  3.57it/s] 73%|███████▎  | 297/405 [02:00<00:30,  3.57it/s] 74%|███████▎  | 298/405 [02:00<00:29,  3.57it/s] 74%|███████▍  | 299/405 [02:00<00:29,  3.57it/s] 74%|███████▍  | 300/405 [02:00<00:29,  3.57it/s] 74%|███████▍  | 301/405 [02:01<00:29,  3.56it/s] 75%|███████▍  | 302/405 [02:01<00:28,  3.56it/s] 75%|███████▍  | 303/405 [02:01<00:28,  3.57it/s] 75%|███████▌  | 304/405 [02:02<00:28,  3.57it/s] 75%|███████▌  | 305/405 [02:02<00:28,  3.57it/s] 76%|███████▌  | 306/405 [02:02<00:27,  3.57it/s] 76%|███████▌  | 307/405 [02:02<00:27,  3.57it/s] 76%|███████▌  | 308/405 [02:03<00:27,  3.57it/s] 76%|███████▋  | 309/405 [02:03<00:26,  3.57it/s] 77%|███████▋  | 310/405 [02:03<00:26,  3.57it/s] 77%|███████▋  | 311/405 [02:04<00:26,  3.57it/s] 77%|███████▋  | 312/405 [02:04<00:26,  3.55it/s] 77%|███████▋  | 313/405 [02:04<00:25,  3.56it/s] 78%|███████▊  | 314/405 [02:04<00:25,  3.56it/s] 78%|███████▊  | 315/405 [02:05<00:25,  3.56it/s] 78%|███████▊  | 316/405 [02:05<00:24,  3.57it/s] 78%|███████▊  | 317/405 [02:05<00:24,  3.57it/s] 79%|███████▊  | 318/405 [02:05<00:24,  3.57it/s] 79%|███████▉  | 319/405 [02:06<00:24,  3.57it/s] 79%|███████▉  | 320/405 [02:06<00:23,  3.57it/s] 79%|███████▉  | 321/405 [02:06<00:23,  3.57it/s] 80%|███████▉  | 322/405 [02:07<00:23,  3.57it/s] 80%|███████▉  | 323/405 [02:07<00:23,  3.56it/s] 80%|████████  | 324/405 [02:07<00:20,  3.91it/s][INFO|trainer.py:2140] 2023-08-28 00:51:22,451 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:51:22,451 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 00:51:22,451 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4797, 'eval_samples_per_second': 367.734, 'eval_steps_per_second': 45.993, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.32it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.85it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.08it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.40it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.03it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.72it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.52it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.00it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.99it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.01it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.08it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.13it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.12it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.10it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.10it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.96it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.76it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.84it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.92it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.89it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.01it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.08it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.11it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.11it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.99it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.93it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.88it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.96it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.95it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.97it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.00it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.06it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.01it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.97it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.82it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.89it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.91it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.90it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.05it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.93it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.01it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.98it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.92it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.92it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.84it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.89it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.88it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.02it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.97it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.97it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.03it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.98it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.95it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.91it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.85it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.91it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.00it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.89it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.94it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.97it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.99it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.95it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.84it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.80it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.94it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.02it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.99it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.00it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.96it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.96it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.96it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.87it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.81it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.81it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.98it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.97it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.98it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.02it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.99it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.04it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.89it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.99it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.98it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.96it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.96it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.97it/s][A                                                 
                                                 [A 80%|████████  | 324/405 [02:17<00:20,  3.91it/s]
100%|██████████| 436/436 [00:09<00:00, 45.97it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:51:31,957 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-324
[INFO|configuration_utils.py:351] 2023-08-28 00:51:31,975 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-324/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:51:34,231 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-324/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:51:34,249 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-324/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:51:34,260 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-324/special_tokens_map.json
 80%|████████  | 325/405 [02:19<05:10,  3.89s/it] 80%|████████  | 326/405 [02:20<03:41,  2.80s/it] 81%|████████  | 327/405 [02:20<02:39,  2.05s/it] 81%|████████  | 328/405 [02:20<01:56,  1.52s/it] 81%|████████  | 329/405 [02:21<01:27,  1.15s/it] 81%|████████▏ | 330/405 [02:21<01:06,  1.13it/s] 82%|████████▏ | 331/405 [02:21<00:52,  1.42it/s] 82%|████████▏ | 332/405 [02:21<00:42,  1.73it/s] 82%|████████▏ | 333/405 [02:22<00:35,  2.05it/s] 82%|████████▏ | 334/405 [02:22<00:30,  2.35it/s] 83%|████████▎ | 335/405 [02:22<00:26,  2.62it/s] 83%|████████▎ | 336/405 [02:22<00:24,  2.85it/s] 83%|████████▎ | 337/405 [02:23<00:22,  3.03it/s] 83%|████████▎ | 338/405 [02:23<00:21,  3.18it/s] 84%|████████▎ | 339/405 [02:23<00:20,  3.29it/s] 84%|████████▍ | 340/405 [02:24<00:19,  3.37it/s] 84%|████████▍ | 341/405 [02:24<00:18,  3.43it/s] 84%|████████▍ | 342/405 [02:24<00:18,  3.47it/s] 85%|████████▍ | 343/405 [02:24<00:17,  3.50it/s] 85%|████████▍ | 344/405 [02:25<00:17,  3.52it/s] 85%|████████▌ | 345/405 [02:25<00:16,  3.54it/s] 85%|████████▌ | 346/405 [02:25<00:16,  3.54it/s] 86%|████████▌ | 347/405 [02:26<00:16,  3.55it/s] 86%|████████▌ | 348/405 [02:26<00:16,  3.56it/s] 86%|████████▌ | 349/405 [02:26<00:15,  3.56it/s] 86%|████████▋ | 350/405 [02:26<00:15,  3.57it/s] 87%|████████▋ | 351/405 [02:27<00:15,  3.57it/s] 87%|████████▋ | 352/405 [02:27<00:14,  3.57it/s] 87%|████████▋ | 353/405 [02:27<00:14,  3.57it/s] 87%|████████▋ | 354/405 [02:28<00:14,  3.57it/s] 88%|████████▊ | 355/405 [02:28<00:13,  3.57it/s] 88%|████████▊ | 356/405 [02:28<00:13,  3.58it/s] 88%|████████▊ | 357/405 [02:28<00:13,  3.57it/s] 88%|████████▊ | 358/405 [02:29<00:13,  3.57it/s] 89%|████████▊ | 359/405 [02:29<00:12,  3.57it/s] 89%|████████▉ | 360/405 [02:29<00:12,  3.57it/s] 89%|████████▉ | 361/405 [02:29<00:12,  3.57it/s] 89%|████████▉ | 362/405 [02:30<00:12,  3.58it/s] 90%|████████▉ | 363/405 [02:30<00:11,  3.57it/s] 90%|████████▉ | 364/405 [02:30<00:11,  3.58it/s] 90%|█████████ | 365/405 [02:31<00:11,  3.58it/s] 90%|█████████ | 366/405 [02:31<00:10,  3.57it/s] 91%|█████████ | 367/405 [02:31<00:10,  3.57it/s] 91%|█████████ | 368/405 [02:31<00:10,  3.57it/s] 91%|█████████ | 369/405 [02:32<00:10,  3.57it/s] 91%|█████████▏| 370/405 [02:32<00:09,  3.57it/s] 92%|█████████▏| 371/405 [02:32<00:09,  3.57it/s] 92%|█████████▏| 372/405 [02:33<00:09,  3.57it/s] 92%|█████████▏| 373/405 [02:33<00:08,  3.57it/s] 92%|█████████▏| 374/405 [02:33<00:08,  3.57it/s] 93%|█████████▎| 375/405 [02:33<00:08,  3.57it/s] 93%|█████████▎| 376/405 [02:34<00:08,  3.57it/s] 93%|█████████▎| 377/405 [02:34<00:07,  3.57it/s] 93%|█████████▎| 378/405 [02:34<00:07,  3.57it/s] 94%|█████████▎| 379/405 [02:35<00:07,  3.57it/s] 94%|█████████▍| 380/405 [02:35<00:07,  3.57it/s] 94%|█████████▍| 381/405 [02:35<00:06,  3.57it/s] 94%|█████████▍| 382/405 [02:35<00:06,  3.57it/s] 95%|█████████▍| 383/405 [02:36<00:06,  3.57it/s] 95%|█████████▍| 384/405 [02:36<00:05,  3.57it/s] 95%|█████████▌| 385/405 [02:36<00:05,  3.57it/s] 95%|█████████▌| 386/405 [02:36<00:05,  3.57it/s] 96%|█████████▌| 387/405 [02:37<00:05,  3.57it/s] 96%|█████████▌| 388/405 [02:37<00:04,  3.57it/s] 96%|█████████▌| 389/405 [02:37<00:04,  3.57it/s] 96%|█████████▋| 390/405 [02:38<00:04,  3.56it/s] 97%|█████████▋| 391/405 [02:38<00:03,  3.57it/s] 97%|█████████▋| 392/405 [02:38<00:03,  3.57it/s] 97%|█████████▋| 393/405 [02:38<00:03,  3.57it/s] 97%|█████████▋| 394/405 [02:39<00:03,  3.57it/s] 98%|█████████▊| 395/405 [02:39<00:02,  3.57it/s] 98%|█████████▊| 396/405 [02:39<00:02,  3.57it/s] 98%|█████████▊| 397/405 [02:40<00:02,  3.57it/s] 98%|█████████▊| 398/405 [02:40<00:01,  3.57it/s] 99%|█████████▊| 399/405 [02:40<00:01,  3.57it/s] 99%|█████████▉| 400/405 [02:40<00:01,  3.57it/s] 99%|█████████▉| 401/405 [02:41<00:01,  3.56it/s] 99%|█████████▉| 402/405 [02:41<00:00,  3.56it/s]100%|█████████▉| 403/405 [02:41<00:00,  3.56it/s]100%|█████████▉| 404/405 [02:42<00:00,  3.56it/s]100%|██████████| 405/405 [02:42<00:00,  3.92it/s][INFO|trainer.py:2140] 2023-08-28 00:51:57,115 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:51:57,115 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 00:51:57,115 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4835, 'eval_samples_per_second': 367.585, 'eval_steps_per_second': 45.974, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.61it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.98it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.13it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.49it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.06it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.79it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.39it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.88it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.94it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.01it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.09it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.05it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.12it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.14it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.16it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.97it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.83it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.79it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.84it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.96it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.08it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.09it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.10it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.02it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.93it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.89it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.81it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.88it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.01it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.97it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.07it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.12it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.08it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.98it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.71it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.73it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.85it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.80it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.80it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.78it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.90it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.85it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.49it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.68it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.67it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.84it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.91it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.99it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.95it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.96it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.02it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.93it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.76it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.85it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.91it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.90it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.01it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.02it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.95it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.98it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.04it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.91it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.85it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.95it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.92it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.02it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.00it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.03it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.94it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.92it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.81it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.79it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.86it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.83it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.93it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.98it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.02it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.99it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.93it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.89it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.92it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.97it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.95it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.96it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.87it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.97it/s][A                                                 
                                                 [A100%|██████████| 405/405 [02:51<00:00,  3.92it/s]
100%|██████████| 436/436 [00:09<00:00, 45.97it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:52:06,616 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-405
[INFO|configuration_utils.py:351] 2023-08-28 00:52:06,632 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-405/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:52:10,915 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-405/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:52:10,937 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-405/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:52:10,946 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-405/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 00:52:11,261 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 00:52:11,261 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81 (score: 1.0161933898925781).
                                                 100%|██████████| 405/405 [02:58<00:00,  3.92it/s]100%|██████████| 405/405 [02:58<00:00,  2.27it/s]
[INFO|trainer.py:1894] 2023-08-28 00:52:13,214 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 00:52:13,233 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:52:15,789 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:52:15,801 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:52:15,811 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:52:15,998 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:15,998 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:15,998 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:15,999 >>   train_runtime            = 0:02:58.33
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:15,999 >>   train_samples            =       5160
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:15,999 >>   train_samples_per_second =    144.674
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:15,999 >>   train_steps_per_second   =      2.271
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4888, 'eval_samples_per_second': 367.38, 'eval_steps_per_second': 45.949, 'epoch': 5.0}
{'train_runtime': 178.3322, 'train_samples_per_second': 144.674, 'train_steps_per_second': 2.271, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 00:52:16 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 00:52:16,031 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:52:16,032 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 00:52:16,032 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 57.00it/s]  3%|▎         | 12/436 [00:00<00:08, 50.24it/s]  4%|▍         | 18/436 [00:00<00:08, 48.47it/s]  5%|▌         | 23/436 [00:00<00:08, 47.76it/s]  6%|▋         | 28/436 [00:00<00:08, 47.32it/s]  8%|▊         | 33/436 [00:00<00:08, 47.03it/s]  9%|▊         | 38/436 [00:00<00:08, 46.84it/s] 10%|▉         | 43/436 [00:00<00:08, 46.59it/s] 11%|█         | 48/436 [00:01<00:08, 46.45it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.44it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.36it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.42it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.31it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.30it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.35it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.26it/s] 20%|██        | 88/436 [00:01<00:07, 46.23it/s] 21%|██▏       | 93/436 [00:01<00:07, 45.83it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.15it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.16it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.13it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.27it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.21it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.31it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.21it/s] 31%|███       | 133/436 [00:02<00:06, 46.11it/s] 32%|███▏      | 138/436 [00:02<00:06, 46.10it/s] 33%|███▎      | 143/436 [00:03<00:06, 46.06it/s] 34%|███▍      | 148/436 [00:03<00:06, 46.31it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.32it/s] 36%|███▌      | 158/436 [00:03<00:06, 46.24it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.26it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.20it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.27it/s] 41%|████      | 178/436 [00:03<00:05, 46.15it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.08it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.07it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.06it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.21it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.20it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.13it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.21it/s] 50%|█████     | 218/436 [00:04<00:04, 46.17it/s] 51%|█████     | 223/436 [00:04<00:04, 46.14it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.07it/s] 53%|█████▎    | 233/436 [00:05<00:04, 45.96it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.00it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.09it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.16it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.13it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.17it/s] 60%|██████    | 263/436 [00:05<00:03, 46.14it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.11it/s] 63%|██████▎   | 273/436 [00:05<00:03, 45.93it/s] 64%|██████▍   | 278/436 [00:05<00:03, 45.97it/s] 65%|██████▍   | 283/436 [00:06<00:03, 46.05it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.17it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.20it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.21it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.18it/s] 71%|███████   | 308/436 [00:06<00:02, 46.15it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.07it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.05it/s] 74%|███████▍  | 323/436 [00:06<00:02, 45.99it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.03it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.08it/s] 78%|███████▊  | 338/436 [00:07<00:02, 45.75it/s] 79%|███████▊  | 343/436 [00:07<00:02, 45.97it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.07it/s] 81%|████████  | 353/436 [00:07<00:01, 46.11it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.10it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.12it/s] 84%|████████▍ | 368/436 [00:07<00:01, 45.91it/s] 86%|████████▌ | 373/436 [00:08<00:01, 45.96it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.03it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.03it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.10it/s] 90%|█████████ | 393/436 [00:08<00:00, 45.82it/s] 91%|█████████▏| 398/436 [00:08<00:00, 45.97it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.01it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.01it/s] 95%|█████████▍| 413/436 [00:08<00:00, 45.93it/s] 96%|█████████▌| 418/436 [00:09<00:00, 46.04it/s] 97%|█████████▋| 423/436 [00:09<00:00, 46.15it/s] 98%|█████████▊| 428/436 [00:09<00:00, 46.05it/s] 99%|█████████▉| 433/436 [00:09<00:00, 45.73it/s]100%|██████████| 436/436 [00:09<00:00, 46.22it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:52:25,489 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:25,489 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:25,489 >>   eval_loss               =     1.0162
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:25,489 >>   eval_runtime            = 0:00:09.45
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:25,489 >>   eval_samples            =       3486
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:25,489 >>   eval_samples_per_second =    368.606
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:25,489 >>   eval_steps_per_second   =     46.102
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:52:25,489 >>   perplexity              =     2.7627
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:31,163 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:31,168 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:31,168 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:31,168 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:31,168 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:52:31,475 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:52:31,476 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:52:32,153 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:52:33,162 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:52:33,162 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:35,344 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:35,349 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:35,349 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:35,349 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:35,349 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:52:35,668 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:52:35,670 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:52:35,938 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:52:36,097 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:52:36,097 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-162
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-81
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-405
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-243
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/checkpoint-324
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'manufacturer', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13015
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13115, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.17it/s]Extractor Predicting: 2it [00:01,  1.20it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.25it/s]Extractor Predicting: 5it [00:04,  1.25it/s]Extractor Predicting: 6it [00:04,  1.25it/s]Extractor Predicting: 7it [00:05,  1.24it/s]Extractor Predicting: 8it [00:06,  1.27it/s]Extractor Predicting: 9it [00:07,  1.27it/s]Extractor Predicting: 10it [00:07,  1.29it/s]Extractor Predicting: 11it [00:08,  1.26it/s]Extractor Predicting: 12it [00:09,  1.26it/s]Extractor Predicting: 13it [00:10,  1.26it/s]Extractor Predicting: 14it [00:11,  1.24it/s]Extractor Predicting: 15it [00:11,  1.27it/s]Extractor Predicting: 16it [00:12,  1.26it/s]Extractor Predicting: 17it [00:13,  1.30it/s]Extractor Predicting: 18it [00:14,  1.30it/s]Extractor Predicting: 19it [00:15,  1.29it/s]Extractor Predicting: 20it [00:15,  1.28it/s]Extractor Predicting: 21it [00:16,  1.29it/s]Extractor Predicting: 22it [00:17,  1.31it/s]Extractor Predicting: 23it [00:18,  1.31it/s]Extractor Predicting: 24it [00:18,  1.27it/s]Extractor Predicting: 25it [00:19,  1.27it/s]Extractor Predicting: 26it [00:20,  1.24it/s]Extractor Predicting: 27it [00:21,  1.26it/s]Extractor Predicting: 28it [00:22,  1.27it/s]Extractor Predicting: 29it [00:22,  1.25it/s]Extractor Predicting: 30it [00:23,  1.22it/s]Extractor Predicting: 31it [00:24,  1.18it/s]Extractor Predicting: 32it [00:25,  1.20it/s]Extractor Predicting: 33it [00:26,  1.21it/s]Extractor Predicting: 34it [00:27,  1.23it/s]Extractor Predicting: 35it [00:27,  1.25it/s]Extractor Predicting: 36it [00:28,  1.27it/s]Extractor Predicting: 37it [00:29,  1.28it/s]Extractor Predicting: 38it [00:30,  1.32it/s]Extractor Predicting: 39it [00:30,  1.32it/s]Extractor Predicting: 40it [00:31,  1.34it/s]Extractor Predicting: 41it [00:32,  1.33it/s]Extractor Predicting: 42it [00:33,  1.32it/s]Extractor Predicting: 43it [00:33,  1.30it/s]Extractor Predicting: 44it [00:34,  1.28it/s]Extractor Predicting: 45it [00:35,  1.33it/s]Extractor Predicting: 46it [00:36,  1.31it/s]Extractor Predicting: 47it [00:36,  1.33it/s]Extractor Predicting: 48it [00:37,  1.32it/s]Extractor Predicting: 49it [00:38,  1.34it/s]Extractor Predicting: 50it [00:39,  1.32it/s]Extractor Predicting: 51it [00:39,  1.31it/s]Extractor Predicting: 52it [00:40,  1.30it/s]Extractor Predicting: 53it [00:41,  1.30it/s]Extractor Predicting: 54it [00:42,  1.28it/s]Extractor Predicting: 55it [00:43,  1.33it/s]Extractor Predicting: 56it [00:43,  1.34it/s]Extractor Predicting: 57it [00:44,  1.31it/s]Extractor Predicting: 58it [00:45,  1.31it/s]Extractor Predicting: 59it [00:46,  1.32it/s]Extractor Predicting: 60it [00:46,  1.30it/s]Extractor Predicting: 61it [00:47,  1.29it/s]Extractor Predicting: 62it [00:48,  1.29it/s]Extractor Predicting: 63it [00:49,  1.30it/s]Extractor Predicting: 64it [00:49,  1.30it/s]Extractor Predicting: 65it [00:50,  1.30it/s]Extractor Predicting: 66it [00:51,  1.31it/s]Extractor Predicting: 67it [00:52,  1.34it/s]Extractor Predicting: 68it [00:52,  1.30it/s]Extractor Predicting: 69it [00:53,  1.30it/s]Extractor Predicting: 70it [00:54,  1.30it/s]Extractor Predicting: 71it [00:55,  1.32it/s]Extractor Predicting: 72it [00:55,  1.33it/s]Extractor Predicting: 73it [00:56,  1.32it/s]Extractor Predicting: 74it [00:57,  1.33it/s]Extractor Predicting: 75it [00:58,  1.30it/s]Extractor Predicting: 76it [00:59,  1.31it/s]Extractor Predicting: 77it [00:59,  1.30it/s]Extractor Predicting: 78it [01:00,  1.31it/s]Extractor Predicting: 79it [01:01,  1.31it/s]Extractor Predicting: 80it [01:02,  1.32it/s]Extractor Predicting: 81it [01:02,  1.31it/s]Extractor Predicting: 82it [01:03,  1.31it/s]Extractor Predicting: 83it [01:04,  1.27it/s]Extractor Predicting: 84it [01:05,  1.29it/s]Extractor Predicting: 85it [01:06,  1.28it/s]Extractor Predicting: 86it [01:06,  1.29it/s]Extractor Predicting: 87it [01:07,  1.28it/s]Extractor Predicting: 88it [01:08,  1.32it/s]Extractor Predicting: 89it [01:09,  1.33it/s]Extractor Predicting: 90it [01:09,  1.37it/s]Extractor Predicting: 91it [01:10,  1.39it/s]Extractor Predicting: 92it [01:11,  1.41it/s]Extractor Predicting: 93it [01:11,  1.37it/s]Extractor Predicting: 94it [01:12,  1.30it/s]Extractor Predicting: 95it [01:13,  1.33it/s]Extractor Predicting: 96it [01:14,  1.34it/s]Extractor Predicting: 97it [01:14,  1.35it/s]Extractor Predicting: 98it [01:15,  1.33it/s]Extractor Predicting: 99it [01:16,  1.31it/s]Extractor Predicting: 100it [01:17,  1.32it/s]Extractor Predicting: 101it [01:17,  1.37it/s]Extractor Predicting: 102it [01:18,  1.40it/s]Extractor Predicting: 103it [01:19,  1.39it/s]Extractor Predicting: 104it [01:20,  1.38it/s]Extractor Predicting: 105it [01:20,  1.39it/s]Extractor Predicting: 106it [01:21,  1.39it/s]Extractor Predicting: 107it [01:22,  1.38it/s]Extractor Predicting: 108it [01:22,  1.37it/s]Extractor Predicting: 109it [01:23,  1.39it/s]Extractor Predicting: 110it [01:24,  1.38it/s]Extractor Predicting: 111it [01:25,  1.39it/s]Extractor Predicting: 112it [01:25,  1.44it/s]Extractor Predicting: 113it [01:26,  1.45it/s]Extractor Predicting: 114it [01:27,  1.42it/s]Extractor Predicting: 115it [01:27,  1.41it/s]Extractor Predicting: 116it [01:28,  1.41it/s]Extractor Predicting: 117it [01:29,  1.41it/s]Extractor Predicting: 118it [01:29,  1.44it/s]Extractor Predicting: 119it [01:30,  1.42it/s]Extractor Predicting: 120it [01:31,  1.40it/s]Extractor Predicting: 121it [01:32,  1.46it/s]Extractor Predicting: 122it [01:32,  1.44it/s]Extractor Predicting: 123it [01:33,  1.38it/s]Extractor Predicting: 124it [01:34,  1.42it/s]Extractor Predicting: 125it [01:34,  1.42it/s]Extractor Predicting: 126it [01:35,  1.47it/s]Extractor Predicting: 127it [01:36,  1.46it/s]Extractor Predicting: 128it [01:36,  1.45it/s]Extractor Predicting: 129it [01:37,  1.41it/s]Extractor Predicting: 130it [01:38,  1.43it/s]Extractor Predicting: 131it [01:38,  1.47it/s]Extractor Predicting: 132it [01:39,  1.45it/s]Extractor Predicting: 133it [01:40,  1.42it/s]Extractor Predicting: 134it [01:41,  1.45it/s]Extractor Predicting: 135it [01:41,  1.45it/s]Extractor Predicting: 136it [01:42,  1.47it/s]Extractor Predicting: 137it [01:43,  1.44it/s]Extractor Predicting: 138it [01:43,  1.49it/s]Extractor Predicting: 139it [01:44,  1.51it/s]Extractor Predicting: 140it [01:45,  1.51it/s]Extractor Predicting: 141it [01:45,  1.54it/s]Extractor Predicting: 142it [01:46,  1.50it/s]Extractor Predicting: 143it [01:47,  1.47it/s]Extractor Predicting: 144it [01:47,  1.82it/s]Extractor Predicting: 144it [01:47,  1.34it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:54:30,972 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:54:30,976 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:54:30,976 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:54:30,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:54:30,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:54:31,561 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:54:31,562 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:54:32,121 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:54:33,142 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:54:33,142 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:54:36,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:54:36,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:54:36,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:54:36,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:54:36,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:54:36,658 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:54:36,659 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:54:37,239 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:54:37,377 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:54:37,377 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13522
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13622, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.34it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:04,  1.37it/s]Extractor Predicting: 7it [00:05,  1.37it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.40it/s]Extractor Predicting: 10it [00:07,  1.43it/s]Extractor Predicting: 11it [00:07,  1.40it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:09,  1.38it/s]Extractor Predicting: 14it [00:10,  1.41it/s]Extractor Predicting: 15it [00:10,  1.40it/s]Extractor Predicting: 16it [00:11,  1.37it/s]Extractor Predicting: 17it [00:12,  1.37it/s]Extractor Predicting: 18it [00:13,  1.36it/s]Extractor Predicting: 19it [00:13,  1.38it/s]Extractor Predicting: 20it [00:14,  1.36it/s]Extractor Predicting: 21it [00:15,  1.38it/s]Extractor Predicting: 22it [00:15,  1.38it/s]Extractor Predicting: 23it [00:16,  1.36it/s]Extractor Predicting: 24it [00:17,  1.35it/s]Extractor Predicting: 25it [00:18,  1.36it/s]Extractor Predicting: 26it [00:18,  1.35it/s]Extractor Predicting: 27it [00:19,  1.35it/s]Extractor Predicting: 28it [00:20,  1.37it/s]Extractor Predicting: 29it [00:21,  1.37it/s]Extractor Predicting: 30it [00:21,  1.34it/s]Extractor Predicting: 31it [00:22,  1.36it/s]Extractor Predicting: 32it [00:23,  1.34it/s]Extractor Predicting: 33it [00:24,  1.34it/s]Extractor Predicting: 34it [00:24,  1.37it/s]Extractor Predicting: 35it [00:25,  1.35it/s]Extractor Predicting: 36it [00:26,  1.32it/s]Extractor Predicting: 37it [00:27,  1.31it/s]Extractor Predicting: 38it [00:27,  1.31it/s]Extractor Predicting: 39it [00:28,  1.31it/s]Extractor Predicting: 40it [00:29,  1.30it/s]Extractor Predicting: 41it [00:30,  1.31it/s]Extractor Predicting: 42it [00:30,  1.32it/s]Extractor Predicting: 43it [00:31,  1.32it/s]Extractor Predicting: 44it [00:32,  1.31it/s]Extractor Predicting: 45it [00:33,  1.32it/s]Extractor Predicting: 46it [00:33,  1.33it/s]Extractor Predicting: 47it [00:34,  1.35it/s]Extractor Predicting: 48it [00:35,  1.35it/s]Extractor Predicting: 49it [00:36,  1.37it/s]Extractor Predicting: 50it [00:36,  1.37it/s]Extractor Predicting: 51it [00:37,  1.40it/s]Extractor Predicting: 52it [00:38,  1.41it/s]Extractor Predicting: 53it [00:38,  1.40it/s]Extractor Predicting: 54it [00:39,  1.40it/s]Extractor Predicting: 55it [00:40,  1.39it/s]Extractor Predicting: 56it [00:41,  1.37it/s]Extractor Predicting: 57it [00:41,  1.32it/s]Extractor Predicting: 58it [00:42,  1.30it/s]Extractor Predicting: 59it [00:43,  1.31it/s]Extractor Predicting: 60it [00:44,  1.33it/s]Extractor Predicting: 61it [00:45,  1.31it/s]Extractor Predicting: 62it [00:45,  1.29it/s]Extractor Predicting: 63it [00:46,  1.35it/s]Extractor Predicting: 64it [00:47,  1.33it/s]Extractor Predicting: 65it [00:47,  1.36it/s]Extractor Predicting: 66it [00:48,  1.34it/s]Extractor Predicting: 67it [00:49,  1.34it/s]Extractor Predicting: 68it [00:50,  1.32it/s]Extractor Predicting: 69it [00:51,  1.31it/s]Extractor Predicting: 70it [00:51,  1.26it/s]Extractor Predicting: 71it [00:52,  1.27it/s]Extractor Predicting: 72it [00:53,  1.28it/s]Extractor Predicting: 73it [00:54,  1.28it/s]Extractor Predicting: 74it [00:54,  1.29it/s]Extractor Predicting: 75it [00:55,  1.27it/s]Extractor Predicting: 76it [00:56,  1.30it/s]Extractor Predicting: 77it [00:57,  1.27it/s]Extractor Predicting: 78it [00:58,  1.28it/s]Extractor Predicting: 79it [00:58,  1.29it/s]Extractor Predicting: 80it [00:59,  1.29it/s]Extractor Predicting: 81it [01:00,  1.29it/s]Extractor Predicting: 82it [01:01,  1.26it/s]Extractor Predicting: 83it [01:02,  1.26it/s]Extractor Predicting: 84it [01:02,  1.29it/s]Extractor Predicting: 85it [01:03,  1.30it/s]Extractor Predicting: 86it [01:04,  1.30it/s]Extractor Predicting: 87it [01:05,  1.28it/s]Extractor Predicting: 88it [01:05,  1.34it/s]Extractor Predicting: 89it [01:06,  1.33it/s]Extractor Predicting: 90it [01:07,  1.31it/s]Extractor Predicting: 91it [01:07,  1.37it/s]Extractor Predicting: 92it [01:08,  1.37it/s]Extractor Predicting: 93it [01:09,  1.39it/s]Extractor Predicting: 94it [01:10,  1.37it/s]Extractor Predicting: 95it [01:10,  1.36it/s]Extractor Predicting: 96it [01:11,  1.36it/s]Extractor Predicting: 97it [01:12,  1.34it/s]Extractor Predicting: 98it [01:13,  1.33it/s]Extractor Predicting: 99it [01:13,  1.33it/s]Extractor Predicting: 100it [01:14,  1.32it/s]Extractor Predicting: 101it [01:15,  1.32it/s]Extractor Predicting: 102it [01:16,  1.27it/s]Extractor Predicting: 103it [01:17,  1.29it/s]Extractor Predicting: 104it [01:17,  1.30it/s]Extractor Predicting: 105it [01:18,  1.33it/s]Extractor Predicting: 106it [01:19,  1.33it/s]Extractor Predicting: 107it [01:20,  1.23it/s]Extractor Predicting: 108it [01:21,  1.26it/s]Extractor Predicting: 109it [01:21,  1.28it/s]Extractor Predicting: 110it [01:22,  1.28it/s]Extractor Predicting: 111it [01:23,  1.30it/s]Extractor Predicting: 112it [01:24,  1.30it/s]Extractor Predicting: 113it [01:24,  1.33it/s]Extractor Predicting: 114it [01:25,  1.32it/s]Extractor Predicting: 115it [01:26,  1.34it/s]Extractor Predicting: 116it [01:27,  1.29it/s]Extractor Predicting: 117it [01:27,  1.27it/s]Extractor Predicting: 118it [01:28,  1.27it/s]Extractor Predicting: 119it [01:29,  1.26it/s]Extractor Predicting: 120it [01:30,  1.26it/s]Extractor Predicting: 121it [01:31,  1.27it/s]Extractor Predicting: 122it [01:31,  1.30it/s]Extractor Predicting: 123it [01:32,  1.29it/s]Extractor Predicting: 124it [01:33,  1.27it/s]Extractor Predicting: 125it [01:34,  1.26it/s]Extractor Predicting: 126it [01:35,  1.23it/s]Extractor Predicting: 127it [01:35,  1.24it/s]Extractor Predicting: 128it [01:36,  1.24it/s]Extractor Predicting: 129it [01:37,  1.23it/s]Extractor Predicting: 130it [01:38,  1.26it/s]Extractor Predicting: 131it [01:39,  1.27it/s]Extractor Predicting: 132it [01:39,  1.26it/s]Extractor Predicting: 133it [01:40,  1.26it/s]Extractor Predicting: 134it [01:41,  1.27it/s]Extractor Predicting: 135it [01:42,  1.29it/s]Extractor Predicting: 136it [01:42,  1.32it/s]Extractor Predicting: 137it [01:43,  1.34it/s]Extractor Predicting: 138it [01:44,  1.33it/s]Extractor Predicting: 139it [01:45,  1.34it/s]Extractor Predicting: 140it [01:45,  1.32it/s]Extractor Predicting: 141it [01:46,  1.28it/s]Extractor Predicting: 142it [01:47,  1.28it/s]Extractor Predicting: 143it [01:48,  1.39it/s]Extractor Predicting: 143it [01:48,  1.32it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:56:32,073 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:56:32,079 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:56:32,079 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:56:32,079 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:56:32,079 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:56:32,663 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:56:32,664 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:56:33,218 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:56:34,271 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:56:34,272 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:56:37,325 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:56:37,327 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:56:37,327 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:56:37,327 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:56:37,327 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:56:37,954 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:56:37,955 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:56:38,516 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:56:38,671 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:56:38,671 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 464
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 564, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.05it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 2it [00:01,  1.38it/s]
[INFO|configuration_utils.py:515] 2023-08-28 00:56:40,458 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:56:40,459 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:56:40,463 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:56:40,464 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 00:56:40,468 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:56:43,630 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 00:56:43,630 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 00:56:43,650 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:56:43,651 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:56:43,661 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:56:43,664 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:56:43,664 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:56:43,664 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:56:43,664 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:56:43,664 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:56:43,664 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 00:56:43,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:44,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:45,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:46,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:47,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:48,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:49,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:50,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:51,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:52,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:53,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:54,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:55,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:56,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:57,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:58,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:56:59,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:00,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:01,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:02,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:02,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:03,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:04,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:05,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:06,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:07,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:08,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:09,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:10,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:27<04:09, 27.71s/it][WARNING|generation_utils.py:914] 2023-08-28 00:57:11,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:12,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:13,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:14,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:15,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:16,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:17,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:18,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:18,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:19,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:20,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:22,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:22,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:24,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:25,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:26,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:27,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:28,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:29,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:30,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:31,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:32,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:33,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:34,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:35,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:52<03:25, 25.74s/it][WARNING|generation_utils.py:914] 2023-08-28 00:57:35,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:37,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:37,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:39,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:40,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:41,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:42,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:43,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:44,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:45,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:46,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:47,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:48,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:49,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:50,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:51,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:52,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:53,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:53,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:54,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:55,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:56,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:57,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:58,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:57:59,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:16<02:55, 25.11s/it][WARNING|generation_utils.py:914] 2023-08-28 00:58:00,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:01,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:02,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:03,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:03,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:04,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:05,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:06,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:07,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:08,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:09,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:10,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:11,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:12,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:13,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:14,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:15,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:16,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:17,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:18,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:19,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:20,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:37<02:20, 23.48s/it][WARNING|generation_utils.py:914] 2023-08-28 00:58:21,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:22,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:23,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:23,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:25,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:26,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:27,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:28,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:29,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:30,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:30,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:31,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:32,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:33,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:34,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:35,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:36,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:37,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:37,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:38,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:39,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:40,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:41,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:42,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:43,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:44,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:45,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:46,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:47,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:48,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:49,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:50,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:51,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:52,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:53,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [02:10<02:14, 26.84s/it][WARNING|generation_utils.py:914] 2023-08-28 00:58:54,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:55,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:56,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:57,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:58,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:58:59,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:00,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:01,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:02,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:03,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:04,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:05,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:06,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:07,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:08,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:09,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:10,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:10,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:11,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:12,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:14,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:14,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:15,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:16,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:33<01:43, 25.76s/it][WARNING|generation_utils.py:914] 2023-08-28 00:59:17,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:18,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:19,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:20,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:21,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:22,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:23,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:24,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:25,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:26,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:27,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:28,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:29,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:30,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:31,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:32,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:33,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:34,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:35,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:36,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:37,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:39,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:40,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:57<01:14, 24.97s/it][WARNING|generation_utils.py:914] 2023-08-28 00:59:41,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:42,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:43,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:44,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:44,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:45,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:46,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:47,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:49,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:50,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:51,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:52,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:53,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:54,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:55,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:56,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:57,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:58,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:59:59,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:00,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:01,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:02,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:03,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [03:20<00:48, 24.44s/it][WARNING|generation_utils.py:914] 2023-08-28 01:00:04,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:05,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:06,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:07,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:08,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:09,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:10,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:11,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:12,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:13,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:14,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:15,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:16,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:17,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:18,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:19,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:20,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:21,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:22,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:23,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:24,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:26,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:43<00:23, 23.92s/it][WARNING|generation_utils.py:914] 2023-08-28 01:00:27,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:28,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:29,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:30,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:31,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:32,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:33,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:34,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:35,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:36,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:37,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:38,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:39,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:40,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:41,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:42,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:43,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:44,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:45,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:46,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:47,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:48,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:00:49,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [04:06<00:00, 23.77s/it]Generating: 100%|██████████| 10/10 [04:06<00:00, 24.67s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:56,684 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:56,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:56,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:56,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:56,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:00:57,282 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:00:57,283 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:00:57,854 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:00:58,914 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:00:58,914 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:02,112 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:02,117 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:02,117 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:02,117 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:01:02,117 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:01:02,734 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:01:02,735 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:01:03,312 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:01:03,467 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:01:03,467 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 188, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 225, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 268, 'raw': 416}
{'target': 600, 'success': 289, 'raw': 448}
{'target': 600, 'success': 309, 'raw': 480}
{'target': 600, 'success': 337, 'raw': 512}
{'target': 600, 'success': 361, 'raw': 544}
{'target': 600, 'success': 378, 'raw': 576}
{'target': 600, 'success': 401, 'raw': 608}
{'target': 600, 'success': 424, 'raw': 640}
{'target': 600, 'success': 446, 'raw': 672}
{'target': 600, 'success': 465, 'raw': 704}
{'target': 600, 'success': 486, 'raw': 736}
{'target': 600, 'success': 502, 'raw': 768}
{'target': 600, 'success': 528, 'raw': 800}
{'target': 600, 'success': 549, 'raw': 832}
{'target': 600, 'success': 570, 'raw': 864}
{'target': 600, 'success': 589, 'raw': 896}
{'target': 600, 'success': 610, 'raw': 928}
{'prompt': 'Relation : country .', 'success_rate': 0.6573275862068966, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 499, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 547, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : genre . Context : Later in 2003 , he played in Steven Spielberg 's The Bourne Ultimatum . Head Entity : The Bourne Ultimatum , Tail Entity : director .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 604, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.755, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : voice type . Context : Later in the year , the band members decided to cancel their performance of " In My Time " on Broadway , in part to play the first time . Head Entity : In My Time , Tail Entity : vocal .\n']
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 102, 'raw': 160}
{'target': 600, 'success': 118, 'raw': 192}
{'target': 600, 'success': 134, 'raw': 224}
{'target': 600, 'success': 146, 'raw': 256}
{'target': 600, 'success': 160, 'raw': 288}
{'target': 600, 'success': 178, 'raw': 320}
{'target': 600, 'success': 193, 'raw': 352}
{'target': 600, 'success': 215, 'raw': 384}
{'target': 600, 'success': 233, 'raw': 416}
{'target': 600, 'success': 245, 'raw': 448}
{'target': 600, 'success': 262, 'raw': 480}
{'target': 600, 'success': 282, 'raw': 512}
{'target': 600, 'success': 304, 'raw': 544}
{'target': 600, 'success': 318, 'raw': 576}
{'target': 600, 'success': 336, 'raw': 608}
{'target': 600, 'success': 355, 'raw': 640}
{'target': 600, 'success': 375, 'raw': 672}
{'target': 600, 'success': 394, 'raw': 704}
{'target': 600, 'success': 413, 'raw': 736}
{'target': 600, 'success': 431, 'raw': 768}
{'target': 600, 'success': 450, 'raw': 800}
{'target': 600, 'success': 466, 'raw': 832}
{'target': 600, 'success': 480, 'raw': 864}
{'target': 600, 'success': 495, 'raw': 896}
{'target': 600, 'success': 511, 'raw': 928}
{'target': 600, 'success': 529, 'raw': 960}
{'target': 600, 'success': 546, 'raw': 992}
{'target': 600, 'success': 561, 'raw': 1024}
{'target': 600, 'success': 580, 'raw': 1056}
{'target': 600, 'success': 596, 'raw': 1088}
{'target': 600, 'success': 612, 'raw': 1120}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5464285714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : contains administrative territorial entity . Context : The city of Marchec is under the rule of the Marchec Municipal Municipality in Romania . Head Entity : Marchec , Tail Entity : Romanian .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8233695652173914, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8707386363636364, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 12301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.09it/s]Extractor Estimating: 2it [00:01,  1.13it/s]Extractor Estimating: 3it [00:02,  1.10it/s]Extractor Estimating: 4it [00:03,  1.17it/s]Extractor Estimating: 5it [00:04,  1.19it/s]Extractor Estimating: 6it [00:05,  1.20it/s]Extractor Estimating: 7it [00:05,  1.19it/s]Extractor Estimating: 8it [00:06,  1.23it/s]Extractor Estimating: 9it [00:07,  1.24it/s]Extractor Estimating: 10it [00:08,  1.24it/s]Extractor Estimating: 11it [00:09,  1.22it/s]Extractor Estimating: 12it [00:10,  1.21it/s]Extractor Estimating: 13it [00:10,  1.25it/s]Extractor Estimating: 14it [00:11,  1.29it/s]Extractor Estimating: 15it [00:12,  1.21it/s]Extractor Estimating: 16it [00:13,  1.21it/s]Extractor Estimating: 17it [00:14,  1.16it/s]Extractor Estimating: 18it [00:15,  1.17it/s]Extractor Estimating: 19it [00:15,  1.17it/s]Extractor Estimating: 20it [00:16,  1.20it/s]Extractor Estimating: 21it [00:17,  1.22it/s]Extractor Estimating: 22it [00:18,  1.22it/s]Extractor Estimating: 23it [00:19,  1.24it/s]Extractor Estimating: 24it [00:19,  1.25it/s]Extractor Estimating: 25it [00:20,  1.26it/s]Extractor Estimating: 26it [00:21,  1.22it/s]Extractor Estimating: 27it [00:22,  1.22it/s]Extractor Estimating: 28it [00:23,  1.23it/s]Extractor Estimating: 29it [00:23,  1.23it/s]Extractor Estimating: 30it [00:24,  1.21it/s]Extractor Estimating: 31it [00:25,  1.22it/s]Extractor Estimating: 32it [00:26,  1.24it/s]Extractor Estimating: 33it [00:27,  1.24it/s]Extractor Estimating: 34it [00:27,  1.25it/s]Extractor Estimating: 35it [00:28,  1.22it/s]Extractor Estimating: 36it [00:29,  1.23it/s]Extractor Estimating: 37it [00:30,  1.18it/s]Extractor Estimating: 38it [00:31,  1.20it/s]Extractor Estimating: 39it [00:32,  1.19it/s]Extractor Estimating: 40it [00:32,  1.21it/s]Extractor Estimating: 41it [00:33,  1.17it/s]Extractor Estimating: 42it [00:34,  1.20it/s]Extractor Estimating: 43it [00:35,  1.21it/s]Extractor Estimating: 44it [00:36,  1.24it/s]Extractor Estimating: 45it [00:37,  1.23it/s]Extractor Estimating: 46it [00:37,  1.22it/s]Extractor Estimating: 47it [00:38,  1.25it/s]Extractor Estimating: 48it [00:39,  1.24it/s]Extractor Estimating: 49it [00:40,  1.24it/s]Extractor Estimating: 50it [00:41,  1.25it/s]Extractor Estimating: 51it [00:41,  1.26it/s]Extractor Estimating: 52it [00:42,  1.25it/s]Extractor Estimating: 53it [00:43,  1.25it/s]Extractor Estimating: 54it [00:44,  1.05it/s]Extractor Estimating: 55it [00:45,  1.10it/s]Extractor Estimating: 56it [00:46,  1.11it/s]Extractor Estimating: 57it [00:47,  1.10it/s]Extractor Estimating: 58it [00:48,  1.16it/s]Extractor Estimating: 59it [00:48,  1.18it/s]Extractor Estimating: 60it [00:49,  1.18it/s]Extractor Estimating: 61it [00:50,  1.12it/s]Extractor Estimating: 62it [00:51,  1.18it/s]Extractor Estimating: 63it [00:52,  1.18it/s]Extractor Estimating: 64it [00:53,  1.17it/s]Extractor Estimating: 65it [00:54,  1.17it/s]Extractor Estimating: 66it [00:55,  1.16it/s]Extractor Estimating: 67it [00:55,  1.19it/s]Extractor Estimating: 68it [00:56,  1.22it/s]Extractor Estimating: 69it [00:57,  1.22it/s]Extractor Estimating: 70it [00:58,  1.22it/s]Extractor Estimating: 71it [00:59,  1.22it/s]Extractor Estimating: 72it [00:59,  1.20it/s]Extractor Estimating: 73it [01:00,  1.19it/s]Extractor Estimating: 74it [01:01,  1.21it/s]Extractor Estimating: 75it [01:02,  1.23it/s]Extractor Estimating: 76it [01:03,  1.26it/s]Extractor Estimating: 77it [01:03,  1.24it/s]Extractor Estimating: 78it [01:04,  1.22it/s]Extractor Estimating: 79it [01:05,  1.23it/s]Extractor Estimating: 80it [01:06,  1.26it/s]Extractor Estimating: 81it [01:07,  1.25it/s]Extractor Estimating: 82it [01:07,  1.27it/s]Extractor Estimating: 83it [01:08,  1.31it/s]Extractor Estimating: 84it [01:09,  1.28it/s]Extractor Estimating: 85it [01:10,  1.28it/s]Extractor Estimating: 86it [01:11,  1.23it/s]Extractor Estimating: 87it [01:11,  1.27it/s]Extractor Estimating: 88it [01:12,  1.25it/s]Extractor Estimating: 89it [01:13,  1.28it/s]Extractor Estimating: 90it [01:14,  1.21it/s]Extractor Estimating: 91it [01:15,  1.11it/s]Extractor Estimating: 92it [01:16,  1.15it/s]Extractor Estimating: 93it [01:17,  1.16it/s]Extractor Estimating: 94it [01:17,  1.16it/s]Extractor Estimating: 95it [01:18,  1.17it/s]Extractor Estimating: 96it [01:19,  1.23it/s]Extractor Estimating: 97it [01:20,  1.18it/s]Extractor Estimating: 98it [01:21,  1.22it/s]Extractor Estimating: 99it [01:21,  1.23it/s]Extractor Estimating: 100it [01:22,  1.26it/s]Extractor Estimating: 101it [01:23,  1.23it/s]Extractor Estimating: 102it [01:24,  1.22it/s]Extractor Estimating: 103it [01:25,  1.19it/s]Extractor Estimating: 104it [01:26,  1.19it/s]Extractor Estimating: 105it [01:26,  1.24it/s]Extractor Estimating: 106it [01:27,  1.29it/s]Extractor Estimating: 107it [01:28,  1.28it/s]Extractor Estimating: 108it [01:29,  1.27it/s]Extractor Estimating: 109it [01:29,  1.28it/s]Extractor Estimating: 110it [01:30,  1.24it/s]Extractor Estimating: 111it [01:31,  1.27it/s]Extractor Estimating: 112it [01:32,  1.27it/s]Extractor Estimating: 113it [01:33,  1.30it/s]Extractor Estimating: 114it [01:33,  1.30it/s]Extractor Estimating: 115it [01:34,  1.28it/s]Extractor Estimating: 116it [01:35,  1.21it/s]Extractor Estimating: 117it [01:36,  1.20it/s]Extractor Estimating: 118it [01:37,  1.19it/s]Extractor Estimating: 119it [01:38,  1.21it/s]Extractor Estimating: 120it [01:38,  1.18it/s]Extractor Estimating: 121it [01:39,  1.20it/s]Extractor Estimating: 122it [01:40,  1.18it/s]Extractor Estimating: 123it [01:41,  1.22it/s]Extractor Estimating: 124it [01:42,  1.24it/s]Extractor Estimating: 125it [01:43,  1.21it/s]Extractor Estimating: 126it [01:43,  1.20it/s]Extractor Estimating: 127it [01:44,  1.20it/s]Extractor Estimating: 128it [01:45,  1.23it/s]Extractor Estimating: 129it [01:46,  1.26it/s]Extractor Estimating: 130it [01:46,  1.27it/s]Extractor Estimating: 131it [01:47,  1.27it/s]Extractor Estimating: 132it [01:48,  1.25it/s]Extractor Estimating: 133it [01:49,  1.28it/s]Extractor Estimating: 134it [01:50,  1.27it/s]Extractor Estimating: 135it [01:50,  1.28it/s]Extractor Estimating: 136it [01:51,  1.29it/s]Extractor Estimating: 137it [01:52,  1.24it/s]Extractor Estimating: 138it [01:53,  1.28it/s]Extractor Estimating: 139it [01:54,  1.24it/s]Extractor Estimating: 140it [01:54,  1.27it/s]Extractor Estimating: 141it [01:55,  1.25it/s]Extractor Estimating: 142it [01:56,  1.26it/s]Extractor Estimating: 143it [01:57,  1.26it/s]Extractor Estimating: 144it [01:58,  1.28it/s]Extractor Estimating: 145it [01:58,  1.30it/s]Extractor Estimating: 146it [01:59,  1.27it/s]Extractor Estimating: 147it [02:00,  1.26it/s]Extractor Estimating: 148it [02:01,  1.26it/s]Extractor Estimating: 149it [02:02,  1.25it/s]Extractor Estimating: 150it [02:02,  1.24it/s]Extractor Estimating: 151it [02:03,  1.23it/s]Extractor Estimating: 152it [02:04,  1.25it/s]Extractor Estimating: 153it [02:05,  1.25it/s]Extractor Estimating: 154it [02:06,  1.24it/s]Extractor Estimating: 155it [02:06,  1.23it/s]Extractor Estimating: 156it [02:07,  1.21it/s]Extractor Estimating: 157it [02:08,  1.17it/s]Extractor Estimating: 158it [02:09,  1.22it/s]Extractor Estimating: 159it [02:10,  1.23it/s]Extractor Estimating: 160it [02:11,  1.23it/s]Extractor Estimating: 161it [02:11,  1.17it/s]Extractor Estimating: 162it [02:12,  1.16it/s]Extractor Estimating: 163it [02:13,  1.19it/s]Extractor Estimating: 164it [02:14,  1.17it/s]Extractor Estimating: 165it [02:15,  1.14it/s]Extractor Estimating: 166it [02:16,  1.16it/s]Extractor Estimating: 167it [02:17,  1.16it/s]Extractor Estimating: 168it [02:18,  1.16it/s]Extractor Estimating: 169it [02:18,  1.12it/s]Extractor Estimating: 170it [02:19,  1.16it/s]Extractor Estimating: 171it [02:20,  1.15it/s]Extractor Estimating: 172it [02:21,  1.16it/s]Extractor Estimating: 173it [02:22,  1.15it/s]Extractor Estimating: 174it [02:23,  1.12it/s]Extractor Estimating: 175it [02:24,  1.13it/s]Extractor Estimating: 176it [02:25,  1.11it/s]Extractor Estimating: 177it [02:26,  1.12it/s]Extractor Estimating: 178it [02:26,  1.18it/s]Extractor Estimating: 179it [02:27,  1.22it/s]Extractor Estimating: 180it [02:28,  1.24it/s]Extractor Estimating: 181it [02:29,  1.18it/s]Extractor Estimating: 182it [02:30,  1.18it/s]Extractor Estimating: 183it [02:30,  1.18it/s]Extractor Estimating: 184it [02:31,  1.19it/s]Extractor Estimating: 185it [02:32,  1.22it/s]Extractor Estimating: 186it [02:33,  1.25it/s]Extractor Estimating: 187it [02:34,  1.23it/s]Extractor Estimating: 188it [02:35,  1.19it/s]Extractor Estimating: 189it [02:35,  1.19it/s]Extractor Estimating: 190it [02:36,  1.21it/s]Extractor Estimating: 191it [02:37,  1.21it/s]Extractor Estimating: 192it [02:38,  1.20it/s]Extractor Estimating: 193it [02:39,  1.20it/s]Extractor Estimating: 194it [02:40,  1.19it/s]Extractor Estimating: 195it [02:40,  1.22it/s]Extractor Estimating: 196it [02:41,  1.23it/s]Extractor Estimating: 197it [02:42,  1.22it/s]Extractor Estimating: 198it [02:43,  1.23it/s]Extractor Estimating: 199it [02:44,  1.23it/s]Extractor Estimating: 200it [02:44,  1.22it/s]Extractor Estimating: 201it [02:45,  1.21it/s]Extractor Estimating: 202it [02:46,  1.20it/s]Extractor Estimating: 203it [02:47,  1.24it/s]Extractor Estimating: 204it [02:48,  1.25it/s]Extractor Estimating: 205it [02:48,  1.25it/s]Extractor Estimating: 206it [02:49,  1.27it/s]Extractor Estimating: 207it [02:50,  1.30it/s]Extractor Estimating: 208it [02:51,  1.27it/s]Extractor Estimating: 209it [02:52,  1.24it/s]Extractor Estimating: 210it [02:52,  1.22it/s]Extractor Estimating: 211it [02:53,  1.24it/s]Extractor Estimating: 212it [02:54,  1.24it/s]Extractor Estimating: 213it [02:55,  1.23it/s]Extractor Estimating: 214it [02:56,  1.20it/s]Extractor Estimating: 215it [02:56,  1.21it/s]Extractor Estimating: 216it [02:57,  1.26it/s]Extractor Estimating: 217it [02:58,  1.24it/s]Extractor Estimating: 218it [02:59,  1.20it/s]Extractor Estimating: 219it [03:00,  1.19it/s]Extractor Estimating: 220it [03:01,  1.18it/s]Extractor Estimating: 221it [03:01,  1.19it/s]Extractor Estimating: 222it [03:02,  1.21it/s]Extractor Estimating: 223it [03:03,  1.24it/s]Extractor Estimating: 224it [03:04,  1.25it/s]Extractor Estimating: 225it [03:05,  1.21it/s]Extractor Estimating: 226it [03:05,  1.25it/s]Extractor Estimating: 227it [03:06,  1.18it/s]Extractor Estimating: 228it [03:07,  1.19it/s]Extractor Estimating: 229it [03:08,  1.23it/s]Extractor Estimating: 230it [03:09,  1.22it/s]Extractor Estimating: 231it [03:10,  1.25it/s]Extractor Estimating: 232it [03:10,  1.25it/s]Extractor Estimating: 233it [03:11,  1.26it/s]Extractor Estimating: 234it [03:12,  1.28it/s]Extractor Estimating: 235it [03:13,  1.25it/s]Extractor Estimating: 236it [03:13,  1.29it/s]Extractor Estimating: 237it [03:14,  1.28it/s]Extractor Estimating: 238it [03:15,  1.29it/s]Extractor Estimating: 239it [03:16,  1.29it/s]Extractor Estimating: 240it [03:16,  1.32it/s]Extractor Estimating: 241it [03:17,  1.27it/s]Extractor Estimating: 242it [03:18,  1.29it/s]Extractor Estimating: 243it [03:19,  1.31it/s]Extractor Estimating: 244it [03:20,  1.22it/s]Extractor Estimating: 245it [03:20,  1.28it/s]Extractor Estimating: 246it [03:21,  1.28it/s]Extractor Estimating: 247it [03:22,  1.28it/s]Extractor Estimating: 248it [03:23,  1.26it/s]Extractor Estimating: 249it [03:24,  1.28it/s]Extractor Estimating: 250it [03:25,  1.22it/s]Extractor Estimating: 250it [03:25,  1.22it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:40,474 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:40,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:40,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:40,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:40,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:04:41,101 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:04:41,102 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:04:41,684 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:04:42,724 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:04:42,724 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:45,617 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:45,621 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:45,621 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:45,621 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:04:45,621 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:04:46,275 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:04:46,276 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:04:46,859 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:04:47,023 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:04:47,023 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 03:06:43,491 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 03:06:43,521 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 5230 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl'}
train vocab size: 22594
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22694, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22694, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.394, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.479, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 82, avg_time 1.396, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 182, avg_time 1.447, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 64, avg_time 1.391, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 164, avg_time 2.847, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 46, avg_time 1.422, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 146, avg_time 1.400, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 28, avg_time 1.404, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 128, avg_time 1.409, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 10, avg_time 2.842, loss:nan
g_step 1200, step 110, avg_time 1.398, loss:nan
g_step 1300, step 210, avg_time 1.422, loss:nan
g_step 1400, step 92, avg_time 1.397, loss:nan
g_step 1500, step 192, avg_time 1.416, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 74, avg_time 2.860, loss:nan
g_step 1700, step 174, avg_time 1.404, loss:nan
g_step 1800, step 56, avg_time 1.404, loss:nan
g_step 1900, step 156, avg_time 1.434, loss:nan
g_step 2000, step 38, avg_time 1.393, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 138, avg_time 2.829, loss:nan
g_step 2200, step 20, avg_time 1.430, loss:nan
g_step 2300, step 120, avg_time 1.390, loss:nan
g_step 2400, step 2, avg_time 1.433, loss:nan
g_step 2500, step 102, avg_time 1.423, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 202, avg_time 2.852, loss:nan
g_step 2700, step 84, avg_time 1.414, loss:nan
g_step 2800, step 184, avg_time 1.428, loss:nan
g_step 2900, step 66, avg_time 1.377, loss:nan
g_step 3000, step 166, avg_time 1.406, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 48, avg_time 2.868, loss:nan
g_step 3200, step 148, avg_time 1.404, loss:nan
g_step 3300, step 30, avg_time 1.430, loss:nan
g_step 3400, step 130, avg_time 1.431, loss:nan
g_step 3500, step 12, avg_time 1.406, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 112, avg_time 2.855, loss:nan
g_step 3700, step 212, avg_time 1.411, loss:nan
g_step 3800, step 94, avg_time 1.413, loss:nan
g_step 3900, step 194, avg_time 1.420, loss:nan
g_step 4000, step 76, avg_time 1.411, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 176, avg_time 2.836, loss:nan
g_step 4200, step 58, avg_time 1.444, loss:nan
g_step 4300, step 158, avg_time 1.408, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 03:06:43 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 03:06:43 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_03-06-43_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 03:06:44 - WARNING - datasets.builder -   Using custom data configuration default-c1efdf831c28ff35
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c1efdf831c28ff35/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 03:06:44,745 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:06:44,746 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:06:44,746 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:06:44,747 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:06:44,756 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:06:44,760 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:06:44,760 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:06:44,760 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:06:44,760 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:06:44,760 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:06:44,760 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 03:06:44,886 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:06:47,979 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 03:06:47,983 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c1efdf831c28ff35/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.09ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.91ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.17ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.31ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.38ba/s]100%|██████████| 6/6 [00:01<00:00,  4.79ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.12ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.37ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.45ba/s]100%|██████████| 4/4 [00:00<00:00,  5.58ba/s]100%|██████████| 4/4 [00:00<00:00,  5.05ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  7.77ba/s] 33%|███▎      | 2/6 [00:00<00:00,  8.58ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.87ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.02ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.18ba/s]100%|██████████| 6/6 [00:00<00:00, 10.24ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.91ba/s] 50%|█████     | 2/4 [00:00<00:00,  8.97ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.81ba/s]100%|██████████| 4/4 [00:00<00:00, 10.14ba/s]
[INFO|trainer.py:414] 2023-08-28 03:06:51,366 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 03:06:51,377 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 03:06:51,377 >>   Num examples = 5240
[INFO|trainer.py:1149] 2023-08-28 03:06:51,377 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 03:06:51,377 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 03:06:51,377 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 03:06:51,377 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 03:06:51,377 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:57,  3.49it/s]  0%|          | 2/410 [00:00<01:55,  3.55it/s]  1%|          | 3/410 [00:00<01:54,  3.57it/s]  1%|          | 4/410 [00:01<01:53,  3.57it/s]  1%|          | 5/410 [00:01<01:53,  3.57it/s]  1%|▏         | 6/410 [00:01<01:53,  3.57it/s]  2%|▏         | 7/410 [00:01<01:52,  3.58it/s]  2%|▏         | 8/410 [00:02<01:52,  3.58it/s]  2%|▏         | 9/410 [00:02<01:51,  3.58it/s]  2%|▏         | 10/410 [00:02<01:51,  3.58it/s]  3%|▎         | 11/410 [00:03<01:52,  3.56it/s]  3%|▎         | 12/410 [00:03<01:51,  3.56it/s]  3%|▎         | 13/410 [00:03<01:51,  3.57it/s]  3%|▎         | 14/410 [00:03<01:50,  3.57it/s]  4%|▎         | 15/410 [00:04<01:50,  3.57it/s]  4%|▍         | 16/410 [00:04<01:50,  3.57it/s]  4%|▍         | 17/410 [00:04<01:49,  3.58it/s]  4%|▍         | 18/410 [00:05<01:49,  3.58it/s]  5%|▍         | 19/410 [00:05<01:49,  3.58it/s]  5%|▍         | 20/410 [00:05<01:48,  3.58it/s]  5%|▌         | 21/410 [00:05<01:48,  3.58it/s]  5%|▌         | 22/410 [00:06<01:48,  3.57it/s]  6%|▌         | 23/410 [00:06<01:48,  3.57it/s]  6%|▌         | 24/410 [00:06<01:47,  3.58it/s]  6%|▌         | 25/410 [00:06<01:47,  3.57it/s]  6%|▋         | 26/410 [00:07<01:47,  3.58it/s]  7%|▋         | 27/410 [00:07<01:47,  3.58it/s]  7%|▋         | 28/410 [00:07<01:46,  3.58it/s]  7%|▋         | 29/410 [00:08<01:46,  3.58it/s]  7%|▋         | 30/410 [00:08<01:46,  3.58it/s]  8%|▊         | 31/410 [00:08<01:45,  3.58it/s]  8%|▊         | 32/410 [00:08<01:45,  3.58it/s]  8%|▊         | 33/410 [00:09<01:45,  3.57it/s]  8%|▊         | 34/410 [00:09<01:45,  3.57it/s]  9%|▊         | 35/410 [00:09<01:44,  3.57it/s]  9%|▉         | 36/410 [00:10<01:44,  3.57it/s]  9%|▉         | 37/410 [00:10<01:44,  3.58it/s]  9%|▉         | 38/410 [00:10<01:44,  3.58it/s] 10%|▉         | 39/410 [00:10<01:43,  3.58it/s] 10%|▉         | 40/410 [00:11<01:43,  3.57it/s] 10%|█         | 41/410 [00:11<01:43,  3.58it/s] 10%|█         | 42/410 [00:11<01:42,  3.57it/s] 10%|█         | 43/410 [00:12<01:42,  3.57it/s] 11%|█         | 44/410 [00:12<01:42,  3.56it/s] 11%|█         | 45/410 [00:12<01:42,  3.56it/s] 11%|█         | 46/410 [00:12<01:42,  3.57it/s] 11%|█▏        | 47/410 [00:13<01:41,  3.57it/s] 12%|█▏        | 48/410 [00:13<01:41,  3.57it/s] 12%|█▏        | 49/410 [00:13<01:40,  3.58it/s] 12%|█▏        | 50/410 [00:13<01:40,  3.57it/s] 12%|█▏        | 51/410 [00:14<01:40,  3.57it/s] 13%|█▎        | 52/410 [00:14<01:40,  3.57it/s] 13%|█▎        | 53/410 [00:14<01:39,  3.57it/s] 13%|█▎        | 54/410 [00:15<01:39,  3.57it/s] 13%|█▎        | 55/410 [00:15<01:40,  3.53it/s] 14%|█▎        | 56/410 [00:15<01:40,  3.54it/s] 14%|█▍        | 57/410 [00:15<01:39,  3.55it/s] 14%|█▍        | 58/410 [00:16<01:39,  3.55it/s] 14%|█▍        | 59/410 [00:16<01:38,  3.55it/s] 15%|█▍        | 60/410 [00:16<01:38,  3.56it/s] 15%|█▍        | 61/410 [00:17<01:37,  3.56it/s] 15%|█▌        | 62/410 [00:17<01:37,  3.56it/s] 15%|█▌        | 63/410 [00:17<01:37,  3.57it/s] 16%|█▌        | 64/410 [00:17<01:37,  3.57it/s] 16%|█▌        | 65/410 [00:18<01:36,  3.56it/s] 16%|█▌        | 66/410 [00:18<01:36,  3.56it/s] 16%|█▋        | 67/410 [00:18<01:36,  3.56it/s] 17%|█▋        | 68/410 [00:19<01:35,  3.56it/s] 17%|█▋        | 69/410 [00:19<01:35,  3.57it/s] 17%|█▋        | 70/410 [00:19<01:35,  3.57it/s] 17%|█▋        | 71/410 [00:19<01:34,  3.57it/s] 18%|█▊        | 72/410 [00:20<01:34,  3.57it/s] 18%|█▊        | 73/410 [00:20<01:34,  3.57it/s] 18%|█▊        | 74/410 [00:20<01:34,  3.57it/s] 18%|█▊        | 75/410 [00:21<01:33,  3.57it/s] 19%|█▊        | 76/410 [00:21<01:33,  3.57it/s] 19%|█▉        | 77/410 [00:21<01:33,  3.56it/s] 19%|█▉        | 78/410 [00:21<01:33,  3.56it/s] 19%|█▉        | 79/410 [00:22<01:32,  3.56it/s] 20%|█▉        | 80/410 [00:22<01:32,  3.57it/s] 20%|█▉        | 81/410 [00:22<01:32,  3.57it/s] 20%|██        | 82/410 [00:22<01:28,  3.70it/s][INFO|trainer.py:2140] 2023-08-28 03:07:14,323 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:07:14,323 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 03:07:14,323 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.36it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.15it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.48it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.63it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.30it/s][A
  8%|▊         | 33/436 [00:00<00:08, 47.08it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.69it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.36it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.27it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.15it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.27it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.28it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.32it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.35it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.35it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.20it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.04it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.01it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.08it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.05it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.24it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.27it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.32it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.38it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.20it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.16it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.01it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.05it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.11it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.06it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.21it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.25it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.25it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.23it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.10it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.12it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.03it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.13it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.12it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.22it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.23it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.18it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.18it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.14it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.19it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.17it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.03it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.08it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.09it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.18it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.20it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.14it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.10it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.04it/s][A
 64%|██████▍   | 278/436 [00:05<00:03, 46.14it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.12it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.14it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.09it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.14it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.14it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.10it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.18it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.07it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.07it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.11it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.13it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.17it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.92it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.10it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.11it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.11it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.14it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.12it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.05it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.04it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.12it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.21it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.19it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.07it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.04it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.07it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.02it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.00it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.96it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.98it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.08it/s][A                                                
                                                 [A 20%|██        | 82/410 [00:32<01:28,  3.70it/s]
100%|██████████| 436/436 [00:09<00:00, 46.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:07:23,787 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 03:07:23,802 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:07:26,168 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:07:26,184 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:07:26,193 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:35<21:22,  3.92s/it] 20%|██        | 84/410 [00:35<15:22,  2.83s/it] 21%|██        | 85/410 [00:35<11:10,  2.06s/it] 21%|██        | 86/410 [00:36<08:15,  1.53s/it] 21%|██        | 87/410 [00:36<06:12,  1.15s/it] 21%|██▏       | 88/410 [00:36<04:47,  1.12it/s] 22%|██▏       | 89/410 [00:37<03:47,  1.41it/s] 22%|██▏       | 90/410 [00:37<03:05,  1.72it/s] 22%|██▏       | 91/410 [00:37<02:36,  2.04it/s] 22%|██▏       | 92/410 [00:37<02:15,  2.34it/s] 23%|██▎       | 93/410 [00:38<02:01,  2.61it/s] 23%|██▎       | 94/410 [00:38<01:51,  2.84it/s] 23%|██▎       | 95/410 [00:38<01:44,  3.02it/s] 23%|██▎       | 96/410 [00:39<01:39,  3.17it/s] 24%|██▎       | 97/410 [00:39<01:35,  3.28it/s] 24%|██▍       | 98/410 [00:39<01:32,  3.36it/s] 24%|██▍       | 99/410 [00:39<01:30,  3.42it/s] 24%|██▍       | 100/410 [00:40<01:29,  3.46it/s] 25%|██▍       | 101/410 [00:40<01:28,  3.50it/s] 25%|██▍       | 102/410 [00:40<01:27,  3.52it/s] 25%|██▌       | 103/410 [00:40<01:26,  3.53it/s] 25%|██▌       | 104/410 [00:41<01:26,  3.53it/s] 26%|██▌       | 105/410 [00:41<01:26,  3.54it/s] 26%|██▌       | 106/410 [00:41<01:25,  3.55it/s] 26%|██▌       | 107/410 [00:42<01:25,  3.56it/s] 26%|██▋       | 108/410 [00:42<01:24,  3.56it/s] 27%|██▋       | 109/410 [00:42<01:24,  3.56it/s] 27%|██▋       | 110/410 [00:42<01:24,  3.57it/s] 27%|██▋       | 111/410 [00:43<01:23,  3.57it/s] 27%|██▋       | 112/410 [00:43<01:23,  3.57it/s] 28%|██▊       | 113/410 [00:43<01:23,  3.56it/s] 28%|██▊       | 114/410 [00:44<01:23,  3.56it/s] 28%|██▊       | 115/410 [00:44<01:23,  3.55it/s] 28%|██▊       | 116/410 [00:44<01:22,  3.55it/s] 29%|██▊       | 117/410 [00:44<01:22,  3.56it/s] 29%|██▉       | 118/410 [00:45<01:22,  3.56it/s] 29%|██▉       | 119/410 [00:45<01:21,  3.56it/s] 29%|██▉       | 120/410 [00:45<01:21,  3.56it/s] 30%|██▉       | 121/410 [00:46<01:21,  3.56it/s] 30%|██▉       | 122/410 [00:46<01:20,  3.57it/s] 30%|███       | 123/410 [00:46<01:20,  3.57it/s] 30%|███       | 124/410 [00:46<01:20,  3.57it/s] 30%|███       | 125/410 [00:47<01:19,  3.57it/s] 31%|███       | 126/410 [00:47<01:19,  3.56it/s] 31%|███       | 127/410 [00:47<01:19,  3.56it/s] 31%|███       | 128/410 [00:48<01:19,  3.56it/s] 31%|███▏      | 129/410 [00:48<01:18,  3.57it/s] 32%|███▏      | 130/410 [00:48<01:18,  3.57it/s] 32%|███▏      | 131/410 [00:48<01:18,  3.57it/s] 32%|███▏      | 132/410 [00:49<01:18,  3.56it/s] 32%|███▏      | 133/410 [00:49<01:17,  3.56it/s] 33%|███▎      | 134/410 [00:49<01:17,  3.56it/s] 33%|███▎      | 135/410 [00:49<01:17,  3.56it/s] 33%|███▎      | 136/410 [00:50<01:16,  3.56it/s] 33%|███▎      | 137/410 [00:50<01:16,  3.56it/s] 34%|███▎      | 138/410 [00:50<01:16,  3.56it/s] 34%|███▍      | 139/410 [00:51<01:16,  3.55it/s] 34%|███▍      | 140/410 [00:51<01:16,  3.55it/s] 34%|███▍      | 141/410 [00:51<01:15,  3.56it/s] 35%|███▍      | 142/410 [00:51<01:15,  3.56it/s] 35%|███▍      | 143/410 [00:52<01:17,  3.45it/s] 35%|███▌      | 144/410 [00:52<01:17,  3.45it/s] 35%|███▌      | 145/410 [00:52<01:16,  3.49it/s] 36%|███▌      | 146/410 [00:53<01:15,  3.51it/s] 36%|███▌      | 147/410 [00:53<01:14,  3.53it/s] 36%|███▌      | 148/410 [00:53<01:14,  3.50it/s] 36%|███▋      | 149/410 [00:53<01:14,  3.52it/s] 37%|███▋      | 150/410 [00:54<01:13,  3.53it/s] 37%|███▋      | 151/410 [00:54<01:13,  3.53it/s] 37%|███▋      | 152/410 [00:54<01:12,  3.54it/s] 37%|███▋      | 153/410 [00:55<01:12,  3.55it/s] 38%|███▊      | 154/410 [00:55<01:12,  3.55it/s] 38%|███▊      | 155/410 [00:55<01:11,  3.56it/s] 38%|███▊      | 156/410 [00:55<01:11,  3.56it/s] 38%|███▊      | 157/410 [00:56<01:11,  3.56it/s] 39%|███▊      | 158/410 [00:56<01:10,  3.56it/s] 39%|███▉      | 159/410 [00:56<01:10,  3.55it/s] 39%|███▉      | 160/410 [00:57<01:10,  3.56it/s] 39%|███▉      | 161/410 [00:57<01:09,  3.56it/s] 40%|███▉      | 162/410 [00:57<01:09,  3.56it/s] 40%|███▉      | 163/410 [00:57<01:09,  3.56it/s] 40%|████      | 164/410 [00:58<01:06,  3.70it/s][INFO|trainer.py:2140] 2023-08-28 03:07:49,513 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:07:49,513 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 03:07:49,513 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4466, 'eval_samples_per_second': 369.023, 'eval_steps_per_second': 46.154, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.90it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.84it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.15it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.45it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.08it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.71it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.41it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.03it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.96it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.95it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.11it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.14it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.22it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.21it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.10it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.95it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.85it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.84it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.80it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.94it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.07it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.12it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.18it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.03it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.99it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.86it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.91it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.96it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.91it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.98it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.05it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.12it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.08it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.01it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.87it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.87it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.96it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.92it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.86it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.91it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.86it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.94it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.99it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.87it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.95it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.93it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.92it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.83it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.92it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.99it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.99it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.05it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.88it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.95it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.93it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.89it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.94it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.94it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.02it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.91it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.76it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.99it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.99it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.95it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.91it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.97it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.06it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.10it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.99it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.04it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.94it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.93it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.93it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.89it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.91it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.99it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.05it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.01it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.95it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.89it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.86it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.82it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.87it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.92it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.97it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.06it/s][A                                                 
                                                 [A 40%|████      | 164/410 [01:07<01:06,  3.70it/s]
100%|██████████| 436/436 [00:09<00:00, 46.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:07:59,038 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 03:07:59,055 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:08:01,279 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:08:01,295 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:08:01,303 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:10<15:54,  3.90s/it] 40%|████      | 166/410 [01:10<11:26,  2.81s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:11<08:18,  2.05s/it] 41%|████      | 168/410 [01:11<06:08,  1.52s/it] 41%|████      | 169/410 [01:11<04:36,  1.15s/it] 41%|████▏     | 170/410 [01:11<03:33,  1.13it/s] 42%|████▏     | 171/410 [01:12<02:48,  1.42it/s] 42%|████▏     | 172/410 [01:12<02:17,  1.73it/s] 42%|████▏     | 173/410 [01:12<01:55,  2.05it/s] 42%|████▏     | 174/410 [01:13<01:40,  2.35it/s] 43%|████▎     | 175/410 [01:13<01:30,  2.61it/s] 43%|████▎     | 176/410 [01:13<01:22,  2.83it/s] 43%|████▎     | 177/410 [01:13<01:17,  3.02it/s] 43%|████▎     | 178/410 [01:14<01:13,  3.16it/s] 44%|████▎     | 179/410 [01:14<01:10,  3.28it/s] 44%|████▍     | 180/410 [01:14<01:08,  3.36it/s] 44%|████▍     | 181/410 [01:14<01:06,  3.42it/s] 44%|████▍     | 182/410 [01:15<01:05,  3.46it/s] 45%|████▍     | 183/410 [01:15<01:05,  3.49it/s] 45%|████▍     | 184/410 [01:15<01:04,  3.51it/s] 45%|████▌     | 185/410 [01:16<01:03,  3.53it/s] 45%|████▌     | 186/410 [01:16<01:03,  3.53it/s] 46%|████▌     | 187/410 [01:16<01:03,  3.53it/s] 46%|████▌     | 188/410 [01:16<01:02,  3.54it/s] 46%|████▌     | 189/410 [01:17<01:02,  3.55it/s] 46%|████▋     | 190/410 [01:17<01:01,  3.55it/s] 47%|████▋     | 191/410 [01:17<01:01,  3.55it/s] 47%|████▋     | 192/410 [01:18<01:01,  3.56it/s] 47%|████▋     | 193/410 [01:18<01:00,  3.56it/s] 47%|████▋     | 194/410 [01:18<01:00,  3.56it/s] 48%|████▊     | 195/410 [01:18<01:00,  3.56it/s] 48%|████▊     | 196/410 [01:19<01:00,  3.57it/s] 48%|████▊     | 197/410 [01:19<00:59,  3.56it/s] 48%|████▊     | 198/410 [01:19<00:59,  3.56it/s] 49%|████▊     | 199/410 [01:20<00:59,  3.56it/s] 49%|████▉     | 200/410 [01:20<00:58,  3.56it/s] 49%|████▉     | 201/410 [01:20<00:58,  3.56it/s] 49%|████▉     | 202/410 [01:20<00:58,  3.56it/s] 50%|████▉     | 203/410 [01:21<00:58,  3.56it/s] 50%|████▉     | 204/410 [01:21<00:57,  3.56it/s] 50%|█████     | 205/410 [01:21<00:57,  3.56it/s] 50%|█████     | 206/410 [01:22<00:57,  3.56it/s] 50%|█████     | 207/410 [01:22<00:57,  3.56it/s] 51%|█████     | 208/410 [01:22<00:56,  3.55it/s] 51%|█████     | 209/410 [01:22<00:56,  3.55it/s] 51%|█████     | 210/410 [01:23<00:56,  3.55it/s] 51%|█████▏    | 211/410 [01:23<00:55,  3.56it/s] 52%|█████▏    | 212/410 [01:23<00:55,  3.56it/s] 52%|█████▏    | 213/410 [01:23<00:55,  3.56it/s] 52%|█████▏    | 214/410 [01:24<00:55,  3.56it/s] 52%|█████▏    | 215/410 [01:24<00:54,  3.56it/s] 53%|█████▎    | 216/410 [01:24<00:54,  3.56it/s] 53%|█████▎    | 217/410 [01:25<00:54,  3.56it/s] 53%|█████▎    | 218/410 [01:25<00:53,  3.57it/s] 53%|█████▎    | 219/410 [01:25<00:53,  3.57it/s] 54%|█████▎    | 220/410 [01:25<00:53,  3.57it/s] 54%|█████▍    | 221/410 [01:26<00:53,  3.57it/s] 54%|█████▍    | 222/410 [01:26<00:52,  3.56it/s] 54%|█████▍    | 223/410 [01:26<00:52,  3.56it/s] 55%|█████▍    | 224/410 [01:27<00:52,  3.56it/s] 55%|█████▍    | 225/410 [01:27<00:51,  3.56it/s] 55%|█████▌    | 226/410 [01:27<00:51,  3.55it/s] 55%|█████▌    | 227/410 [01:27<00:51,  3.55it/s] 56%|█████▌    | 228/410 [01:28<00:51,  3.55it/s] 56%|█████▌    | 229/410 [01:28<00:50,  3.55it/s] 56%|█████▌    | 230/410 [01:28<00:50,  3.55it/s] 56%|█████▋    | 231/410 [01:29<00:50,  3.56it/s] 57%|█████▋    | 232/410 [01:29<00:50,  3.56it/s] 57%|█████▋    | 233/410 [01:29<00:49,  3.56it/s] 57%|█████▋    | 234/410 [01:29<00:49,  3.56it/s] 57%|█████▋    | 235/410 [01:30<00:49,  3.56it/s] 58%|█████▊    | 236/410 [01:30<00:48,  3.56it/s] 58%|█████▊    | 237/410 [01:30<00:48,  3.55it/s] 58%|█████▊    | 238/410 [01:30<00:48,  3.55it/s] 58%|█████▊    | 239/410 [01:31<00:48,  3.56it/s] 59%|█████▊    | 240/410 [01:31<00:47,  3.56it/s] 59%|█████▉    | 241/410 [01:31<00:47,  3.56it/s] 59%|█████▉    | 242/410 [01:32<00:47,  3.56it/s] 59%|█████▉    | 243/410 [01:32<00:46,  3.56it/s] 60%|█████▉    | 244/410 [01:32<00:46,  3.56it/s] 60%|█████▉    | 245/410 [01:32<00:46,  3.56it/s] 60%|██████    | 246/410 [01:33<00:44,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 03:08:24,592 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:08:24,592 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 03:08:24,592 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4829, 'eval_samples_per_second': 367.609, 'eval_steps_per_second': 45.977, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.69it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.12it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.26it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.53it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.99it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.65it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.20it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.01it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.97it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.96it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.03it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.07it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.18it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.14it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.08it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.93it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.75it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.82it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.89it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.94it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.01it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.01it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.00it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.05it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.94it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.96it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.96it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.91it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.96it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.09it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.01it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.00it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.00it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.87it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.96it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.95it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.92it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.93it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.03it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.02it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.02it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.03it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.78it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.92it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.84it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.90it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.79it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.98it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.04it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.02it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.97it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.89it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.97it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.92it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.97it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.94it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.91it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.93it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.92it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.91it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.94it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.95it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.92it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.95it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.96it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.94it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.04it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.00it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.99it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.82it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.88it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.96it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.94it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.93it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.98it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.91it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.91it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.95it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.87it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.98it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.95it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.98it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.89it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.94it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.00it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.87it/s][A                                                 
                                                 [A 60%|██████    | 246/410 [01:42<00:44,  3.69it/s]
100%|██████████| 436/436 [00:09<00:00, 45.87it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:08:34,230 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 03:08:34,256 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:08:36,683 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:08:36,695 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:08:36,707 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [01:45<10:51,  3.99s/it] 60%|██████    | 248/410 [01:46<07:46,  2.88s/it] 61%|██████    | 249/410 [01:46<05:38,  2.10s/it] 61%|██████    | 250/410 [01:46<04:08,  1.55s/it] 61%|██████    | 251/410 [01:47<03:06,  1.17s/it] 61%|██████▏   | 252/410 [01:47<02:22,  1.11it/s] 62%|██████▏   | 253/410 [01:47<01:52,  1.39it/s] 62%|██████▏   | 254/410 [01:47<01:31,  1.70it/s] 62%|██████▏   | 255/410 [01:48<01:16,  2.02it/s] 62%|██████▏   | 256/410 [01:48<01:06,  2.32it/s] 63%|██████▎   | 257/410 [01:48<00:58,  2.60it/s] 63%|██████▎   | 258/410 [01:48<00:53,  2.82it/s] 63%|██████▎   | 259/410 [01:49<00:50,  3.01it/s] 63%|██████▎   | 260/410 [01:49<00:47,  3.16it/s] 64%|██████▎   | 261/410 [01:49<00:45,  3.27it/s] 64%|██████▍   | 262/410 [01:50<00:44,  3.35it/s] 64%|██████▍   | 263/410 [01:50<00:43,  3.41it/s] 64%|██████▍   | 264/410 [01:50<00:42,  3.46it/s] 65%|██████▍   | 265/410 [01:50<00:41,  3.49it/s] 65%|██████▍   | 266/410 [01:51<00:41,  3.50it/s] 65%|██████▌   | 267/410 [01:51<00:40,  3.52it/s] 65%|██████▌   | 268/410 [01:51<00:40,  3.53it/s] 66%|██████▌   | 269/410 [01:52<00:39,  3.53it/s] 66%|██████▌   | 270/410 [01:52<00:40,  3.44it/s] 66%|██████▌   | 271/410 [01:52<00:40,  3.46it/s] 66%|██████▋   | 272/410 [01:52<00:39,  3.49it/s] 67%|██████▋   | 273/410 [01:53<00:39,  3.51it/s] 67%|██████▋   | 274/410 [01:53<00:38,  3.52it/s] 67%|██████▋   | 275/410 [01:53<00:38,  3.53it/s] 67%|██████▋   | 276/410 [01:54<00:37,  3.54it/s] 68%|██████▊   | 277/410 [01:54<00:37,  3.54it/s] 68%|██████▊   | 278/410 [01:54<00:37,  3.55it/s] 68%|██████▊   | 279/410 [01:54<00:36,  3.55it/s] 68%|██████▊   | 280/410 [01:55<00:36,  3.55it/s] 69%|██████▊   | 281/410 [01:55<00:36,  3.55it/s] 69%|██████▉   | 282/410 [01:55<00:35,  3.56it/s] 69%|██████▉   | 283/410 [01:56<00:35,  3.56it/s] 69%|██████▉   | 284/410 [01:56<00:35,  3.56it/s] 70%|██████▉   | 285/410 [01:56<00:35,  3.56it/s] 70%|██████▉   | 286/410 [01:56<00:34,  3.56it/s] 70%|███████   | 287/410 [01:57<00:34,  3.56it/s] 70%|███████   | 288/410 [01:57<00:34,  3.56it/s] 70%|███████   | 289/410 [01:57<00:33,  3.56it/s] 71%|███████   | 290/410 [01:58<00:33,  3.56it/s] 71%|███████   | 291/410 [01:58<00:33,  3.55it/s] 71%|███████   | 292/410 [01:58<00:33,  3.55it/s] 71%|███████▏  | 293/410 [01:58<00:32,  3.55it/s] 72%|███████▏  | 294/410 [01:59<00:32,  3.55it/s] 72%|███████▏  | 295/410 [01:59<00:32,  3.55it/s] 72%|███████▏  | 296/410 [01:59<00:32,  3.55it/s] 72%|███████▏  | 297/410 [01:59<00:31,  3.55it/s] 73%|███████▎  | 298/410 [02:00<00:31,  3.55it/s] 73%|███████▎  | 299/410 [02:00<00:31,  3.56it/s] 73%|███████▎  | 300/410 [02:00<00:30,  3.56it/s] 73%|███████▎  | 301/410 [02:01<00:30,  3.56it/s] 74%|███████▎  | 302/410 [02:01<00:30,  3.56it/s] 74%|███████▍  | 303/410 [02:01<00:30,  3.56it/s] 74%|███████▍  | 304/410 [02:01<00:29,  3.56it/s] 74%|███████▍  | 305/410 [02:02<00:29,  3.56it/s] 75%|███████▍  | 306/410 [02:02<00:29,  3.56it/s] 75%|███████▍  | 307/410 [02:02<00:28,  3.56it/s] 75%|███████▌  | 308/410 [02:03<00:28,  3.56it/s] 75%|███████▌  | 309/410 [02:03<00:28,  3.52it/s] 76%|███████▌  | 310/410 [02:03<00:28,  3.53it/s] 76%|███████▌  | 311/410 [02:03<00:27,  3.54it/s] 76%|███████▌  | 312/410 [02:04<00:27,  3.55it/s] 76%|███████▋  | 313/410 [02:04<00:27,  3.55it/s] 77%|███████▋  | 314/410 [02:04<00:27,  3.55it/s] 77%|███████▋  | 315/410 [02:05<00:26,  3.55it/s] 77%|███████▋  | 316/410 [02:05<00:26,  3.55it/s] 77%|███████▋  | 317/410 [02:05<00:26,  3.55it/s] 78%|███████▊  | 318/410 [02:05<00:25,  3.55it/s] 78%|███████▊  | 319/410 [02:06<00:25,  3.55it/s] 78%|███████▊  | 320/410 [02:06<00:25,  3.55it/s] 78%|███████▊  | 321/410 [02:06<00:25,  3.55it/s] 79%|███████▊  | 322/410 [02:07<00:24,  3.55it/s] 79%|███████▉  | 323/410 [02:07<00:24,  3.55it/s] 79%|███████▉  | 324/410 [02:07<00:24,  3.56it/s] 79%|███████▉  | 325/410 [02:07<00:23,  3.55it/s] 80%|███████▉  | 326/410 [02:08<00:23,  3.56it/s] 80%|███████▉  | 327/410 [02:08<00:23,  3.52it/s] 80%|████████  | 328/410 [02:08<00:22,  3.67it/s][INFO|trainer.py:2140] 2023-08-28 03:09:00,060 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:09:00,060 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 03:09:00,060 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.6253, 'eval_samples_per_second': 362.169, 'eval_steps_per_second': 45.297, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.71it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.93it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.17it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.34it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.95it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.70it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.38it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.10it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.01it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.09it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.11it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.07it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.07it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.02it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.08it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.98it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.91it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.95it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.95it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.97it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.99it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.01it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 45.99it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.08it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.01it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.93it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.98it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.93it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.98it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.98it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.90it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.00it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.06it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.99it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.91it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.94it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.95it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.01it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.99it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.94it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.93it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.01it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.97it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.99it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.97it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.98it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.04it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.92it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.02it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.97it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.89it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.89it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.86it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.93it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.95it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.98it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.89it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.96it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.99it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.99it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.03it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.93it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.95it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.91it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.91it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.98it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.03it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.02it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.98it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.97it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.03it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.00it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.92it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.96it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.96it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.97it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.61it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.07it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.05it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.02it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.02it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.02it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.00it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.93it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.00it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.01it/s][A                                                 
                                                 [A 80%|████████  | 328/410 [02:18<00:22,  3.67it/s]
100%|██████████| 436/436 [00:09<00:00, 46.01it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:09:09,563 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 03:09:09,579 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:09:12,019 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:09:12,035 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:09:12,045 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [02:21<05:20,  3.96s/it] 80%|████████  | 330/410 [02:21<03:48,  2.85s/it] 81%|████████  | 331/410 [02:21<02:44,  2.08s/it] 81%|████████  | 332/410 [02:22<02:00,  1.54s/it] 81%|████████  | 333/410 [02:22<01:29,  1.16s/it] 81%|████████▏ | 334/410 [02:22<01:08,  1.11it/s] 82%|████████▏ | 335/410 [02:22<00:53,  1.40it/s] 82%|████████▏ | 336/410 [02:23<00:43,  1.72it/s] 82%|████████▏ | 337/410 [02:23<00:35,  2.03it/s] 82%|████████▏ | 338/410 [02:23<00:30,  2.33it/s] 83%|████████▎ | 339/410 [02:24<00:27,  2.60it/s] 83%|████████▎ | 340/410 [02:24<00:24,  2.83it/s] 83%|████████▎ | 341/410 [02:24<00:22,  3.01it/s] 83%|████████▎ | 342/410 [02:24<00:21,  3.16it/s] 84%|████████▎ | 343/410 [02:25<00:20,  3.27it/s] 84%|████████▍ | 344/410 [02:25<00:19,  3.35it/s] 84%|████████▍ | 345/410 [02:25<00:19,  3.41it/s] 84%|████████▍ | 346/410 [02:26<00:18,  3.45it/s] 85%|████████▍ | 347/410 [02:26<00:18,  3.49it/s] 85%|████████▍ | 348/410 [02:26<00:17,  3.51it/s] 85%|████████▌ | 349/410 [02:26<00:17,  3.52it/s] 85%|████████▌ | 350/410 [02:27<00:17,  3.52it/s] 86%|████████▌ | 351/410 [02:27<00:16,  3.53it/s] 86%|████████▌ | 352/410 [02:27<00:16,  3.54it/s] 86%|████████▌ | 353/410 [02:27<00:16,  3.54it/s] 86%|████████▋ | 354/410 [02:28<00:15,  3.55it/s] 87%|████████▋ | 355/410 [02:28<00:15,  3.55it/s] 87%|████████▋ | 356/410 [02:28<00:15,  3.56it/s] 87%|████████▋ | 357/410 [02:29<00:14,  3.56it/s] 87%|████████▋ | 358/410 [02:29<00:14,  3.56it/s] 88%|████████▊ | 359/410 [02:29<00:14,  3.57it/s] 88%|████████▊ | 360/410 [02:29<00:14,  3.57it/s] 88%|████████▊ | 361/410 [02:30<00:13,  3.55it/s] 88%|████████▊ | 362/410 [02:30<00:13,  3.56it/s] 89%|████████▊ | 363/410 [02:30<00:13,  3.56it/s] 89%|████████▉ | 364/410 [02:31<00:12,  3.56it/s] 89%|████████▉ | 365/410 [02:31<00:12,  3.56it/s] 89%|████████▉ | 366/410 [02:31<00:12,  3.56it/s] 90%|████████▉ | 367/410 [02:31<00:12,  3.56it/s] 90%|████████▉ | 368/410 [02:32<00:11,  3.56it/s] 90%|█████████ | 369/410 [02:32<00:11,  3.56it/s] 90%|█████████ | 370/410 [02:32<00:11,  3.56it/s] 90%|█████████ | 371/410 [02:33<00:10,  3.56it/s] 91%|█████████ | 372/410 [02:33<00:10,  3.55it/s] 91%|█████████ | 373/410 [02:33<00:10,  3.55it/s] 91%|█████████ | 374/410 [02:33<00:10,  3.56it/s] 91%|█████████▏| 375/410 [02:34<00:09,  3.56it/s] 92%|█████████▏| 376/410 [02:34<00:09,  3.56it/s] 92%|█████████▏| 377/410 [02:34<00:09,  3.56it/s] 92%|█████████▏| 378/410 [02:34<00:08,  3.57it/s] 92%|█████████▏| 379/410 [02:35<00:08,  3.56it/s] 93%|█████████▎| 380/410 [02:35<00:08,  3.56it/s] 93%|█████████▎| 381/410 [02:35<00:08,  3.56it/s] 93%|█████████▎| 382/410 [02:36<00:07,  3.56it/s] 93%|█████████▎| 383/410 [02:36<00:07,  3.52it/s] 94%|█████████▎| 384/410 [02:36<00:07,  3.54it/s] 94%|█████████▍| 385/410 [02:36<00:07,  3.54it/s] 94%|█████████▍| 386/410 [02:37<00:06,  3.55it/s] 94%|█████████▍| 387/410 [02:37<00:06,  3.56it/s] 95%|█████████▍| 388/410 [02:37<00:06,  3.56it/s] 95%|█████████▍| 389/410 [02:38<00:05,  3.56it/s] 95%|█████████▌| 390/410 [02:38<00:05,  3.56it/s] 95%|█████████▌| 391/410 [02:38<00:05,  3.56it/s] 96%|█████████▌| 392/410 [02:38<00:05,  3.56it/s] 96%|█████████▌| 393/410 [02:39<00:04,  3.56it/s] 96%|█████████▌| 394/410 [02:39<00:04,  3.55it/s] 96%|█████████▋| 395/410 [02:39<00:04,  3.55it/s] 97%|█████████▋| 396/410 [02:40<00:03,  3.55it/s] 97%|█████████▋| 397/410 [02:40<00:03,  3.55it/s] 97%|█████████▋| 398/410 [02:40<00:03,  3.55it/s] 97%|█████████▋| 399/410 [02:40<00:03,  3.56it/s] 98%|█████████▊| 400/410 [02:41<00:02,  3.56it/s] 98%|█████████▊| 401/410 [02:41<00:02,  3.56it/s] 98%|█████████▊| 402/410 [02:41<00:02,  3.56it/s] 98%|█████████▊| 403/410 [02:42<00:01,  3.56it/s] 99%|█████████▊| 404/410 [02:42<00:01,  3.56it/s] 99%|█████████▉| 405/410 [02:42<00:01,  3.55it/s] 99%|█████████▉| 406/410 [02:42<00:01,  3.56it/s] 99%|█████████▉| 407/410 [02:43<00:00,  3.56it/s]100%|█████████▉| 408/410 [02:43<00:00,  3.56it/s]100%|█████████▉| 409/410 [02:43<00:00,  3.56it/s]100%|██████████| 410/410 [02:43<00:00,  3.70it/s][INFO|trainer.py:2140] 2023-08-28 03:09:35,343 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:09:35,343 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 03:09:35,343 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4802, 'eval_samples_per_second': 367.715, 'eval_steps_per_second': 45.991, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.99it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.62it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.07it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.38it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.90it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.71it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.39it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.17it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.10it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.03it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.14it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.12it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.17it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.17it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.08it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.01it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.85it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.91it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.97it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.01it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.03it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.04it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.05it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.01it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.94it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.82it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.86it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.93it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.02it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.03it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.05it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.14it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.00it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.99it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.95it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.95it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.94it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.97it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.00it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.07it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.01it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.01it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.02it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.95it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.95it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.03it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.96it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.96it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.05it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.07it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.99it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.06it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.03it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.05it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.03it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.97it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.05it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.00it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.78it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.06it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.97it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.00it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.03it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.05it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.95it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.99it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.00it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.03it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.99it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.99it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.01it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.97it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.03it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.05it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.01it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.05it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 42.27it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 43.78it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 44.47it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 44.91it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.29it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.53it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.78it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.84it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.91it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.70it/s][A                                                 
                                                 [A100%|██████████| 410/410 [02:53<00:00,  3.70it/s]
100%|██████████| 436/436 [00:09<00:00, 45.70it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:09:44,862 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 03:09:44,875 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:09:47,410 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:09:47,435 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:09:47,444 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 03:09:47,744 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 03:09:47,745 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82 (score: 1.0161933898925781).
                                                 100%|██████████| 410/410 [02:58<00:00,  3.70it/s]100%|██████████| 410/410 [02:58<00:00,  2.30it/s]
[INFO|trainer.py:1894] 2023-08-28 03:09:49,508 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 03:09:49,532 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:09:51,883 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:09:51,913 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:09:51,928 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:09:52,161 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:09:52,161 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:09:52,161 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:09:52,161 >>   train_runtime            = 0:02:58.12
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:09:52,161 >>   train_samples            =       5240
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:09:52,161 >>   train_samples_per_second =    147.086
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:09:52,161 >>   train_steps_per_second   =      2.302
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.504, 'eval_samples_per_second': 366.794, 'eval_steps_per_second': 45.876, 'epoch': 5.0}
{'train_runtime': 178.1274, 'train_samples_per_second': 147.086, 'train_steps_per_second': 2.302, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 03:09:52 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 03:09:52,227 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:09:52,227 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 03:09:52,227 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 57.29it/s]  3%|▎         | 12/436 [00:00<00:08, 50.55it/s]  4%|▍         | 18/436 [00:00<00:08, 48.49it/s]  5%|▌         | 23/436 [00:00<00:08, 47.79it/s]  6%|▋         | 28/436 [00:00<00:08, 47.17it/s]  8%|▊         | 33/436 [00:00<00:08, 46.99it/s]  9%|▊         | 38/436 [00:00<00:08, 46.84it/s] 10%|▉         | 43/436 [00:00<00:08, 46.71it/s] 11%|█         | 48/436 [00:01<00:08, 46.51it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.50it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.46it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.34it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.29it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.22it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.21it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.23it/s] 20%|██        | 88/436 [00:01<00:07, 46.23it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.18it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.25it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.32it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.29it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.29it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.19it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.22it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.07it/s] 31%|███       | 133/436 [00:02<00:06, 46.06it/s] 32%|███▏      | 138/436 [00:02<00:06, 46.08it/s] 33%|███▎      | 143/436 [00:03<00:06, 46.17it/s] 34%|███▍      | 148/436 [00:03<00:06, 46.23it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.30it/s] 36%|███▌      | 158/436 [00:03<00:06, 46.05it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.16it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.17it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.27it/s] 41%|████      | 178/436 [00:03<00:05, 46.18it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.12it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.17it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.12it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.22it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.18it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.25it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.28it/s] 50%|█████     | 218/436 [00:04<00:04, 46.24it/s] 51%|█████     | 223/436 [00:04<00:04, 46.24it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.21it/s] 53%|█████▎    | 233/436 [00:05<00:04, 46.17it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.17it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.12it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.14it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.19it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.27it/s] 60%|██████    | 263/436 [00:05<00:03, 46.31it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.27it/s] 63%|██████▎   | 273/436 [00:05<00:03, 46.22it/s] 64%|██████▍   | 278/436 [00:05<00:03, 46.11it/s] 65%|██████▍   | 283/436 [00:06<00:03, 46.13it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.21it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.21it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.16it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.17it/s] 71%|███████   | 308/436 [00:06<00:02, 46.24it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.22it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.28it/s] 74%|███████▍  | 323/436 [00:06<00:02, 46.17it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.15it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.18it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.09it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.14it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.23it/s] 81%|████████  | 353/436 [00:07<00:01, 46.24it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.16it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.16it/s] 84%|████████▍ | 368/436 [00:07<00:01, 46.14it/s] 86%|████████▌ | 373/436 [00:08<00:01, 46.06it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.19it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.16it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.22it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.24it/s] 91%|█████████▏| 398/436 [00:08<00:00, 46.12it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.18it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.18it/s] 95%|█████████▍| 413/436 [00:08<00:00, 46.12it/s] 96%|█████████▌| 418/436 [00:09<00:00, 46.06it/s] 97%|█████████▋| 423/436 [00:09<00:00, 45.74it/s] 98%|█████████▊| 428/436 [00:09<00:00, 45.87it/s] 99%|█████████▉| 433/436 [00:09<00:00, 45.96it/s]100%|██████████| 436/436 [00:09<00:00, 46.30it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:10:01,670 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:10:01,670 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:10:01,670 >>   eval_loss               =     1.0162
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:10:01,670 >>   eval_runtime            = 0:00:09.44
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:10:01,670 >>   eval_samples            =       3486
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:10:01,670 >>   eval_samples_per_second =    369.181
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:10:01,670 >>   eval_steps_per_second   =     46.174
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:10:01,670 >>   perplexity              =     2.7627
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:10:08,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:10:08,348 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:10:08,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:10:08,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:10:08,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:10:09,057 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:10:09,058 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:10:09,636 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:10:10,685 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:10:10,685 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:10:13,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:10:13,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:10:13,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:10:13,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:10:13,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:10:14,218 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:10:14,220 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:10:14,816 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:10:14,977 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:10:14,977 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-164
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-410
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-82
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/checkpoint-328
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'manufacturer', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13015
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13115, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.13it/s]Extractor Predicting: 2it [00:01,  1.17it/s]Extractor Predicting: 3it [00:02,  1.23it/s]Extractor Predicting: 4it [00:03,  1.23it/s]Extractor Predicting: 5it [00:04,  1.24it/s]Extractor Predicting: 6it [00:04,  1.24it/s]Extractor Predicting: 7it [00:05,  1.23it/s]Extractor Predicting: 8it [00:06,  1.26it/s]Extractor Predicting: 9it [00:07,  1.26it/s]Extractor Predicting: 10it [00:08,  1.28it/s]Extractor Predicting: 11it [00:08,  1.25it/s]Extractor Predicting: 12it [00:09,  1.25it/s]Extractor Predicting: 13it [00:10,  1.24it/s]Extractor Predicting: 14it [00:11,  1.23it/s]Extractor Predicting: 15it [00:12,  1.25it/s]Extractor Predicting: 16it [00:12,  1.25it/s]Extractor Predicting: 17it [00:13,  1.28it/s]Extractor Predicting: 18it [00:14,  1.29it/s]Extractor Predicting: 19it [00:15,  1.29it/s]Extractor Predicting: 20it [00:15,  1.28it/s]Extractor Predicting: 21it [00:16,  1.28it/s]Extractor Predicting: 22it [00:17,  1.30it/s]Extractor Predicting: 23it [00:18,  1.30it/s]Extractor Predicting: 24it [00:19,  1.26it/s]Extractor Predicting: 25it [00:19,  1.25it/s]Extractor Predicting: 26it [00:20,  1.23it/s]Extractor Predicting: 27it [00:21,  1.25it/s]Extractor Predicting: 28it [00:22,  1.25it/s]Extractor Predicting: 29it [00:23,  1.24it/s]Extractor Predicting: 30it [00:24,  1.21it/s]Extractor Predicting: 31it [00:24,  1.22it/s]Extractor Predicting: 32it [00:25,  1.22it/s]Extractor Predicting: 33it [00:26,  1.22it/s]Extractor Predicting: 34it [00:27,  1.24it/s]Extractor Predicting: 35it [00:28,  1.25it/s]Extractor Predicting: 36it [00:28,  1.25it/s]Extractor Predicting: 37it [00:29,  1.27it/s]Extractor Predicting: 38it [00:30,  1.30it/s]Extractor Predicting: 39it [00:31,  1.31it/s]Extractor Predicting: 40it [00:31,  1.31it/s]Extractor Predicting: 41it [00:32,  1.31it/s]Extractor Predicting: 42it [00:33,  1.30it/s]Extractor Predicting: 43it [00:34,  1.28it/s]Extractor Predicting: 44it [00:34,  1.26it/s]Extractor Predicting: 45it [00:35,  1.31it/s]Extractor Predicting: 46it [00:36,  1.29it/s]Extractor Predicting: 47it [00:37,  1.31it/s]Extractor Predicting: 48it [00:38,  1.30it/s]Extractor Predicting: 49it [00:38,  1.32it/s]Extractor Predicting: 50it [00:39,  1.32it/s]Extractor Predicting: 51it [00:40,  1.30it/s]Extractor Predicting: 52it [00:41,  1.29it/s]Extractor Predicting: 53it [00:41,  1.29it/s]Extractor Predicting: 54it [00:42,  1.27it/s]Extractor Predicting: 55it [00:43,  1.31it/s]Extractor Predicting: 56it [00:44,  1.32it/s]Extractor Predicting: 57it [00:44,  1.29it/s]Extractor Predicting: 58it [00:45,  1.21it/s]Extractor Predicting: 59it [00:46,  1.24it/s]Extractor Predicting: 60it [00:47,  1.24it/s]Extractor Predicting: 61it [00:48,  1.25it/s]Extractor Predicting: 62it [00:49,  1.27it/s]Extractor Predicting: 63it [00:49,  1.28it/s]Extractor Predicting: 64it [00:50,  1.29it/s]Extractor Predicting: 65it [00:51,  1.28it/s]Extractor Predicting: 66it [00:52,  1.29it/s]Extractor Predicting: 67it [00:52,  1.31it/s]Extractor Predicting: 68it [00:53,  1.28it/s]Extractor Predicting: 69it [00:54,  1.29it/s]Extractor Predicting: 70it [00:55,  1.28it/s]Extractor Predicting: 71it [00:55,  1.31it/s]Extractor Predicting: 72it [00:56,  1.32it/s]Extractor Predicting: 73it [00:57,  1.31it/s]Extractor Predicting: 74it [00:58,  1.32it/s]Extractor Predicting: 75it [00:59,  1.28it/s]Extractor Predicting: 76it [00:59,  1.29it/s]Extractor Predicting: 77it [01:00,  1.28it/s]Extractor Predicting: 78it [01:01,  1.29it/s]Extractor Predicting: 79it [01:02,  1.29it/s]Extractor Predicting: 80it [01:02,  1.30it/s]Extractor Predicting: 81it [01:03,  1.29it/s]Extractor Predicting: 82it [01:04,  1.30it/s]Extractor Predicting: 83it [01:05,  1.26it/s]Extractor Predicting: 84it [01:06,  1.28it/s]Extractor Predicting: 85it [01:06,  1.26it/s]Extractor Predicting: 86it [01:07,  1.27it/s]Extractor Predicting: 87it [01:08,  1.26it/s]Extractor Predicting: 88it [01:09,  1.30it/s]Extractor Predicting: 89it [01:10,  1.23it/s]Extractor Predicting: 90it [01:10,  1.29it/s]Extractor Predicting: 91it [01:11,  1.33it/s]Extractor Predicting: 92it [01:12,  1.36it/s]Extractor Predicting: 93it [01:12,  1.33it/s]Extractor Predicting: 94it [01:13,  1.38it/s]Extractor Predicting: 95it [01:14,  1.37it/s]Extractor Predicting: 96it [01:15,  1.37it/s]Extractor Predicting: 97it [01:15,  1.36it/s]Extractor Predicting: 98it [01:16,  1.34it/s]Extractor Predicting: 99it [01:17,  1.31it/s]Extractor Predicting: 100it [01:18,  1.31it/s]Extractor Predicting: 101it [01:18,  1.36it/s]Extractor Predicting: 102it [01:19,  1.39it/s]Extractor Predicting: 103it [01:20,  1.38it/s]Extractor Predicting: 104it [01:20,  1.37it/s]Extractor Predicting: 105it [01:21,  1.38it/s]Extractor Predicting: 106it [01:22,  1.38it/s]Extractor Predicting: 107it [01:23,  1.37it/s]Extractor Predicting: 108it [01:23,  1.37it/s]Extractor Predicting: 109it [01:24,  1.38it/s]Extractor Predicting: 110it [01:25,  1.38it/s]Extractor Predicting: 111it [01:26,  1.38it/s]Extractor Predicting: 112it [01:26,  1.43it/s]Extractor Predicting: 113it [01:27,  1.45it/s]Extractor Predicting: 114it [01:28,  1.43it/s]Extractor Predicting: 115it [01:28,  1.41it/s]Extractor Predicting: 116it [01:29,  1.40it/s]Extractor Predicting: 117it [01:30,  1.41it/s]Extractor Predicting: 118it [01:30,  1.44it/s]Extractor Predicting: 119it [01:31,  1.42it/s]Extractor Predicting: 120it [01:32,  1.39it/s]Extractor Predicting: 121it [01:32,  1.45it/s]Extractor Predicting: 122it [01:33,  1.44it/s]Extractor Predicting: 123it [01:34,  1.38it/s]Extractor Predicting: 124it [01:35,  1.41it/s]Extractor Predicting: 125it [01:35,  1.41it/s]Extractor Predicting: 126it [01:36,  1.47it/s]Extractor Predicting: 127it [01:37,  1.45it/s]Extractor Predicting: 128it [01:37,  1.45it/s]Extractor Predicting: 129it [01:38,  1.41it/s]Extractor Predicting: 130it [01:39,  1.43it/s]Extractor Predicting: 131it [01:39,  1.46it/s]Extractor Predicting: 132it [01:40,  1.44it/s]Extractor Predicting: 133it [01:41,  1.41it/s]Extractor Predicting: 134it [01:42,  1.44it/s]Extractor Predicting: 135it [01:42,  1.45it/s]Extractor Predicting: 136it [01:43,  1.46it/s]Extractor Predicting: 137it [01:44,  1.44it/s]Extractor Predicting: 138it [01:44,  1.49it/s]Extractor Predicting: 139it [01:45,  1.50it/s]Extractor Predicting: 140it [01:46,  1.51it/s]Extractor Predicting: 141it [01:46,  1.54it/s]Extractor Predicting: 142it [01:47,  1.51it/s]Extractor Predicting: 143it [01:48,  1.47it/s]Extractor Predicting: 144it [01:48,  1.68it/s]Extractor Predicting: 144it [01:48,  1.33it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:12:11,079 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:12:11,085 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:12:11,085 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:12:11,085 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:12:11,085 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:12:11,718 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:12:11,719 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:12:12,300 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:12:13,335 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:12:13,335 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:12:16,171 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:12:16,175 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:12:16,175 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:12:16,175 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:12:16,175 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:12:16,823 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:12:16,825 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:12:17,405 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:12:17,562 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:12:17,562 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13522
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13622, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.32it/s]Extractor Predicting: 3it [00:02,  1.34it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.38it/s]Extractor Predicting: 6it [00:04,  1.35it/s]Extractor Predicting: 7it [00:05,  1.35it/s]Extractor Predicting: 8it [00:05,  1.39it/s]Extractor Predicting: 9it [00:06,  1.39it/s]Extractor Predicting: 10it [00:07,  1.41it/s]Extractor Predicting: 11it [00:07,  1.39it/s]Extractor Predicting: 12it [00:08,  1.40it/s]Extractor Predicting: 13it [00:09,  1.36it/s]Extractor Predicting: 14it [00:10,  1.40it/s]Extractor Predicting: 15it [00:10,  1.39it/s]Extractor Predicting: 16it [00:11,  1.36it/s]Extractor Predicting: 17it [00:12,  1.35it/s]Extractor Predicting: 18it [00:13,  1.35it/s]Extractor Predicting: 19it [00:13,  1.37it/s]Extractor Predicting: 20it [00:14,  1.35it/s]Extractor Predicting: 21it [00:15,  1.36it/s]Extractor Predicting: 22it [00:16,  1.37it/s]Extractor Predicting: 23it [00:16,  1.35it/s]Extractor Predicting: 24it [00:17,  1.33it/s]Extractor Predicting: 25it [00:18,  1.34it/s]Extractor Predicting: 26it [00:19,  1.34it/s]Extractor Predicting: 27it [00:19,  1.34it/s]Extractor Predicting: 28it [00:20,  1.36it/s]Extractor Predicting: 29it [00:21,  1.36it/s]Extractor Predicting: 30it [00:22,  1.32it/s]Extractor Predicting: 31it [00:22,  1.35it/s]Extractor Predicting: 32it [00:23,  1.32it/s]Extractor Predicting: 33it [00:24,  1.33it/s]Extractor Predicting: 34it [00:25,  1.35it/s]Extractor Predicting: 35it [00:25,  1.34it/s]Extractor Predicting: 36it [00:26,  1.37it/s]Extractor Predicting: 37it [00:27,  1.34it/s]Extractor Predicting: 38it [00:28,  1.32it/s]Extractor Predicting: 39it [00:28,  1.31it/s]Extractor Predicting: 40it [00:29,  1.29it/s]Extractor Predicting: 41it [00:30,  1.30it/s]Extractor Predicting: 42it [00:31,  1.31it/s]Extractor Predicting: 43it [00:31,  1.30it/s]Extractor Predicting: 44it [00:32,  1.28it/s]Extractor Predicting: 45it [00:33,  1.30it/s]Extractor Predicting: 46it [00:34,  1.30it/s]Extractor Predicting: 47it [00:34,  1.32it/s]Extractor Predicting: 48it [00:35,  1.32it/s]Extractor Predicting: 49it [00:36,  1.35it/s]Extractor Predicting: 50it [00:37,  1.35it/s]Extractor Predicting: 51it [00:37,  1.37it/s]Extractor Predicting: 52it [00:38,  1.38it/s]Extractor Predicting: 53it [00:39,  1.38it/s]Extractor Predicting: 54it [00:40,  1.38it/s]Extractor Predicting: 55it [00:40,  1.37it/s]Extractor Predicting: 56it [00:41,  1.35it/s]Extractor Predicting: 57it [00:42,  1.30it/s]Extractor Predicting: 58it [00:43,  1.28it/s]Extractor Predicting: 59it [00:43,  1.29it/s]Extractor Predicting: 60it [00:44,  1.32it/s]Extractor Predicting: 61it [00:45,  1.29it/s]Extractor Predicting: 62it [00:46,  1.28it/s]Extractor Predicting: 63it [00:47,  1.25it/s]Extractor Predicting: 64it [00:47,  1.26it/s]Extractor Predicting: 65it [00:48,  1.30it/s]Extractor Predicting: 66it [00:49,  1.30it/s]Extractor Predicting: 67it [00:50,  1.30it/s]Extractor Predicting: 68it [00:50,  1.30it/s]Extractor Predicting: 69it [00:51,  1.29it/s]Extractor Predicting: 70it [00:52,  1.24it/s]Extractor Predicting: 71it [00:53,  1.26it/s]Extractor Predicting: 72it [00:54,  1.26it/s]Extractor Predicting: 73it [00:54,  1.26it/s]Extractor Predicting: 74it [00:55,  1.27it/s]Extractor Predicting: 75it [00:56,  1.25it/s]Extractor Predicting: 76it [00:57,  1.29it/s]Extractor Predicting: 77it [00:58,  1.26it/s]Extractor Predicting: 78it [00:58,  1.27it/s]Extractor Predicting: 79it [00:59,  1.28it/s]Extractor Predicting: 80it [01:00,  1.27it/s]Extractor Predicting: 81it [01:01,  1.28it/s]Extractor Predicting: 82it [01:02,  1.25it/s]Extractor Predicting: 83it [01:02,  1.25it/s]Extractor Predicting: 84it [01:03,  1.28it/s]Extractor Predicting: 85it [01:04,  1.28it/s]Extractor Predicting: 86it [01:05,  1.29it/s]Extractor Predicting: 87it [01:05,  1.27it/s]Extractor Predicting: 88it [01:06,  1.32it/s]Extractor Predicting: 89it [01:07,  1.32it/s]Extractor Predicting: 90it [01:08,  1.30it/s]Extractor Predicting: 91it [01:08,  1.35it/s]Extractor Predicting: 92it [01:09,  1.35it/s]Extractor Predicting: 93it [01:10,  1.38it/s]Extractor Predicting: 94it [01:11,  1.36it/s]Extractor Predicting: 95it [01:11,  1.35it/s]Extractor Predicting: 96it [01:12,  1.34it/s]Extractor Predicting: 97it [01:13,  1.32it/s]Extractor Predicting: 98it [01:14,  1.31it/s]Extractor Predicting: 99it [01:14,  1.31it/s]Extractor Predicting: 100it [01:15,  1.30it/s]Extractor Predicting: 101it [01:16,  1.30it/s]Extractor Predicting: 102it [01:17,  1.25it/s]Extractor Predicting: 103it [01:18,  1.27it/s]Extractor Predicting: 104it [01:18,  1.27it/s]Extractor Predicting: 105it [01:19,  1.30it/s]Extractor Predicting: 106it [01:20,  1.31it/s]Extractor Predicting: 107it [01:21,  1.30it/s]Extractor Predicting: 108it [01:21,  1.30it/s]Extractor Predicting: 109it [01:22,  1.31it/s]Extractor Predicting: 110it [01:23,  1.29it/s]Extractor Predicting: 111it [01:24,  1.30it/s]Extractor Predicting: 112it [01:24,  1.29it/s]Extractor Predicting: 113it [01:25,  1.33it/s]Extractor Predicting: 114it [01:26,  1.31it/s]Extractor Predicting: 115it [01:27,  1.33it/s]Extractor Predicting: 116it [01:28,  1.28it/s]Extractor Predicting: 117it [01:28,  1.26it/s]Extractor Predicting: 118it [01:29,  1.26it/s]Extractor Predicting: 119it [01:30,  1.25it/s]Extractor Predicting: 120it [01:31,  1.26it/s]Extractor Predicting: 121it [01:32,  1.26it/s]Extractor Predicting: 122it [01:32,  1.30it/s]Extractor Predicting: 123it [01:33,  1.29it/s]Extractor Predicting: 124it [01:34,  1.27it/s]Extractor Predicting: 125it [01:35,  1.25it/s]Extractor Predicting: 126it [01:36,  1.23it/s]Extractor Predicting: 127it [01:36,  1.23it/s]Extractor Predicting: 128it [01:37,  1.24it/s]Extractor Predicting: 129it [01:38,  1.23it/s]Extractor Predicting: 130it [01:39,  1.26it/s]Extractor Predicting: 131it [01:40,  1.27it/s]Extractor Predicting: 132it [01:40,  1.26it/s]Extractor Predicting: 133it [01:41,  1.26it/s]Extractor Predicting: 134it [01:42,  1.27it/s]Extractor Predicting: 135it [01:43,  1.29it/s]Extractor Predicting: 136it [01:43,  1.31it/s]Extractor Predicting: 137it [01:44,  1.34it/s]Extractor Predicting: 138it [01:45,  1.33it/s]Extractor Predicting: 139it [01:46,  1.34it/s]Extractor Predicting: 140it [01:46,  1.32it/s]Extractor Predicting: 141it [01:47,  1.28it/s]Extractor Predicting: 142it [01:48,  1.29it/s]Extractor Predicting: 143it [01:49,  1.38it/s]Extractor Predicting: 143it [01:49,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:12,773 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:12,777 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:12,777 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:12,777 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:12,777 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:14:13,398 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:14:13,399 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:14:13,972 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:14:14,976 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:14:14,976 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:17,826 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:17,831 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:17,831 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:17,831 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:14:17,831 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:14:18,475 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:14:18,476 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:14:19,050 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:14:19,213 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:14:19,213 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 464
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 564, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.05it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 2it [00:01,  1.38it/s]
[INFO|configuration_utils.py:515] 2023-08-28 03:14:20,999 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:14:21,001 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:14:21,006 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:14:21,007 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 03:14:21,009 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:14:23,977 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 03:14:23,979 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 03:14:23,993 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:14:23,994 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:14:24,000 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:14:24,004 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:14:24,004 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:14:24,004 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:14:24,005 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:14:24,005 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:14:24,005 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 03:14:24,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:25,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:26,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:27,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:28,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:29,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:30,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:31,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:32,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:33,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:34,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:35,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:35,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:36,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:37,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:38,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:39,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:40,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:41,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:42,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:43,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:44,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:45,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:46,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:47,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:48,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:49,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:50,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:50,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:27<04:10, 27.78s/it][WARNING|generation_utils.py:914] 2023-08-28 03:14:52,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:53,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:53,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:54,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:55,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:56,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:57,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:58,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:14:59,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:00,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:01,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:02,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:03,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:04,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:05,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:06,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:07,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:08,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:10,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:10,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:11,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:12,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:13,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:14,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:15,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:52<03:26, 25.80s/it][WARNING|generation_utils.py:914] 2023-08-28 03:15:16,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:17,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:18,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:19,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:21,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:22,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:23,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:23,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:24,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:25,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:26,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:27,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:28,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:29,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:30,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:31,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:32,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:33,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:34,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:35,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:36,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:37,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:38,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:39,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:39,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:16<02:56, 25.19s/it][WARNING|generation_utils.py:914] 2023-08-28 03:15:40,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:41,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:42,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:43,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:44,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:45,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:46,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:47,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:48,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:49,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:50,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:51,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:52,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:53,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:53,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:54,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:55,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:57,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:58,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:15:59,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:00,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:00,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:37<02:21, 23.55s/it][WARNING|generation_utils.py:914] 2023-08-28 03:16:01,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:02,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:03,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:04,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:05,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:06,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:07,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:08,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:09,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:10,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:11,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:12,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:13,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:13,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:14,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:16,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:16,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:17,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:18,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:19,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:20,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:21,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:22,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:23,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:24,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:25,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:26,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:27,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:28,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:29,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:30,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:31,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:32,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:32,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:33,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [02:10<02:14, 26.91s/it][WARNING|generation_utils.py:914] 2023-08-28 03:16:34,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:35,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:37,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:38,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:38,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:39,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:40,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:41,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:42,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:43,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:44,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:45,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:46,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:47,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:48,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:49,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:50,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:51,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:52,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:53,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:54,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:55,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:56,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:57,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:34<01:42, 25.74s/it][WARNING|generation_utils.py:914] 2023-08-28 03:16:58,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:16:59,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:00,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:01,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:02,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:02,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:04,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:04,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:05,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:06,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:07,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:09,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:10,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:11,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:12,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:13,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:14,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:15,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:16,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:17,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:18,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:19,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:20,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:57<01:14, 24.93s/it][WARNING|generation_utils.py:914] 2023-08-28 03:17:21,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:22,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:23,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:24,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:25,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:26,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:27,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:28,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:30,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:30,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:31,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:32,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:33,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:34,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:35,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:36,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:37,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:38,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:39,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:40,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:41,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:42,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:43,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [03:20<00:48, 24.40s/it][WARNING|generation_utils.py:914] 2023-08-28 03:17:44,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:45,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:46,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:47,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:48,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:49,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:50,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:51,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:52,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:53,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:54,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:55,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:56,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:57,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:58,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:17:59,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:01,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:02,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:03,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:04,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:05,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:06,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:43<00:23, 23.88s/it][WARNING|generation_utils.py:914] 2023-08-28 03:18:07,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:08,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:09,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:10,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:11,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:12,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:13,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:14,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:15,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:16,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:17,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:18,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:19,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:20,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:21,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:22,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:23,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:24,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:25,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:26,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:27,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:28,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:18:29,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [04:06<00:00, 23.69s/it]Generating: 100%|██████████| 10/10 [04:06<00:00, 24.66s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:36,921 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:36,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:36,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:36,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:36,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:18:37,552 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:18:37,553 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:18:38,127 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:18:39,200 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:18:39,200 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:41,615 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:41,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:41,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:41,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:18:41,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:18:42,092 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:18:42,093 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:18:42,491 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:18:42,658 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:18:42,658 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 188, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 225, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 268, 'raw': 416}
{'target': 600, 'success': 289, 'raw': 448}
{'target': 600, 'success': 309, 'raw': 480}
{'target': 600, 'success': 337, 'raw': 512}
{'target': 600, 'success': 361, 'raw': 544}
{'target': 600, 'success': 378, 'raw': 576}
{'target': 600, 'success': 401, 'raw': 608}
{'target': 600, 'success': 424, 'raw': 640}
{'target': 600, 'success': 446, 'raw': 672}
{'target': 600, 'success': 465, 'raw': 704}
{'target': 600, 'success': 486, 'raw': 736}
{'target': 600, 'success': 502, 'raw': 768}
{'target': 600, 'success': 528, 'raw': 800}
{'target': 600, 'success': 549, 'raw': 832}
{'target': 600, 'success': 570, 'raw': 864}
{'target': 600, 'success': 589, 'raw': 896}
{'target': 600, 'success': 610, 'raw': 928}
{'prompt': 'Relation : country .', 'success_rate': 0.6573275862068966, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 499, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 547, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : genre . Context : Later in 2003 , he played in Steven Spielberg 's The Bourne Ultimatum . Head Entity : The Bourne Ultimatum , Tail Entity : director .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 604, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.755, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : voice type . Context : Later in the year , the band members decided to cancel their performance of " In My Time " on Broadway , in part to play the first time . Head Entity : In My Time , Tail Entity : vocal .\n']
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 102, 'raw': 160}
{'target': 600, 'success': 118, 'raw': 192}
{'target': 600, 'success': 134, 'raw': 224}
{'target': 600, 'success': 146, 'raw': 256}
{'target': 600, 'success': 160, 'raw': 288}
{'target': 600, 'success': 178, 'raw': 320}
{'target': 600, 'success': 193, 'raw': 352}
{'target': 600, 'success': 215, 'raw': 384}
{'target': 600, 'success': 233, 'raw': 416}
{'target': 600, 'success': 245, 'raw': 448}
{'target': 600, 'success': 262, 'raw': 480}
{'target': 600, 'success': 282, 'raw': 512}
{'target': 600, 'success': 304, 'raw': 544}
{'target': 600, 'success': 318, 'raw': 576}
{'target': 600, 'success': 336, 'raw': 608}
{'target': 600, 'success': 355, 'raw': 640}
{'target': 600, 'success': 375, 'raw': 672}
{'target': 600, 'success': 394, 'raw': 704}
{'target': 600, 'success': 413, 'raw': 736}
{'target': 600, 'success': 431, 'raw': 768}
{'target': 600, 'success': 450, 'raw': 800}
{'target': 600, 'success': 466, 'raw': 832}
{'target': 600, 'success': 480, 'raw': 864}
{'target': 600, 'success': 495, 'raw': 896}
{'target': 600, 'success': 511, 'raw': 928}
{'target': 600, 'success': 529, 'raw': 960}
{'target': 600, 'success': 546, 'raw': 992}
{'target': 600, 'success': 561, 'raw': 1024}
{'target': 600, 'success': 580, 'raw': 1056}
{'target': 600, 'success': 596, 'raw': 1088}
{'target': 600, 'success': 612, 'raw': 1120}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5464285714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : contains administrative territorial entity . Context : The city of Marchec is under the rule of the Marchec Municipal Municipality in Romania . Head Entity : Marchec , Tail Entity : Romanian .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8233695652173914, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8707386363636364, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 12301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.06it/s]Extractor Estimating: 2it [00:01,  1.11it/s]Extractor Estimating: 3it [00:02,  1.08it/s]Extractor Estimating: 4it [00:03,  1.15it/s]Extractor Estimating: 5it [00:04,  1.17it/s]Extractor Estimating: 6it [00:05,  1.18it/s]Extractor Estimating: 7it [00:06,  1.17it/s]Extractor Estimating: 8it [00:06,  1.21it/s]Extractor Estimating: 9it [00:07,  1.23it/s]Extractor Estimating: 10it [00:08,  1.23it/s]Extractor Estimating: 11it [00:09,  1.21it/s]Extractor Estimating: 12it [00:10,  1.20it/s]Extractor Estimating: 13it [00:10,  1.24it/s]Extractor Estimating: 14it [00:11,  1.27it/s]Extractor Estimating: 15it [00:12,  1.23it/s]Extractor Estimating: 16it [00:13,  1.23it/s]Extractor Estimating: 17it [00:14,  1.16it/s]Extractor Estimating: 18it [00:15,  1.17it/s]Extractor Estimating: 19it [00:16,  1.16it/s]Extractor Estimating: 20it [00:16,  1.19it/s]Extractor Estimating: 21it [00:17,  1.21it/s]Extractor Estimating: 22it [00:18,  1.21it/s]Extractor Estimating: 23it [00:19,  1.23it/s]Extractor Estimating: 24it [00:19,  1.24it/s]Extractor Estimating: 25it [00:20,  1.25it/s]Extractor Estimating: 26it [00:21,  1.21it/s]Extractor Estimating: 27it [00:22,  1.21it/s]Extractor Estimating: 28it [00:23,  1.22it/s]Extractor Estimating: 29it [00:24,  1.22it/s]Extractor Estimating: 30it [00:24,  1.20it/s]Extractor Estimating: 31it [00:25,  1.21it/s]Extractor Estimating: 32it [00:26,  1.22it/s]Extractor Estimating: 33it [00:27,  1.22it/s]Extractor Estimating: 34it [00:28,  1.23it/s]Extractor Estimating: 35it [00:29,  1.20it/s]Extractor Estimating: 36it [00:29,  1.22it/s]Extractor Estimating: 37it [00:30,  1.17it/s]Extractor Estimating: 38it [00:31,  1.18it/s]Extractor Estimating: 39it [00:32,  1.18it/s]Extractor Estimating: 40it [00:33,  1.20it/s]Extractor Estimating: 41it [00:34,  1.16it/s]Extractor Estimating: 42it [00:35,  1.18it/s]Extractor Estimating: 43it [00:35,  1.20it/s]Extractor Estimating: 44it [00:36,  1.23it/s]Extractor Estimating: 45it [00:37,  1.22it/s]Extractor Estimating: 46it [00:38,  1.21it/s]Extractor Estimating: 47it [00:39,  1.24it/s]Extractor Estimating: 48it [00:39,  1.23it/s]Extractor Estimating: 49it [00:40,  1.23it/s]Extractor Estimating: 50it [00:41,  1.17it/s]Extractor Estimating: 51it [00:42,  1.21it/s]Extractor Estimating: 52it [00:43,  1.20it/s]Extractor Estimating: 53it [00:44,  1.22it/s]Extractor Estimating: 54it [00:45,  1.02it/s]Extractor Estimating: 55it [00:46,  1.08it/s]Extractor Estimating: 56it [00:47,  1.09it/s]Extractor Estimating: 57it [00:48,  1.08it/s]Extractor Estimating: 58it [00:48,  1.15it/s]Extractor Estimating: 59it [00:49,  1.16it/s]Extractor Estimating: 60it [00:50,  1.16it/s]Extractor Estimating: 61it [00:51,  1.11it/s]Extractor Estimating: 62it [00:52,  1.17it/s]Extractor Estimating: 63it [00:53,  1.16it/s]Extractor Estimating: 64it [00:53,  1.15it/s]Extractor Estimating: 65it [00:54,  1.15it/s]Extractor Estimating: 66it [00:55,  1.15it/s]Extractor Estimating: 67it [00:56,  1.18it/s]Extractor Estimating: 68it [00:57,  1.20it/s]Extractor Estimating: 69it [00:58,  1.20it/s]Extractor Estimating: 70it [00:58,  1.20it/s]Extractor Estimating: 71it [00:59,  1.21it/s]Extractor Estimating: 72it [01:00,  1.19it/s]Extractor Estimating: 73it [01:01,  1.18it/s]Extractor Estimating: 74it [01:02,  1.20it/s]Extractor Estimating: 75it [01:03,  1.21it/s]Extractor Estimating: 76it [01:03,  1.24it/s]Extractor Estimating: 77it [01:04,  1.22it/s]Extractor Estimating: 78it [01:05,  1.20it/s]Extractor Estimating: 79it [01:06,  1.21it/s]Extractor Estimating: 80it [01:07,  1.23it/s]Extractor Estimating: 81it [01:08,  1.23it/s]Extractor Estimating: 82it [01:08,  1.26it/s]Extractor Estimating: 83it [01:09,  1.29it/s]Extractor Estimating: 84it [01:10,  1.27it/s]Extractor Estimating: 85it [01:11,  1.26it/s]Extractor Estimating: 86it [01:12,  1.22it/s]Extractor Estimating: 87it [01:12,  1.25it/s]Extractor Estimating: 88it [01:13,  1.25it/s]Extractor Estimating: 89it [01:14,  1.27it/s]Extractor Estimating: 90it [01:15,  1.20it/s]Extractor Estimating: 91it [01:16,  1.18it/s]Extractor Estimating: 92it [01:16,  1.20it/s]Extractor Estimating: 93it [01:17,  1.20it/s]Extractor Estimating: 94it [01:18,  1.18it/s]Extractor Estimating: 95it [01:19,  1.18it/s]Extractor Estimating: 96it [01:20,  1.24it/s]Extractor Estimating: 97it [01:21,  1.19it/s]Extractor Estimating: 98it [01:21,  1.22it/s]Extractor Estimating: 99it [01:22,  1.23it/s]Extractor Estimating: 100it [01:23,  1.26it/s]Extractor Estimating: 101it [01:24,  1.23it/s]Extractor Estimating: 102it [01:25,  1.22it/s]Extractor Estimating: 103it [01:25,  1.22it/s]Extractor Estimating: 104it [01:26,  1.21it/s]Extractor Estimating: 105it [01:27,  1.26it/s]Extractor Estimating: 106it [01:28,  1.30it/s]Extractor Estimating: 107it [01:29,  1.28it/s]Extractor Estimating: 108it [01:29,  1.27it/s]Extractor Estimating: 109it [01:30,  1.29it/s]Extractor Estimating: 110it [01:31,  1.24it/s]Extractor Estimating: 111it [01:32,  1.27it/s]Extractor Estimating: 112it [01:33,  1.27it/s]Extractor Estimating: 113it [01:33,  1.30it/s]Extractor Estimating: 114it [01:34,  1.30it/s]Extractor Estimating: 115it [01:35,  1.28it/s]Extractor Estimating: 116it [01:36,  1.21it/s]Extractor Estimating: 117it [01:37,  1.20it/s]Extractor Estimating: 118it [01:37,  1.18it/s]Extractor Estimating: 119it [01:38,  1.21it/s]Extractor Estimating: 120it [01:39,  1.18it/s]Extractor Estimating: 121it [01:40,  1.19it/s]Extractor Estimating: 122it [01:41,  1.18it/s]Extractor Estimating: 123it [01:42,  1.22it/s]Extractor Estimating: 124it [01:42,  1.24it/s]Extractor Estimating: 125it [01:43,  1.21it/s]Extractor Estimating: 126it [01:44,  1.19it/s]Extractor Estimating: 127it [01:45,  1.12it/s]Extractor Estimating: 128it [01:46,  1.17it/s]Extractor Estimating: 129it [01:47,  1.20it/s]Extractor Estimating: 130it [01:47,  1.23it/s]Extractor Estimating: 131it [01:48,  1.24it/s]Extractor Estimating: 132it [01:49,  1.23it/s]Extractor Estimating: 133it [01:50,  1.26it/s]Extractor Estimating: 134it [01:51,  1.25it/s]Extractor Estimating: 135it [01:51,  1.26it/s]Extractor Estimating: 136it [01:52,  1.28it/s]Extractor Estimating: 137it [01:53,  1.23it/s]Extractor Estimating: 138it [01:54,  1.27it/s]Extractor Estimating: 139it [01:55,  1.22it/s]Extractor Estimating: 140it [01:55,  1.25it/s]Extractor Estimating: 141it [01:56,  1.23it/s]Extractor Estimating: 142it [01:57,  1.24it/s]Extractor Estimating: 143it [01:58,  1.24it/s]Extractor Estimating: 144it [01:59,  1.26it/s]Extractor Estimating: 145it [01:59,  1.28it/s]Extractor Estimating: 146it [02:00,  1.25it/s]Extractor Estimating: 147it [02:01,  1.25it/s]Extractor Estimating: 148it [02:02,  1.25it/s]Extractor Estimating: 149it [02:03,  1.24it/s]Extractor Estimating: 150it [02:04,  1.22it/s]Extractor Estimating: 151it [02:04,  1.21it/s]Extractor Estimating: 152it [02:05,  1.23it/s]Extractor Estimating: 153it [02:06,  1.23it/s]Extractor Estimating: 154it [02:07,  1.23it/s]Extractor Estimating: 155it [02:08,  1.21it/s]Extractor Estimating: 156it [02:08,  1.20it/s]Extractor Estimating: 157it [02:09,  1.16it/s]Extractor Estimating: 158it [02:10,  1.21it/s]Extractor Estimating: 159it [02:11,  1.22it/s]Extractor Estimating: 160it [02:12,  1.22it/s]Extractor Estimating: 161it [02:13,  1.24it/s]Extractor Estimating: 162it [02:13,  1.21it/s]Extractor Estimating: 163it [02:14,  1.22it/s]Extractor Estimating: 164it [02:15,  1.19it/s]Extractor Estimating: 165it [02:16,  1.15it/s]Extractor Estimating: 166it [02:17,  1.17it/s]Extractor Estimating: 167it [02:18,  1.16it/s]Extractor Estimating: 168it [02:19,  1.16it/s]Extractor Estimating: 169it [02:20,  1.12it/s]Extractor Estimating: 170it [02:20,  1.16it/s]Extractor Estimating: 171it [02:21,  1.15it/s]Extractor Estimating: 172it [02:22,  1.16it/s]Extractor Estimating: 173it [02:23,  1.15it/s]Extractor Estimating: 174it [02:24,  1.12it/s]Extractor Estimating: 175it [02:25,  1.13it/s]Extractor Estimating: 176it [02:26,  1.11it/s]Extractor Estimating: 177it [02:27,  1.12it/s]Extractor Estimating: 178it [02:27,  1.18it/s]Extractor Estimating: 179it [02:28,  1.22it/s]Extractor Estimating: 180it [02:29,  1.24it/s]Extractor Estimating: 181it [02:30,  1.18it/s]Extractor Estimating: 182it [02:31,  1.18it/s]Extractor Estimating: 183it [02:32,  1.18it/s]Extractor Estimating: 184it [02:32,  1.19it/s]Extractor Estimating: 185it [02:33,  1.22it/s]Extractor Estimating: 186it [02:34,  1.25it/s]Extractor Estimating: 187it [02:35,  1.22it/s]Extractor Estimating: 188it [02:36,  1.18it/s]Extractor Estimating: 189it [02:36,  1.18it/s]Extractor Estimating: 190it [02:37,  1.13it/s]Extractor Estimating: 191it [02:38,  1.14it/s]Extractor Estimating: 192it [02:39,  1.15it/s]Extractor Estimating: 193it [02:40,  1.17it/s]Extractor Estimating: 194it [02:41,  1.16it/s]Extractor Estimating: 195it [02:42,  1.20it/s]Extractor Estimating: 196it [02:42,  1.21it/s]Extractor Estimating: 197it [02:43,  1.21it/s]Extractor Estimating: 198it [02:44,  1.21it/s]Extractor Estimating: 199it [02:45,  1.21it/s]Extractor Estimating: 200it [02:46,  1.21it/s]Extractor Estimating: 201it [02:47,  1.19it/s]Extractor Estimating: 202it [02:47,  1.18it/s]Extractor Estimating: 203it [02:48,  1.22it/s]Extractor Estimating: 204it [02:49,  1.23it/s]Extractor Estimating: 205it [02:50,  1.23it/s]Extractor Estimating: 206it [02:51,  1.25it/s]Extractor Estimating: 207it [02:51,  1.28it/s]Extractor Estimating: 208it [02:52,  1.25it/s]Extractor Estimating: 209it [02:53,  1.24it/s]Extractor Estimating: 210it [02:54,  1.22it/s]Extractor Estimating: 211it [02:55,  1.23it/s]Extractor Estimating: 212it [02:55,  1.23it/s]Extractor Estimating: 213it [02:56,  1.22it/s]Extractor Estimating: 214it [02:57,  1.19it/s]Extractor Estimating: 215it [02:58,  1.19it/s]Extractor Estimating: 216it [02:59,  1.25it/s]Extractor Estimating: 217it [03:00,  1.23it/s]Extractor Estimating: 218it [03:01,  1.19it/s]Extractor Estimating: 219it [03:01,  1.17it/s]Extractor Estimating: 220it [03:02,  1.17it/s]Extractor Estimating: 221it [03:03,  1.18it/s]Extractor Estimating: 222it [03:04,  1.20it/s]Extractor Estimating: 223it [03:05,  1.22it/s]Extractor Estimating: 224it [03:05,  1.24it/s]Extractor Estimating: 225it [03:06,  1.19it/s]Extractor Estimating: 226it [03:07,  1.24it/s]Extractor Estimating: 227it [03:08,  1.23it/s]Extractor Estimating: 228it [03:09,  1.22it/s]Extractor Estimating: 229it [03:10,  1.25it/s]Extractor Estimating: 230it [03:10,  1.23it/s]Extractor Estimating: 231it [03:11,  1.25it/s]Extractor Estimating: 232it [03:12,  1.24it/s]Extractor Estimating: 233it [03:13,  1.25it/s]Extractor Estimating: 234it [03:13,  1.27it/s]Extractor Estimating: 235it [03:14,  1.23it/s]Extractor Estimating: 236it [03:15,  1.27it/s]Extractor Estimating: 237it [03:16,  1.25it/s]Extractor Estimating: 238it [03:17,  1.26it/s]Extractor Estimating: 239it [03:17,  1.26it/s]Extractor Estimating: 240it [03:18,  1.29it/s]Extractor Estimating: 241it [03:19,  1.25it/s]Extractor Estimating: 242it [03:20,  1.26it/s]Extractor Estimating: 243it [03:21,  1.28it/s]Extractor Estimating: 244it [03:22,  1.20it/s]Extractor Estimating: 245it [03:22,  1.26it/s]Extractor Estimating: 246it [03:23,  1.26it/s]Extractor Estimating: 247it [03:24,  1.26it/s]Extractor Estimating: 248it [03:25,  1.24it/s]Extractor Estimating: 249it [03:25,  1.26it/s]Extractor Estimating: 250it [03:26,  1.20it/s]Extractor Estimating: 250it [03:26,  1.21it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:22,213 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:22,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:22,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:22,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:22,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:22:22,910 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:22:22,911 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:22:23,493 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:22:24,571 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:22:24,571 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:27,518 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:27,528 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:27,528 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:27,528 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:27,528 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:22:28,174 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:22:28,175 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:22:28,743 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:22:28,902 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:22:28,903 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 05:27:24,202 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 05:27:24,231 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 5295 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl'}
train vocab size: 21580
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21680, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21680, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.452, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.431, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 79, avg_time 1.439, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 179, avg_time 1.431, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 58, avg_time 1.466, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 158, avg_time 2.846, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 37, avg_time 1.430, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 137, avg_time 1.430, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 16, avg_time 1.452, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 116, avg_time 1.438, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 216, avg_time 2.853, loss:nan
g_step 1200, step 95, avg_time 1.449, loss:nan
g_step 1300, step 195, avg_time 1.414, loss:nan
g_step 1400, step 74, avg_time 1.456, loss:nan
g_step 1500, step 174, avg_time 1.424, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 53, avg_time 2.882, loss:nan
g_step 1700, step 153, avg_time 1.422, loss:nan
g_step 1800, step 32, avg_time 1.444, loss:nan
g_step 1900, step 132, avg_time 1.448, loss:nan
g_step 2000, step 11, avg_time 1.430, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 111, avg_time 2.879, loss:nan
g_step 2200, step 211, avg_time 1.426, loss:nan
g_step 2300, step 90, avg_time 1.427, loss:nan
g_step 2400, step 190, avg_time 1.421, loss:nan
g_step 2500, step 69, avg_time 1.460, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 169, avg_time 2.856, loss:nan
g_step 2700, step 48, avg_time 1.444, loss:nan
g_step 2800, step 148, avg_time 1.412, loss:nan
g_step 2900, step 27, avg_time 1.454, loss:nan
g_step 3000, step 127, avg_time 1.417, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 6, avg_time 2.875, loss:nan
g_step 3200, step 106, avg_time 1.458, loss:nan
g_step 3300, step 206, avg_time 1.424, loss:nan
g_step 3400, step 85, avg_time 1.470, loss:nan
g_step 3500, step 185, avg_time 1.421, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 64, avg_time 2.872, loss:nan
g_step 3700, step 164, avg_time 1.429, loss:nan
g_step 3800, step 43, avg_time 1.408, loss:nan
g_step 3900, step 143, avg_time 1.426, loss:nan
g_step 4000, step 22, avg_time 1.454, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 122, avg_time 2.846, loss:nan
g_step 4200, step 1, avg_time 1.452, loss:nan
g_step 4300, step 101, avg_time 1.445, loss:nan
g_step 4400, step 201, avg_time 1.415, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 05:27:24 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 05:27:24 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_05-27-24_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 05:27:25 - WARNING - datasets.builder -   Using custom data configuration default-7e3a400cb49d8cee
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7e3a400cb49d8cee/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  3.17 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 05:27:26,021 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:27:26,022 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 05:27:26,022 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:27:26,023 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 05:27:26,032 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:27:26,037 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:27:26,037 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:27:26,037 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:27:26,037 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:27:26,037 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:27:26,037 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 05:27:26,202 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 05:27:29,376 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 05:27:29,381 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7e3a400cb49d8cee/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.69ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.62ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.05ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.26ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.38ba/s]100%|██████████| 6/6 [00:01<00:00,  4.63ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.15ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.39ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.62ba/s]100%|██████████| 4/4 [00:00<00:00,  4.78ba/s]100%|██████████| 4/4 [00:00<00:00,  4.44ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  7.85ba/s] 33%|███▎      | 2/6 [00:00<00:00,  8.54ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.82ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.17ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.20ba/s]100%|██████████| 6/6 [00:00<00:00, 10.16ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.90ba/s] 50%|█████     | 2/4 [00:00<00:00,  8.57ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.88ba/s]100%|██████████| 4/4 [00:00<00:00, 10.13ba/s]
[INFO|trainer.py:414] 2023-08-28 05:27:32,948 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 05:27:32,966 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 05:27:32,966 >>   Num examples = 5300
[INFO|trainer.py:1149] 2023-08-28 05:27:32,966 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 05:27:32,967 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 05:27:32,967 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 05:27:32,967 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 05:27:32,967 >>   Total optimization steps = 415
  0%|          | 0/415 [00:00<?, ?it/s]  0%|          | 1/415 [00:00<02:25,  2.84it/s]  0%|          | 2/415 [00:00<02:07,  3.24it/s]  1%|          | 3/415 [00:00<02:01,  3.40it/s]  1%|          | 4/415 [00:01<01:58,  3.48it/s]  1%|          | 5/415 [00:01<01:56,  3.52it/s]  1%|▏         | 6/415 [00:01<01:55,  3.55it/s]  2%|▏         | 7/415 [00:02<01:54,  3.56it/s]  2%|▏         | 8/415 [00:02<01:53,  3.57it/s]  2%|▏         | 9/415 [00:02<01:53,  3.58it/s]  2%|▏         | 10/415 [00:02<01:53,  3.58it/s]  3%|▎         | 11/415 [00:03<01:52,  3.59it/s]  3%|▎         | 12/415 [00:03<01:52,  3.59it/s]  3%|▎         | 13/415 [00:03<01:51,  3.59it/s]  3%|▎         | 14/415 [00:03<01:51,  3.59it/s]  4%|▎         | 15/415 [00:04<01:51,  3.60it/s]  4%|▍         | 16/415 [00:04<01:50,  3.60it/s]  4%|▍         | 17/415 [00:04<01:50,  3.60it/s]  4%|▍         | 18/415 [00:05<01:50,  3.60it/s]  5%|▍         | 19/415 [00:05<01:50,  3.60it/s]  5%|▍         | 20/415 [00:05<01:49,  3.60it/s]  5%|▌         | 21/415 [00:05<01:49,  3.59it/s]  5%|▌         | 22/415 [00:06<01:49,  3.59it/s]  6%|▌         | 23/415 [00:06<01:49,  3.60it/s]  6%|▌         | 24/415 [00:06<01:48,  3.60it/s]  6%|▌         | 25/415 [00:07<01:48,  3.60it/s]  6%|▋         | 26/415 [00:07<01:48,  3.60it/s]  7%|▋         | 27/415 [00:07<01:47,  3.60it/s]  7%|▋         | 28/415 [00:07<01:47,  3.59it/s]  7%|▋         | 29/415 [00:08<01:47,  3.60it/s]  7%|▋         | 30/415 [00:08<01:47,  3.59it/s]  7%|▋         | 31/415 [00:08<01:46,  3.59it/s]  8%|▊         | 32/415 [00:08<01:46,  3.58it/s]  8%|▊         | 33/415 [00:09<01:46,  3.59it/s]  8%|▊         | 34/415 [00:09<01:46,  3.59it/s]  8%|▊         | 35/415 [00:09<01:45,  3.59it/s]  9%|▊         | 36/415 [00:10<01:45,  3.59it/s]  9%|▉         | 37/415 [00:10<01:45,  3.60it/s]  9%|▉         | 38/415 [00:10<01:44,  3.60it/s]  9%|▉         | 39/415 [00:10<01:44,  3.60it/s] 10%|▉         | 40/415 [00:11<01:44,  3.60it/s] 10%|▉         | 41/415 [00:11<01:44,  3.59it/s] 10%|█         | 42/415 [00:11<01:43,  3.60it/s] 10%|█         | 43/415 [00:12<01:43,  3.59it/s] 11%|█         | 44/415 [00:12<01:43,  3.59it/s] 11%|█         | 45/415 [00:12<01:43,  3.59it/s] 11%|█         | 46/415 [00:12<01:42,  3.59it/s] 11%|█▏        | 47/415 [00:13<01:42,  3.59it/s] 12%|█▏        | 48/415 [00:13<01:42,  3.59it/s] 12%|█▏        | 49/415 [00:13<01:41,  3.59it/s] 12%|█▏        | 50/415 [00:13<01:41,  3.59it/s] 12%|█▏        | 51/415 [00:14<01:41,  3.59it/s] 13%|█▎        | 52/415 [00:14<01:41,  3.59it/s] 13%|█▎        | 53/415 [00:14<01:40,  3.59it/s] 13%|█▎        | 54/415 [00:15<01:41,  3.57it/s] 13%|█▎        | 55/415 [00:15<01:40,  3.58it/s] 13%|█▎        | 56/415 [00:15<01:40,  3.58it/s] 14%|█▎        | 57/415 [00:15<01:39,  3.58it/s] 14%|█▍        | 58/415 [00:16<01:39,  3.58it/s] 14%|█▍        | 59/415 [00:16<01:39,  3.58it/s] 14%|█▍        | 60/415 [00:16<01:39,  3.58it/s] 15%|█▍        | 61/415 [00:17<01:38,  3.58it/s] 15%|█▍        | 62/415 [00:17<01:38,  3.58it/s] 15%|█▌        | 63/415 [00:17<01:38,  3.58it/s] 15%|█▌        | 64/415 [00:17<01:38,  3.58it/s] 16%|█▌        | 65/415 [00:18<01:37,  3.58it/s] 16%|█▌        | 66/415 [00:18<01:37,  3.58it/s] 16%|█▌        | 67/415 [00:18<01:37,  3.58it/s] 16%|█▋        | 68/415 [00:19<01:36,  3.58it/s] 17%|█▋        | 69/415 [00:19<01:36,  3.59it/s] 17%|█▋        | 70/415 [00:19<01:36,  3.59it/s] 17%|█▋        | 71/415 [00:19<01:35,  3.59it/s] 17%|█▋        | 72/415 [00:20<01:35,  3.58it/s] 18%|█▊        | 73/415 [00:20<01:35,  3.59it/s] 18%|█▊        | 74/415 [00:20<01:35,  3.59it/s] 18%|█▊        | 75/415 [00:20<01:34,  3.59it/s] 18%|█▊        | 76/415 [00:21<01:34,  3.59it/s] 19%|█▊        | 77/415 [00:21<01:34,  3.58it/s] 19%|█▉        | 78/415 [00:21<01:34,  3.58it/s] 19%|█▉        | 79/415 [00:22<01:33,  3.59it/s] 19%|█▉        | 80/415 [00:22<01:33,  3.58it/s] 20%|█▉        | 81/415 [00:22<01:33,  3.58it/s] 20%|█▉        | 82/415 [00:22<01:32,  3.58it/s] 20%|██        | 83/415 [00:23<01:27,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 05:27:56,112 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:27:56,112 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 05:27:56,112 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 57.09it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.25it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.52it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.78it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.41it/s][A
  8%|▊         | 33/436 [00:00<00:08, 47.04it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.74it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.35it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.36it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.36it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.38it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.38it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.42it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.50it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.46it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.35it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.15it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.21it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.22it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.23it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.33it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.33it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.30it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.38it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.31it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.25it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.10it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.21it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.16it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.28it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.32it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.24it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.38it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.29it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.22it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.13it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.17it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.20it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.22it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.30it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.35it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.31it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.27it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.23it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.22it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.19it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.21it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.23it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.27it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.31it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.35it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.01it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.30it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.35it/s][A
 64%|██████▍   | 278/436 [00:05<00:03, 46.28it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.22it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.24it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.24it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.28it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.25it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.17it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.19it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.17it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.15it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.22it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.24it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.25it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.21it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.20it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.19it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.14it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.20it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.13it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.22it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.19it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.21it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.26it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.19it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.24it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.25it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.18it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.28it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.28it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.26it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.27it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.19it/s][A                                                
                                                 [A 20%|██        | 83/415 [00:32<01:27,  3.78it/s]
100%|██████████| 436/436 [00:09<00:00, 46.19it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:28:05,547 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83
[INFO|configuration_utils.py:351] 2023-08-28 05:28:05,568 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:28:08,111 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:28:08,125 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:28:08,137 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83/special_tokens_map.json
 20%|██        | 84/415 [00:35<21:54,  3.97s/it] 20%|██        | 85/415 [00:36<15:44,  2.86s/it] 21%|██        | 86/415 [00:36<11:26,  2.09s/it] 21%|██        | 87/415 [00:36<08:26,  1.55s/it] 21%|██        | 88/415 [00:36<06:21,  1.17s/it] 21%|██▏       | 89/415 [00:37<04:53,  1.11it/s] 22%|██▏       | 90/415 [00:37<03:51,  1.40it/s] 22%|██▏       | 91/415 [00:37<03:08,  1.72it/s] 22%|██▏       | 92/415 [00:37<02:38,  2.03it/s] 22%|██▏       | 93/415 [00:38<02:17,  2.34it/s] 23%|██▎       | 94/415 [00:38<02:03,  2.60it/s] 23%|██▎       | 95/415 [00:38<01:52,  2.84it/s] 23%|██▎       | 96/415 [00:39<01:45,  3.03it/s] 23%|██▎       | 97/415 [00:39<01:40,  3.17it/s] 24%|██▎       | 98/415 [00:39<01:36,  3.28it/s] 24%|██▍       | 99/415 [00:39<01:33,  3.37it/s] 24%|██▍       | 100/415 [00:40<01:31,  3.43it/s] 24%|██▍       | 101/415 [00:40<01:30,  3.47it/s] 25%|██▍       | 102/415 [00:40<01:29,  3.51it/s] 25%|██▍       | 103/415 [00:41<01:28,  3.53it/s] 25%|██▌       | 104/415 [00:41<01:27,  3.55it/s] 25%|██▌       | 105/415 [00:41<01:27,  3.55it/s] 26%|██▌       | 106/415 [00:41<01:26,  3.56it/s] 26%|██▌       | 107/415 [00:42<01:26,  3.57it/s] 26%|██▌       | 108/415 [00:42<01:26,  3.56it/s] 26%|██▋       | 109/415 [00:42<01:25,  3.57it/s] 27%|██▋       | 110/415 [00:43<01:25,  3.57it/s] 27%|██▋       | 111/415 [00:43<01:25,  3.58it/s] 27%|██▋       | 112/415 [00:43<01:24,  3.58it/s] 27%|██▋       | 113/415 [00:43<01:24,  3.58it/s] 27%|██▋       | 114/415 [00:44<01:24,  3.58it/s] 28%|██▊       | 115/415 [00:44<01:23,  3.59it/s] 28%|██▊       | 116/415 [00:44<01:23,  3.57it/s] 28%|██▊       | 117/415 [00:44<01:23,  3.58it/s] 28%|██▊       | 118/415 [00:45<01:22,  3.58it/s] 29%|██▊       | 119/415 [00:45<01:22,  3.58it/s] 29%|██▉       | 120/415 [00:45<01:22,  3.58it/s] 29%|██▉       | 121/415 [00:46<01:21,  3.59it/s] 29%|██▉       | 122/415 [00:46<01:21,  3.58it/s] 30%|██▉       | 123/415 [00:46<01:21,  3.58it/s] 30%|██▉       | 124/415 [00:46<01:21,  3.58it/s] 30%|███       | 125/415 [00:47<01:20,  3.58it/s] 30%|███       | 126/415 [00:47<01:20,  3.58it/s] 31%|███       | 127/415 [00:47<01:20,  3.57it/s] 31%|███       | 128/415 [00:48<01:20,  3.57it/s] 31%|███       | 129/415 [00:48<01:19,  3.58it/s] 31%|███▏      | 130/415 [00:48<01:19,  3.58it/s] 32%|███▏      | 131/415 [00:48<01:19,  3.58it/s] 32%|███▏      | 132/415 [00:49<01:19,  3.58it/s] 32%|███▏      | 133/415 [00:49<01:18,  3.58it/s] 32%|███▏      | 134/415 [00:49<01:18,  3.58it/s] 33%|███▎      | 135/415 [00:50<01:18,  3.58it/s] 33%|███▎      | 136/415 [00:50<01:18,  3.57it/s] 33%|███▎      | 137/415 [00:50<01:17,  3.57it/s] 33%|███▎      | 138/415 [00:50<01:17,  3.57it/s] 33%|███▎      | 139/415 [00:51<01:17,  3.57it/s] 34%|███▎      | 140/415 [00:51<01:18,  3.51it/s] 34%|███▍      | 141/415 [00:51<01:18,  3.51it/s] 34%|███▍      | 142/415 [00:51<01:17,  3.53it/s] 34%|███▍      | 143/415 [00:52<01:16,  3.55it/s] 35%|███▍      | 144/415 [00:52<01:16,  3.56it/s] 35%|███▍      | 145/415 [00:52<01:15,  3.56it/s] 35%|███▌      | 146/415 [00:53<01:15,  3.57it/s] 35%|███▌      | 147/415 [00:53<01:15,  3.57it/s] 36%|███▌      | 148/415 [00:53<01:14,  3.57it/s] 36%|███▌      | 149/415 [00:53<01:14,  3.56it/s] 36%|███▌      | 150/415 [00:54<01:14,  3.57it/s] 36%|███▋      | 151/415 [00:54<01:13,  3.57it/s] 37%|███▋      | 152/415 [00:54<01:13,  3.57it/s] 37%|███▋      | 153/415 [00:55<01:13,  3.57it/s] 37%|███▋      | 154/415 [00:55<01:13,  3.57it/s] 37%|███▋      | 155/415 [00:55<01:12,  3.58it/s] 38%|███▊      | 156/415 [00:55<01:12,  3.57it/s] 38%|███▊      | 157/415 [00:56<01:12,  3.58it/s] 38%|███▊      | 158/415 [00:56<01:11,  3.58it/s] 38%|███▊      | 159/415 [00:56<01:11,  3.58it/s] 39%|███▊      | 160/415 [00:57<01:11,  3.56it/s] 39%|███▉      | 161/415 [00:57<01:11,  3.56it/s] 39%|███▉      | 162/415 [00:57<01:10,  3.57it/s] 39%|███▉      | 163/415 [00:57<01:10,  3.57it/s] 40%|███▉      | 164/415 [00:58<01:10,  3.58it/s] 40%|███▉      | 165/415 [00:58<01:09,  3.57it/s] 40%|████      | 166/415 [00:58<01:06,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 05:28:31,626 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:28:31,626 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 05:28:31,626 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4239, 'eval_samples_per_second': 369.91, 'eval_steps_per_second': 46.265, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.75it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.09it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.26it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.49it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.10it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.66it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.38it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.04it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.16it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.07it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.07it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.16it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.09it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.15it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.13it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.07it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.04it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.93it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.04it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.03it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.07it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.11it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.06it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.09it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.94it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.00it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.04it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.04it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.06it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.03it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.07it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.02it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.09it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.01it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.00it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.98it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.02it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.07it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.08it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.03it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.07it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.04it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.00it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.99it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.00it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.02it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.03it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.04it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.07it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.09it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.02it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.96it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.98it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.04it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.99it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.03it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.00it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.01it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.99it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.03it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.08it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.98it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.03it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.00it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.04it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.05it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.00it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.01it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.98it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.01it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.01it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.00it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.03it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.02it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.05it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.02it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.01it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.01it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.07it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.02it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.99it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.03it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.97it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.99it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.03it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.06it/s][A                                                 
                                                 [A 40%|████      | 166/415 [01:08<01:06,  3.77it/s]
100%|██████████| 436/436 [00:09<00:00, 46.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:28:41,117 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-166
[INFO|configuration_utils.py:351] 2023-08-28 05:28:41,139 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-166/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:28:43,597 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-166/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:28:43,608 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-166/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:28:43,619 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-166/special_tokens_map.json
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 40%|████      | 167/415 [01:11<16:23,  3.97s/it] 40%|████      | 168/415 [01:11<11:46,  2.86s/it] 41%|████      | 169/415 [01:11<08:33,  2.09s/it] 41%|████      | 170/415 [01:12<06:18,  1.54s/it] 41%|████      | 171/415 [01:12<04:44,  1.17s/it] 41%|████▏     | 172/415 [01:12<03:38,  1.11it/s] 42%|████▏     | 173/415 [01:12<02:52,  1.40it/s] 42%|████▏     | 174/415 [01:13<02:20,  1.71it/s] 42%|████▏     | 175/415 [01:13<01:58,  2.03it/s] 42%|████▏     | 176/415 [01:13<01:42,  2.34it/s] 43%|████▎     | 177/415 [01:14<01:31,  2.60it/s] 43%|████▎     | 178/415 [01:14<01:23,  2.83it/s] 43%|████▎     | 179/415 [01:14<01:18,  3.02it/s] 43%|████▎     | 180/415 [01:14<01:14,  3.17it/s] 44%|████▎     | 181/415 [01:15<01:11,  3.29it/s] 44%|████▍     | 182/415 [01:15<01:09,  3.37it/s] 44%|████▍     | 183/415 [01:15<01:07,  3.43it/s] 44%|████▍     | 184/415 [01:16<01:06,  3.47it/s] 45%|████▍     | 185/415 [01:16<01:05,  3.50it/s] 45%|████▍     | 186/415 [01:16<01:04,  3.53it/s] 45%|████▌     | 187/415 [01:16<01:04,  3.55it/s] 45%|████▌     | 188/415 [01:17<01:03,  3.55it/s] 46%|████▌     | 189/415 [01:17<01:03,  3.56it/s] 46%|████▌     | 190/415 [01:17<01:03,  3.57it/s] 46%|████▌     | 191/415 [01:17<01:02,  3.57it/s] 46%|████▋     | 192/415 [01:18<01:02,  3.58it/s] 47%|████▋     | 193/415 [01:18<01:02,  3.58it/s] 47%|████▋     | 194/415 [01:18<01:01,  3.58it/s] 47%|████▋     | 195/415 [01:19<01:01,  3.58it/s] 47%|████▋     | 196/415 [01:19<01:01,  3.58it/s] 47%|████▋     | 197/415 [01:19<01:00,  3.58it/s] 48%|████▊     | 198/415 [01:19<01:00,  3.58it/s] 48%|████▊     | 199/415 [01:20<01:00,  3.55it/s] 48%|████▊     | 200/415 [01:20<01:00,  3.56it/s] 48%|████▊     | 201/415 [01:20<00:59,  3.57it/s] 49%|████▊     | 202/415 [01:21<00:59,  3.57it/s] 49%|████▉     | 203/415 [01:21<00:59,  3.58it/s] 49%|████▉     | 204/415 [01:21<00:58,  3.58it/s] 49%|████▉     | 205/415 [01:21<00:58,  3.58it/s] 50%|████▉     | 206/415 [01:22<00:58,  3.58it/s] 50%|████▉     | 207/415 [01:22<00:58,  3.58it/s] 50%|█████     | 208/415 [01:22<00:57,  3.58it/s] 50%|█████     | 209/415 [01:23<00:57,  3.58it/s] 51%|█████     | 210/415 [01:23<00:57,  3.57it/s] 51%|█████     | 211/415 [01:23<00:57,  3.57it/s] 51%|█████     | 212/415 [01:23<00:56,  3.57it/s] 51%|█████▏    | 213/415 [01:24<00:56,  3.58it/s] 52%|█████▏    | 214/415 [01:24<00:56,  3.58it/s] 52%|█████▏    | 215/415 [01:24<00:55,  3.58it/s] 52%|█████▏    | 216/415 [01:24<00:55,  3.58it/s] 52%|█████▏    | 217/415 [01:25<00:55,  3.58it/s] 53%|█████▎    | 218/415 [01:25<00:55,  3.58it/s] 53%|█████▎    | 219/415 [01:25<00:54,  3.58it/s] 53%|█████▎    | 220/415 [01:26<00:54,  3.58it/s] 53%|█████▎    | 221/415 [01:26<00:54,  3.57it/s] 53%|█████▎    | 222/415 [01:26<00:54,  3.57it/s] 54%|█████▎    | 223/415 [01:26<00:53,  3.57it/s] 54%|█████▍    | 224/415 [01:27<00:53,  3.57it/s] 54%|█████▍    | 225/415 [01:27<00:53,  3.57it/s] 54%|█████▍    | 226/415 [01:27<00:52,  3.57it/s] 55%|█████▍    | 227/415 [01:28<00:52,  3.57it/s] 55%|█████▍    | 228/415 [01:28<00:52,  3.57it/s] 55%|█████▌    | 229/415 [01:28<00:52,  3.57it/s] 55%|█████▌    | 230/415 [01:28<00:51,  3.58it/s] 56%|█████▌    | 231/415 [01:29<00:51,  3.58it/s] 56%|█████▌    | 232/415 [01:29<00:51,  3.57it/s] 56%|█████▌    | 233/415 [01:29<00:50,  3.57it/s] 56%|█████▋    | 234/415 [01:29<00:50,  3.57it/s] 57%|█████▋    | 235/415 [01:30<00:50,  3.57it/s] 57%|█████▋    | 236/415 [01:30<00:50,  3.58it/s] 57%|█████▋    | 237/415 [01:30<00:49,  3.58it/s] 57%|█████▋    | 238/415 [01:31<00:49,  3.57it/s] 58%|█████▊    | 239/415 [01:31<00:49,  3.58it/s] 58%|█████▊    | 240/415 [01:31<00:48,  3.57it/s] 58%|█████▊    | 241/415 [01:31<00:48,  3.58it/s] 58%|█████▊    | 242/415 [01:32<00:48,  3.57it/s] 59%|█████▊    | 243/415 [01:32<00:48,  3.56it/s] 59%|█████▉    | 244/415 [01:32<00:47,  3.57it/s] 59%|█████▉    | 245/415 [01:33<00:47,  3.57it/s] 59%|█████▉    | 246/415 [01:33<00:47,  3.57it/s] 60%|█████▉    | 247/415 [01:33<00:47,  3.57it/s] 60%|█████▉    | 248/415 [01:33<00:46,  3.57it/s] 60%|██████    | 249/415 [01:34<00:44,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 05:29:07,117 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:29:07,117 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 05:29:07,117 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4679, 'eval_samples_per_second': 368.19, 'eval_steps_per_second': 46.05, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.50it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.99it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.14it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.42it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.96it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.67it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.46it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.11it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.07it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.04it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.11it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.09it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.10it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.17it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.13it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.11it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.97it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.02it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.02it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.02it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.00it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.03it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.15it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.10it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.06it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.05it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.00it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.00it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.99it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.05it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.08it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.10it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.10it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.01it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.03it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.08it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.05it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.06it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.00it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.04it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.03it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.10it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.00it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.03it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.07it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.99it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.98it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.02it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.99it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.05it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.07it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.99it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.00it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.04it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.06it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.08it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.99it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.07it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.08it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.06it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.02it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.01it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.04it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 45.99it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.08it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.10it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.03it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.05it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.06it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.99it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.96it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.04it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.04it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.00it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.07it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.08it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.06it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.98it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.02it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.95it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.04it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.07it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.10it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.08it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.95it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.03it/s][A                                                 
                                                 [A 60%|██████    | 249/415 [01:43<00:44,  3.77it/s]
100%|██████████| 436/436 [00:09<00:00, 46.03it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:29:16,597 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-249
[INFO|configuration_utils.py:351] 2023-08-28 05:29:16,617 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-249/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:29:18,858 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-249/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:29:18,875 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-249/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:29:18,888 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-249/special_tokens_map.json
 60%|██████    | 250/415 [01:46<10:40,  3.88s/it] 60%|██████    | 251/415 [01:46<07:39,  2.80s/it] 61%|██████    | 252/415 [01:47<05:33,  2.05s/it] 61%|██████    | 253/415 [01:47<04:05,  1.52s/it] 61%|██████    | 254/415 [01:47<03:04,  1.15s/it] 61%|██████▏   | 255/415 [01:47<02:21,  1.13it/s] 62%|██████▏   | 256/415 [01:48<01:51,  1.42it/s] 62%|██████▏   | 257/415 [01:48<01:31,  1.74it/s] 62%|██████▏   | 258/415 [01:48<01:16,  2.05it/s] 62%|██████▏   | 259/415 [01:48<01:06,  2.36it/s] 63%|██████▎   | 260/415 [01:49<00:59,  2.63it/s] 63%|██████▎   | 261/415 [01:49<00:53,  2.85it/s] 63%|██████▎   | 262/415 [01:49<00:50,  3.04it/s] 63%|██████▎   | 263/415 [01:50<00:47,  3.18it/s] 64%|██████▎   | 264/415 [01:50<00:46,  3.26it/s] 64%|██████▍   | 265/415 [01:50<00:44,  3.35it/s] 64%|██████▍   | 266/415 [01:50<00:43,  3.42it/s] 64%|██████▍   | 267/415 [01:51<00:42,  3.47it/s] 65%|██████▍   | 268/415 [01:51<00:42,  3.44it/s] 65%|██████▍   | 269/415 [01:51<00:42,  3.47it/s] 65%|██████▌   | 270/415 [01:52<00:41,  3.51it/s] 65%|██████▌   | 271/415 [01:52<00:40,  3.53it/s] 66%|██████▌   | 272/415 [01:52<00:40,  3.55it/s] 66%|██████▌   | 273/415 [01:52<00:39,  3.56it/s] 66%|██████▌   | 274/415 [01:53<00:39,  3.57it/s] 66%|██████▋   | 275/415 [01:53<00:39,  3.56it/s] 67%|██████▋   | 276/415 [01:53<00:38,  3.57it/s] 67%|██████▋   | 277/415 [01:54<00:38,  3.57it/s] 67%|██████▋   | 278/415 [01:54<00:38,  3.57it/s] 67%|██████▋   | 279/415 [01:54<00:38,  3.57it/s] 67%|██████▋   | 280/415 [01:54<00:37,  3.57it/s] 68%|██████▊   | 281/415 [01:55<00:37,  3.58it/s] 68%|██████▊   | 282/415 [01:55<00:37,  3.58it/s] 68%|██████▊   | 283/415 [01:55<00:36,  3.58it/s] 68%|██████▊   | 284/415 [01:56<00:36,  3.58it/s] 69%|██████▊   | 285/415 [01:56<00:36,  3.58it/s] 69%|██████▉   | 286/415 [01:56<00:36,  3.57it/s] 69%|██████▉   | 287/415 [01:56<00:35,  3.58it/s] 69%|██████▉   | 288/415 [01:57<00:35,  3.58it/s] 70%|██████▉   | 289/415 [01:57<00:35,  3.58it/s] 70%|██████▉   | 290/415 [01:57<00:34,  3.58it/s] 70%|███████   | 291/415 [01:57<00:34,  3.58it/s] 70%|███████   | 292/415 [01:58<00:34,  3.58it/s] 71%|███████   | 293/415 [01:58<00:34,  3.58it/s] 71%|███████   | 294/415 [01:58<00:33,  3.59it/s] 71%|███████   | 295/415 [01:59<00:33,  3.59it/s] 71%|███████▏  | 296/415 [01:59<00:33,  3.59it/s] 72%|███████▏  | 297/415 [01:59<00:33,  3.57it/s] 72%|███████▏  | 298/415 [01:59<00:32,  3.58it/s] 72%|███████▏  | 299/415 [02:00<00:32,  3.58it/s] 72%|███████▏  | 300/415 [02:00<00:32,  3.58it/s] 73%|███████▎  | 301/415 [02:00<00:31,  3.58it/s] 73%|███████▎  | 302/415 [02:01<00:31,  3.58it/s] 73%|███████▎  | 303/415 [02:01<00:31,  3.58it/s] 73%|███████▎  | 304/415 [02:01<00:31,  3.58it/s] 73%|███████▎  | 305/415 [02:01<00:30,  3.58it/s] 74%|███████▎  | 306/415 [02:02<00:30,  3.58it/s] 74%|███████▍  | 307/415 [02:02<00:30,  3.58it/s] 74%|███████▍  | 308/415 [02:02<00:29,  3.57it/s] 74%|███████▍  | 309/415 [02:02<00:29,  3.57it/s] 75%|███████▍  | 310/415 [02:03<00:29,  3.58it/s] 75%|███████▍  | 311/415 [02:03<00:29,  3.58it/s] 75%|███████▌  | 312/415 [02:03<00:28,  3.58it/s] 75%|███████▌  | 313/415 [02:04<00:28,  3.57it/s] 76%|███████▌  | 314/415 [02:04<00:28,  3.57it/s] 76%|███████▌  | 315/415 [02:04<00:27,  3.57it/s] 76%|███████▌  | 316/415 [02:04<00:27,  3.58it/s] 76%|███████▋  | 317/415 [02:05<00:27,  3.57it/s] 77%|███████▋  | 318/415 [02:05<00:27,  3.58it/s] 77%|███████▋  | 319/415 [02:05<00:26,  3.56it/s] 77%|███████▋  | 320/415 [02:06<00:26,  3.57it/s] 77%|███████▋  | 321/415 [02:06<00:26,  3.57it/s] 78%|███████▊  | 322/415 [02:06<00:26,  3.57it/s] 78%|███████▊  | 323/415 [02:06<00:25,  3.57it/s] 78%|███████▊  | 324/415 [02:07<00:25,  3.57it/s] 78%|███████▊  | 325/415 [02:07<00:25,  3.57it/s] 79%|███████▊  | 326/415 [02:07<00:24,  3.57it/s] 79%|███████▉  | 327/415 [02:08<00:24,  3.58it/s] 79%|███████▉  | 328/415 [02:08<00:24,  3.58it/s] 79%|███████▉  | 329/415 [02:08<00:24,  3.58it/s] 80%|███████▉  | 330/415 [02:08<00:23,  3.57it/s] 80%|███████▉  | 331/415 [02:09<00:23,  3.56it/s] 80%|████████  | 332/415 [02:09<00:22,  3.76it/s][INFO|trainer.py:2140] 2023-08-28 05:29:42,348 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:29:42,349 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 05:29:42,349 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4675, 'eval_samples_per_second': 368.206, 'eval_steps_per_second': 46.052, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.60it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.09it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.15it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.47it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.09it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.76it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.43it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.10it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.19it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.21it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.18it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.28it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.25it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.23it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.07it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.99it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.94it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.95it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.07it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.05it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.18it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.18it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.18it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.08it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.93it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.89it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.93it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.08it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.10it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.17it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.17it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.11it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.03it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.96it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.94it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.80it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.97it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.06it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.11it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.18it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.03it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.08it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.93it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.98it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.93it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.02it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.98it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.03it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.14it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.08it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.10it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.95it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.94it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.94it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.96it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.04it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.04it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.08it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.12it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.04it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.04it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.94it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.92it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 45.91it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.06it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.07it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.10it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.04it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.05it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.02it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.90it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.97it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.93it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.04it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.07it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.10it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.12it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.05it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.04it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.94it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.93it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.84it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.99it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.03it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.06it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.12it/s][A                                                 
                                                 [A 80%|████████  | 332/415 [02:18<00:22,  3.76it/s]
100%|██████████| 436/436 [00:09<00:00, 46.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:29:51,842 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-332
[INFO|configuration_utils.py:351] 2023-08-28 05:29:51,863 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-332/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:29:54,183 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-332/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:29:54,199 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-332/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:29:54,208 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-332/special_tokens_map.json
 80%|████████  | 333/415 [02:21<05:20,  3.91s/it] 80%|████████  | 334/415 [02:22<03:48,  2.82s/it] 81%|████████  | 335/415 [02:22<02:44,  2.06s/it] 81%|████████  | 336/415 [02:22<02:00,  1.53s/it] 81%|████████  | 337/415 [02:22<01:29,  1.15s/it] 81%|████████▏ | 338/415 [02:23<01:08,  1.12it/s] 82%|████████▏ | 339/415 [02:23<00:53,  1.41it/s] 82%|████████▏ | 340/415 [02:23<00:43,  1.73it/s] 82%|████████▏ | 341/415 [02:24<00:36,  2.05it/s] 82%|████████▏ | 342/415 [02:24<00:31,  2.35it/s] 83%|████████▎ | 343/415 [02:24<00:27,  2.61it/s] 83%|████████▎ | 344/415 [02:24<00:24,  2.84it/s] 83%|████████▎ | 345/415 [02:25<00:23,  3.03it/s] 83%|████████▎ | 346/415 [02:25<00:21,  3.18it/s] 84%|████████▎ | 347/415 [02:25<00:20,  3.29it/s] 84%|████████▍ | 348/415 [02:25<00:19,  3.37it/s] 84%|████████▍ | 349/415 [02:26<00:19,  3.43it/s] 84%|████████▍ | 350/415 [02:26<00:18,  3.47it/s] 85%|████████▍ | 351/415 [02:26<00:18,  3.50it/s] 85%|████████▍ | 352/415 [02:27<00:17,  3.52it/s] 85%|████████▌ | 353/415 [02:27<00:17,  3.54it/s] 85%|████████▌ | 354/415 [02:27<00:17,  3.54it/s] 86%|████████▌ | 355/415 [02:27<00:16,  3.55it/s] 86%|████████▌ | 356/415 [02:28<00:16,  3.56it/s] 86%|████████▌ | 357/415 [02:28<00:16,  3.56it/s] 86%|████████▋ | 358/415 [02:28<00:15,  3.57it/s] 87%|████████▋ | 359/415 [02:29<00:15,  3.56it/s] 87%|████████▋ | 360/415 [02:29<00:15,  3.57it/s] 87%|████████▋ | 361/415 [02:29<00:15,  3.57it/s] 87%|████████▋ | 362/415 [02:29<00:14,  3.57it/s] 87%|████████▋ | 363/415 [02:30<00:14,  3.57it/s] 88%|████████▊ | 364/415 [02:30<00:14,  3.57it/s] 88%|████████▊ | 365/415 [02:30<00:14,  3.56it/s] 88%|████████▊ | 366/415 [02:31<00:13,  3.57it/s] 88%|████████▊ | 367/415 [02:31<00:13,  3.57it/s] 89%|████████▊ | 368/415 [02:31<00:13,  3.57it/s] 89%|████████▉ | 369/415 [02:31<00:12,  3.57it/s] 89%|████████▉ | 370/415 [02:32<00:12,  3.57it/s] 89%|████████▉ | 371/415 [02:32<00:12,  3.57it/s] 90%|████████▉ | 372/415 [02:32<00:12,  3.57it/s] 90%|████████▉ | 373/415 [02:32<00:11,  3.57it/s] 90%|█████████ | 374/415 [02:33<00:11,  3.57it/s] 90%|█████████ | 375/415 [02:33<00:11,  3.57it/s] 91%|█████████ | 376/415 [02:33<00:10,  3.56it/s] 91%|█████████ | 377/415 [02:34<00:10,  3.56it/s] 91%|█████████ | 378/415 [02:34<00:10,  3.57it/s] 91%|█████████▏| 379/415 [02:34<00:10,  3.57it/s] 92%|█████████▏| 380/415 [02:34<00:09,  3.58it/s] 92%|█████████▏| 381/415 [02:35<00:09,  3.58it/s] 92%|█████████▏| 382/415 [02:35<00:09,  3.58it/s] 92%|█████████▏| 383/415 [02:35<00:08,  3.58it/s] 93%|█████████▎| 384/415 [02:36<00:08,  3.58it/s] 93%|█████████▎| 385/415 [02:36<00:08,  3.58it/s] 93%|█████████▎| 386/415 [02:36<00:08,  3.58it/s] 93%|█████████▎| 387/415 [02:36<00:07,  3.56it/s] 93%|█████████▎| 388/415 [02:37<00:07,  3.56it/s] 94%|█████████▎| 389/415 [02:37<00:07,  3.57it/s] 94%|█████████▍| 390/415 [02:37<00:06,  3.57it/s] 94%|█████████▍| 391/415 [02:38<00:06,  3.57it/s] 94%|█████████▍| 392/415 [02:38<00:06,  3.57it/s] 95%|█████████▍| 393/415 [02:38<00:06,  3.57it/s] 95%|█████████▍| 394/415 [02:38<00:05,  3.57it/s] 95%|█████████▌| 395/415 [02:39<00:05,  3.58it/s] 95%|█████████▌| 396/415 [02:39<00:05,  3.57it/s] 96%|█████████▌| 397/415 [02:39<00:05,  3.57it/s] 96%|█████████▌| 398/415 [02:39<00:04,  3.56it/s] 96%|█████████▌| 399/415 [02:40<00:04,  3.57it/s] 96%|█████████▋| 400/415 [02:40<00:04,  3.57it/s] 97%|█████████▋| 401/415 [02:40<00:03,  3.57it/s] 97%|█████████▋| 402/415 [02:41<00:03,  3.57it/s] 97%|█████████▋| 403/415 [02:41<00:03,  3.57it/s] 97%|█████████▋| 404/415 [02:41<00:03,  3.57it/s] 98%|█████████▊| 405/415 [02:41<00:02,  3.58it/s] 98%|█████████▊| 406/415 [02:42<00:02,  3.58it/s] 98%|█████████▊| 407/415 [02:42<00:02,  3.58it/s] 98%|█████████▊| 408/415 [02:42<00:01,  3.57it/s] 99%|█████████▊| 409/415 [02:43<00:01,  3.57it/s] 99%|█████████▉| 410/415 [02:43<00:01,  3.57it/s] 99%|█████████▉| 411/415 [02:43<00:01,  3.56it/s] 99%|█████████▉| 412/415 [02:43<00:00,  3.56it/s]100%|█████████▉| 413/415 [02:44<00:00,  3.56it/s]100%|█████████▉| 414/415 [02:44<00:00,  3.57it/s]100%|██████████| 415/415 [02:44<00:00,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 05:30:17,680 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:30:17,680 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 05:30:17,680 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4666, 'eval_samples_per_second': 368.243, 'eval_steps_per_second': 46.057, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.68it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.15it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.24it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.53it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.19it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.81it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.44it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.03it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.98it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.11it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.19it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.24it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.23it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.26it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.23it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.08it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.93it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.90it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.98it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.05it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.13it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.20it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.22it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.12it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.94it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.93it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.88it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.03it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.07it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.08it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.19it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.23it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.17it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.05it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.96it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.88it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.88it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.00it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.00it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.11it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.07it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.08it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.00it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.94it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.93it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.02it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.98it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.05it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.16it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.07it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.17it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.04it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.98it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.93it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.00it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.00it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.97it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.96it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.02it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.05it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.03it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.97it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 44.40it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 44.88it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.31it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.55it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.61it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.78it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.79it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.91it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.77it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.85it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.92it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.87it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.99it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.99it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.06it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.10it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.01it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.95it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.90it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.00it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.01it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.08it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.04it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.07it/s][A                                                 
                                                 [A100%|██████████| 415/415 [02:54<00:00,  3.77it/s]
100%|██████████| 436/436 [00:09<00:00, 46.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:30:27,177 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-415
[INFO|configuration_utils.py:351] 2023-08-28 05:30:27,197 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-415/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:30:29,738 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-415/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:30:29,750 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-415/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:30:29,765 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-415/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 05:30:30,050 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 05:30:30,050 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83 (score: 1.0161933898925781).
                                                 100%|██████████| 415/415 [02:58<00:00,  3.77it/s]100%|██████████| 415/415 [02:58<00:00,  2.32it/s]
[INFO|trainer.py:1894] 2023-08-28 05:30:31,923 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 05:30:31,939 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:30:34,318 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:30:34,330 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:30:34,338 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 05:30:34,520 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:34,520 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:34,520 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:34,520 >>   train_runtime            = 0:02:58.94
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:34,520 >>   train_samples            =       5300
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:34,520 >>   train_samples_per_second =    148.091
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:34,520 >>   train_steps_per_second   =      2.319
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4802, 'eval_samples_per_second': 367.712, 'eval_steps_per_second': 45.99, 'epoch': 5.0}
{'train_runtime': 178.9438, 'train_samples_per_second': 148.091, 'train_steps_per_second': 2.319, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 05:30:34 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 05:30:34,551 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:30:34,552 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 05:30:34,552 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 57.20it/s]  3%|▎         | 12/436 [00:00<00:08, 50.44it/s]  4%|▍         | 18/436 [00:00<00:08, 48.58it/s]  5%|▌         | 23/436 [00:00<00:08, 47.77it/s]  6%|▋         | 28/436 [00:00<00:08, 47.40it/s]  8%|▊         | 33/436 [00:00<00:08, 47.15it/s]  9%|▊         | 38/436 [00:00<00:08, 46.99it/s] 10%|▉         | 43/436 [00:00<00:08, 46.71it/s] 11%|█         | 48/436 [00:01<00:08, 46.31it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.29it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.20it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.33it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.37it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.39it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.23it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.57it/s] 20%|██        | 88/436 [00:01<00:07, 46.41it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.23it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.18it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.11it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.16it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.22it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.27it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.37it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.39it/s] 31%|███       | 133/436 [00:02<00:06, 46.31it/s] 32%|███▏      | 138/436 [00:02<00:06, 46.19it/s] 33%|███▎      | 143/436 [00:03<00:06, 46.14it/s] 34%|███▍      | 148/436 [00:03<00:06, 46.19it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.25it/s] 36%|███▌      | 158/436 [00:03<00:06, 46.27it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.27it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.30it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.34it/s] 41%|████      | 178/436 [00:03<00:05, 46.28it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.21it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.11it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.04it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.21it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.27it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.33it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.32it/s] 50%|█████     | 218/436 [00:04<00:04, 46.22it/s] 51%|█████     | 223/436 [00:04<00:04, 46.21it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.15it/s] 53%|█████▎    | 233/436 [00:05<00:04, 46.05it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.08it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.14it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.19it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.19it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.26it/s] 60%|██████    | 263/436 [00:05<00:03, 46.21it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.27it/s] 63%|██████▎   | 273/436 [00:05<00:03, 46.19it/s] 64%|██████▍   | 278/436 [00:05<00:03, 46.04it/s] 65%|██████▍   | 283/436 [00:06<00:03, 46.13it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.13it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.22it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.28it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.16it/s] 71%|███████   | 308/436 [00:06<00:02, 46.20it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.18it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.16it/s] 74%|███████▍  | 323/436 [00:06<00:02, 46.17it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.18it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.04it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.08it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.18it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.21it/s] 81%|████████  | 353/436 [00:07<00:01, 46.27it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.21it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.13it/s] 84%|████████▍ | 368/436 [00:07<00:01, 46.17it/s] 86%|████████▌ | 373/436 [00:08<00:01, 45.96it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.05it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.07it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.08it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.12it/s] 91%|█████████▏| 398/436 [00:08<00:00, 46.12it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.12it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.11it/s] 95%|█████████▍| 413/436 [00:08<00:00, 46.16it/s] 96%|█████████▌| 418/436 [00:09<00:00, 46.11it/s] 97%|█████████▋| 423/436 [00:09<00:00, 46.16it/s] 98%|█████████▊| 428/436 [00:09<00:00, 46.18it/s] 99%|█████████▉| 433/436 [00:09<00:00, 46.10it/s]100%|██████████| 436/436 [00:09<00:00, 46.32it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 05:30:43,990 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:43,990 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:43,990 >>   eval_loss               =     1.0162
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:43,990 >>   eval_runtime            = 0:00:09.43
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:43,990 >>   eval_samples            =       3486
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:43,990 >>   eval_samples_per_second =    369.359
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:43,990 >>   eval_steps_per_second   =     46.196
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:30:43,990 >>   perplexity              =     2.7627
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:30:55,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:30:55,351 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:30:55,351 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:30:55,351 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:30:55,351 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:30:55,955 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:30:55,956 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:30:56,549 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:30:58,461 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:30:58,465 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:01,338 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:01,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:01,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:01,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:01,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:31:02,017 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:31:02,021 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:31:02,608 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:31:03,308 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:31:03,308 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-249
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-415
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-166
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-83
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/checkpoint-332
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'manufacturer', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13015
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13115, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.14it/s]Extractor Predicting: 2it [00:01,  1.18it/s]Extractor Predicting: 3it [00:02,  1.24it/s]Extractor Predicting: 4it [00:03,  1.23it/s]Extractor Predicting: 5it [00:04,  1.23it/s]Extractor Predicting: 6it [00:04,  1.24it/s]Extractor Predicting: 7it [00:05,  1.23it/s]Extractor Predicting: 8it [00:06,  1.25it/s]Extractor Predicting: 9it [00:07,  1.25it/s]Extractor Predicting: 10it [00:08,  1.28it/s]Extractor Predicting: 11it [00:08,  1.24it/s]Extractor Predicting: 12it [00:09,  1.24it/s]Extractor Predicting: 13it [00:10,  1.23it/s]Extractor Predicting: 14it [00:11,  1.23it/s]Extractor Predicting: 15it [00:12,  1.24it/s]Extractor Predicting: 16it [00:12,  1.24it/s]Extractor Predicting: 17it [00:13,  1.27it/s]Extractor Predicting: 18it [00:14,  1.28it/s]Extractor Predicting: 19it [00:15,  1.27it/s]Extractor Predicting: 20it [00:16,  1.27it/s]Extractor Predicting: 21it [00:16,  1.27it/s]Extractor Predicting: 22it [00:17,  1.30it/s]Extractor Predicting: 23it [00:18,  1.29it/s]Extractor Predicting: 24it [00:19,  1.25it/s]Extractor Predicting: 25it [00:19,  1.25it/s]Extractor Predicting: 26it [00:20,  1.23it/s]Extractor Predicting: 27it [00:21,  1.25it/s]Extractor Predicting: 28it [00:22,  1.25it/s]Extractor Predicting: 29it [00:23,  1.24it/s]Extractor Predicting: 30it [00:24,  1.21it/s]Extractor Predicting: 31it [00:24,  1.22it/s]Extractor Predicting: 32it [00:25,  1.23it/s]Extractor Predicting: 33it [00:26,  1.23it/s]Extractor Predicting: 34it [00:27,  1.24it/s]Extractor Predicting: 35it [00:28,  1.25it/s]Extractor Predicting: 36it [00:28,  1.21it/s]Extractor Predicting: 37it [00:29,  1.24it/s]Extractor Predicting: 38it [00:30,  1.27it/s]Extractor Predicting: 39it [00:31,  1.29it/s]Extractor Predicting: 40it [00:31,  1.31it/s]Extractor Predicting: 41it [00:32,  1.30it/s]Extractor Predicting: 42it [00:33,  1.30it/s]Extractor Predicting: 43it [00:34,  1.29it/s]Extractor Predicting: 44it [00:35,  1.27it/s]Extractor Predicting: 45it [00:35,  1.31it/s]Extractor Predicting: 46it [00:36,  1.30it/s]Extractor Predicting: 47it [00:37,  1.31it/s]Extractor Predicting: 48it [00:38,  1.30it/s]Extractor Predicting: 49it [00:38,  1.33it/s]Extractor Predicting: 50it [00:39,  1.31it/s]Extractor Predicting: 51it [00:40,  1.30it/s]Extractor Predicting: 52it [00:41,  1.29it/s]Extractor Predicting: 53it [00:41,  1.29it/s]Extractor Predicting: 54it [00:42,  1.27it/s]Extractor Predicting: 55it [00:43,  1.31it/s]Extractor Predicting: 56it [00:44,  1.33it/s]Extractor Predicting: 57it [00:45,  1.30it/s]Extractor Predicting: 58it [00:45,  1.29it/s]Extractor Predicting: 59it [00:46,  1.31it/s]Extractor Predicting: 60it [00:47,  1.29it/s]Extractor Predicting: 61it [00:48,  1.28it/s]Extractor Predicting: 62it [00:48,  1.29it/s]Extractor Predicting: 63it [00:49,  1.22it/s]Extractor Predicting: 64it [00:50,  1.25it/s]Extractor Predicting: 65it [00:51,  1.26it/s]Extractor Predicting: 66it [00:52,  1.28it/s]Extractor Predicting: 67it [00:52,  1.31it/s]Extractor Predicting: 68it [00:53,  1.28it/s]Extractor Predicting: 69it [00:54,  1.29it/s]Extractor Predicting: 70it [00:55,  1.28it/s]Extractor Predicting: 71it [00:55,  1.31it/s]Extractor Predicting: 72it [00:56,  1.32it/s]Extractor Predicting: 73it [00:57,  1.31it/s]Extractor Predicting: 74it [00:58,  1.32it/s]Extractor Predicting: 75it [00:59,  1.29it/s]Extractor Predicting: 76it [00:59,  1.29it/s]Extractor Predicting: 77it [01:00,  1.29it/s]Extractor Predicting: 78it [01:01,  1.30it/s]Extractor Predicting: 79it [01:02,  1.30it/s]Extractor Predicting: 80it [01:02,  1.30it/s]Extractor Predicting: 81it [01:03,  1.30it/s]Extractor Predicting: 82it [01:04,  1.30it/s]Extractor Predicting: 83it [01:05,  1.26it/s]Extractor Predicting: 84it [01:06,  1.28it/s]Extractor Predicting: 85it [01:06,  1.27it/s]Extractor Predicting: 86it [01:07,  1.27it/s]Extractor Predicting: 87it [01:08,  1.27it/s]Extractor Predicting: 88it [01:09,  1.30it/s]Extractor Predicting: 89it [01:09,  1.31it/s]Extractor Predicting: 90it [01:10,  1.35it/s]Extractor Predicting: 91it [01:11,  1.38it/s]Extractor Predicting: 92it [01:11,  1.40it/s]Extractor Predicting: 93it [01:12,  1.36it/s]Extractor Predicting: 94it [01:13,  1.40it/s]Extractor Predicting: 95it [01:14,  1.39it/s]Extractor Predicting: 96it [01:14,  1.38it/s]Extractor Predicting: 97it [01:15,  1.38it/s]Extractor Predicting: 98it [01:16,  1.35it/s]Extractor Predicting: 99it [01:17,  1.32it/s]Extractor Predicting: 100it [01:17,  1.32it/s]Extractor Predicting: 101it [01:18,  1.37it/s]Extractor Predicting: 102it [01:19,  1.40it/s]Extractor Predicting: 103it [01:19,  1.40it/s]Extractor Predicting: 104it [01:20,  1.38it/s]Extractor Predicting: 105it [01:21,  1.39it/s]Extractor Predicting: 106it [01:22,  1.39it/s]Extractor Predicting: 107it [01:22,  1.38it/s]Extractor Predicting: 108it [01:23,  1.38it/s]Extractor Predicting: 109it [01:24,  1.39it/s]Extractor Predicting: 110it [01:25,  1.38it/s]Extractor Predicting: 111it [01:25,  1.39it/s]Extractor Predicting: 112it [01:26,  1.43it/s]Extractor Predicting: 113it [01:27,  1.45it/s]Extractor Predicting: 114it [01:27,  1.43it/s]Extractor Predicting: 115it [01:28,  1.42it/s]Extractor Predicting: 116it [01:29,  1.41it/s]Extractor Predicting: 117it [01:29,  1.42it/s]Extractor Predicting: 118it [01:30,  1.44it/s]Extractor Predicting: 119it [01:31,  1.42it/s]Extractor Predicting: 120it [01:32,  1.40it/s]Extractor Predicting: 121it [01:32,  1.46it/s]Extractor Predicting: 122it [01:33,  1.45it/s]Extractor Predicting: 123it [01:34,  1.39it/s]Extractor Predicting: 124it [01:34,  1.43it/s]Extractor Predicting: 125it [01:35,  1.42it/s]Extractor Predicting: 126it [01:36,  1.47it/s]Extractor Predicting: 127it [01:36,  1.47it/s]Extractor Predicting: 128it [01:37,  1.46it/s]Extractor Predicting: 129it [01:38,  1.42it/s]Extractor Predicting: 130it [01:38,  1.44it/s]Extractor Predicting: 131it [01:39,  1.47it/s]Extractor Predicting: 132it [01:40,  1.45it/s]Extractor Predicting: 133it [01:41,  1.42it/s]Extractor Predicting: 134it [01:41,  1.45it/s]Extractor Predicting: 135it [01:42,  1.46it/s]Extractor Predicting: 136it [01:43,  1.47it/s]Extractor Predicting: 137it [01:43,  1.34it/s]Extractor Predicting: 138it [01:44,  1.41it/s]Extractor Predicting: 139it [01:45,  1.44it/s]Extractor Predicting: 140it [01:45,  1.46it/s]Extractor Predicting: 141it [01:46,  1.50it/s]Extractor Predicting: 142it [01:47,  1.47it/s]Extractor Predicting: 143it [01:47,  1.44it/s]Extractor Predicting: 144it [01:48,  1.79it/s]Extractor Predicting: 144it [01:48,  1.33it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:33:00,101 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:33:00,105 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:33:00,105 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:33:00,105 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:33:00,105 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:33:00,811 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:33:00,812 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:33:01,408 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:33:02,460 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:33:02,460 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:33:05,349 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:33:05,355 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:33:05,355 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:33:05,355 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:33:05,355 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:33:06,094 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:33:06,095 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:33:06,680 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:33:06,845 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:33:06,846 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13522
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13622, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.33it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.38it/s]Extractor Predicting: 6it [00:04,  1.35it/s]Extractor Predicting: 7it [00:05,  1.35it/s]Extractor Predicting: 8it [00:05,  1.38it/s]Extractor Predicting: 9it [00:06,  1.38it/s]Extractor Predicting: 10it [00:07,  1.41it/s]Extractor Predicting: 11it [00:08,  1.38it/s]Extractor Predicting: 12it [00:08,  1.38it/s]Extractor Predicting: 13it [00:09,  1.35it/s]Extractor Predicting: 14it [00:10,  1.38it/s]Extractor Predicting: 15it [00:10,  1.38it/s]Extractor Predicting: 16it [00:11,  1.33it/s]Extractor Predicting: 17it [00:12,  1.34it/s]Extractor Predicting: 18it [00:13,  1.33it/s]Extractor Predicting: 19it [00:13,  1.35it/s]Extractor Predicting: 20it [00:14,  1.34it/s]Extractor Predicting: 21it [00:15,  1.35it/s]Extractor Predicting: 22it [00:16,  1.36it/s]Extractor Predicting: 23it [00:16,  1.34it/s]Extractor Predicting: 24it [00:17,  1.33it/s]Extractor Predicting: 25it [00:18,  1.33it/s]Extractor Predicting: 26it [00:19,  1.33it/s]Extractor Predicting: 27it [00:19,  1.33it/s]Extractor Predicting: 28it [00:20,  1.35it/s]Extractor Predicting: 29it [00:21,  1.35it/s]Extractor Predicting: 30it [00:22,  1.32it/s]Extractor Predicting: 31it [00:22,  1.34it/s]Extractor Predicting: 32it [00:23,  1.32it/s]Extractor Predicting: 33it [00:24,  1.33it/s]Extractor Predicting: 34it [00:25,  1.34it/s]Extractor Predicting: 35it [00:25,  1.34it/s]Extractor Predicting: 36it [00:26,  1.38it/s]Extractor Predicting: 37it [00:27,  1.34it/s]Extractor Predicting: 38it [00:28,  1.32it/s]Extractor Predicting: 39it [00:28,  1.32it/s]Extractor Predicting: 40it [00:29,  1.30it/s]Extractor Predicting: 41it [00:30,  1.30it/s]Extractor Predicting: 42it [00:31,  1.31it/s]Extractor Predicting: 43it [00:32,  1.31it/s]Extractor Predicting: 44it [00:32,  1.30it/s]Extractor Predicting: 45it [00:33,  1.31it/s]Extractor Predicting: 46it [00:34,  1.31it/s]Extractor Predicting: 47it [00:35,  1.33it/s]Extractor Predicting: 48it [00:35,  1.33it/s]Extractor Predicting: 49it [00:36,  1.36it/s]Extractor Predicting: 50it [00:37,  1.35it/s]Extractor Predicting: 51it [00:37,  1.39it/s]Extractor Predicting: 52it [00:38,  1.40it/s]Extractor Predicting: 53it [00:39,  1.39it/s]Extractor Predicting: 54it [00:40,  1.38it/s]Extractor Predicting: 55it [00:40,  1.38it/s]Extractor Predicting: 56it [00:41,  1.35it/s]Extractor Predicting: 57it [00:42,  1.30it/s]Extractor Predicting: 58it [00:43,  1.28it/s]Extractor Predicting: 59it [00:43,  1.30it/s]Extractor Predicting: 60it [00:44,  1.32it/s]Extractor Predicting: 61it [00:45,  1.22it/s]Extractor Predicting: 62it [00:46,  1.23it/s]Extractor Predicting: 63it [00:47,  1.30it/s]Extractor Predicting: 64it [00:47,  1.29it/s]Extractor Predicting: 65it [00:48,  1.32it/s]Extractor Predicting: 66it [00:49,  1.31it/s]Extractor Predicting: 67it [00:50,  1.31it/s]Extractor Predicting: 68it [00:50,  1.30it/s]Extractor Predicting: 69it [00:51,  1.29it/s]Extractor Predicting: 70it [00:52,  1.24it/s]Extractor Predicting: 71it [00:53,  1.26it/s]Extractor Predicting: 72it [00:54,  1.26it/s]Extractor Predicting: 73it [00:54,  1.26it/s]Extractor Predicting: 74it [00:55,  1.27it/s]Extractor Predicting: 75it [00:56,  1.25it/s]Extractor Predicting: 76it [00:57,  1.28it/s]Extractor Predicting: 77it [00:58,  1.26it/s]Extractor Predicting: 78it [00:58,  1.27it/s]Extractor Predicting: 79it [00:59,  1.28it/s]Extractor Predicting: 80it [01:00,  1.27it/s]Extractor Predicting: 81it [01:01,  1.28it/s]Extractor Predicting: 82it [01:02,  1.25it/s]Extractor Predicting: 83it [01:02,  1.25it/s]Extractor Predicting: 84it [01:03,  1.28it/s]Extractor Predicting: 85it [01:04,  1.28it/s]Extractor Predicting: 86it [01:05,  1.29it/s]Extractor Predicting: 87it [01:06,  1.26it/s]Extractor Predicting: 88it [01:06,  1.31it/s]Extractor Predicting: 89it [01:07,  1.31it/s]Extractor Predicting: 90it [01:08,  1.30it/s]Extractor Predicting: 91it [01:08,  1.35it/s]Extractor Predicting: 92it [01:09,  1.34it/s]Extractor Predicting: 93it [01:10,  1.37it/s]Extractor Predicting: 94it [01:11,  1.35it/s]Extractor Predicting: 95it [01:11,  1.34it/s]Extractor Predicting: 96it [01:12,  1.34it/s]Extractor Predicting: 97it [01:13,  1.32it/s]Extractor Predicting: 98it [01:14,  1.31it/s]Extractor Predicting: 99it [01:14,  1.31it/s]Extractor Predicting: 100it [01:15,  1.30it/s]Extractor Predicting: 101it [01:16,  1.30it/s]Extractor Predicting: 102it [01:17,  1.26it/s]Extractor Predicting: 103it [01:18,  1.28it/s]Extractor Predicting: 104it [01:18,  1.29it/s]Extractor Predicting: 105it [01:19,  1.32it/s]Extractor Predicting: 106it [01:20,  1.32it/s]Extractor Predicting: 107it [01:21,  1.32it/s]Extractor Predicting: 108it [01:21,  1.33it/s]Extractor Predicting: 109it [01:22,  1.33it/s]Extractor Predicting: 110it [01:23,  1.31it/s]Extractor Predicting: 111it [01:24,  1.32it/s]Extractor Predicting: 112it [01:24,  1.31it/s]Extractor Predicting: 113it [01:25,  1.34it/s]Extractor Predicting: 114it [01:26,  1.32it/s]Extractor Predicting: 115it [01:27,  1.35it/s]Extractor Predicting: 116it [01:27,  1.29it/s]Extractor Predicting: 117it [01:28,  1.27it/s]Extractor Predicting: 118it [01:29,  1.27it/s]Extractor Predicting: 119it [01:30,  1.26it/s]Extractor Predicting: 120it [01:31,  1.26it/s]Extractor Predicting: 121it [01:31,  1.27it/s]Extractor Predicting: 122it [01:32,  1.31it/s]Extractor Predicting: 123it [01:33,  1.29it/s]Extractor Predicting: 124it [01:34,  1.27it/s]Extractor Predicting: 125it [01:35,  1.26it/s]Extractor Predicting: 126it [01:35,  1.23it/s]Extractor Predicting: 127it [01:36,  1.24it/s]Extractor Predicting: 128it [01:37,  1.24it/s]Extractor Predicting: 129it [01:38,  1.23it/s]Extractor Predicting: 130it [01:39,  1.27it/s]Extractor Predicting: 131it [01:40,  1.18it/s]Extractor Predicting: 132it [01:40,  1.19it/s]Extractor Predicting: 133it [01:41,  1.20it/s]Extractor Predicting: 134it [01:42,  1.23it/s]Extractor Predicting: 135it [01:43,  1.26it/s]Extractor Predicting: 136it [01:43,  1.28it/s]Extractor Predicting: 137it [01:44,  1.32it/s]Extractor Predicting: 138it [01:45,  1.30it/s]Extractor Predicting: 139it [01:46,  1.31it/s]Extractor Predicting: 140it [01:47,  1.30it/s]Extractor Predicting: 141it [01:47,  1.26it/s]Extractor Predicting: 142it [01:48,  1.27it/s]Extractor Predicting: 143it [01:49,  1.36it/s]Extractor Predicting: 143it [01:49,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:35:02,884 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:35:02,890 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:35:02,891 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:35:02,891 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:35:02,891 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:35:03,527 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:35:03,528 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:35:04,236 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:35:05,250 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:35:05,250 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:35:08,053 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:35:08,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:35:08,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:35:08,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:35:08,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:35:08,711 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:35:08,713 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:35:09,292 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:35:09,460 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:35:09,461 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 464
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 564, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.06it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 2it [00:01,  1.39it/s]
[INFO|configuration_utils.py:515] 2023-08-28 05:35:11,254 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:35:11,255 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 05:35:11,262 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:35:11,263 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 05:35:11,265 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 05:35:14,234 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 05:35:14,236 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 05:35:14,244 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:35:14,245 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 05:35:14,254 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:35:14,259 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:35:14,259 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:35:14,259 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:35:14,259 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:35:14,260 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:35:14,260 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 05:35:14,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:15,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:16,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:17,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:18,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:19,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:20,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:21,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:22,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:23,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:24,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:25,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:26,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:27,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:27,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:28,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:29,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:30,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:31,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:32,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:33,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:34,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:35,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:36,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:37,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:38,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:39,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:40,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:41,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:27<04:08, 27.58s/it][WARNING|generation_utils.py:914] 2023-08-28 05:35:42,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:43,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:43,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:44,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:45,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:46,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:47,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:48,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:49,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:50,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:51,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:52,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:53,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:54,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:55,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:56,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:57,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:58,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:35:59,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:00,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:01,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:02,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:03,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:04,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:05,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:51<03:24, 25.60s/it][WARNING|generation_utils.py:914] 2023-08-28 05:36:06,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:07,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:08,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:09,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:10,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:11,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:12,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:13,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:14,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:15,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:16,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:17,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:18,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:19,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:20,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:21,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:22,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:23,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:24,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:25,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:25,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:26,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:27,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:28,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:29,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:16<02:54, 24.98s/it][WARNING|generation_utils.py:914] 2023-08-28 05:36:30,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:31,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:32,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:33,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:34,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:35,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:36,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:36,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:37,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:38,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:39,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:40,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:41,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:42,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:43,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:44,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:45,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:46,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:47,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:48,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:49,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:50,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:36<02:20, 23.36s/it][WARNING|generation_utils.py:914] 2023-08-28 05:36:51,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:52,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:53,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:54,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:55,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:56,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:57,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:58,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:36:59,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:00,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:00,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:01,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:02,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:03,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:04,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:05,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:06,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:07,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:07,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:08,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:09,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:10,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:11,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:12,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:13,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:14,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:15,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:16,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:17,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:18,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:19,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:20,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:21,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:22,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:23,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [02:09<02:13, 26.69s/it][WARNING|generation_utils.py:914] 2023-08-28 05:37:23,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:25,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:26,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:27,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:28,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:29,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:30,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:30,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:31,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:32,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:33,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:35,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:35,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:36,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:37,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:38,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:39,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:40,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:41,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:42,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:43,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:44,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:45,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:46,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:32<01:42, 25.54s/it][WARNING|generation_utils.py:914] 2023-08-28 05:37:47,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:48,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:49,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:50,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:50,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:51,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:52,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:53,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:54,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:55,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:56,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:58,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:37:58,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:00,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:00,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:02,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:03,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:04,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:05,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:06,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:07,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:08,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:09,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:55<01:14, 24.73s/it][WARNING|generation_utils.py:914] 2023-08-28 05:38:10,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:11,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:12,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:13,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:14,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:15,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:16,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:17,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:18,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:19,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:20,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:21,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:22,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:23,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:24,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:25,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:26,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:27,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:28,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:29,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:30,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:31,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:32,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [03:19<00:48, 24.26s/it][WARNING|generation_utils.py:914] 2023-08-28 05:38:33,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:34,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:35,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:36,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:37,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:38,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:39,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:40,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:41,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:42,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:43,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:44,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:45,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:46,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:47,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:48,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:49,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:50,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:51,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:52,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:53,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:55,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:41<00:23, 23.73s/it][WARNING|generation_utils.py:914] 2023-08-28 05:38:56,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:57,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:58,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:38:59,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:00,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:01,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:02,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:02,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:03,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:05,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:06,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:07,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:08,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:08,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:10,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:11,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:12,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:13,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:14,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:15,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:16,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:17,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:39:18,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [04:04<00:00, 23.54s/it]Generating: 100%|██████████| 10/10 [04:04<00:00, 24.48s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:39:25,490 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:39:25,494 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:39:25,494 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:39:25,494 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:39:25,494 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:39:26,124 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:39:26,125 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:39:26,698 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:39:27,999 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:39:27,999 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:39:30,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:39:30,883 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:39:30,883 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:39:30,883 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:39:30,883 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:39:31,526 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:39:31,527 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:39:32,111 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:39:32,278 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:39:32,278 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 188, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 225, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 268, 'raw': 416}
{'target': 600, 'success': 289, 'raw': 448}
{'target': 600, 'success': 309, 'raw': 480}
{'target': 600, 'success': 337, 'raw': 512}
{'target': 600, 'success': 361, 'raw': 544}
{'target': 600, 'success': 378, 'raw': 576}
{'target': 600, 'success': 401, 'raw': 608}
{'target': 600, 'success': 424, 'raw': 640}
{'target': 600, 'success': 446, 'raw': 672}
{'target': 600, 'success': 465, 'raw': 704}
{'target': 600, 'success': 486, 'raw': 736}
{'target': 600, 'success': 502, 'raw': 768}
{'target': 600, 'success': 528, 'raw': 800}
{'target': 600, 'success': 549, 'raw': 832}
{'target': 600, 'success': 570, 'raw': 864}
{'target': 600, 'success': 589, 'raw': 896}
{'target': 600, 'success': 610, 'raw': 928}
{'prompt': 'Relation : country .', 'success_rate': 0.6573275862068966, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 499, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 547, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : genre . Context : Later in 2003 , he played in Steven Spielberg 's The Bourne Ultimatum . Head Entity : The Bourne Ultimatum , Tail Entity : director .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 604, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.755, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : voice type . Context : Later in the year , the band members decided to cancel their performance of " In My Time " on Broadway , in part to play the first time . Head Entity : In My Time , Tail Entity : vocal .\n']
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 102, 'raw': 160}
{'target': 600, 'success': 118, 'raw': 192}
{'target': 600, 'success': 134, 'raw': 224}
{'target': 600, 'success': 146, 'raw': 256}
{'target': 600, 'success': 160, 'raw': 288}
{'target': 600, 'success': 178, 'raw': 320}
{'target': 600, 'success': 193, 'raw': 352}
{'target': 600, 'success': 215, 'raw': 384}
{'target': 600, 'success': 233, 'raw': 416}
{'target': 600, 'success': 245, 'raw': 448}
{'target': 600, 'success': 262, 'raw': 480}
{'target': 600, 'success': 282, 'raw': 512}
{'target': 600, 'success': 304, 'raw': 544}
{'target': 600, 'success': 318, 'raw': 576}
{'target': 600, 'success': 336, 'raw': 608}
{'target': 600, 'success': 355, 'raw': 640}
{'target': 600, 'success': 375, 'raw': 672}
{'target': 600, 'success': 394, 'raw': 704}
{'target': 600, 'success': 413, 'raw': 736}
{'target': 600, 'success': 431, 'raw': 768}
{'target': 600, 'success': 450, 'raw': 800}
{'target': 600, 'success': 466, 'raw': 832}
{'target': 600, 'success': 480, 'raw': 864}
{'target': 600, 'success': 495, 'raw': 896}
{'target': 600, 'success': 511, 'raw': 928}
{'target': 600, 'success': 529, 'raw': 960}
{'target': 600, 'success': 546, 'raw': 992}
{'target': 600, 'success': 561, 'raw': 1024}
{'target': 600, 'success': 580, 'raw': 1056}
{'target': 600, 'success': 596, 'raw': 1088}
{'target': 600, 'success': 612, 'raw': 1120}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5464285714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : contains administrative territorial entity . Context : The city of Marchec is under the rule of the Marchec Municipal Municipality in Romania . Head Entity : Marchec , Tail Entity : Romanian .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8233695652173914, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8707386363636364, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 12301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.07it/s]Extractor Estimating: 2it [00:01,  1.11it/s]Extractor Estimating: 3it [00:02,  1.08it/s]Extractor Estimating: 4it [00:03,  1.15it/s]Extractor Estimating: 5it [00:04,  1.17it/s]Extractor Estimating: 6it [00:05,  1.18it/s]Extractor Estimating: 7it [00:06,  1.17it/s]Extractor Estimating: 8it [00:06,  1.21it/s]Extractor Estimating: 9it [00:07,  1.22it/s]Extractor Estimating: 10it [00:08,  1.23it/s]Extractor Estimating: 11it [00:09,  1.20it/s]Extractor Estimating: 12it [00:10,  1.20it/s]Extractor Estimating: 13it [00:10,  1.24it/s]Extractor Estimating: 14it [00:11,  1.27it/s]Extractor Estimating: 15it [00:12,  1.24it/s]Extractor Estimating: 16it [00:13,  1.23it/s]Extractor Estimating: 17it [00:14,  1.16it/s]Extractor Estimating: 18it [00:15,  1.17it/s]Extractor Estimating: 19it [00:16,  1.17it/s]Extractor Estimating: 20it [00:16,  1.19it/s]Extractor Estimating: 21it [00:17,  1.21it/s]Extractor Estimating: 22it [00:18,  1.21it/s]Extractor Estimating: 23it [00:19,  1.23it/s]Extractor Estimating: 24it [00:20,  1.19it/s]Extractor Estimating: 25it [00:20,  1.21it/s]Extractor Estimating: 26it [00:21,  1.19it/s]Extractor Estimating: 27it [00:22,  1.19it/s]Extractor Estimating: 28it [00:23,  1.20it/s]Extractor Estimating: 29it [00:24,  1.21it/s]Extractor Estimating: 30it [00:25,  1.20it/s]Extractor Estimating: 31it [00:25,  1.20it/s]Extractor Estimating: 32it [00:26,  1.22it/s]Extractor Estimating: 33it [00:27,  1.22it/s]Extractor Estimating: 34it [00:28,  1.23it/s]Extractor Estimating: 35it [00:29,  1.20it/s]Extractor Estimating: 36it [00:30,  1.21it/s]Extractor Estimating: 37it [00:30,  1.17it/s]Extractor Estimating: 38it [00:31,  1.18it/s]Extractor Estimating: 39it [00:32,  1.17it/s]Extractor Estimating: 40it [00:33,  1.19it/s]Extractor Estimating: 41it [00:34,  1.15it/s]Extractor Estimating: 42it [00:35,  1.18it/s]Extractor Estimating: 43it [00:36,  1.19it/s]Extractor Estimating: 44it [00:36,  1.22it/s]Extractor Estimating: 45it [00:37,  1.21it/s]Extractor Estimating: 46it [00:38,  1.20it/s]Extractor Estimating: 47it [00:39,  1.23it/s]Extractor Estimating: 48it [00:40,  1.22it/s]Extractor Estimating: 49it [00:40,  1.21it/s]Extractor Estimating: 50it [00:41,  1.23it/s]Extractor Estimating: 51it [00:42,  1.25it/s]Extractor Estimating: 52it [00:43,  1.22it/s]Extractor Estimating: 53it [00:44,  1.23it/s]Extractor Estimating: 54it [00:45,  1.04it/s]Extractor Estimating: 55it [00:46,  1.10it/s]Extractor Estimating: 56it [00:47,  1.10it/s]Extractor Estimating: 57it [00:48,  1.09it/s]Extractor Estimating: 58it [00:48,  1.15it/s]Extractor Estimating: 59it [00:49,  1.17it/s]Extractor Estimating: 60it [00:50,  1.17it/s]Extractor Estimating: 61it [00:51,  1.11it/s]Extractor Estimating: 62it [00:52,  1.17it/s]Extractor Estimating: 63it [00:53,  1.17it/s]Extractor Estimating: 64it [00:54,  1.15it/s]Extractor Estimating: 65it [00:54,  1.15it/s]Extractor Estimating: 66it [00:55,  1.15it/s]Extractor Estimating: 67it [00:56,  1.18it/s]Extractor Estimating: 68it [00:57,  1.21it/s]Extractor Estimating: 69it [00:58,  1.21it/s]Extractor Estimating: 70it [00:58,  1.21it/s]Extractor Estimating: 71it [00:59,  1.21it/s]Extractor Estimating: 72it [01:00,  1.20it/s]Extractor Estimating: 73it [01:01,  1.19it/s]Extractor Estimating: 74it [01:02,  1.21it/s]Extractor Estimating: 75it [01:03,  1.22it/s]Extractor Estimating: 76it [01:03,  1.26it/s]Extractor Estimating: 77it [01:04,  1.24it/s]Extractor Estimating: 78it [01:05,  1.22it/s]Extractor Estimating: 79it [01:06,  1.23it/s]Extractor Estimating: 80it [01:07,  1.26it/s]Extractor Estimating: 81it [01:07,  1.26it/s]Extractor Estimating: 82it [01:08,  1.28it/s]Extractor Estimating: 83it [01:09,  1.31it/s]Extractor Estimating: 84it [01:10,  1.29it/s]Extractor Estimating: 85it [01:10,  1.29it/s]Extractor Estimating: 86it [01:11,  1.24it/s]Extractor Estimating: 87it [01:12,  1.28it/s]Extractor Estimating: 88it [01:13,  1.26it/s]Extractor Estimating: 89it [01:14,  1.28it/s]Extractor Estimating: 90it [01:15,  1.21it/s]Extractor Estimating: 91it [01:15,  1.19it/s]Extractor Estimating: 92it [01:16,  1.21it/s]Extractor Estimating: 93it [01:17,  1.21it/s]Extractor Estimating: 94it [01:18,  1.19it/s]Extractor Estimating: 95it [01:19,  1.19it/s]Extractor Estimating: 96it [01:19,  1.25it/s]Extractor Estimating: 97it [01:20,  1.20it/s]Extractor Estimating: 98it [01:21,  1.23it/s]Extractor Estimating: 99it [01:22,  1.16it/s]Extractor Estimating: 100it [01:23,  1.21it/s]Extractor Estimating: 101it [01:24,  1.19it/s]Extractor Estimating: 102it [01:25,  1.19it/s]Extractor Estimating: 103it [01:25,  1.20it/s]Extractor Estimating: 104it [01:26,  1.20it/s]Extractor Estimating: 105it [01:27,  1.24it/s]Extractor Estimating: 106it [01:28,  1.29it/s]Extractor Estimating: 107it [01:28,  1.27it/s]Extractor Estimating: 108it [01:29,  1.27it/s]Extractor Estimating: 109it [01:30,  1.27it/s]Extractor Estimating: 110it [01:31,  1.23it/s]Extractor Estimating: 111it [01:32,  1.26it/s]Extractor Estimating: 112it [01:32,  1.26it/s]Extractor Estimating: 113it [01:33,  1.28it/s]Extractor Estimating: 114it [01:34,  1.28it/s]Extractor Estimating: 115it [01:35,  1.26it/s]Extractor Estimating: 116it [01:36,  1.19it/s]Extractor Estimating: 117it [01:37,  1.18it/s]Extractor Estimating: 118it [01:38,  1.17it/s]Extractor Estimating: 119it [01:38,  1.19it/s]Extractor Estimating: 120it [01:39,  1.16it/s]Extractor Estimating: 121it [01:40,  1.17it/s]Extractor Estimating: 122it [01:41,  1.16it/s]Extractor Estimating: 123it [01:42,  1.20it/s]Extractor Estimating: 124it [01:42,  1.22it/s]Extractor Estimating: 125it [01:43,  1.19it/s]Extractor Estimating: 126it [01:44,  1.18it/s]Extractor Estimating: 127it [01:45,  1.18it/s]Extractor Estimating: 128it [01:46,  1.21it/s]Extractor Estimating: 129it [01:47,  1.24it/s]Extractor Estimating: 130it [01:47,  1.26it/s]Extractor Estimating: 131it [01:48,  1.26it/s]Extractor Estimating: 132it [01:49,  1.24it/s]Extractor Estimating: 133it [01:50,  1.27it/s]Extractor Estimating: 134it [01:51,  1.26it/s]Extractor Estimating: 135it [01:51,  1.26it/s]Extractor Estimating: 136it [01:52,  1.28it/s]Extractor Estimating: 137it [01:53,  1.23it/s]Extractor Estimating: 138it [01:54,  1.26it/s]Extractor Estimating: 139it [01:55,  1.23it/s]Extractor Estimating: 140it [01:55,  1.25it/s]Extractor Estimating: 141it [01:56,  1.24it/s]Extractor Estimating: 142it [01:57,  1.25it/s]Extractor Estimating: 143it [01:58,  1.25it/s]Extractor Estimating: 144it [01:59,  1.27it/s]Extractor Estimating: 145it [01:59,  1.29it/s]Extractor Estimating: 146it [02:00,  1.26it/s]Extractor Estimating: 147it [02:01,  1.26it/s]Extractor Estimating: 148it [02:02,  1.26it/s]Extractor Estimating: 149it [02:03,  1.25it/s]Extractor Estimating: 150it [02:03,  1.24it/s]Extractor Estimating: 151it [02:05,  1.06it/s]Extractor Estimating: 152it [02:05,  1.12it/s]Extractor Estimating: 153it [02:06,  1.16it/s]Extractor Estimating: 154it [02:07,  1.18it/s]Extractor Estimating: 155it [02:08,  1.18it/s]Extractor Estimating: 156it [02:09,  1.18it/s]Extractor Estimating: 157it [02:10,  1.16it/s]Extractor Estimating: 158it [02:10,  1.21it/s]Extractor Estimating: 159it [02:11,  1.22it/s]Extractor Estimating: 160it [02:12,  1.22it/s]Extractor Estimating: 161it [02:13,  1.25it/s]Extractor Estimating: 162it [02:14,  1.21it/s]Extractor Estimating: 163it [02:14,  1.22it/s]Extractor Estimating: 164it [02:15,  1.20it/s]Extractor Estimating: 165it [02:16,  1.16it/s]Extractor Estimating: 166it [02:17,  1.18it/s]Extractor Estimating: 167it [02:18,  1.17it/s]Extractor Estimating: 168it [02:19,  1.17it/s]Extractor Estimating: 169it [02:20,  1.13it/s]Extractor Estimating: 170it [02:21,  1.16it/s]Extractor Estimating: 171it [02:22,  1.09it/s]Extractor Estimating: 172it [02:22,  1.11it/s]Extractor Estimating: 173it [02:23,  1.12it/s]Extractor Estimating: 174it [02:24,  1.09it/s]Extractor Estimating: 175it [02:25,  1.10it/s]Extractor Estimating: 176it [02:26,  1.10it/s]Extractor Estimating: 177it [02:27,  1.11it/s]Extractor Estimating: 178it [02:28,  1.16it/s]Extractor Estimating: 179it [02:28,  1.21it/s]Extractor Estimating: 180it [02:29,  1.23it/s]Extractor Estimating: 181it [02:30,  1.17it/s]Extractor Estimating: 182it [02:31,  1.16it/s]Extractor Estimating: 183it [02:32,  1.16it/s]Extractor Estimating: 184it [02:33,  1.18it/s]Extractor Estimating: 185it [02:34,  1.20it/s]Extractor Estimating: 186it [02:34,  1.23it/s]Extractor Estimating: 187it [02:35,  1.20it/s]Extractor Estimating: 188it [02:36,  1.17it/s]Extractor Estimating: 189it [02:37,  1.16it/s]Extractor Estimating: 190it [02:38,  1.19it/s]Extractor Estimating: 191it [02:39,  1.19it/s]Extractor Estimating: 192it [02:39,  1.18it/s]Extractor Estimating: 193it [02:40,  1.18it/s]Extractor Estimating: 194it [02:41,  1.18it/s]Extractor Estimating: 195it [02:42,  1.21it/s]Extractor Estimating: 196it [02:43,  1.22it/s]Extractor Estimating: 197it [02:44,  1.21it/s]Extractor Estimating: 198it [02:44,  1.21it/s]Extractor Estimating: 199it [02:45,  1.22it/s]Extractor Estimating: 200it [02:46,  1.21it/s]Extractor Estimating: 201it [02:47,  1.21it/s]Extractor Estimating: 202it [02:48,  1.19it/s]Extractor Estimating: 203it [02:49,  1.24it/s]Extractor Estimating: 204it [02:49,  1.24it/s]Extractor Estimating: 205it [02:50,  1.24it/s]Extractor Estimating: 206it [02:51,  1.26it/s]Extractor Estimating: 207it [02:52,  1.30it/s]Extractor Estimating: 208it [02:52,  1.27it/s]Extractor Estimating: 209it [02:53,  1.25it/s]Extractor Estimating: 210it [02:54,  1.22it/s]Extractor Estimating: 211it [02:55,  1.24it/s]Extractor Estimating: 212it [02:56,  1.24it/s]Extractor Estimating: 213it [02:57,  1.23it/s]Extractor Estimating: 214it [02:57,  1.21it/s]Extractor Estimating: 215it [02:58,  1.21it/s]Extractor Estimating: 216it [02:59,  1.26it/s]Extractor Estimating: 217it [03:00,  1.24it/s]Extractor Estimating: 218it [03:01,  1.20it/s]Extractor Estimating: 219it [03:02,  1.19it/s]Extractor Estimating: 220it [03:02,  1.18it/s]Extractor Estimating: 221it [03:03,  1.19it/s]Extractor Estimating: 222it [03:04,  1.22it/s]Extractor Estimating: 223it [03:05,  1.24it/s]Extractor Estimating: 224it [03:06,  1.26it/s]Extractor Estimating: 225it [03:06,  1.22it/s]Extractor Estimating: 226it [03:07,  1.26it/s]Extractor Estimating: 227it [03:08,  1.26it/s]Extractor Estimating: 228it [03:09,  1.24it/s]Extractor Estimating: 229it [03:10,  1.27it/s]Extractor Estimating: 230it [03:10,  1.25it/s]Extractor Estimating: 231it [03:11,  1.27it/s]Extractor Estimating: 232it [03:12,  1.27it/s]Extractor Estimating: 233it [03:13,  1.28it/s]Extractor Estimating: 234it [03:13,  1.29it/s]Extractor Estimating: 235it [03:14,  1.26it/s]Extractor Estimating: 236it [03:15,  1.30it/s]Extractor Estimating: 237it [03:16,  1.28it/s]Extractor Estimating: 238it [03:17,  1.29it/s]Extractor Estimating: 239it [03:18,  1.20it/s]Extractor Estimating: 240it [03:18,  1.25it/s]Extractor Estimating: 241it [03:19,  1.22it/s]Extractor Estimating: 242it [03:20,  1.25it/s]Extractor Estimating: 243it [03:21,  1.28it/s]Extractor Estimating: 244it [03:22,  1.20it/s]Extractor Estimating: 245it [03:22,  1.26it/s]Extractor Estimating: 246it [03:23,  1.27it/s]Extractor Estimating: 247it [03:24,  1.27it/s]Extractor Estimating: 248it [03:25,  1.25it/s]Extractor Estimating: 249it [03:25,  1.26it/s]Extractor Estimating: 250it [03:26,  1.20it/s]Extractor Estimating: 250it [03:26,  1.21it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:43:11,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:43:11,060 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:43:11,060 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:43:11,060 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:43:11,060 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:43:11,669 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:43:11,670 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:43:12,314 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:43:13,356 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:43:13,356 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:43:16,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:43:16,218 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:43:16,218 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:43:16,218 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:43:16,218 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:43:16,846 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:43:16,847 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:43:17,412 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:43:17,569 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:43:17,570 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 07:48:21,237 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 07:48:21,256 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 5236 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl'}
train vocab size: 20361
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20461, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20461, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.436, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.476, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 81, avg_time 1.447, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 181, avg_time 1.478, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 62, avg_time 1.455, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 162, avg_time 2.900, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 43, avg_time 1.431, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 143, avg_time 1.440, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 24, avg_time 1.451, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 124, avg_time 1.468, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 5, avg_time 2.862, loss:nan
g_step 1200, step 105, avg_time 1.455, loss:nan
g_step 1300, step 205, avg_time 1.447, loss:nan
g_step 1400, step 86, avg_time 1.470, loss:nan
g_step 1500, step 186, avg_time 1.447, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 67, avg_time 2.853, loss:nan
g_step 1700, step 167, avg_time 1.457, loss:nan
g_step 1800, step 48, avg_time 1.450, loss:nan
g_step 1900, step 148, avg_time 1.460, loss:nan
g_step 2000, step 29, avg_time 1.456, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 129, avg_time 2.886, loss:nan
g_step 2200, step 10, avg_time 1.437, loss:nan
g_step 2300, step 110, avg_time 1.454, loss:nan
g_step 2400, step 210, avg_time 1.451, loss:nan
g_step 2500, step 91, avg_time 1.440, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 191, avg_time 2.891, loss:nan
g_step 2700, step 72, avg_time 1.463, loss:nan
g_step 2800, step 172, avg_time 1.458, loss:nan
g_step 2900, step 53, avg_time 1.429, loss:nan
g_step 3000, step 153, avg_time 1.474, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 34, avg_time 2.845, loss:nan
g_step 3200, step 134, avg_time 1.478, loss:nan
g_step 3300, step 15, avg_time 1.432, loss:nan
g_step 3400, step 115, avg_time 1.439, loss:nan
g_step 3500, step 215, avg_time 1.476, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 96, avg_time 2.844, loss:nan
g_step 3700, step 196, avg_time 1.470, loss:nan
g_step 3800, step 77, avg_time 1.457, loss:nan
g_step 3900, step 177, avg_time 1.451, loss:nan
g_step 4000, step 58, avg_time 1.429, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 158, avg_time 2.872, loss:nan
g_step 4200, step 39, avg_time 1.453, loss:nan
g_step 4300, step 139, avg_time 1.446, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 07:48:21 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 07:48:21 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_07-48-21_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 07:48:22 - WARNING - datasets.builder -   Using custom data configuration default-461dc460d27a9b1b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-461dc460d27a9b1b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 07:48:22,599 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:48:22,600 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:48:22,601 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:48:22,602 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:48:22,609 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:48:22,612 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:48:22,613 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:48:22,613 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:48:22,613 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:48:22,613 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:48:22,613 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 07:48:22,745 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:48:26,008 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 07:48:26,008 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-461dc460d27a9b1b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.13ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.96ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.26ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.40ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.50ba/s]100%|██████████| 6/6 [00:01<00:00,  4.91ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.21ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.42ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.49ba/s]100%|██████████| 4/4 [00:00<00:00,  5.67ba/s]100%|██████████| 4/4 [00:00<00:00,  5.13ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  8.41ba/s] 33%|███▎      | 2/6 [00:00<00:00,  8.87ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.14ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.20ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.24ba/s]100%|██████████| 6/6 [00:00<00:00, 10.44ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.84ba/s] 50%|█████     | 2/4 [00:00<00:00,  8.57ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.88ba/s]100%|██████████| 4/4 [00:00<00:00, 10.09ba/s]
[INFO|trainer.py:414] 2023-08-28 07:48:29,350 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 07:48:29,362 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 07:48:29,363 >>   Num examples = 5239
[INFO|trainer.py:1149] 2023-08-28 07:48:29,363 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 07:48:29,363 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 07:48:29,363 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 07:48:29,363 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 07:48:29,363 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:59,  3.41it/s]  0%|          | 2/410 [00:00<01:55,  3.53it/s]  1%|          | 3/410 [00:00<01:53,  3.57it/s]  1%|          | 4/410 [00:01<01:53,  3.59it/s]  1%|          | 5/410 [00:01<01:52,  3.60it/s]  1%|▏         | 6/410 [00:01<01:52,  3.60it/s]  2%|▏         | 7/410 [00:01<01:51,  3.60it/s]  2%|▏         | 8/410 [00:02<01:51,  3.60it/s]  2%|▏         | 9/410 [00:02<01:51,  3.61it/s]  2%|▏         | 10/410 [00:02<01:51,  3.60it/s]  3%|▎         | 11/410 [00:03<01:50,  3.61it/s]  3%|▎         | 12/410 [00:03<01:50,  3.60it/s]  3%|▎         | 13/410 [00:03<01:50,  3.61it/s]  3%|▎         | 14/410 [00:03<01:49,  3.61it/s]  4%|▎         | 15/410 [00:04<01:49,  3.60it/s]  4%|▍         | 16/410 [00:04<01:49,  3.60it/s]  4%|▍         | 17/410 [00:04<01:49,  3.60it/s]  4%|▍         | 18/410 [00:05<01:48,  3.60it/s]  5%|▍         | 19/410 [00:05<01:48,  3.61it/s]  5%|▍         | 20/410 [00:05<01:48,  3.60it/s]  5%|▌         | 21/410 [00:05<01:47,  3.61it/s]  5%|▌         | 22/410 [00:06<01:47,  3.61it/s]  6%|▌         | 23/410 [00:06<01:47,  3.61it/s]  6%|▌         | 24/410 [00:06<01:47,  3.61it/s]  6%|▌         | 25/410 [00:06<01:46,  3.60it/s]  6%|▋         | 26/410 [00:07<01:46,  3.60it/s]  7%|▋         | 27/410 [00:07<01:46,  3.60it/s]  7%|▋         | 28/410 [00:07<01:45,  3.61it/s]  7%|▋         | 29/410 [00:08<01:46,  3.59it/s]  7%|▋         | 30/410 [00:08<01:45,  3.60it/s]  8%|▊         | 31/410 [00:08<01:45,  3.60it/s]  8%|▊         | 32/410 [00:08<01:44,  3.60it/s]  8%|▊         | 33/410 [00:09<01:44,  3.61it/s]  8%|▊         | 34/410 [00:09<01:44,  3.61it/s]  9%|▊         | 35/410 [00:09<01:43,  3.61it/s]  9%|▉         | 36/410 [00:09<01:43,  3.61it/s]  9%|▉         | 37/410 [00:10<01:43,  3.61it/s]  9%|▉         | 38/410 [00:10<01:43,  3.60it/s] 10%|▉         | 39/410 [00:10<01:43,  3.60it/s] 10%|▉         | 40/410 [00:11<01:42,  3.60it/s] 10%|█         | 41/410 [00:11<01:42,  3.60it/s] 10%|█         | 42/410 [00:11<01:42,  3.61it/s] 10%|█         | 43/410 [00:11<01:41,  3.60it/s] 11%|█         | 44/410 [00:12<01:41,  3.61it/s] 11%|█         | 45/410 [00:12<01:41,  3.60it/s] 11%|█         | 46/410 [00:12<01:40,  3.61it/s] 11%|█▏        | 47/410 [00:13<01:40,  3.60it/s] 12%|█▏        | 48/410 [00:13<01:40,  3.60it/s] 12%|█▏        | 49/410 [00:13<01:40,  3.60it/s] 12%|█▏        | 50/410 [00:13<01:40,  3.60it/s] 12%|█▏        | 51/410 [00:14<01:39,  3.60it/s] 13%|█▎        | 52/410 [00:14<01:39,  3.60it/s] 13%|█▎        | 53/410 [00:14<01:39,  3.60it/s] 13%|█▎        | 54/410 [00:14<01:38,  3.60it/s] 13%|█▎        | 55/410 [00:15<01:38,  3.60it/s] 14%|█▎        | 56/410 [00:15<01:38,  3.60it/s] 14%|█▍        | 57/410 [00:15<01:38,  3.59it/s] 14%|█▍        | 58/410 [00:16<01:37,  3.60it/s] 14%|█▍        | 59/410 [00:16<01:37,  3.60it/s] 15%|█▍        | 60/410 [00:16<01:37,  3.59it/s] 15%|█▍        | 61/410 [00:16<01:36,  3.60it/s] 15%|█▌        | 62/410 [00:17<01:36,  3.60it/s] 15%|█▌        | 63/410 [00:17<01:36,  3.60it/s] 16%|█▌        | 64/410 [00:17<01:36,  3.60it/s] 16%|█▌        | 65/410 [00:18<01:35,  3.60it/s] 16%|█▌        | 66/410 [00:18<01:35,  3.59it/s] 16%|█▋        | 67/410 [00:18<01:35,  3.60it/s] 17%|█▋        | 68/410 [00:18<01:35,  3.60it/s] 17%|█▋        | 69/410 [00:19<01:34,  3.59it/s] 17%|█▋        | 70/410 [00:19<01:34,  3.60it/s] 17%|█▋        | 71/410 [00:19<01:34,  3.60it/s] 18%|█▊        | 72/410 [00:19<01:33,  3.60it/s] 18%|█▊        | 73/410 [00:20<01:33,  3.60it/s] 18%|█▊        | 74/410 [00:20<01:33,  3.60it/s] 18%|█▊        | 75/410 [00:20<01:33,  3.60it/s] 19%|█▊        | 76/410 [00:21<01:32,  3.60it/s] 19%|█▉        | 77/410 [00:21<01:32,  3.60it/s] 19%|█▉        | 78/410 [00:21<01:32,  3.59it/s] 19%|█▉        | 79/410 [00:21<01:32,  3.60it/s] 20%|█▉        | 80/410 [00:22<01:31,  3.59it/s] 20%|█▉        | 81/410 [00:22<01:31,  3.60it/s] 20%|██        | 82/410 [00:22<01:27,  3.75it/s][INFO|trainer.py:2140] 2023-08-28 07:48:52,110 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:48:52,110 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 07:48:52,110 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 57.28it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.39it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.26it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.58it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.40it/s][A
  8%|▊         | 33/436 [00:00<00:08, 47.20it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.81it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.52it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.51it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.44it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.52it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.56it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.68it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.73it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.52it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.45it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.37it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.37it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.33it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.45it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.34it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.55it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.62it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.56it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.52it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.33it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.35it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.32it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.42it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.47it/s][A
 36%|███▌      | 158/436 [00:03<00:05, 46.45it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.58it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.53it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.53it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.50it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.44it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.29it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.30it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.40it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.46it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.55it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.59it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.44it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.47it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.39it/s][A
 53%|█████▎    | 233/436 [00:04<00:04, 46.30it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.34it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.34it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.36it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.42it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.48it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.51it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.43it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.40it/s][A
 64%|██████▍   | 278/436 [00:05<00:03, 46.27it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.25it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.31it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.34it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.45it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.49it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.40it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.45it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.36it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.33it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.32it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.18it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.08it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.50it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.53it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.52it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.46it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.36it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.37it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.32it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.25it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.32it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.41it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.40it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.38it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.42it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.37it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.39it/s][A
 96%|█████████▌| 418/436 [00:08<00:00, 46.36it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.30it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.35it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.37it/s][A                                                
                                                 [A 20%|██        | 82/410 [00:32<01:27,  3.75it/s]
100%|██████████| 436/436 [00:09<00:00, 46.37it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:49:01,521 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 07:49:01,534 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:49:04,371 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:49:04,388 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:49:04,400 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:35<22:05,  4.06s/it] 20%|██        | 84/410 [00:35<15:52,  2.92s/it] 21%|██        | 85/410 [00:36<11:31,  2.13s/it] 21%|██        | 86/410 [00:36<08:29,  1.57s/it] 21%|██        | 87/410 [00:36<06:22,  1.18s/it] 21%|██▏       | 88/410 [00:37<04:53,  1.10it/s] 22%|██▏       | 89/410 [00:37<03:51,  1.38it/s] 22%|██▏       | 90/410 [00:37<03:08,  1.70it/s] 22%|██▏       | 91/410 [00:37<02:38,  2.02it/s] 22%|██▏       | 92/410 [00:38<02:16,  2.32it/s] 23%|██▎       | 93/410 [00:38<02:02,  2.58it/s] 23%|██▎       | 94/410 [00:38<01:52,  2.82it/s] 23%|██▎       | 95/410 [00:38<01:44,  3.01it/s] 23%|██▎       | 96/410 [00:39<01:39,  3.17it/s] 24%|██▎       | 97/410 [00:39<01:35,  3.27it/s] 24%|██▍       | 98/410 [00:39<01:32,  3.37it/s] 24%|██▍       | 99/410 [00:40<01:30,  3.43it/s] 24%|██▍       | 100/410 [00:40<01:29,  3.48it/s] 25%|██▍       | 101/410 [00:40<01:27,  3.51it/s] 25%|██▍       | 102/410 [00:40<01:26,  3.55it/s] 25%|██▌       | 103/410 [00:41<01:26,  3.56it/s] 25%|██▌       | 104/410 [00:41<01:25,  3.56it/s] 26%|██▌       | 105/410 [00:41<01:25,  3.57it/s] 26%|██▌       | 106/410 [00:42<01:24,  3.58it/s] 26%|██▌       | 107/410 [00:42<01:24,  3.58it/s] 26%|██▋       | 108/410 [00:42<01:24,  3.58it/s] 27%|██▋       | 109/410 [00:42<01:24,  3.58it/s] 27%|██▋       | 110/410 [00:43<01:24,  3.55it/s] 27%|██▋       | 111/410 [00:43<01:23,  3.56it/s] 27%|██▋       | 112/410 [00:43<01:23,  3.57it/s] 28%|██▊       | 113/410 [00:44<01:23,  3.58it/s] 28%|██▊       | 114/410 [00:44<01:22,  3.58it/s] 28%|██▊       | 115/410 [00:44<01:22,  3.57it/s] 28%|██▊       | 116/410 [00:44<01:22,  3.57it/s] 29%|██▊       | 117/410 [00:45<01:21,  3.58it/s] 29%|██▉       | 118/410 [00:45<01:21,  3.58it/s] 29%|██▉       | 119/410 [00:45<01:21,  3.58it/s] 29%|██▉       | 120/410 [00:45<01:20,  3.58it/s] 30%|██▉       | 121/410 [00:46<01:20,  3.58it/s] 30%|██▉       | 122/410 [00:46<01:20,  3.59it/s] 30%|███       | 123/410 [00:46<01:19,  3.59it/s] 30%|███       | 124/410 [00:47<01:19,  3.59it/s] 30%|███       | 125/410 [00:47<01:19,  3.59it/s] 31%|███       | 126/410 [00:47<01:19,  3.56it/s] 31%|███       | 127/410 [00:47<01:19,  3.57it/s] 31%|███       | 128/410 [00:48<01:18,  3.57it/s] 31%|███▏      | 129/410 [00:48<01:18,  3.58it/s] 32%|███▏      | 130/410 [00:48<01:18,  3.58it/s] 32%|███▏      | 131/410 [00:49<01:17,  3.59it/s] 32%|███▏      | 132/410 [00:49<01:17,  3.58it/s] 32%|███▏      | 133/410 [00:49<01:17,  3.58it/s] 33%|███▎      | 134/410 [00:49<01:16,  3.59it/s] 33%|███▎      | 135/410 [00:50<01:16,  3.59it/s] 33%|███▎      | 136/410 [00:50<01:16,  3.59it/s] 33%|███▎      | 137/410 [00:50<01:16,  3.58it/s] 34%|███▎      | 138/410 [00:50<01:15,  3.58it/s] 34%|███▍      | 139/410 [00:51<01:15,  3.58it/s] 34%|███▍      | 140/410 [00:51<01:15,  3.58it/s] 34%|███▍      | 141/410 [00:51<01:15,  3.59it/s] 35%|███▍      | 142/410 [00:52<01:15,  3.54it/s] 35%|███▍      | 143/410 [00:52<01:15,  3.55it/s] 35%|███▌      | 144/410 [00:52<01:14,  3.56it/s] 35%|███▌      | 145/410 [00:52<01:14,  3.57it/s] 36%|███▌      | 146/410 [00:53<01:13,  3.58it/s] 36%|███▌      | 147/410 [00:53<01:13,  3.58it/s] 36%|███▌      | 148/410 [00:53<01:13,  3.57it/s] 36%|███▋      | 149/410 [00:54<01:12,  3.58it/s] 37%|███▋      | 150/410 [00:54<01:12,  3.58it/s] 37%|███▋      | 151/410 [00:54<01:12,  3.58it/s] 37%|███▋      | 152/410 [00:54<01:11,  3.59it/s] 37%|███▋      | 153/410 [00:55<01:11,  3.58it/s] 38%|███▊      | 154/410 [00:55<01:11,  3.58it/s] 38%|███▊      | 155/410 [00:55<01:11,  3.58it/s] 38%|███▊      | 156/410 [00:56<01:10,  3.59it/s] 38%|███▊      | 157/410 [00:56<01:10,  3.58it/s] 39%|███▊      | 158/410 [00:56<01:10,  3.58it/s] 39%|███▉      | 159/410 [00:56<01:10,  3.57it/s] 39%|███▉      | 160/410 [00:57<01:09,  3.58it/s] 39%|███▉      | 161/410 [00:57<01:09,  3.58it/s] 40%|███▉      | 162/410 [00:57<01:09,  3.58it/s] 40%|███▉      | 163/410 [00:57<01:09,  3.58it/s] 40%|████      | 164/410 [00:58<01:05,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 07:49:27,585 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:49:27,585 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 07:49:27,585 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.3922, 'eval_samples_per_second': 371.159, 'eval_steps_per_second': 46.422, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.55it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.93it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.23it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.63it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.22it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.81it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.44it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.03it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.07it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.15it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.21it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.16it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.27it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.30it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.31it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.29it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.02it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.94it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.07it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.13it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.23it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.27it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.32it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.26it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.16it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.10it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.96it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.10it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.14it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.19it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.12it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.19it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.25it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.20it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.11it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.00it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.06it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.11it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.11it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.23it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.13it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.05it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.97it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.98it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.99it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.04it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.15it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.16it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.17it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.16it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.15it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.16it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.09it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.05it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.05it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.07it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.15it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.18it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.12it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.11it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.09it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.09it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.07it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.08it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.00it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.12it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.21it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.26it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.18it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.03it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.03it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.89it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.95it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.98it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.11it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.96it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.10it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.08it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.05it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.11it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.02it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.05it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.06it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.14it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.16it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.12it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 46.12it/s][A 40%|████      | 164/410 [01:07<01:05,  3.73it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:49:37,056 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 07:49:37,075 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:49:39,956 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:49:39,975 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:49:39,983 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:11<16:42,  4.09s/it] 40%|████      | 166/410 [01:11<11:59,  2.95s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:11<08:41,  2.15s/it] 41%|████      | 168/410 [01:12<06:23,  1.59s/it] 41%|████      | 169/410 [01:12<04:47,  1.19s/it] 41%|████▏     | 170/410 [01:12<03:40,  1.09it/s] 42%|████▏     | 171/410 [01:12<02:53,  1.38it/s] 42%|████▏     | 172/410 [01:13<02:20,  1.69it/s] 42%|████▏     | 173/410 [01:13<01:57,  2.01it/s] 42%|████▏     | 174/410 [01:13<01:42,  2.31it/s] 43%|████▎     | 175/410 [01:14<01:31,  2.58it/s] 43%|████▎     | 176/410 [01:14<01:23,  2.82it/s] 43%|████▎     | 177/410 [01:14<01:17,  3.02it/s] 43%|████▎     | 178/410 [01:14<01:13,  3.17it/s] 44%|████▎     | 179/410 [01:15<01:10,  3.28it/s] 44%|████▍     | 180/410 [01:15<01:08,  3.36it/s] 44%|████▍     | 181/410 [01:15<01:06,  3.42it/s] 44%|████▍     | 182/410 [01:15<01:05,  3.46it/s] 45%|████▍     | 183/410 [01:16<01:04,  3.50it/s] 45%|████▍     | 184/410 [01:16<01:04,  3.53it/s] 45%|████▌     | 185/410 [01:16<01:03,  3.55it/s] 45%|████▌     | 186/410 [01:17<01:02,  3.56it/s] 46%|████▌     | 187/410 [01:17<01:02,  3.56it/s] 46%|████▌     | 188/410 [01:17<01:02,  3.58it/s] 46%|████▌     | 189/410 [01:17<01:01,  3.58it/s] 46%|████▋     | 190/410 [01:18<01:01,  3.59it/s] 47%|████▋     | 191/410 [01:18<01:00,  3.59it/s] 47%|████▋     | 192/410 [01:18<01:00,  3.59it/s] 47%|████▋     | 193/410 [01:19<01:00,  3.59it/s] 47%|████▋     | 194/410 [01:19<01:00,  3.59it/s] 48%|████▊     | 195/410 [01:19<00:59,  3.59it/s] 48%|████▊     | 196/410 [01:19<00:59,  3.59it/s] 48%|████▊     | 197/410 [01:20<00:59,  3.57it/s] 48%|████▊     | 198/410 [01:20<00:59,  3.57it/s] 49%|████▊     | 199/410 [01:20<00:58,  3.58it/s] 49%|████▉     | 200/410 [01:20<00:58,  3.58it/s] 49%|████▉     | 201/410 [01:21<00:58,  3.58it/s] 49%|████▉     | 202/410 [01:21<00:58,  3.58it/s] 50%|████▉     | 203/410 [01:21<00:57,  3.58it/s] 50%|████▉     | 204/410 [01:22<00:57,  3.57it/s] 50%|█████     | 205/410 [01:22<00:57,  3.57it/s] 50%|█████     | 206/410 [01:22<00:57,  3.57it/s] 50%|█████     | 207/410 [01:22<00:56,  3.57it/s] 51%|█████     | 208/410 [01:23<00:56,  3.57it/s] 51%|█████     | 209/410 [01:23<00:56,  3.57it/s] 51%|█████     | 210/410 [01:23<00:55,  3.58it/s] 51%|█████▏    | 211/410 [01:24<00:55,  3.58it/s] 52%|█████▏    | 212/410 [01:24<00:55,  3.58it/s] 52%|█████▏    | 213/410 [01:24<00:55,  3.58it/s] 52%|█████▏    | 214/410 [01:24<00:54,  3.58it/s] 52%|█████▏    | 215/410 [01:25<00:54,  3.59it/s] 53%|█████▎    | 216/410 [01:25<00:54,  3.59it/s] 53%|█████▎    | 217/410 [01:25<00:53,  3.59it/s] 53%|█████▎    | 218/410 [01:26<00:53,  3.59it/s] 53%|█████▎    | 219/410 [01:26<00:53,  3.55it/s] 54%|█████▎    | 220/410 [01:26<00:53,  3.56it/s] 54%|█████▍    | 221/410 [01:26<00:52,  3.57it/s] 54%|█████▍    | 222/410 [01:27<00:52,  3.57it/s] 54%|█████▍    | 223/410 [01:27<00:52,  3.58it/s] 55%|█████▍    | 224/410 [01:27<00:51,  3.58it/s] 55%|█████▍    | 225/410 [01:27<00:51,  3.58it/s] 55%|█████▌    | 226/410 [01:28<00:51,  3.58it/s] 55%|█████▌    | 227/410 [01:28<00:51,  3.58it/s] 56%|█████▌    | 228/410 [01:28<00:50,  3.58it/s] 56%|█████▌    | 229/410 [01:29<00:50,  3.58it/s] 56%|█████▌    | 230/410 [01:29<00:50,  3.57it/s] 56%|█████▋    | 231/410 [01:29<00:50,  3.57it/s] 57%|█████▋    | 232/410 [01:29<00:49,  3.58it/s] 57%|█████▋    | 233/410 [01:30<00:49,  3.58it/s] 57%|█████▋    | 234/410 [01:30<00:49,  3.59it/s] 57%|█████▋    | 235/410 [01:30<00:48,  3.59it/s] 58%|█████▊    | 236/410 [01:31<00:48,  3.59it/s] 58%|█████▊    | 237/410 [01:31<00:48,  3.59it/s] 58%|█████▊    | 238/410 [01:31<00:47,  3.59it/s] 58%|█████▊    | 239/410 [01:31<00:47,  3.59it/s] 59%|█████▊    | 240/410 [01:32<00:47,  3.59it/s] 59%|█████▉    | 241/410 [01:32<00:47,  3.58it/s] 59%|█████▉    | 242/410 [01:32<00:46,  3.58it/s] 59%|█████▉    | 243/410 [01:33<00:46,  3.58it/s] 60%|█████▉    | 244/410 [01:33<00:46,  3.58it/s] 60%|█████▉    | 245/410 [01:33<00:46,  3.58it/s] 60%|██████    | 246/410 [01:33<00:44,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 07:50:03,177 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:50:03,177 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 07:50:03,177 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4522, 'eval_samples_per_second': 368.804, 'eval_steps_per_second': 46.127, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.51it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.06it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.23it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.58it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.17it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.87it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.59it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.26it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.21it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.17it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.26it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.29it/s][A
 16%|█▌        | 68/436 [00:01<00:08, 41.91it/s][A
 17%|█▋        | 73/436 [00:01<00:08, 43.65it/s][A
 18%|█▊        | 78/436 [00:01<00:08, 44.43it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 44.92it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.34it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.60it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.85it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.00it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.03it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.83it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 45.86it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 45.99it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.10it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.11it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.19it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.21it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.27it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.15it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.04it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.02it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.07it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.09it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.10it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.17it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.14it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.19it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.11it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.92it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.00it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.98it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.08it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.02it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.00it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.07it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.13it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.08it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.96it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.06it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.01it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.07it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.01it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.08it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.11it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.12it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.05it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.00it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.03it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.01it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.00it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.98it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.03it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 46.11it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.13it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.14it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.11it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.07it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.08it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.91it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.01it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.01it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.10it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.08it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.07it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.06it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.04it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.08it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.94it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.96it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.98it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.07it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.07it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.06it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.12it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.80it/s][A                                                 
                                                 [A 60%|██████    | 246/410 [01:43<00:44,  3.73it/s]
100%|██████████| 436/436 [00:09<00:00, 45.80it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:50:12,678 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 07:50:12,702 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:50:15,226 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:50:15,246 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:50:15,259 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [01:46<10:48,  3.98s/it] 60%|██████    | 248/410 [01:46<07:44,  2.87s/it] 61%|██████    | 249/410 [01:47<05:36,  2.09s/it] 61%|██████    | 250/410 [01:47<04:07,  1.55s/it] 61%|██████    | 251/410 [01:47<03:05,  1.17s/it] 61%|██████▏   | 252/410 [01:47<02:22,  1.11it/s] 62%|██████▏   | 253/410 [01:48<01:51,  1.40it/s] 62%|██████▏   | 254/410 [01:48<01:30,  1.72it/s] 62%|██████▏   | 255/410 [01:48<01:16,  2.04it/s] 62%|██████▏   | 256/410 [01:48<01:05,  2.34it/s] 63%|██████▎   | 257/410 [01:49<00:58,  2.61it/s] 63%|██████▎   | 258/410 [01:49<00:53,  2.85it/s] 63%|██████▎   | 259/410 [01:49<00:49,  3.04it/s] 63%|██████▎   | 260/410 [01:50<00:47,  3.19it/s] 64%|██████▎   | 261/410 [01:50<00:45,  3.30it/s] 64%|██████▍   | 262/410 [01:50<00:43,  3.39it/s] 64%|██████▍   | 263/410 [01:50<00:42,  3.45it/s] 64%|██████▍   | 264/410 [01:51<00:41,  3.49it/s] 65%|██████▍   | 265/410 [01:51<00:41,  3.52it/s] 65%|██████▍   | 266/410 [01:51<00:40,  3.54it/s] 65%|██████▌   | 267/410 [01:52<00:40,  3.56it/s] 65%|██████▌   | 268/410 [01:52<00:39,  3.57it/s] 66%|██████▌   | 269/410 [01:52<00:39,  3.58it/s] 66%|██████▌   | 270/410 [01:52<00:39,  3.58it/s] 66%|██████▌   | 271/410 [01:53<00:38,  3.59it/s] 66%|██████▋   | 272/410 [01:53<00:38,  3.59it/s] 67%|██████▋   | 273/410 [01:53<00:38,  3.59it/s] 67%|██████▋   | 274/410 [01:53<00:37,  3.60it/s] 67%|██████▋   | 275/410 [01:54<00:37,  3.60it/s] 67%|██████▋   | 276/410 [01:54<00:37,  3.60it/s] 68%|██████▊   | 277/410 [01:54<00:36,  3.60it/s] 68%|██████▊   | 278/410 [01:55<00:36,  3.60it/s] 68%|██████▊   | 279/410 [01:55<00:36,  3.59it/s] 68%|██████▊   | 280/410 [01:55<00:36,  3.59it/s] 69%|██████▊   | 281/410 [01:55<00:35,  3.60it/s] 69%|██████▉   | 282/410 [01:56<00:35,  3.60it/s] 69%|██████▉   | 283/410 [01:56<00:35,  3.60it/s] 69%|██████▉   | 284/410 [01:56<00:34,  3.60it/s] 70%|██████▉   | 285/410 [01:57<00:34,  3.60it/s] 70%|██████▉   | 286/410 [01:57<00:34,  3.61it/s] 70%|███████   | 287/410 [01:57<00:34,  3.60it/s] 70%|███████   | 288/410 [01:57<00:33,  3.60it/s] 70%|███████   | 289/410 [01:58<00:33,  3.60it/s] 71%|███████   | 290/410 [01:58<00:33,  3.59it/s] 71%|███████   | 291/410 [01:58<00:33,  3.59it/s] 71%|███████   | 292/410 [01:58<00:32,  3.60it/s] 71%|███████▏  | 293/410 [01:59<00:32,  3.60it/s] 72%|███████▏  | 294/410 [01:59<00:32,  3.59it/s] 72%|███████▏  | 295/410 [01:59<00:31,  3.60it/s] 72%|███████▏  | 296/410 [02:00<00:31,  3.60it/s] 72%|███████▏  | 297/410 [02:00<00:31,  3.60it/s] 73%|███████▎  | 298/410 [02:00<00:31,  3.60it/s] 73%|███████▎  | 299/410 [02:00<00:30,  3.60it/s] 73%|███████▎  | 300/410 [02:01<00:30,  3.60it/s] 73%|███████▎  | 301/410 [02:01<00:30,  3.58it/s] 74%|███████▎  | 302/410 [02:01<00:30,  3.59it/s] 74%|███████▍  | 303/410 [02:02<00:29,  3.59it/s] 74%|███████▍  | 304/410 [02:02<00:29,  3.59it/s] 74%|███████▍  | 305/410 [02:02<00:29,  3.60it/s] 75%|███████▍  | 306/410 [02:02<00:28,  3.60it/s] 75%|███████▍  | 307/410 [02:03<00:28,  3.60it/s] 75%|███████▌  | 308/410 [02:03<00:28,  3.60it/s] 75%|███████▌  | 309/410 [02:03<00:28,  3.60it/s] 76%|███████▌  | 310/410 [02:03<00:27,  3.60it/s] 76%|███████▌  | 311/410 [02:04<00:27,  3.60it/s] 76%|███████▌  | 312/410 [02:04<00:27,  3.58it/s] 76%|███████▋  | 313/410 [02:04<00:27,  3.59it/s] 77%|███████▋  | 314/410 [02:05<00:26,  3.59it/s] 77%|███████▋  | 315/410 [02:05<00:26,  3.60it/s] 77%|███████▋  | 316/410 [02:05<00:26,  3.60it/s] 77%|███████▋  | 317/410 [02:05<00:25,  3.60it/s] 78%|███████▊  | 318/410 [02:06<00:25,  3.60it/s] 78%|███████▊  | 319/410 [02:06<00:25,  3.60it/s] 78%|███████▊  | 320/410 [02:06<00:24,  3.60it/s] 78%|███████▊  | 321/410 [02:07<00:24,  3.60it/s] 79%|███████▊  | 322/410 [02:07<00:24,  3.59it/s] 79%|███████▉  | 323/410 [02:07<00:24,  3.57it/s] 79%|███████▉  | 324/410 [02:07<00:24,  3.57it/s] 79%|███████▉  | 325/410 [02:08<00:23,  3.58it/s] 80%|███████▉  | 326/410 [02:08<00:23,  3.59it/s] 80%|███████▉  | 327/410 [02:08<00:23,  3.59it/s] 80%|████████  | 328/410 [02:08<00:21,  3.75it/s][INFO|trainer.py:2140] 2023-08-28 07:50:38,299 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:50:38,299 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 07:50:38,299 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4897, 'eval_samples_per_second': 367.346, 'eval_steps_per_second': 45.945, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.75it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.96it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.27it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.57it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.93it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.95it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.57it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.24it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.20it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.21it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.17it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.26it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.30it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.31it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.26it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.09it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.05it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.99it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.09it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.09it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.14it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.21it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.25it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.23it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.12it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.03it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.99it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.01it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.11it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.09it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.21it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.26it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.22it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.14it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.02it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.01it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.00it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.11it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.10it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.13it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.21it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.22it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.20it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.09it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.98it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.94it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.90it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.90it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.95it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.11it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.17it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.17it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.09it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.02it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.04it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.03it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.10it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.09it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.18it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.18it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.16it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 42.73it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 43.79it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 44.54it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.02it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.30it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.53it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.69it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.91it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.69it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.75it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.90it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.04it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.11it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.20it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.18it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.25it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.11it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.99it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.03it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.03it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.12it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.11it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.20it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.19it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.15it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 46.15it/s][A 80%|████████  | 328/410 [02:18<00:21,  3.75it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:50:47,793 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 07:50:47,807 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:50:50,196 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:50:50,206 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:50:50,215 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [02:21<05:18,  3.93s/it] 80%|████████  | 330/410 [02:21<03:46,  2.84s/it] 81%|████████  | 331/410 [02:21<02:43,  2.07s/it] 81%|████████  | 332/410 [02:22<01:59,  1.53s/it] 81%|████████  | 333/410 [02:22<01:28,  1.15s/it] 81%|████████▏ | 334/410 [02:22<01:07,  1.12it/s] 82%|████████▏ | 335/410 [02:23<00:53,  1.41it/s] 82%|████████▏ | 336/410 [02:23<00:42,  1.73it/s] 82%|████████▏ | 337/410 [02:23<00:35,  2.05it/s] 82%|████████▏ | 338/410 [02:23<00:30,  2.35it/s] 83%|████████▎ | 339/410 [02:24<00:27,  2.62it/s] 83%|████████▎ | 340/410 [02:24<00:24,  2.86it/s] 83%|████████▎ | 341/410 [02:24<00:22,  3.04it/s] 83%|████████▎ | 342/410 [02:25<00:21,  3.19it/s] 84%|████████▎ | 343/410 [02:25<00:20,  3.31it/s] 84%|████████▍ | 344/410 [02:25<00:19,  3.39it/s] 84%|████████▍ | 345/410 [02:25<00:18,  3.45it/s] 84%|████████▍ | 346/410 [02:26<00:18,  3.48it/s] 85%|████████▍ | 347/410 [02:26<00:17,  3.51it/s] 85%|████████▍ | 348/410 [02:26<00:17,  3.52it/s] 85%|████████▌ | 349/410 [02:26<00:17,  3.53it/s] 85%|████████▌ | 350/410 [02:27<00:16,  3.54it/s] 86%|████████▌ | 351/410 [02:27<00:16,  3.56it/s] 86%|████████▌ | 352/410 [02:27<00:16,  3.57it/s] 86%|████████▌ | 353/410 [02:28<00:15,  3.58it/s] 86%|████████▋ | 354/410 [02:28<00:15,  3.59it/s] 87%|████████▋ | 355/410 [02:28<00:15,  3.59it/s] 87%|████████▋ | 356/410 [02:28<00:15,  3.60it/s] 87%|████████▋ | 357/410 [02:29<00:14,  3.60it/s] 87%|████████▋ | 358/410 [02:29<00:14,  3.61it/s] 88%|████████▊ | 359/410 [02:29<00:14,  3.61it/s] 88%|████████▊ | 360/410 [02:30<00:13,  3.60it/s] 88%|████████▊ | 361/410 [02:30<00:13,  3.59it/s] 88%|████████▊ | 362/410 [02:30<00:13,  3.60it/s] 89%|████████▊ | 363/410 [02:30<00:13,  3.60it/s] 89%|████████▉ | 364/410 [02:31<00:12,  3.60it/s] 89%|████████▉ | 365/410 [02:31<00:12,  3.59it/s] 89%|████████▉ | 366/410 [02:31<00:12,  3.58it/s] 90%|████████▉ | 367/410 [02:31<00:12,  3.58it/s] 90%|████████▉ | 368/410 [02:32<00:11,  3.58it/s] 90%|█████████ | 369/410 [02:32<00:11,  3.58it/s] 90%|█████████ | 370/410 [02:32<00:11,  3.58it/s] 90%|█████████ | 371/410 [02:33<00:10,  3.58it/s] 91%|█████████ | 372/410 [02:33<00:10,  3.56it/s] 91%|█████████ | 373/410 [02:33<00:10,  3.56it/s] 91%|█████████ | 374/410 [02:33<00:10,  3.57it/s] 91%|█████████▏| 375/410 [02:34<00:09,  3.57it/s] 92%|█████████▏| 376/410 [02:34<00:09,  3.57it/s] 92%|█████████▏| 377/410 [02:34<00:09,  3.56it/s] 92%|█████████▏| 378/410 [02:35<00:08,  3.57it/s] 92%|█████████▏| 379/410 [02:35<00:08,  3.52it/s] 93%|█████████▎| 380/410 [02:35<00:08,  3.53it/s] 93%|█████████▎| 381/410 [02:35<00:08,  3.54it/s] 93%|█████████▎| 382/410 [02:36<00:07,  3.55it/s] 93%|█████████▎| 383/410 [02:36<00:07,  3.54it/s] 94%|█████████▎| 384/410 [02:36<00:07,  3.55it/s] 94%|█████████▍| 385/410 [02:37<00:07,  3.55it/s] 94%|█████████▍| 386/410 [02:37<00:06,  3.56it/s] 94%|█████████▍| 387/410 [02:37<00:06,  3.57it/s] 95%|█████████▍| 388/410 [02:37<00:06,  3.57it/s] 95%|█████████▍| 389/410 [02:38<00:05,  3.57it/s] 95%|█████████▌| 390/410 [02:38<00:05,  3.57it/s] 95%|█████████▌| 391/410 [02:38<00:05,  3.57it/s] 96%|█████████▌| 392/410 [02:39<00:05,  3.57it/s] 96%|█████████▌| 393/410 [02:39<00:04,  3.57it/s] 96%|█████████▌| 394/410 [02:39<00:04,  3.56it/s] 96%|█████████▋| 395/410 [02:39<00:04,  3.56it/s] 97%|█████████▋| 396/410 [02:40<00:03,  3.57it/s] 97%|█████████▋| 397/410 [02:40<00:03,  3.57it/s] 97%|█████████▋| 398/410 [02:40<00:03,  3.57it/s] 97%|█████████▋| 399/410 [02:40<00:03,  3.57it/s] 98%|█████████▊| 400/410 [02:41<00:02,  3.58it/s] 98%|█████████▊| 401/410 [02:41<00:02,  3.58it/s] 98%|█████████▊| 402/410 [02:41<00:02,  3.59it/s] 98%|█████████▊| 403/410 [02:42<00:01,  3.60it/s] 99%|█████████▊| 404/410 [02:42<00:01,  3.60it/s] 99%|█████████▉| 405/410 [02:42<00:01,  3.36it/s] 99%|█████████▉| 406/410 [02:42<00:01,  3.43it/s] 99%|█████████▉| 407/410 [02:43<00:00,  3.26it/s]100%|█████████▉| 408/410 [02:43<00:00,  3.07it/s]100%|█████████▉| 409/410 [02:43<00:00,  3.21it/s]100%|██████████| 410/410 [02:44<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 07:51:13,581 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:51:13,581 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 07:51:13,581 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4789, 'eval_samples_per_second': 367.763, 'eval_steps_per_second': 45.997, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 57.04it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.16it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.39it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.65it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.18it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.95it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.69it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.39it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.28it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.23it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.29it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.32it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.36it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.37it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.29it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.17it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.00it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.94it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.97it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.98it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.01it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.15it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.18it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.14it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.16it/s][A
 31%|███       | 133/436 [00:02<00:07, 42.97it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 43.92it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 44.60it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.15it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.47it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.76it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.95it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.08it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.01it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.83it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.91it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.96it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.99it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.60it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.77it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.02it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.10it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.99it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.88it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.97it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.08it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.06it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.20it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.26it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.32it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.28it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.17it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.15it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.12it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.16it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.13it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.19it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.25it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.29it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.36it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.29it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.19it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.16it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 46.10it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.14it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.98it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.10it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.15it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.21it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.27it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.24it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.25it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.18it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.16it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.12it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.12it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.22it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.24it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.30it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.24it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.20it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.21it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.14it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.15it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.11it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.20it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 46.20it/s][A100%|██████████| 410/410 [02:53<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:51:23,066 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 07:51:23,084 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:51:26,073 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:51:26,090 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:51:26,099 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 07:51:26,385 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 07:51:26,386 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82 (score: 1.0161933898925781).
                                                 100%|██████████| 410/410 [02:59<00:00,  3.45it/s]100%|██████████| 410/410 [02:59<00:00,  2.29it/s]
[INFO|trainer.py:1894] 2023-08-28 07:51:28,431 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 07:51:28,448 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:51:30,867 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:51:30,893 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:51:30,904 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 07:51:31,095 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:31,095 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:31,095 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:31,095 >>   train_runtime            = 0:02:59.05
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:31,095 >>   train_samples            =       5239
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:31,095 >>   train_samples_per_second =    146.293
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:31,095 >>   train_steps_per_second   =       2.29
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4682, 'eval_samples_per_second': 368.18, 'eval_steps_per_second': 46.049, 'epoch': 5.0}
{'train_runtime': 179.0584, 'train_samples_per_second': 146.293, 'train_steps_per_second': 2.29, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 07:51:31 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 07:51:31,147 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:51:31,147 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 07:51:31,147 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 57.84it/s]  3%|▎         | 12/436 [00:00<00:08, 50.78it/s]  4%|▍         | 18/436 [00:00<00:08, 48.84it/s]  5%|▌         | 23/436 [00:00<00:08, 48.09it/s]  6%|▋         | 28/436 [00:00<00:08, 47.66it/s]  8%|▊         | 33/436 [00:00<00:08, 47.34it/s]  9%|▊         | 38/436 [00:00<00:08, 47.18it/s] 10%|▉         | 43/436 [00:00<00:08, 46.92it/s] 11%|█         | 48/436 [00:01<00:08, 46.64it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.63it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.59it/s] 14%|█▍        | 63/436 [00:01<00:07, 46.63it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.62it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.59it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.59it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.52it/s] 20%|██        | 88/436 [00:01<00:07, 46.41it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.38it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.30it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.42it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.47it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.49it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.52it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.51it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.54it/s] 31%|███       | 133/436 [00:02<00:06, 46.38it/s] 32%|███▏      | 138/436 [00:02<00:06, 46.32it/s] 33%|███▎      | 143/436 [00:03<00:06, 46.36it/s] 34%|███▍      | 148/436 [00:03<00:06, 46.38it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.46it/s] 36%|███▌      | 158/436 [00:03<00:05, 46.51it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.47it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.50it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.44it/s] 41%|████      | 178/436 [00:03<00:05, 46.39it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.35it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.32it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.38it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.45it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.42it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.44it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.41it/s] 50%|█████     | 218/436 [00:04<00:04, 46.36it/s] 51%|█████     | 223/436 [00:04<00:04, 46.36it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.32it/s] 53%|█████▎    | 233/436 [00:04<00:04, 46.33it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.29it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.42it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.45it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.40it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.42it/s] 60%|██████    | 263/436 [00:05<00:03, 46.38it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.46it/s] 63%|██████▎   | 273/436 [00:05<00:03, 46.45it/s] 64%|██████▍   | 278/436 [00:05<00:03, 46.26it/s] 65%|██████▍   | 283/436 [00:06<00:03, 46.36it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.34it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.42it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.38it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.35it/s] 71%|███████   | 308/436 [00:06<00:02, 46.36it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.35it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.40it/s] 74%|███████▍  | 323/436 [00:06<00:02, 46.37it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.35it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.42it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.43it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.39it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.42it/s] 81%|████████  | 353/436 [00:07<00:01, 46.39it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.40it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.38it/s] 84%|████████▍ | 368/436 [00:07<00:01, 46.36it/s] 86%|████████▌ | 373/436 [00:08<00:01, 46.36it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.36it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.37it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.32it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.31it/s] 91%|█████████▏| 398/436 [00:08<00:00, 46.39it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.37it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.38it/s] 95%|█████████▍| 413/436 [00:08<00:00, 46.37it/s] 96%|█████████▌| 418/436 [00:08<00:00, 46.28it/s] 97%|█████████▋| 423/436 [00:09<00:00, 46.33it/s] 98%|█████████▊| 428/436 [00:09<00:00, 46.34it/s] 99%|█████████▉| 433/436 [00:09<00:00, 46.36it/s]100%|██████████| 436/436 [00:09<00:00, 46.53it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 07:51:40,543 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:40,543 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:40,543 >>   eval_loss               =     1.0162
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:40,543 >>   eval_runtime            = 0:00:09.39
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:40,544 >>   eval_samples            =       3486
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:40,544 >>   eval_samples_per_second =    370.988
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:40,544 >>   eval_steps_per_second   =       46.4
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:51:40,544 >>   perplexity              =     2.7627
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:51:46,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:51:46,860 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:51:46,860 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:51:46,860 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:51:46,860 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:51:47,548 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:51:47,549 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:51:48,120 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:51:49,152 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:51:49,152 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:51:52,075 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:51:52,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:51:52,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:51:52,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:51:52,080 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:51:52,830 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:51:52,832 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:51:53,436 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:51:53,589 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:51:53,589 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-328
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-410
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-164
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/checkpoint-82
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'manufacturer', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13015
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13115, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.13it/s]Extractor Predicting: 2it [00:01,  1.18it/s]Extractor Predicting: 3it [00:02,  1.24it/s]Extractor Predicting: 4it [00:03,  1.24it/s]Extractor Predicting: 5it [00:04,  1.25it/s]Extractor Predicting: 6it [00:04,  1.25it/s]Extractor Predicting: 7it [00:05,  1.24it/s]Extractor Predicting: 8it [00:06,  1.26it/s]Extractor Predicting: 9it [00:07,  1.26it/s]Extractor Predicting: 10it [00:07,  1.29it/s]Extractor Predicting: 11it [00:08,  1.26it/s]Extractor Predicting: 12it [00:09,  1.26it/s]Extractor Predicting: 13it [00:10,  1.25it/s]Extractor Predicting: 14it [00:11,  1.24it/s]Extractor Predicting: 15it [00:11,  1.26it/s]Extractor Predicting: 16it [00:12,  1.26it/s]Extractor Predicting: 17it [00:13,  1.29it/s]Extractor Predicting: 18it [00:14,  1.30it/s]Extractor Predicting: 19it [00:15,  1.30it/s]Extractor Predicting: 20it [00:15,  1.29it/s]Extractor Predicting: 21it [00:16,  1.29it/s]Extractor Predicting: 22it [00:17,  1.31it/s]Extractor Predicting: 23it [00:18,  1.31it/s]Extractor Predicting: 24it [00:18,  1.27it/s]Extractor Predicting: 25it [00:19,  1.24it/s]Extractor Predicting: 26it [00:20,  1.23it/s]Extractor Predicting: 27it [00:21,  1.25it/s]Extractor Predicting: 28it [00:22,  1.26it/s]Extractor Predicting: 29it [00:22,  1.25it/s]Extractor Predicting: 30it [00:23,  1.22it/s]Extractor Predicting: 31it [00:24,  1.24it/s]Extractor Predicting: 32it [00:25,  1.24it/s]Extractor Predicting: 33it [00:26,  1.24it/s]Extractor Predicting: 34it [00:27,  1.26it/s]Extractor Predicting: 35it [00:27,  1.27it/s]Extractor Predicting: 36it [00:28,  1.28it/s]Extractor Predicting: 37it [00:29,  1.30it/s]Extractor Predicting: 38it [00:30,  1.32it/s]Extractor Predicting: 39it [00:30,  1.33it/s]Extractor Predicting: 40it [00:31,  1.34it/s]Extractor Predicting: 41it [00:32,  1.32it/s]Extractor Predicting: 42it [00:33,  1.30it/s]Extractor Predicting: 43it [00:33,  1.29it/s]Extractor Predicting: 44it [00:34,  1.20it/s]Extractor Predicting: 45it [00:35,  1.27it/s]Extractor Predicting: 46it [00:36,  1.27it/s]Extractor Predicting: 47it [00:37,  1.30it/s]Extractor Predicting: 48it [00:37,  1.30it/s]Extractor Predicting: 49it [00:38,  1.33it/s]Extractor Predicting: 50it [00:39,  1.33it/s]Extractor Predicting: 51it [00:40,  1.32it/s]Extractor Predicting: 52it [00:40,  1.30it/s]Extractor Predicting: 53it [00:41,  1.31it/s]Extractor Predicting: 54it [00:42,  1.29it/s]Extractor Predicting: 55it [00:43,  1.33it/s]Extractor Predicting: 56it [00:43,  1.34it/s]Extractor Predicting: 57it [00:44,  1.31it/s]Extractor Predicting: 58it [00:45,  1.31it/s]Extractor Predicting: 59it [00:46,  1.32it/s]Extractor Predicting: 60it [00:46,  1.30it/s]Extractor Predicting: 61it [00:47,  1.29it/s]Extractor Predicting: 62it [00:48,  1.30it/s]Extractor Predicting: 63it [00:49,  1.31it/s]Extractor Predicting: 64it [00:49,  1.31it/s]Extractor Predicting: 65it [00:50,  1.30it/s]Extractor Predicting: 66it [00:51,  1.31it/s]Extractor Predicting: 67it [00:52,  1.34it/s]Extractor Predicting: 68it [00:53,  1.30it/s]Extractor Predicting: 69it [00:53,  1.30it/s]Extractor Predicting: 70it [00:54,  1.29it/s]Extractor Predicting: 71it [00:55,  1.32it/s]Extractor Predicting: 72it [00:56,  1.32it/s]Extractor Predicting: 73it [00:56,  1.31it/s]Extractor Predicting: 74it [00:57,  1.33it/s]Extractor Predicting: 75it [00:58,  1.30it/s]Extractor Predicting: 76it [00:59,  1.30it/s]Extractor Predicting: 77it [00:59,  1.29it/s]Extractor Predicting: 78it [01:00,  1.30it/s]Extractor Predicting: 79it [01:01,  1.31it/s]Extractor Predicting: 80it [01:02,  1.31it/s]Extractor Predicting: 81it [01:03,  1.30it/s]Extractor Predicting: 82it [01:03,  1.31it/s]Extractor Predicting: 83it [01:04,  1.27it/s]Extractor Predicting: 84it [01:05,  1.29it/s]Extractor Predicting: 85it [01:06,  1.28it/s]Extractor Predicting: 86it [01:06,  1.29it/s]Extractor Predicting: 87it [01:07,  1.29it/s]Extractor Predicting: 88it [01:08,  1.30it/s]Extractor Predicting: 89it [01:09,  1.31it/s]Extractor Predicting: 90it [01:09,  1.35it/s]Extractor Predicting: 91it [01:10,  1.38it/s]Extractor Predicting: 92it [01:11,  1.39it/s]Extractor Predicting: 93it [01:12,  1.35it/s]Extractor Predicting: 94it [01:12,  1.39it/s]Extractor Predicting: 95it [01:13,  1.38it/s]Extractor Predicting: 96it [01:14,  1.38it/s]Extractor Predicting: 97it [01:14,  1.38it/s]Extractor Predicting: 98it [01:15,  1.34it/s]Extractor Predicting: 99it [01:16,  1.32it/s]Extractor Predicting: 100it [01:17,  1.32it/s]Extractor Predicting: 101it [01:17,  1.38it/s]Extractor Predicting: 102it [01:18,  1.41it/s]Extractor Predicting: 103it [01:19,  1.40it/s]Extractor Predicting: 104it [01:20,  1.38it/s]Extractor Predicting: 105it [01:20,  1.39it/s]Extractor Predicting: 106it [01:21,  1.39it/s]Extractor Predicting: 107it [01:22,  1.39it/s]Extractor Predicting: 108it [01:22,  1.37it/s]Extractor Predicting: 109it [01:23,  1.39it/s]Extractor Predicting: 110it [01:24,  1.39it/s]Extractor Predicting: 111it [01:25,  1.39it/s]Extractor Predicting: 112it [01:25,  1.44it/s]Extractor Predicting: 113it [01:26,  1.45it/s]Extractor Predicting: 114it [01:27,  1.43it/s]Extractor Predicting: 115it [01:27,  1.42it/s]Extractor Predicting: 116it [01:28,  1.41it/s]Extractor Predicting: 117it [01:29,  1.42it/s]Extractor Predicting: 118it [01:29,  1.44it/s]Extractor Predicting: 119it [01:30,  1.42it/s]Extractor Predicting: 120it [01:31,  1.39it/s]Extractor Predicting: 121it [01:32,  1.45it/s]Extractor Predicting: 122it [01:32,  1.44it/s]Extractor Predicting: 123it [01:33,  1.38it/s]Extractor Predicting: 124it [01:34,  1.42it/s]Extractor Predicting: 125it [01:34,  1.41it/s]Extractor Predicting: 126it [01:35,  1.47it/s]Extractor Predicting: 127it [01:36,  1.45it/s]Extractor Predicting: 128it [01:36,  1.45it/s]Extractor Predicting: 129it [01:37,  1.42it/s]Extractor Predicting: 130it [01:38,  1.44it/s]Extractor Predicting: 131it [01:38,  1.47it/s]Extractor Predicting: 132it [01:39,  1.45it/s]Extractor Predicting: 133it [01:40,  1.42it/s]Extractor Predicting: 134it [01:41,  1.45it/s]Extractor Predicting: 135it [01:41,  1.46it/s]Extractor Predicting: 136it [01:42,  1.47it/s]Extractor Predicting: 137it [01:43,  1.45it/s]Extractor Predicting: 138it [01:43,  1.50it/s]Extractor Predicting: 139it [01:44,  1.38it/s]Extractor Predicting: 140it [01:45,  1.42it/s]Extractor Predicting: 141it [01:45,  1.48it/s]Extractor Predicting: 142it [01:46,  1.35it/s]Extractor Predicting: 143it [01:47,  1.36it/s]Extractor Predicting: 144it [01:47,  1.70it/s]Extractor Predicting: 144it [01:47,  1.34it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:53:49,919 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:53:49,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:53:49,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:53:49,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:53:49,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:53:50,627 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:53:50,629 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:53:51,204 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:53:52,235 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:53:52,235 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:53:55,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:53:55,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:53:55,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:53:55,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:53:55,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:53:55,852 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:53:55,853 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:53:56,424 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:53:56,579 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:53:56,579 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13522
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13622, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 4it [00:02,  1.40it/s]Extractor Predicting: 5it [00:03,  1.39it/s]Extractor Predicting: 6it [00:04,  1.36it/s]Extractor Predicting: 7it [00:05,  1.36it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.39it/s]Extractor Predicting: 10it [00:07,  1.42it/s]Extractor Predicting: 11it [00:07,  1.39it/s]Extractor Predicting: 12it [00:08,  1.40it/s]Extractor Predicting: 13it [00:09,  1.37it/s]Extractor Predicting: 14it [00:10,  1.40it/s]Extractor Predicting: 15it [00:10,  1.39it/s]Extractor Predicting: 16it [00:11,  1.36it/s]Extractor Predicting: 17it [00:12,  1.36it/s]Extractor Predicting: 18it [00:13,  1.35it/s]Extractor Predicting: 19it [00:13,  1.37it/s]Extractor Predicting: 20it [00:14,  1.35it/s]Extractor Predicting: 21it [00:15,  1.37it/s]Extractor Predicting: 22it [00:15,  1.38it/s]Extractor Predicting: 23it [00:16,  1.35it/s]Extractor Predicting: 24it [00:17,  1.34it/s]Extractor Predicting: 25it [00:18,  1.35it/s]Extractor Predicting: 26it [00:18,  1.35it/s]Extractor Predicting: 27it [00:19,  1.34it/s]Extractor Predicting: 28it [00:20,  1.37it/s]Extractor Predicting: 29it [00:21,  1.37it/s]Extractor Predicting: 30it [00:21,  1.33it/s]Extractor Predicting: 31it [00:22,  1.35it/s]Extractor Predicting: 32it [00:23,  1.34it/s]Extractor Predicting: 33it [00:24,  1.34it/s]Extractor Predicting: 34it [00:24,  1.36it/s]Extractor Predicting: 35it [00:25,  1.35it/s]Extractor Predicting: 36it [00:26,  1.40it/s]Extractor Predicting: 37it [00:27,  1.36it/s]Extractor Predicting: 38it [00:27,  1.34it/s]Extractor Predicting: 39it [00:28,  1.33it/s]Extractor Predicting: 40it [00:29,  1.31it/s]Extractor Predicting: 41it [00:30,  1.31it/s]Extractor Predicting: 42it [00:30,  1.32it/s]Extractor Predicting: 43it [00:31,  1.32it/s]Extractor Predicting: 44it [00:32,  1.30it/s]Extractor Predicting: 45it [00:33,  1.32it/s]Extractor Predicting: 46it [00:33,  1.32it/s]Extractor Predicting: 47it [00:34,  1.34it/s]Extractor Predicting: 48it [00:35,  1.34it/s]Extractor Predicting: 49it [00:36,  1.36it/s]Extractor Predicting: 50it [00:36,  1.36it/s]Extractor Predicting: 51it [00:37,  1.40it/s]Extractor Predicting: 52it [00:38,  1.40it/s]Extractor Predicting: 53it [00:39,  1.31it/s]Extractor Predicting: 54it [00:39,  1.33it/s]Extractor Predicting: 55it [00:40,  1.35it/s]Extractor Predicting: 56it [00:41,  1.33it/s]Extractor Predicting: 57it [00:42,  1.29it/s]Extractor Predicting: 58it [00:42,  1.27it/s]Extractor Predicting: 59it [00:43,  1.29it/s]Extractor Predicting: 60it [00:44,  1.32it/s]Extractor Predicting: 61it [00:45,  1.29it/s]Extractor Predicting: 62it [00:46,  1.28it/s]Extractor Predicting: 63it [00:46,  1.35it/s]Extractor Predicting: 64it [00:47,  1.33it/s]Extractor Predicting: 65it [00:48,  1.34it/s]Extractor Predicting: 66it [00:48,  1.33it/s]Extractor Predicting: 67it [00:49,  1.32it/s]Extractor Predicting: 68it [00:50,  1.31it/s]Extractor Predicting: 69it [00:51,  1.30it/s]Extractor Predicting: 70it [00:52,  1.24it/s]Extractor Predicting: 71it [00:52,  1.26it/s]Extractor Predicting: 72it [00:53,  1.27it/s]Extractor Predicting: 73it [00:54,  1.26it/s]Extractor Predicting: 74it [00:55,  1.27it/s]Extractor Predicting: 75it [00:56,  1.25it/s]Extractor Predicting: 76it [00:56,  1.29it/s]Extractor Predicting: 77it [00:57,  1.26it/s]Extractor Predicting: 78it [00:58,  1.27it/s]Extractor Predicting: 79it [00:59,  1.28it/s]Extractor Predicting: 80it [01:00,  1.28it/s]Extractor Predicting: 81it [01:00,  1.28it/s]Extractor Predicting: 82it [01:01,  1.26it/s]Extractor Predicting: 83it [01:02,  1.26it/s]Extractor Predicting: 84it [01:03,  1.29it/s]Extractor Predicting: 85it [01:03,  1.28it/s]Extractor Predicting: 86it [01:04,  1.29it/s]Extractor Predicting: 87it [01:05,  1.27it/s]Extractor Predicting: 88it [01:06,  1.32it/s]Extractor Predicting: 89it [01:06,  1.32it/s]Extractor Predicting: 90it [01:07,  1.31it/s]Extractor Predicting: 91it [01:08,  1.36it/s]Extractor Predicting: 92it [01:09,  1.36it/s]Extractor Predicting: 93it [01:09,  1.38it/s]Extractor Predicting: 94it [01:10,  1.36it/s]Extractor Predicting: 95it [01:11,  1.35it/s]Extractor Predicting: 96it [01:12,  1.35it/s]Extractor Predicting: 97it [01:12,  1.33it/s]Extractor Predicting: 98it [01:13,  1.32it/s]Extractor Predicting: 99it [01:14,  1.33it/s]Extractor Predicting: 100it [01:15,  1.32it/s]Extractor Predicting: 101it [01:15,  1.31it/s]Extractor Predicting: 102it [01:16,  1.27it/s]Extractor Predicting: 103it [01:17,  1.28it/s]Extractor Predicting: 104it [01:18,  1.29it/s]Extractor Predicting: 105it [01:19,  1.32it/s]Extractor Predicting: 106it [01:19,  1.33it/s]Extractor Predicting: 107it [01:20,  1.32it/s]Extractor Predicting: 108it [01:21,  1.33it/s]Extractor Predicting: 109it [01:22,  1.33it/s]Extractor Predicting: 110it [01:22,  1.31it/s]Extractor Predicting: 111it [01:23,  1.32it/s]Extractor Predicting: 112it [01:24,  1.31it/s]Extractor Predicting: 113it [01:25,  1.34it/s]Extractor Predicting: 114it [01:25,  1.32it/s]Extractor Predicting: 115it [01:26,  1.35it/s]Extractor Predicting: 116it [01:27,  1.29it/s]Extractor Predicting: 117it [01:28,  1.27it/s]Extractor Predicting: 118it [01:29,  1.27it/s]Extractor Predicting: 119it [01:29,  1.26it/s]Extractor Predicting: 120it [01:30,  1.26it/s]Extractor Predicting: 121it [01:31,  1.27it/s]Extractor Predicting: 122it [01:32,  1.30it/s]Extractor Predicting: 123it [01:32,  1.29it/s]Extractor Predicting: 124it [01:33,  1.27it/s]Extractor Predicting: 125it [01:34,  1.25it/s]Extractor Predicting: 126it [01:35,  1.23it/s]Extractor Predicting: 127it [01:36,  1.23it/s]Extractor Predicting: 128it [01:37,  1.17it/s]Extractor Predicting: 129it [01:37,  1.18it/s]Extractor Predicting: 130it [01:38,  1.22it/s]Extractor Predicting: 131it [01:39,  1.25it/s]Extractor Predicting: 132it [01:40,  1.24it/s]Extractor Predicting: 133it [01:41,  1.16it/s]Extractor Predicting: 134it [01:42,  1.20it/s]Extractor Predicting: 135it [01:42,  1.24it/s]Extractor Predicting: 136it [01:43,  1.28it/s]Extractor Predicting: 137it [01:44,  1.31it/s]Extractor Predicting: 138it [01:45,  1.30it/s]Extractor Predicting: 139it [01:45,  1.31it/s]Extractor Predicting: 140it [01:46,  1.31it/s]Extractor Predicting: 141it [01:47,  1.27it/s]Extractor Predicting: 142it [01:48,  1.27it/s]Extractor Predicting: 143it [01:48,  1.36it/s]Extractor Predicting: 143it [01:48,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:55:52,172 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:55:52,177 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:55:52,178 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:55:52,178 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:55:52,178 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:55:52,803 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:55:52,804 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:55:53,376 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:55:54,397 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:55:54,397 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:55:57,253 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:55:57,256 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:55:57,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:55:57,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:55:57,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:55:57,909 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:55:57,910 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:55:58,483 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:55:58,636 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:55:58,636 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 464
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 564, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.06it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 2it [00:01,  1.39it/s]
[INFO|configuration_utils.py:515] 2023-08-28 07:56:00,423 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:56:00,424 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:56:00,431 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:56:00,432 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 07:56:00,438 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:56:03,519 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 07:56:03,521 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 07:56:03,538 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:56:03,539 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:56:03,546 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:56:03,549 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:56:03,549 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:56:03,549 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:56:03,549 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:56:03,550 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:56:03,550 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 07:56:03,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:04,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:05,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:06,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:07,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:08,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:09,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:10,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:11,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:12,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:13,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:14,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:15,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:16,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:17,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:18,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:18,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:19,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:20,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:21,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:22,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:23,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:24,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:25,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:26,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:27,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:28,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:29,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:30,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:27<04:07, 27.46s/it][WARNING|generation_utils.py:914] 2023-08-28 07:56:31,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:32,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:33,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:33,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:34,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:35,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:36,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:37,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:38,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:39,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:40,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:41,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:42,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:43,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:44,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:46,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:46,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:47,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:49,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:49,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:50,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:51,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:52,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:53,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:54,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:51<03:23, 25.49s/it][WARNING|generation_utils.py:914] 2023-08-28 07:56:55,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:56,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:57,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:58,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:56:59,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:00,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:03,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:04,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:04,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:05,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:06,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:07,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:08,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:09,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:10,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:11,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:12,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:13,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:14,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:15,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:16,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:17,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:18,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:19,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:19,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:17<02:58, 25.51s/it][WARNING|generation_utils.py:914] 2023-08-28 07:57:20,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:21,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:22,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:23,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:24,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:25,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:26,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:27,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:28,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:29,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:30,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:30,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:31,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:32,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:33,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:34,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:35,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:36,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:37,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:39,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:39,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:40,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:37<02:22, 23.68s/it][WARNING|generation_utils.py:914] 2023-08-28 07:57:41,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:42,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:43,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:44,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:45,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:46,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:47,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:48,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:49,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:50,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:51,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:52,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:52,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:53,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:54,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:55,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:56,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:57,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:58,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:57:59,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:00,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:00,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:01,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:02,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:03,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:04,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:05,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:06,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:07,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:08,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:09,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:10,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:11,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:12,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:13,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [02:10<02:14, 26.85s/it][WARNING|generation_utils.py:914] 2023-08-28 07:58:14,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:15,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:16,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:17,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:18,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:19,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:20,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:21,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:22,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:23,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:24,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:25,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:26,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:27,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:28,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:29,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:29,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:30,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:31,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:32,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:33,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:34,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:35,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:36,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:33<01:42, 25.62s/it][WARNING|generation_utils.py:914] 2023-08-28 07:58:37,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:38,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:39,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:40,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:41,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:42,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:43,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:44,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:45,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:45,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:47,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:48,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:49,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:50,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:51,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:52,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:53,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:54,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:55,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:56,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:57,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:58,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:58:59,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:56<01:14, 24.82s/it][WARNING|generation_utils.py:914] 2023-08-28 07:59:00,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:01,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:02,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:03,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:04,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:05,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:06,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:07,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:09,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:10,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:11,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:12,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:13,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:13,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:14,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:16,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:16,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:18,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:18,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:19,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:21,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:21,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:23,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [03:20<00:48, 24.35s/it][WARNING|generation_utils.py:914] 2023-08-28 07:59:23,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:25,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:26,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:27,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:28,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:28,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:29,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:30,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:31,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:32,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:33,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:34,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:35,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:36,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:37,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:38,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:40,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:41,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:42,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:43,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:44,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:45,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:42<00:23, 23.76s/it][WARNING|generation_utils.py:914] 2023-08-28 07:59:46,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:47,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:48,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:49,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:50,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:51,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:52,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:53,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:54,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:55,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:56,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:57,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:58,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:59:59,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:00:00,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:00:01,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:00:02,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:00:03,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:00:04,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:00:05,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:00:06,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:00:07,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:00:08,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [04:05<00:00, 23.52s/it]Generating: 100%|██████████| 10/10 [04:05<00:00, 24.56s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:00:15,387 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:00:15,389 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:00:15,390 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:00:15,390 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:00:15,390 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:00:16,103 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:00:16,104 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:00:16,373 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:00:17,426 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:00:17,426 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:00:20,299 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:00:20,305 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:00:20,305 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:00:20,305 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:00:20,305 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:00:20,943 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:00:20,944 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:00:21,539 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:00:21,706 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:00:21,706 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 188, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 225, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 268, 'raw': 416}
{'target': 600, 'success': 289, 'raw': 448}
{'target': 600, 'success': 309, 'raw': 480}
{'target': 600, 'success': 337, 'raw': 512}
{'target': 600, 'success': 361, 'raw': 544}
{'target': 600, 'success': 378, 'raw': 576}
{'target': 600, 'success': 401, 'raw': 608}
{'target': 600, 'success': 424, 'raw': 640}
{'target': 600, 'success': 446, 'raw': 672}
{'target': 600, 'success': 465, 'raw': 704}
{'target': 600, 'success': 486, 'raw': 736}
{'target': 600, 'success': 502, 'raw': 768}
{'target': 600, 'success': 528, 'raw': 800}
{'target': 600, 'success': 549, 'raw': 832}
{'target': 600, 'success': 570, 'raw': 864}
{'target': 600, 'success': 589, 'raw': 896}
{'target': 600, 'success': 610, 'raw': 928}
{'prompt': 'Relation : country .', 'success_rate': 0.6573275862068966, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 499, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 547, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : genre . Context : Later in 2003 , he played in Steven Spielberg 's The Bourne Ultimatum . Head Entity : The Bourne Ultimatum , Tail Entity : director .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 604, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.755, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : voice type . Context : Later in the year , the band members decided to cancel their performance of " In My Time " on Broadway , in part to play the first time . Head Entity : In My Time , Tail Entity : vocal .\n']
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 102, 'raw': 160}
{'target': 600, 'success': 118, 'raw': 192}
{'target': 600, 'success': 134, 'raw': 224}
{'target': 600, 'success': 146, 'raw': 256}
{'target': 600, 'success': 160, 'raw': 288}
{'target': 600, 'success': 178, 'raw': 320}
{'target': 600, 'success': 193, 'raw': 352}
{'target': 600, 'success': 215, 'raw': 384}
{'target': 600, 'success': 233, 'raw': 416}
{'target': 600, 'success': 245, 'raw': 448}
{'target': 600, 'success': 262, 'raw': 480}
{'target': 600, 'success': 282, 'raw': 512}
{'target': 600, 'success': 304, 'raw': 544}
{'target': 600, 'success': 318, 'raw': 576}
{'target': 600, 'success': 336, 'raw': 608}
{'target': 600, 'success': 355, 'raw': 640}
{'target': 600, 'success': 375, 'raw': 672}
{'target': 600, 'success': 394, 'raw': 704}
{'target': 600, 'success': 413, 'raw': 736}
{'target': 600, 'success': 431, 'raw': 768}
{'target': 600, 'success': 450, 'raw': 800}
{'target': 600, 'success': 466, 'raw': 832}
{'target': 600, 'success': 480, 'raw': 864}
{'target': 600, 'success': 495, 'raw': 896}
{'target': 600, 'success': 511, 'raw': 928}
{'target': 600, 'success': 529, 'raw': 960}
{'target': 600, 'success': 546, 'raw': 992}
{'target': 600, 'success': 561, 'raw': 1024}
{'target': 600, 'success': 580, 'raw': 1056}
{'target': 600, 'success': 596, 'raw': 1088}
{'target': 600, 'success': 612, 'raw': 1120}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5464285714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : contains administrative territorial entity . Context : The city of Marchec is under the rule of the Marchec Municipal Municipality in Romania . Head Entity : Marchec , Tail Entity : Romanian .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8233695652173914, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8707386363636364, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 12301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.09it/s]Extractor Estimating: 2it [00:01,  1.13it/s]Extractor Estimating: 3it [00:02,  1.09it/s]Extractor Estimating: 4it [00:03,  1.16it/s]Extractor Estimating: 5it [00:04,  1.18it/s]Extractor Estimating: 6it [00:05,  1.19it/s]Extractor Estimating: 7it [00:06,  1.19it/s]Extractor Estimating: 8it [00:06,  1.22it/s]Extractor Estimating: 9it [00:07,  1.23it/s]Extractor Estimating: 10it [00:08,  1.24it/s]Extractor Estimating: 11it [00:09,  1.22it/s]Extractor Estimating: 12it [00:10,  1.21it/s]Extractor Estimating: 13it [00:10,  1.25it/s]Extractor Estimating: 14it [00:11,  1.28it/s]Extractor Estimating: 15it [00:12,  1.25it/s]Extractor Estimating: 16it [00:13,  1.24it/s]Extractor Estimating: 17it [00:14,  1.18it/s]Extractor Estimating: 18it [00:14,  1.18it/s]Extractor Estimating: 19it [00:15,  1.18it/s]Extractor Estimating: 20it [00:16,  1.20it/s]Extractor Estimating: 21it [00:17,  1.22it/s]Extractor Estimating: 22it [00:18,  1.22it/s]Extractor Estimating: 23it [00:18,  1.24it/s]Extractor Estimating: 24it [00:19,  1.21it/s]Extractor Estimating: 25it [00:20,  1.22it/s]Extractor Estimating: 26it [00:21,  1.20it/s]Extractor Estimating: 27it [00:22,  1.20it/s]Extractor Estimating: 28it [00:23,  1.21it/s]Extractor Estimating: 29it [00:24,  1.21it/s]Extractor Estimating: 30it [00:24,  1.21it/s]Extractor Estimating: 31it [00:25,  1.21it/s]Extractor Estimating: 32it [00:26,  1.23it/s]Extractor Estimating: 33it [00:27,  1.23it/s]Extractor Estimating: 34it [00:28,  1.24it/s]Extractor Estimating: 35it [00:28,  1.21it/s]Extractor Estimating: 36it [00:29,  1.22it/s]Extractor Estimating: 37it [00:30,  1.17it/s]Extractor Estimating: 38it [00:31,  1.18it/s]Extractor Estimating: 39it [00:32,  1.18it/s]Extractor Estimating: 40it [00:33,  1.20it/s]Extractor Estimating: 41it [00:34,  1.15it/s]Extractor Estimating: 42it [00:34,  1.18it/s]Extractor Estimating: 43it [00:35,  1.20it/s]Extractor Estimating: 44it [00:36,  1.23it/s]Extractor Estimating: 45it [00:37,  1.22it/s]Extractor Estimating: 46it [00:38,  1.21it/s]Extractor Estimating: 47it [00:38,  1.24it/s]Extractor Estimating: 48it [00:39,  1.23it/s]Extractor Estimating: 49it [00:40,  1.23it/s]Extractor Estimating: 50it [00:41,  1.24it/s]Extractor Estimating: 51it [00:42,  1.26it/s]Extractor Estimating: 52it [00:42,  1.24it/s]Extractor Estimating: 53it [00:43,  1.25it/s]Extractor Estimating: 54it [00:45,  1.05it/s]Extractor Estimating: 55it [00:45,  1.10it/s]Extractor Estimating: 56it [00:46,  1.10it/s]Extractor Estimating: 57it [00:47,  1.09it/s]Extractor Estimating: 58it [00:48,  1.16it/s]Extractor Estimating: 59it [00:49,  1.17it/s]Extractor Estimating: 60it [00:50,  1.17it/s]Extractor Estimating: 61it [00:51,  1.11it/s]Extractor Estimating: 62it [00:51,  1.17it/s]Extractor Estimating: 63it [00:52,  1.17it/s]Extractor Estimating: 64it [00:53,  1.16it/s]Extractor Estimating: 65it [00:54,  1.16it/s]Extractor Estimating: 66it [00:55,  1.16it/s]Extractor Estimating: 67it [00:56,  1.19it/s]Extractor Estimating: 68it [00:56,  1.21it/s]Extractor Estimating: 69it [00:57,  1.22it/s]Extractor Estimating: 70it [00:58,  1.21it/s]Extractor Estimating: 71it [00:59,  1.22it/s]Extractor Estimating: 72it [01:00,  1.20it/s]Extractor Estimating: 73it [01:01,  1.19it/s]Extractor Estimating: 74it [01:01,  1.21it/s]Extractor Estimating: 75it [01:02,  1.22it/s]Extractor Estimating: 76it [01:03,  1.26it/s]Extractor Estimating: 77it [01:04,  1.24it/s]Extractor Estimating: 78it [01:05,  1.22it/s]Extractor Estimating: 79it [01:05,  1.23it/s]Extractor Estimating: 80it [01:06,  1.26it/s]Extractor Estimating: 81it [01:07,  1.25it/s]Extractor Estimating: 82it [01:08,  1.27it/s]Extractor Estimating: 83it [01:08,  1.31it/s]Extractor Estimating: 84it [01:09,  1.29it/s]Extractor Estimating: 85it [01:10,  1.29it/s]Extractor Estimating: 86it [01:11,  1.24it/s]Extractor Estimating: 87it [01:12,  1.27it/s]Extractor Estimating: 88it [01:12,  1.26it/s]Extractor Estimating: 89it [01:13,  1.28it/s]Extractor Estimating: 90it [01:14,  1.21it/s]Extractor Estimating: 91it [01:15,  1.19it/s]Extractor Estimating: 92it [01:16,  1.21it/s]Extractor Estimating: 93it [01:17,  1.21it/s]Extractor Estimating: 94it [01:17,  1.19it/s]Extractor Estimating: 95it [01:18,  1.19it/s]Extractor Estimating: 96it [01:19,  1.24it/s]Extractor Estimating: 97it [01:20,  1.19it/s]Extractor Estimating: 98it [01:21,  1.22it/s]Extractor Estimating: 99it [01:22,  1.16it/s]Extractor Estimating: 100it [01:22,  1.21it/s]Extractor Estimating: 101it [01:23,  1.19it/s]Extractor Estimating: 102it [01:24,  1.18it/s]Extractor Estimating: 103it [01:25,  1.20it/s]Extractor Estimating: 104it [01:26,  1.20it/s]Extractor Estimating: 105it [01:27,  1.24it/s]Extractor Estimating: 106it [01:27,  1.29it/s]Extractor Estimating: 107it [01:28,  1.27it/s]Extractor Estimating: 108it [01:29,  1.27it/s]Extractor Estimating: 109it [01:30,  1.28it/s]Extractor Estimating: 110it [01:30,  1.23it/s]Extractor Estimating: 111it [01:31,  1.26it/s]Extractor Estimating: 112it [01:32,  1.27it/s]Extractor Estimating: 113it [01:33,  1.30it/s]Extractor Estimating: 114it [01:34,  1.29it/s]Extractor Estimating: 115it [01:34,  1.27it/s]Extractor Estimating: 116it [01:35,  1.20it/s]Extractor Estimating: 117it [01:36,  1.19it/s]Extractor Estimating: 118it [01:37,  1.18it/s]Extractor Estimating: 119it [01:38,  1.20it/s]Extractor Estimating: 120it [01:39,  1.17it/s]Extractor Estimating: 121it [01:40,  1.19it/s]Extractor Estimating: 122it [01:40,  1.17it/s]Extractor Estimating: 123it [01:41,  1.22it/s]Extractor Estimating: 124it [01:42,  1.24it/s]Extractor Estimating: 125it [01:43,  1.21it/s]Extractor Estimating: 126it [01:44,  1.19it/s]Extractor Estimating: 127it [01:44,  1.19it/s]Extractor Estimating: 128it [01:45,  1.22it/s]Extractor Estimating: 129it [01:47,  1.07s/it]Extractor Estimating: 130it [01:48,  1.02it/s]Extractor Estimating: 131it [01:48,  1.08it/s]Extractor Estimating: 132it [01:49,  1.12it/s]Extractor Estimating: 133it [01:50,  1.18it/s]Extractor Estimating: 134it [01:51,  1.20it/s]Extractor Estimating: 135it [01:52,  1.22it/s]Extractor Estimating: 136it [01:52,  1.25it/s]Extractor Estimating: 137it [01:53,  1.22it/s]Extractor Estimating: 138it [01:54,  1.26it/s]Extractor Estimating: 139it [01:55,  1.23it/s]Extractor Estimating: 140it [01:56,  1.26it/s]Extractor Estimating: 141it [01:56,  1.24it/s]Extractor Estimating: 142it [01:57,  1.25it/s]Extractor Estimating: 143it [01:58,  1.25it/s]Extractor Estimating: 144it [01:59,  1.28it/s]Extractor Estimating: 145it [02:00,  1.29it/s]Extractor Estimating: 146it [02:00,  1.27it/s]Extractor Estimating: 147it [02:01,  1.26it/s]Extractor Estimating: 148it [02:02,  1.26it/s]Extractor Estimating: 149it [02:03,  1.25it/s]Extractor Estimating: 150it [02:04,  1.24it/s]Extractor Estimating: 151it [02:04,  1.23it/s]Extractor Estimating: 152it [02:05,  1.25it/s]Extractor Estimating: 153it [02:06,  1.25it/s]Extractor Estimating: 154it [02:07,  1.24it/s]Extractor Estimating: 155it [02:08,  1.23it/s]Extractor Estimating: 156it [02:08,  1.22it/s]Extractor Estimating: 157it [02:09,  1.18it/s]Extractor Estimating: 158it [02:10,  1.23it/s]Extractor Estimating: 159it [02:11,  1.24it/s]Extractor Estimating: 160it [02:12,  1.23it/s]Extractor Estimating: 161it [02:12,  1.25it/s]Extractor Estimating: 162it [02:13,  1.23it/s]Extractor Estimating: 163it [02:14,  1.23it/s]Extractor Estimating: 164it [02:15,  1.21it/s]Extractor Estimating: 165it [02:16,  1.17it/s]Extractor Estimating: 166it [02:17,  1.18it/s]Extractor Estimating: 167it [02:18,  1.17it/s]Extractor Estimating: 168it [02:18,  1.17it/s]Extractor Estimating: 169it [02:19,  1.12it/s]Extractor Estimating: 170it [02:20,  1.16it/s]Extractor Estimating: 171it [02:21,  1.09it/s]Extractor Estimating: 172it [02:22,  1.11it/s]Extractor Estimating: 173it [02:23,  1.12it/s]Extractor Estimating: 174it [02:24,  1.09it/s]Extractor Estimating: 175it [02:25,  1.11it/s]Extractor Estimating: 176it [02:26,  1.10it/s]Extractor Estimating: 177it [02:27,  1.11it/s]Extractor Estimating: 178it [02:27,  1.16it/s]Extractor Estimating: 179it [02:28,  1.21it/s]Extractor Estimating: 180it [02:29,  1.23it/s]Extractor Estimating: 181it [02:30,  1.17it/s]Extractor Estimating: 182it [02:31,  1.17it/s]Extractor Estimating: 183it [02:32,  1.16it/s]Extractor Estimating: 184it [02:32,  1.18it/s]Extractor Estimating: 185it [02:33,  1.21it/s]Extractor Estimating: 186it [02:34,  1.24it/s]Extractor Estimating: 187it [02:35,  1.21it/s]Extractor Estimating: 188it [02:36,  1.18it/s]Extractor Estimating: 189it [02:37,  1.18it/s]Extractor Estimating: 190it [02:37,  1.20it/s]Extractor Estimating: 191it [02:38,  1.20it/s]Extractor Estimating: 192it [02:39,  1.19it/s]Extractor Estimating: 193it [02:40,  1.19it/s]Extractor Estimating: 194it [02:41,  1.19it/s]Extractor Estimating: 195it [02:42,  1.22it/s]Extractor Estimating: 196it [02:42,  1.23it/s]Extractor Estimating: 197it [02:43,  1.22it/s]Extractor Estimating: 198it [02:44,  1.22it/s]Extractor Estimating: 199it [02:45,  1.23it/s]Extractor Estimating: 200it [02:46,  1.22it/s]Extractor Estimating: 201it [02:47,  1.21it/s]Extractor Estimating: 202it [02:47,  1.20it/s]Extractor Estimating: 203it [02:48,  1.24it/s]Extractor Estimating: 204it [02:49,  1.25it/s]Extractor Estimating: 205it [02:50,  1.24it/s]Extractor Estimating: 206it [02:50,  1.26it/s]Extractor Estimating: 207it [02:51,  1.29it/s]Extractor Estimating: 208it [02:52,  1.26it/s]Extractor Estimating: 209it [02:53,  1.25it/s]Extractor Estimating: 210it [02:54,  1.23it/s]Extractor Estimating: 211it [02:54,  1.24it/s]Extractor Estimating: 212it [02:55,  1.24it/s]Extractor Estimating: 213it [02:56,  1.23it/s]Extractor Estimating: 214it [02:57,  1.20it/s]Extractor Estimating: 215it [02:58,  1.20it/s]Extractor Estimating: 216it [02:59,  1.26it/s]Extractor Estimating: 217it [02:59,  1.24it/s]Extractor Estimating: 218it [03:00,  1.20it/s]Extractor Estimating: 219it [03:01,  1.19it/s]Extractor Estimating: 220it [03:02,  1.18it/s]Extractor Estimating: 221it [03:03,  1.19it/s]Extractor Estimating: 222it [03:04,  1.22it/s]Extractor Estimating: 223it [03:04,  1.24it/s]Extractor Estimating: 224it [03:05,  1.26it/s]Extractor Estimating: 225it [03:06,  1.22it/s]Extractor Estimating: 226it [03:07,  1.26it/s]Extractor Estimating: 227it [03:08,  1.26it/s]Extractor Estimating: 228it [03:08,  1.24it/s]Extractor Estimating: 229it [03:09,  1.27it/s]Extractor Estimating: 230it [03:10,  1.26it/s]Extractor Estimating: 231it [03:11,  1.27it/s]Extractor Estimating: 232it [03:11,  1.27it/s]Extractor Estimating: 233it [03:12,  1.28it/s]Extractor Estimating: 234it [03:13,  1.29it/s]Extractor Estimating: 235it [03:14,  1.26it/s]Extractor Estimating: 236it [03:15,  1.30it/s]Extractor Estimating: 237it [03:15,  1.28it/s]Extractor Estimating: 238it [03:16,  1.29it/s]Extractor Estimating: 239it [03:17,  1.20it/s]Extractor Estimating: 240it [03:18,  1.25it/s]Extractor Estimating: 241it [03:19,  1.22it/s]Extractor Estimating: 242it [03:19,  1.24it/s]Extractor Estimating: 243it [03:20,  1.27it/s]Extractor Estimating: 244it [03:21,  1.19it/s]Extractor Estimating: 245it [03:22,  1.25it/s]Extractor Estimating: 246it [03:23,  1.26it/s]Extractor Estimating: 247it [03:23,  1.26it/s]Extractor Estimating: 248it [03:24,  1.25it/s]Extractor Estimating: 249it [03:25,  1.27it/s]Extractor Estimating: 250it [03:26,  1.19it/s]Extractor Estimating: 250it [03:26,  1.21it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:03:59,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:03:59,923 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:03:59,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:03:59,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:03:59,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:04:00,552 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:04:00,553 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:04:01,141 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:04:02,176 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:04:02,176 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:04:05,006 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:04:05,010 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:04:05,010 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:04:05,010 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:04:05,010 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:04:05,648 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:04:05,649 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:04:06,232 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:04:06,388 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:04:06,388 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 10:08:39,069 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 10:08:39,101 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 5099 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl'}
train vocab size: 18967
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19067, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19067, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.507, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.466, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 87, avg_time 1.480, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 187, avg_time 1.495, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 74, avg_time 1.471, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 174, avg_time 2.914, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 61, avg_time 1.528, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 161, avg_time 1.482, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 48, avg_time 1.479, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 148, avg_time 1.488, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 35, avg_time 2.901, loss:nan
g_step 1200, step 135, avg_time 1.510, loss:nan
g_step 1300, step 22, avg_time 1.489, loss:nan
g_step 1400, step 122, avg_time 1.471, loss:nan
g_step 1500, step 9, avg_time 1.499, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 109, avg_time 2.874, loss:nan
g_step 1700, step 209, avg_time 1.503, loss:nan
g_step 1800, step 96, avg_time 1.486, loss:nan
g_step 1900, step 196, avg_time 1.470, loss:nan
g_step 2000, step 83, avg_time 1.533, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 183, avg_time 2.885, loss:nan
g_step 2200, step 70, avg_time 1.515, loss:nan
g_step 2300, step 170, avg_time 1.463, loss:nan
g_step 2400, step 57, avg_time 1.521, loss:nan
g_step 2500, step 157, avg_time 1.478, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 44, avg_time 2.891, loss:nan
g_step 2700, step 144, avg_time 1.469, loss:nan
g_step 2800, step 31, avg_time 1.474, loss:nan
g_step 2900, step 131, avg_time 1.461, loss:nan
g_step 3000, step 18, avg_time 1.528, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 118, avg_time 2.872, loss:nan
g_step 3200, step 5, avg_time 1.494, loss:nan
g_step 3300, step 105, avg_time 1.510, loss:nan
g_step 3400, step 205, avg_time 1.466, loss:nan
g_step 3500, step 92, avg_time 1.517, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 192, avg_time 2.888, loss:nan
g_step 3700, step 79, avg_time 1.473, loss:nan
g_step 3800, step 179, avg_time 1.455, loss:nan
g_step 3900, step 66, avg_time 1.535, loss:nan
g_step 4000, step 166, avg_time 1.468, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 53, avg_time 2.915, loss:nan
g_step 4200, step 153, avg_time 1.467, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 10:08:39 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 10:08:39 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_10-08-39_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 10:08:40 - WARNING - datasets.builder -   Using custom data configuration default-f28abcd32fd0e5d6
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f28abcd32fd0e5d6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 10:08:40,453 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:08:40,454 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:08:40,454 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:08:40,455 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:08:40,466 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:08:40,473 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:08:40,473 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:08:40,473 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:08:40,473 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:08:40,473 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:08:40,473 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 10:08:40,623 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:08:43,841 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 10:08:43,844 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f28abcd32fd0e5d6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.12ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.94ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.31ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.47ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.56ba/s]100%|██████████| 6/6 [00:01<00:00,  5.08ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.22ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.42ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.49ba/s]100%|██████████| 4/4 [00:00<00:00,  4.58ba/s]100%|██████████| 4/4 [00:00<00:00,  4.51ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  7.51ba/s] 33%|███▎      | 2/6 [00:00<00:00,  8.60ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.86ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.00ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.18ba/s]100%|██████████| 6/6 [00:00<00:00, 10.49ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.98ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.08ba/s]100%|██████████| 4/4 [00:00<00:00, 10.38ba/s]
[INFO|trainer.py:414] 2023-08-28 10:08:47,254 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 10:08:47,272 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 10:08:47,273 >>   Num examples = 5100
[INFO|trainer.py:1149] 2023-08-28 10:08:47,273 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 10:08:47,273 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 10:08:47,273 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 10:08:47,273 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 10:08:47,273 >>   Total optimization steps = 400
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<01:55,  3.45it/s]  0%|          | 2/400 [00:00<01:52,  3.54it/s]  1%|          | 3/400 [00:00<01:51,  3.58it/s]  1%|          | 4/400 [00:01<01:50,  3.59it/s]  1%|▏         | 5/400 [00:01<01:49,  3.60it/s]  2%|▏         | 6/400 [00:01<01:49,  3.60it/s]  2%|▏         | 7/400 [00:01<01:49,  3.60it/s]  2%|▏         | 8/400 [00:02<01:49,  3.59it/s]  2%|▏         | 9/400 [00:02<01:48,  3.60it/s]  2%|▎         | 10/400 [00:02<01:48,  3.60it/s]  3%|▎         | 11/400 [00:03<01:48,  3.59it/s]  3%|▎         | 12/400 [00:03<01:48,  3.59it/s]  3%|▎         | 13/400 [00:03<01:47,  3.59it/s]  4%|▎         | 14/400 [00:03<01:47,  3.60it/s]  4%|▍         | 15/400 [00:04<01:47,  3.59it/s]  4%|▍         | 16/400 [00:04<01:46,  3.60it/s]  4%|▍         | 17/400 [00:04<01:46,  3.60it/s]  4%|▍         | 18/400 [00:05<01:46,  3.60it/s]  5%|▍         | 19/400 [00:05<01:45,  3.60it/s]  5%|▌         | 20/400 [00:05<01:45,  3.60it/s]  5%|▌         | 21/400 [00:05<01:45,  3.60it/s]  6%|▌         | 22/400 [00:06<01:45,  3.60it/s]  6%|▌         | 23/400 [00:06<01:44,  3.60it/s]  6%|▌         | 24/400 [00:06<01:44,  3.60it/s]  6%|▋         | 25/400 [00:06<01:44,  3.60it/s]  6%|▋         | 26/400 [00:07<01:44,  3.60it/s]  7%|▋         | 27/400 [00:07<01:43,  3.59it/s]  7%|▋         | 28/400 [00:07<01:43,  3.60it/s]  7%|▋         | 29/400 [00:08<01:43,  3.59it/s]  8%|▊         | 30/400 [00:08<01:42,  3.60it/s]  8%|▊         | 31/400 [00:08<01:42,  3.59it/s]  8%|▊         | 32/400 [00:08<01:42,  3.60it/s]  8%|▊         | 33/400 [00:09<01:41,  3.60it/s]  8%|▊         | 34/400 [00:09<01:41,  3.60it/s]  9%|▉         | 35/400 [00:09<01:41,  3.60it/s]  9%|▉         | 36/400 [00:10<01:41,  3.60it/s]  9%|▉         | 37/400 [00:10<01:40,  3.60it/s] 10%|▉         | 38/400 [00:10<01:40,  3.60it/s] 10%|▉         | 39/400 [00:10<01:40,  3.60it/s] 10%|█         | 40/400 [00:11<01:40,  3.60it/s] 10%|█         | 41/400 [00:11<01:39,  3.60it/s] 10%|█         | 42/400 [00:11<01:39,  3.60it/s] 11%|█         | 43/400 [00:11<01:39,  3.60it/s] 11%|█         | 44/400 [00:12<01:38,  3.60it/s] 11%|█▏        | 45/400 [00:12<01:38,  3.59it/s] 12%|█▏        | 46/400 [00:12<01:38,  3.59it/s] 12%|█▏        | 47/400 [00:13<01:38,  3.59it/s] 12%|█▏        | 48/400 [00:13<01:38,  3.59it/s] 12%|█▏        | 49/400 [00:13<01:37,  3.59it/s] 12%|█▎        | 50/400 [00:13<01:37,  3.59it/s] 13%|█▎        | 51/400 [00:14<01:37,  3.60it/s] 13%|█▎        | 52/400 [00:14<01:36,  3.60it/s] 13%|█▎        | 53/400 [00:14<01:36,  3.60it/s] 14%|█▎        | 54/400 [00:15<01:36,  3.60it/s] 14%|█▍        | 55/400 [00:15<01:36,  3.59it/s] 14%|█▍        | 56/400 [00:15<01:35,  3.59it/s] 14%|█▍        | 57/400 [00:15<01:35,  3.59it/s] 14%|█▍        | 58/400 [00:16<01:35,  3.59it/s] 15%|█▍        | 59/400 [00:16<01:35,  3.59it/s] 15%|█▌        | 60/400 [00:16<01:34,  3.59it/s] 15%|█▌        | 61/400 [00:16<01:34,  3.59it/s] 16%|█▌        | 62/400 [00:17<01:34,  3.58it/s] 16%|█▌        | 63/400 [00:17<01:33,  3.59it/s] 16%|█▌        | 64/400 [00:17<01:33,  3.59it/s] 16%|█▋        | 65/400 [00:18<01:33,  3.59it/s] 16%|█▋        | 66/400 [00:18<01:33,  3.59it/s] 17%|█▋        | 67/400 [00:18<01:32,  3.59it/s] 17%|█▋        | 68/400 [00:18<01:32,  3.59it/s] 17%|█▋        | 69/400 [00:19<01:32,  3.59it/s] 18%|█▊        | 70/400 [00:19<01:31,  3.59it/s] 18%|█▊        | 71/400 [00:19<01:31,  3.59it/s] 18%|█▊        | 72/400 [00:20<01:31,  3.59it/s] 18%|█▊        | 73/400 [00:20<01:31,  3.59it/s] 18%|█▊        | 74/400 [00:20<01:30,  3.59it/s] 19%|█▉        | 75/400 [00:20<01:30,  3.58it/s] 19%|█▉        | 76/400 [00:21<01:30,  3.59it/s] 19%|█▉        | 77/400 [00:21<01:30,  3.58it/s] 20%|█▉        | 78/400 [00:21<01:29,  3.58it/s] 20%|█▉        | 79/400 [00:21<01:29,  3.58it/s] 20%|██        | 80/400 [00:22<01:22,  3.90it/s][INFO|trainer.py:2140] 2023-08-28 10:09:09,474 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:09:09,475 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 10:09:09,475 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.03it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.30it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.66it/s][A
  5%|▌         | 23/436 [00:00<00:08, 48.01it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.65it/s][A
  8%|▊         | 33/436 [00:00<00:08, 47.34it/s][A
  9%|▊         | 38/436 [00:00<00:08, 47.11it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.58it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.50it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.50it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.46it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.51it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.56it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.57it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.59it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.50it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.34it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.22it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.27it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.42it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.43it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.55it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.57it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.63it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.61it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.45it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.29it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.38it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.47it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.45it/s][A
 36%|███▌      | 158/436 [00:03<00:05, 46.53it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.52it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.58it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.62it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.52it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.47it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.42it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.46it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.48it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.30it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.38it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.41it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.49it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.48it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.42it/s][A
 53%|█████▎    | 233/436 [00:04<00:04, 46.41it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.40it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.36it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.47it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.53it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.45it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.53it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.47it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.42it/s][A
 64%|██████▍   | 278/436 [00:05<00:03, 46.40it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.39it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.38it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.42it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.51it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.52it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.47it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.52it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.45it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.41it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.39it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.31it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.36it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.43it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.13it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.43it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.40it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.45it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.42it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.40it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.36it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.33it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.37it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.34it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.43it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.51it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.43it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.42it/s][A
 96%|█████████▌| 418/436 [00:08<00:00, 46.41it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.35it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.38it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.36it/s][A                                                
                                                 [A 20%|██        | 80/400 [00:31<01:22,  3.90it/s]
100%|██████████| 436/436 [00:09<00:00, 46.36it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:09:18,877 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80
[INFO|configuration_utils.py:351] 2023-08-28 10:09:18,894 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:09:21,339 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:09:21,355 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:09:21,367 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80/special_tokens_map.json
 20%|██        | 81/400 [00:34<20:50,  3.92s/it] 20%|██        | 82/400 [00:34<14:58,  2.83s/it] 21%|██        | 83/400 [00:35<10:53,  2.06s/it] 21%|██        | 84/400 [00:35<08:02,  1.53s/it] 21%|██▏       | 85/400 [00:35<06:03,  1.15s/it] 22%|██▏       | 86/400 [00:36<04:39,  1.12it/s] 22%|██▏       | 87/400 [00:36<03:41,  1.41it/s] 22%|██▏       | 88/400 [00:36<03:00,  1.73it/s] 22%|██▏       | 89/400 [00:36<02:31,  2.05it/s] 22%|██▎       | 90/400 [00:37<02:12,  2.35it/s] 23%|██▎       | 91/400 [00:37<01:57,  2.62it/s] 23%|██▎       | 92/400 [00:37<01:48,  2.85it/s] 23%|██▎       | 93/400 [00:38<01:41,  3.04it/s] 24%|██▎       | 94/400 [00:38<01:36,  3.18it/s] 24%|██▍       | 95/400 [00:38<01:32,  3.30it/s] 24%|██▍       | 96/400 [00:38<01:29,  3.38it/s] 24%|██▍       | 97/400 [00:39<01:28,  3.44it/s] 24%|██▍       | 98/400 [00:39<01:26,  3.48it/s] 25%|██▍       | 99/400 [00:39<01:25,  3.52it/s] 25%|██▌       | 100/400 [00:39<01:24,  3.54it/s] 25%|██▌       | 101/400 [00:40<01:24,  3.56it/s] 26%|██▌       | 102/400 [00:40<01:23,  3.56it/s] 26%|██▌       | 103/400 [00:40<01:23,  3.57it/s] 26%|██▌       | 104/400 [00:41<01:22,  3.57it/s] 26%|██▋       | 105/400 [00:41<01:22,  3.58it/s] 26%|██▋       | 106/400 [00:41<01:22,  3.58it/s] 27%|██▋       | 107/400 [00:41<01:21,  3.58it/s] 27%|██▋       | 108/400 [00:42<01:21,  3.58it/s] 27%|██▋       | 109/400 [00:42<01:21,  3.58it/s] 28%|██▊       | 110/400 [00:42<01:20,  3.58it/s] 28%|██▊       | 111/400 [00:43<01:20,  3.59it/s] 28%|██▊       | 112/400 [00:43<01:20,  3.58it/s] 28%|██▊       | 113/400 [00:43<01:20,  3.59it/s] 28%|██▊       | 114/400 [00:43<01:19,  3.59it/s] 29%|██▉       | 115/400 [00:44<01:19,  3.59it/s] 29%|██▉       | 116/400 [00:44<01:19,  3.59it/s] 29%|██▉       | 117/400 [00:44<01:18,  3.59it/s] 30%|██▉       | 118/400 [00:44<01:18,  3.59it/s] 30%|██▉       | 119/400 [00:45<01:18,  3.59it/s] 30%|███       | 120/400 [00:45<01:18,  3.58it/s] 30%|███       | 121/400 [00:45<01:17,  3.58it/s] 30%|███       | 122/400 [00:46<01:17,  3.58it/s] 31%|███       | 123/400 [00:46<01:17,  3.58it/s] 31%|███       | 124/400 [00:46<01:17,  3.57it/s] 31%|███▏      | 125/400 [00:46<01:16,  3.57it/s] 32%|███▏      | 126/400 [00:47<01:16,  3.58it/s] 32%|███▏      | 127/400 [00:47<01:16,  3.58it/s] 32%|███▏      | 128/400 [00:47<01:16,  3.58it/s] 32%|███▏      | 129/400 [00:48<01:15,  3.58it/s] 32%|███▎      | 130/400 [00:48<01:15,  3.58it/s] 33%|███▎      | 131/400 [00:48<01:15,  3.58it/s] 33%|███▎      | 132/400 [00:48<01:14,  3.58it/s] 33%|███▎      | 133/400 [00:49<01:14,  3.58it/s] 34%|███▎      | 134/400 [00:49<01:14,  3.58it/s] 34%|███▍      | 135/400 [00:49<01:14,  3.56it/s] 34%|███▍      | 136/400 [00:50<01:13,  3.57it/s] 34%|███▍      | 137/400 [00:50<01:13,  3.57it/s] 34%|███▍      | 138/400 [00:50<01:13,  3.57it/s] 35%|███▍      | 139/400 [00:50<01:13,  3.57it/s] 35%|███▌      | 140/400 [00:51<01:12,  3.57it/s] 35%|███▌      | 141/400 [00:51<01:12,  3.57it/s] 36%|███▌      | 142/400 [00:51<01:12,  3.57it/s] 36%|███▌      | 143/400 [00:51<01:14,  3.47it/s] 36%|███▌      | 144/400 [00:52<01:13,  3.47it/s] 36%|███▋      | 145/400 [00:52<01:12,  3.51it/s] 36%|███▋      | 146/400 [00:52<01:12,  3.51it/s] 37%|███▋      | 147/400 [00:53<01:11,  3.53it/s] 37%|███▋      | 148/400 [00:53<01:10,  3.55it/s] 37%|███▋      | 149/400 [00:53<01:10,  3.56it/s] 38%|███▊      | 150/400 [00:53<01:10,  3.57it/s] 38%|███▊      | 151/400 [00:54<01:09,  3.57it/s] 38%|███▊      | 152/400 [00:54<01:09,  3.58it/s] 38%|███▊      | 153/400 [00:54<01:09,  3.57it/s] 38%|███▊      | 154/400 [00:55<01:08,  3.57it/s] 39%|███▉      | 155/400 [00:55<01:08,  3.58it/s] 39%|███▉      | 156/400 [00:55<01:08,  3.58it/s] 39%|███▉      | 157/400 [00:55<01:07,  3.58it/s] 40%|███▉      | 158/400 [00:56<01:07,  3.58it/s] 40%|███▉      | 159/400 [00:56<01:07,  3.58it/s] 40%|████      | 160/400 [00:56<01:01,  3.90it/s][INFO|trainer.py:2140] 2023-08-28 10:09:43,956 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:09:43,956 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 10:09:43,956 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.3857, 'eval_samples_per_second': 371.417, 'eval_steps_per_second': 46.454, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.72it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.10it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.46it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.80it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.37it/s][A
  8%|▊         | 33/436 [00:00<00:08, 47.07it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.63it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.25it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.25it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.31it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.30it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.35it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.42it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.49it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.46it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.28it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.13it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.10it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.11it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.24it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.18it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.30it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.39it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.39it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.31it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.19it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.13it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.15it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.20it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.26it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.23it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.35it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.31it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.20it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.12it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.00it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.07it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.07it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.15it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.16it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.21it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.22it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.21it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.15it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.08it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.09it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.10it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.07it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.19it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.19it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.22it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.18it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.05it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.08it/s][A
 64%|██████▍   | 278/436 [00:05<00:03, 46.06it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.01it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.04it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.14it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.22it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.19it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.25it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.14it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.16it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.14it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.06it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.06it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.05it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.15it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.22it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.20it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.16it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.08it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.12it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.08it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.10it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.08it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.16it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.18it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.15it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.12it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.14it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.16it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.07it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.04it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.03it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.06it/s][A                                                 
                                                 [A 40%|████      | 160/400 [01:06<01:01,  3.90it/s]
100%|██████████| 436/436 [00:09<00:00, 46.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:09:53,422 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-160
[INFO|configuration_utils.py:351] 2023-08-28 10:09:53,443 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-160/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:09:55,963 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:09:55,983 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:09:55,991 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-160/special_tokens_map.json
 40%|████      | 161/400 [01:09<15:46,  3.96s/it] 40%|████      | 162/400 [01:09<11:19,  2.86s/it] 41%|████      | 163/400 [01:09<08:13,  2.08s/it] 41%|████      | 164/400 [01:10<06:03,  1.54s/it] 41%|████▏     | 165/400 [01:10<04:33,  1.16s/it] 42%|████▏     | 166/400 [01:10<03:29,  1.11it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 42%|████▏     | 167/400 [01:10<02:45,  1.40it/s] 42%|████▏     | 168/400 [01:11<02:14,  1.72it/s] 42%|████▏     | 169/400 [01:11<01:53,  2.04it/s] 42%|████▎     | 170/400 [01:11<01:38,  2.34it/s] 43%|████▎     | 171/400 [01:12<01:27,  2.61it/s] 43%|████▎     | 172/400 [01:12<01:20,  2.84it/s] 43%|████▎     | 173/400 [01:12<01:14,  3.03it/s] 44%|████▎     | 174/400 [01:12<01:11,  3.18it/s] 44%|████▍     | 175/400 [01:13<01:08,  3.29it/s] 44%|████▍     | 176/400 [01:13<01:06,  3.37it/s] 44%|████▍     | 177/400 [01:13<01:04,  3.43it/s] 44%|████▍     | 178/400 [01:14<01:03,  3.48it/s] 45%|████▍     | 179/400 [01:14<01:03,  3.51it/s] 45%|████▌     | 180/400 [01:14<01:02,  3.52it/s] 45%|████▌     | 181/400 [01:14<01:01,  3.54it/s] 46%|████▌     | 182/400 [01:15<01:01,  3.54it/s] 46%|████▌     | 183/400 [01:15<01:01,  3.55it/s] 46%|████▌     | 184/400 [01:15<01:00,  3.56it/s] 46%|████▋     | 185/400 [01:15<01:00,  3.56it/s] 46%|████▋     | 186/400 [01:16<00:59,  3.57it/s] 47%|████▋     | 187/400 [01:16<00:59,  3.57it/s] 47%|████▋     | 188/400 [01:16<00:59,  3.57it/s] 47%|████▋     | 189/400 [01:17<00:59,  3.58it/s] 48%|████▊     | 190/400 [01:17<00:58,  3.58it/s] 48%|████▊     | 191/400 [01:17<00:58,  3.58it/s] 48%|████▊     | 192/400 [01:17<00:58,  3.58it/s] 48%|████▊     | 193/400 [01:18<00:57,  3.57it/s] 48%|████▊     | 194/400 [01:18<00:57,  3.57it/s] 49%|████▉     | 195/400 [01:18<00:57,  3.57it/s] 49%|████▉     | 196/400 [01:19<00:57,  3.57it/s] 49%|████▉     | 197/400 [01:19<00:56,  3.58it/s] 50%|████▉     | 198/400 [01:19<00:56,  3.57it/s] 50%|████▉     | 199/400 [01:19<00:56,  3.58it/s] 50%|█████     | 200/400 [01:20<00:55,  3.57it/s] 50%|█████     | 201/400 [01:20<00:55,  3.58it/s] 50%|█████     | 202/400 [01:20<00:55,  3.57it/s] 51%|█████     | 203/400 [01:21<00:55,  3.57it/s] 51%|█████     | 204/400 [01:21<00:54,  3.56it/s] 51%|█████▏    | 205/400 [01:21<00:54,  3.57it/s] 52%|█████▏    | 206/400 [01:21<00:54,  3.57it/s] 52%|█████▏    | 207/400 [01:22<00:54,  3.57it/s] 52%|█████▏    | 208/400 [01:22<00:53,  3.57it/s] 52%|█████▏    | 209/400 [01:22<00:53,  3.57it/s] 52%|█████▎    | 210/400 [01:22<00:53,  3.57it/s] 53%|█████▎    | 211/400 [01:23<00:52,  3.57it/s] 53%|█████▎    | 212/400 [01:23<00:52,  3.58it/s] 53%|█████▎    | 213/400 [01:23<00:52,  3.58it/s] 54%|█████▎    | 214/400 [01:24<00:52,  3.57it/s] 54%|█████▍    | 215/400 [01:24<00:51,  3.58it/s] 54%|█████▍    | 216/400 [01:24<00:51,  3.58it/s] 54%|█████▍    | 217/400 [01:24<00:51,  3.58it/s] 55%|█████▍    | 218/400 [01:25<00:50,  3.57it/s] 55%|█████▍    | 219/400 [01:25<00:50,  3.57it/s] 55%|█████▌    | 220/400 [01:25<00:50,  3.57it/s] 55%|█████▌    | 221/400 [01:26<00:50,  3.57it/s] 56%|█████▌    | 222/400 [01:26<00:49,  3.58it/s] 56%|█████▌    | 223/400 [01:26<00:49,  3.57it/s] 56%|█████▌    | 224/400 [01:26<00:49,  3.58it/s] 56%|█████▋    | 225/400 [01:27<00:49,  3.56it/s] 56%|█████▋    | 226/400 [01:27<00:48,  3.57it/s] 57%|█████▋    | 227/400 [01:27<00:48,  3.57it/s] 57%|█████▋    | 228/400 [01:28<00:48,  3.58it/s] 57%|█████▋    | 229/400 [01:28<00:47,  3.57it/s] 57%|█████▊    | 230/400 [01:28<00:47,  3.57it/s] 58%|█████▊    | 231/400 [01:28<00:47,  3.57it/s] 58%|█████▊    | 232/400 [01:29<00:46,  3.57it/s] 58%|█████▊    | 233/400 [01:29<00:46,  3.57it/s] 58%|█████▊    | 234/400 [01:29<00:46,  3.57it/s] 59%|█████▉    | 235/400 [01:29<00:46,  3.57it/s] 59%|█████▉    | 236/400 [01:30<00:46,  3.56it/s] 59%|█████▉    | 237/400 [01:30<00:45,  3.56it/s] 60%|█████▉    | 238/400 [01:30<00:45,  3.56it/s] 60%|█████▉    | 239/400 [01:31<00:45,  3.57it/s] 60%|██████    | 240/400 [01:31<00:41,  3.89it/s][INFO|trainer.py:2140] 2023-08-28 10:10:18,583 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:10:18,583 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 10:10:18,583 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4384, 'eval_samples_per_second': 369.341, 'eval_steps_per_second': 46.194, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.68it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.09it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.25it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.57it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.20it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.88it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.52it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.17it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.13it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.15it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.23it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.19it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.24it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.24it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.30it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.23it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.08it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.09it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.09it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.12it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.15it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.16it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.21it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.19it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.11it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.00it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.03it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.05it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.14it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.11it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.15it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.21it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.20it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.13it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.04it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.98it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.00it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.08it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.08it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.06it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.19it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.16it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.19it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.08it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.04it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.08it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.08it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.14it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.12it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.06it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.16it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.14it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.10it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.02it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.08it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.08it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.12it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.15it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.09it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.16it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.15it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.14it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.11it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.06it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.12it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.88it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.00it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.01it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.08it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.04it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.09it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.11it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.05it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.06it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.00it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.09it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.11it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.11it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.13it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.06it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.15it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.13it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.00it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.04it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.07it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.09it/s][A                                                 
                                                 [A 60%|██████    | 240/400 [01:40<00:41,  3.89it/s]
100%|██████████| 436/436 [00:09<00:00, 46.09it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:10:28,054 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-28 10:10:28,068 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:10:30,463 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:10:30,478 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:10:30,488 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-240/special_tokens_map.json
 60%|██████    | 241/400 [01:43<10:25,  3.93s/it] 60%|██████    | 242/400 [01:44<07:28,  2.84s/it] 61%|██████    | 243/400 [01:44<05:24,  2.07s/it] 61%|██████    | 244/400 [01:44<03:59,  1.53s/it] 61%|██████▏   | 245/400 [01:44<02:59,  1.16s/it] 62%|██████▏   | 246/400 [01:45<02:17,  1.12it/s] 62%|██████▏   | 247/400 [01:45<01:48,  1.41it/s] 62%|██████▏   | 248/400 [01:45<01:28,  1.72it/s] 62%|██████▏   | 249/400 [01:46<01:13,  2.04it/s] 62%|██████▎   | 250/400 [01:46<01:04,  2.34it/s] 63%|██████▎   | 251/400 [01:46<00:57,  2.60it/s] 63%|██████▎   | 252/400 [01:46<00:52,  2.83it/s] 63%|██████▎   | 253/400 [01:47<00:48,  3.02it/s] 64%|██████▎   | 254/400 [01:47<00:46,  3.17it/s] 64%|██████▍   | 255/400 [01:47<00:44,  3.28it/s] 64%|██████▍   | 256/400 [01:48<00:42,  3.36it/s] 64%|██████▍   | 257/400 [01:48<00:41,  3.42it/s] 64%|██████▍   | 258/400 [01:48<00:40,  3.47it/s] 65%|██████▍   | 259/400 [01:48<00:40,  3.50it/s] 65%|██████▌   | 260/400 [01:49<00:39,  3.52it/s] 65%|██████▌   | 261/400 [01:49<00:39,  3.54it/s] 66%|██████▌   | 262/400 [01:49<00:39,  3.53it/s] 66%|██████▌   | 263/400 [01:49<00:38,  3.54it/s] 66%|██████▌   | 264/400 [01:50<00:38,  3.55it/s] 66%|██████▋   | 265/400 [01:50<00:37,  3.56it/s] 66%|██████▋   | 266/400 [01:50<00:37,  3.56it/s] 67%|██████▋   | 267/400 [01:51<00:37,  3.56it/s] 67%|██████▋   | 268/400 [01:51<00:37,  3.56it/s] 67%|██████▋   | 269/400 [01:51<00:36,  3.57it/s] 68%|██████▊   | 270/400 [01:51<00:36,  3.57it/s] 68%|██████▊   | 271/400 [01:52<00:37,  3.47it/s] 68%|██████▊   | 272/400 [01:52<00:36,  3.47it/s] 68%|██████▊   | 273/400 [01:52<00:36,  3.49it/s] 68%|██████▊   | 274/400 [01:53<00:35,  3.51it/s] 69%|██████▉   | 275/400 [01:53<00:35,  3.53it/s] 69%|██████▉   | 276/400 [01:53<00:35,  3.54it/s] 69%|██████▉   | 277/400 [01:53<00:34,  3.55it/s] 70%|██████▉   | 278/400 [01:54<00:34,  3.56it/s] 70%|██████▉   | 279/400 [01:54<00:33,  3.56it/s] 70%|███████   | 280/400 [01:54<00:33,  3.56it/s] 70%|███████   | 281/400 [01:55<00:33,  3.57it/s] 70%|███████   | 282/400 [01:55<00:33,  3.57it/s] 71%|███████   | 283/400 [01:55<00:32,  3.57it/s] 71%|███████   | 284/400 [01:55<00:32,  3.57it/s] 71%|███████▏  | 285/400 [01:56<00:32,  3.57it/s] 72%|███████▏  | 286/400 [01:56<00:31,  3.57it/s] 72%|███████▏  | 287/400 [01:56<00:31,  3.57it/s] 72%|███████▏  | 288/400 [01:57<00:31,  3.57it/s] 72%|███████▏  | 289/400 [01:57<00:31,  3.57it/s] 72%|███████▎  | 290/400 [01:57<00:30,  3.57it/s] 73%|███████▎  | 291/400 [01:57<00:30,  3.57it/s] 73%|███████▎  | 292/400 [01:58<00:30,  3.57it/s] 73%|███████▎  | 293/400 [01:58<00:29,  3.57it/s] 74%|███████▎  | 294/400 [01:58<00:29,  3.57it/s] 74%|███████▍  | 295/400 [01:58<00:29,  3.57it/s] 74%|███████▍  | 296/400 [01:59<00:29,  3.57it/s] 74%|███████▍  | 297/400 [01:59<00:28,  3.57it/s] 74%|███████▍  | 298/400 [01:59<00:28,  3.57it/s] 75%|███████▍  | 299/400 [02:00<00:28,  3.57it/s] 75%|███████▌  | 300/400 [02:00<00:27,  3.57it/s] 75%|███████▌  | 301/400 [02:00<00:27,  3.57it/s] 76%|███████▌  | 302/400 [02:00<00:27,  3.56it/s] 76%|███████▌  | 303/400 [02:01<00:27,  3.57it/s] 76%|███████▌  | 304/400 [02:01<00:26,  3.57it/s] 76%|███████▋  | 305/400 [02:01<00:26,  3.57it/s] 76%|███████▋  | 306/400 [02:02<00:26,  3.57it/s] 77%|███████▋  | 307/400 [02:02<00:26,  3.57it/s] 77%|███████▋  | 308/400 [02:02<00:25,  3.57it/s] 77%|███████▋  | 309/400 [02:02<00:25,  3.57it/s] 78%|███████▊  | 310/400 [02:03<00:25,  3.57it/s] 78%|███████▊  | 311/400 [02:03<00:24,  3.57it/s] 78%|███████▊  | 312/400 [02:03<00:24,  3.57it/s] 78%|███████▊  | 313/400 [02:04<00:24,  3.57it/s] 78%|███████▊  | 314/400 [02:04<00:24,  3.57it/s] 79%|███████▉  | 315/400 [02:04<00:23,  3.57it/s] 79%|███████▉  | 316/400 [02:04<00:23,  3.57it/s] 79%|███████▉  | 317/400 [02:05<00:23,  3.57it/s] 80%|███████▉  | 318/400 [02:05<00:22,  3.57it/s] 80%|███████▉  | 319/400 [02:05<00:22,  3.57it/s] 80%|████████  | 320/400 [02:05<00:20,  3.89it/s][INFO|trainer.py:2140] 2023-08-28 10:10:53,174 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:10:53,174 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 10:10:53,175 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4534, 'eval_samples_per_second': 368.754, 'eval_steps_per_second': 46.121, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.70it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.86it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.24it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.57it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.08it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.58it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.37it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.01it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.00it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.00it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.00it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.12it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.20it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.22it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.11it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.00it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.95it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.88it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.93it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.97it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.08it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.12it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.15it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.15it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.06it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.97it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.90it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.93it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.97it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.04it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.09it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.08it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.12it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.03it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.98it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.83it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.86it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.91it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.99it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.03it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.00it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.12it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.09it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.04it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.94it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.85it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.92it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.99it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.04it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.04it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.13it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.10it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.03it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.97it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.95it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.94it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.95it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.02it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.00it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.13it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.11it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.02it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.04it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.96it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.00it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.94it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.98it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.01it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.07it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.08it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.05it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.07it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.00it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.04it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.03it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.97it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.00it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.01it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.06it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.05it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.08it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.06it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.03it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.00it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.89it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.00it/s][A                                                 
                                                 [A 80%|████████  | 320/400 [02:15<00:20,  3.89it/s]
100%|██████████| 436/436 [00:09<00:00, 46.00it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:11:02,672 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-320
[INFO|configuration_utils.py:351] 2023-08-28 10:11:02,693 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-320/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:11:05,425 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-320/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:11:05,442 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:11:05,450 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-320/special_tokens_map.json
 80%|████████  | 321/400 [02:18<05:18,  4.04s/it] 80%|████████  | 322/400 [02:19<03:46,  2.91s/it] 81%|████████  | 323/400 [02:19<02:43,  2.12s/it] 81%|████████  | 324/400 [02:19<01:59,  1.57s/it] 81%|████████▏ | 325/400 [02:19<01:28,  1.18s/it] 82%|████████▏ | 326/400 [02:20<01:07,  1.10it/s] 82%|████████▏ | 327/400 [02:20<00:52,  1.39it/s] 82%|████████▏ | 328/400 [02:20<00:42,  1.70it/s] 82%|████████▏ | 329/400 [02:20<00:35,  2.02it/s] 82%|████████▎ | 330/400 [02:21<00:30,  2.32it/s] 83%|████████▎ | 331/400 [02:21<00:26,  2.59it/s] 83%|████████▎ | 332/400 [02:21<00:24,  2.82it/s] 83%|████████▎ | 333/400 [02:22<00:22,  3.01it/s] 84%|████████▎ | 334/400 [02:22<00:20,  3.17it/s] 84%|████████▍ | 335/400 [02:22<00:19,  3.28it/s] 84%|████████▍ | 336/400 [02:22<00:19,  3.36it/s] 84%|████████▍ | 337/400 [02:23<00:18,  3.43it/s] 84%|████████▍ | 338/400 [02:23<00:17,  3.47it/s] 85%|████████▍ | 339/400 [02:23<00:17,  3.50it/s] 85%|████████▌ | 340/400 [02:24<00:17,  3.52it/s] 85%|████████▌ | 341/400 [02:24<00:16,  3.54it/s] 86%|████████▌ | 342/400 [02:24<00:16,  3.54it/s] 86%|████████▌ | 343/400 [02:24<00:16,  3.55it/s] 86%|████████▌ | 344/400 [02:25<00:15,  3.56it/s] 86%|████████▋ | 345/400 [02:25<00:15,  3.57it/s] 86%|████████▋ | 346/400 [02:26<00:29,  1.82it/s] 87%|████████▋ | 347/400 [02:26<00:24,  2.14it/s] 87%|████████▋ | 348/400 [02:27<00:21,  2.43it/s] 87%|████████▋ | 349/400 [02:27<00:18,  2.69it/s] 88%|████████▊ | 350/400 [02:27<00:17,  2.91it/s] 88%|████████▊ | 351/400 [02:28<00:15,  3.08it/s] 88%|████████▊ | 352/400 [02:28<00:14,  3.21it/s] 88%|████████▊ | 353/400 [02:28<00:14,  3.30it/s] 88%|████████▊ | 354/400 [02:28<00:13,  3.38it/s] 89%|████████▉ | 355/400 [02:29<00:13,  3.43it/s] 89%|████████▉ | 356/400 [02:29<00:12,  3.47it/s] 89%|████████▉ | 357/400 [02:29<00:12,  3.50it/s] 90%|████████▉ | 358/400 [02:29<00:11,  3.52it/s] 90%|████████▉ | 359/400 [02:30<00:11,  3.54it/s] 90%|█████████ | 360/400 [02:30<00:11,  3.55it/s] 90%|█████████ | 361/400 [02:30<00:10,  3.56it/s] 90%|█████████ | 362/400 [02:31<00:10,  3.56it/s] 91%|█████████ | 363/400 [02:31<00:10,  3.56it/s] 91%|█████████ | 364/400 [02:31<00:10,  3.54it/s] 91%|█████████▏| 365/400 [02:31<00:09,  3.55it/s] 92%|█████████▏| 366/400 [02:32<00:09,  3.56it/s] 92%|█████████▏| 367/400 [02:32<00:09,  3.56it/s] 92%|█████████▏| 368/400 [02:32<00:08,  3.57it/s] 92%|█████████▏| 369/400 [02:33<00:08,  3.57it/s] 92%|█████████▎| 370/400 [02:33<00:08,  3.57it/s] 93%|█████████▎| 371/400 [02:33<00:08,  3.57it/s] 93%|█████████▎| 372/400 [02:33<00:07,  3.57it/s] 93%|█████████▎| 373/400 [02:34<00:07,  3.57it/s] 94%|█████████▎| 374/400 [02:34<00:07,  3.58it/s] 94%|█████████▍| 375/400 [02:34<00:07,  3.57it/s] 94%|█████████▍| 376/400 [02:35<00:06,  3.57it/s] 94%|█████████▍| 377/400 [02:35<00:06,  3.57it/s] 94%|█████████▍| 378/400 [02:35<00:06,  3.58it/s] 95%|█████████▍| 379/400 [02:35<00:05,  3.57it/s] 95%|█████████▌| 380/400 [02:36<00:05,  3.57it/s] 95%|█████████▌| 381/400 [02:36<00:05,  3.58it/s] 96%|█████████▌| 382/400 [02:36<00:05,  3.58it/s] 96%|█████████▌| 383/400 [02:36<00:04,  3.57it/s] 96%|█████████▌| 384/400 [02:37<00:04,  3.57it/s] 96%|█████████▋| 385/400 [02:37<00:04,  3.58it/s] 96%|█████████▋| 386/400 [02:37<00:03,  3.56it/s] 97%|█████████▋| 387/400 [02:38<00:03,  3.57it/s] 97%|█████████▋| 388/400 [02:38<00:03,  3.57it/s] 97%|█████████▋| 389/400 [02:38<00:03,  3.58it/s] 98%|█████████▊| 390/400 [02:38<00:02,  3.57it/s] 98%|█████████▊| 391/400 [02:39<00:02,  3.57it/s] 98%|█████████▊| 392/400 [02:39<00:02,  3.57it/s] 98%|█████████▊| 393/400 [02:39<00:01,  3.57it/s] 98%|█████████▊| 394/400 [02:40<00:01,  3.57it/s] 99%|█████████▉| 395/400 [02:40<00:01,  3.57it/s] 99%|█████████▉| 396/400 [02:40<00:01,  3.57it/s] 99%|█████████▉| 397/400 [02:40<00:00,  3.55it/s]100%|█████████▉| 398/400 [02:41<00:00,  3.56it/s]100%|█████████▉| 399/400 [02:41<00:00,  3.57it/s]100%|██████████| 400/400 [02:41<00:00,  3.89it/s][INFO|trainer.py:2140] 2023-08-28 10:11:28,965 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:11:28,965 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 10:11:28,965 >>   Batch size = 8
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4716, 'eval_samples_per_second': 368.049, 'eval_steps_per_second': 46.033, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.79it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.80it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.19it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.49it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.12it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.77it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.48it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.03it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.08it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.10it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.09it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.20it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.07it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.07it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.12it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.03it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.01it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.93it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.04it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.05it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.10it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.06it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.12it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.11it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.99it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.02it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.02it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.06it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.06it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.10it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.08it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.05it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.01it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.06it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.03it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.02it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.06it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.06it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.05it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.10it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.10it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.07it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.08it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.03it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.00it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.96it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.00it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.99it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.01it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.00it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.06it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.03it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.02it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.04it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.00it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.02it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.00it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.05it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.07it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.05it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.04it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.00it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.89it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 45.91it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.94it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.84it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.90it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.95it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.99it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.95it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.92it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.97it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.01it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.04it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.06it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.06it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.05it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.02it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.04it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.97it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.02it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.00it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.05it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.06it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.00it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.08it/s][A                                                 
                                                 [A100%|██████████| 400/400 [02:51<00:00,  3.89it/s]
100%|██████████| 436/436 [00:09<00:00, 46.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:11:38,452 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-400
[INFO|configuration_utils.py:351] 2023-08-28 10:11:38,470 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-400/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:11:40,749 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:11:40,791 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:11:40,801 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 10:11:41,094 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 10:11:41,094 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80 (score: 1.0161933898925781).
                                                 100%|██████████| 400/400 [02:55<00:00,  3.89it/s]100%|██████████| 400/400 [02:55<00:00,  2.27it/s]
[INFO|trainer.py:1894] 2023-08-28 10:11:43,147 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 10:11:43,166 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:11:45,836 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:11:45,851 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:11:45,863 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:11:46,053 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:46,053 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:46,053 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:46,053 >>   train_runtime            = 0:02:55.86
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:46,053 >>   train_samples            =       5100
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:46,053 >>   train_samples_per_second =      145.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:46,053 >>   train_steps_per_second   =      2.275
{'eval_loss': 1.0161933898925781, 'eval_runtime': 9.4691, 'eval_samples_per_second': 368.144, 'eval_steps_per_second': 46.044, 'epoch': 5.0}
{'train_runtime': 175.862, 'train_samples_per_second': 145.0, 'train_steps_per_second': 2.275, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 10:11:46 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 10:11:46,088 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:11:46,088 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 10:11:46,088 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 57.73it/s]  3%|▎         | 12/436 [00:00<00:08, 50.44it/s]  4%|▍         | 18/436 [00:00<00:08, 48.66it/s]  5%|▌         | 23/436 [00:00<00:08, 47.91it/s]  6%|▋         | 28/436 [00:00<00:08, 47.48it/s]  8%|▊         | 33/436 [00:00<00:08, 47.20it/s]  9%|▊         | 38/436 [00:00<00:08, 47.03it/s] 10%|▉         | 43/436 [00:00<00:08, 46.61it/s] 11%|█         | 48/436 [00:01<00:08, 46.34it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.26it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.34it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.29it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.35it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.45it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.48it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.51it/s] 20%|██        | 88/436 [00:01<00:07, 46.39it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.22it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.18it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.17it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.26it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.22it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.33it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.44it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.44it/s] 31%|███       | 133/436 [00:02<00:06, 46.36it/s] 32%|███▏      | 138/436 [00:02<00:06, 46.22it/s] 33%|███▎      | 143/436 [00:03<00:06, 46.25it/s] 34%|███▍      | 148/436 [00:03<00:06, 46.23it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.20it/s] 36%|███▌      | 158/436 [00:03<00:06, 46.28it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.25it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.35it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.39it/s] 41%|████      | 178/436 [00:03<00:05, 46.29it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.24it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.20it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.16it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.22it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.21it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.29it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.04it/s] 50%|█████     | 218/436 [00:04<00:04, 46.17it/s] 51%|█████     | 223/436 [00:04<00:04, 46.18it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.14it/s] 53%|█████▎    | 233/436 [00:05<00:04, 46.18it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.14it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.23it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.19it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.30it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.27it/s] 60%|██████    | 263/436 [00:05<00:03, 46.18it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.24it/s] 63%|██████▎   | 273/436 [00:05<00:03, 46.14it/s] 64%|██████▍   | 278/436 [00:05<00:03, 46.20it/s] 65%|██████▍   | 283/436 [00:06<00:03, 46.13it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.17it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.18it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.21it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.25it/s] 71%|███████   | 308/436 [00:06<00:02, 46.21it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.19it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.14it/s] 74%|███████▍  | 323/436 [00:06<00:02, 46.15it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.17it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.15it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.25it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.23it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.25it/s] 81%|████████  | 353/436 [00:07<00:01, 46.27it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.20it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.14it/s] 84%|████████▍ | 368/436 [00:07<00:01, 46.11it/s] 86%|████████▌ | 373/436 [00:08<00:01, 46.12it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.20it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.22it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.21it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.25it/s] 91%|█████████▏| 398/436 [00:08<00:00, 46.29it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.24it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.18it/s] 95%|█████████▍| 413/436 [00:08<00:00, 46.19it/s] 96%|█████████▌| 418/436 [00:09<00:00, 46.09it/s] 97%|█████████▋| 423/436 [00:09<00:00, 46.21it/s] 98%|█████████▊| 428/436 [00:09<00:00, 46.18it/s] 99%|█████████▉| 433/436 [00:09<00:00, 46.19it/s]100%|██████████| 436/436 [00:09<00:00, 46.35it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:11:55,521 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:55,521 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:55,521 >>   eval_loss               =     1.0162
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:55,521 >>   eval_runtime            = 0:00:09.43
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:55,521 >>   eval_samples            =       3486
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:55,521 >>   eval_samples_per_second =    369.574
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:55,521 >>   eval_steps_per_second   =     46.223
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:11:55,521 >>   perplexity              =     2.7627
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:12:00,341 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:12:00,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:12:00,347 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:12:00,347 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:12:00,347 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:12:00,657 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:12:00,658 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:12:01,238 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:12:02,268 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:12:02,268 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:12:05,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:12:05,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:12:05,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:12:05,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:12:05,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:12:05,764 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:12:05,765 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:12:06,341 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:12:06,505 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:12:06,505 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-160
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-400
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-320
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-240
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/generator/iter5/model/checkpoint-80
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'manufacturer', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13015
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13115, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.10it/s]Extractor Predicting: 2it [00:01,  1.16it/s]Extractor Predicting: 3it [00:02,  1.23it/s]Extractor Predicting: 4it [00:03,  1.23it/s]Extractor Predicting: 5it [00:04,  1.24it/s]Extractor Predicting: 6it [00:04,  1.24it/s]Extractor Predicting: 7it [00:05,  1.23it/s]Extractor Predicting: 8it [00:06,  1.26it/s]Extractor Predicting: 9it [00:07,  1.27it/s]Extractor Predicting: 10it [00:08,  1.29it/s]Extractor Predicting: 11it [00:08,  1.26it/s]Extractor Predicting: 12it [00:09,  1.26it/s]Extractor Predicting: 13it [00:10,  1.25it/s]Extractor Predicting: 14it [00:11,  1.24it/s]Extractor Predicting: 15it [00:12,  1.26it/s]Extractor Predicting: 16it [00:12,  1.26it/s]Extractor Predicting: 17it [00:13,  1.29it/s]Extractor Predicting: 18it [00:14,  1.30it/s]Extractor Predicting: 19it [00:15,  1.29it/s]Extractor Predicting: 20it [00:15,  1.28it/s]Extractor Predicting: 21it [00:16,  1.28it/s]Extractor Predicting: 22it [00:17,  1.31it/s]Extractor Predicting: 23it [00:18,  1.31it/s]Extractor Predicting: 24it [00:19,  1.27it/s]Extractor Predicting: 25it [00:19,  1.26it/s]Extractor Predicting: 26it [00:20,  1.24it/s]Extractor Predicting: 27it [00:21,  1.25it/s]Extractor Predicting: 28it [00:22,  1.26it/s]Extractor Predicting: 29it [00:23,  1.25it/s]Extractor Predicting: 30it [00:23,  1.22it/s]Extractor Predicting: 31it [00:24,  1.23it/s]Extractor Predicting: 32it [00:25,  1.23it/s]Extractor Predicting: 33it [00:26,  1.23it/s]Extractor Predicting: 34it [00:27,  1.25it/s]Extractor Predicting: 35it [00:27,  1.26it/s]Extractor Predicting: 36it [00:28,  1.21it/s]Extractor Predicting: 37it [00:29,  1.24it/s]Extractor Predicting: 38it [00:30,  1.28it/s]Extractor Predicting: 39it [00:31,  1.29it/s]Extractor Predicting: 40it [00:31,  1.31it/s]Extractor Predicting: 41it [00:32,  1.31it/s]Extractor Predicting: 42it [00:33,  1.31it/s]Extractor Predicting: 43it [00:34,  1.30it/s]Extractor Predicting: 44it [00:34,  1.28it/s]Extractor Predicting: 45it [00:35,  1.32it/s]Extractor Predicting: 46it [00:36,  1.31it/s]Extractor Predicting: 47it [00:37,  1.32it/s]Extractor Predicting: 48it [00:37,  1.31it/s]Extractor Predicting: 49it [00:38,  1.34it/s]Extractor Predicting: 50it [00:39,  1.33it/s]Extractor Predicting: 51it [00:40,  1.32it/s]Extractor Predicting: 52it [00:40,  1.30it/s]Extractor Predicting: 53it [00:41,  1.30it/s]Extractor Predicting: 54it [00:42,  1.28it/s]Extractor Predicting: 55it [00:43,  1.33it/s]Extractor Predicting: 56it [00:43,  1.34it/s]Extractor Predicting: 57it [00:44,  1.30it/s]Extractor Predicting: 58it [00:45,  1.31it/s]Extractor Predicting: 59it [00:46,  1.32it/s]Extractor Predicting: 60it [00:47,  1.29it/s]Extractor Predicting: 61it [00:47,  1.28it/s]Extractor Predicting: 62it [00:48,  1.30it/s]Extractor Predicting: 63it [00:49,  1.30it/s]Extractor Predicting: 64it [00:50,  1.31it/s]Extractor Predicting: 65it [00:50,  1.30it/s]Extractor Predicting: 66it [00:51,  1.31it/s]Extractor Predicting: 67it [00:52,  1.34it/s]Extractor Predicting: 68it [00:53,  1.30it/s]Extractor Predicting: 69it [00:53,  1.30it/s]Extractor Predicting: 70it [00:54,  1.29it/s]Extractor Predicting: 71it [00:55,  1.32it/s]Extractor Predicting: 72it [00:56,  1.32it/s]Extractor Predicting: 73it [00:56,  1.31it/s]Extractor Predicting: 74it [00:57,  1.33it/s]Extractor Predicting: 75it [00:58,  1.29it/s]Extractor Predicting: 76it [00:59,  1.30it/s]Extractor Predicting: 77it [01:00,  1.29it/s]Extractor Predicting: 78it [01:00,  1.30it/s]Extractor Predicting: 79it [01:01,  1.31it/s]Extractor Predicting: 80it [01:02,  1.31it/s]Extractor Predicting: 81it [01:03,  1.30it/s]Extractor Predicting: 82it [01:03,  1.31it/s]Extractor Predicting: 83it [01:04,  1.27it/s]Extractor Predicting: 84it [01:05,  1.29it/s]Extractor Predicting: 85it [01:06,  1.28it/s]Extractor Predicting: 86it [01:07,  1.28it/s]Extractor Predicting: 87it [01:07,  1.28it/s]Extractor Predicting: 88it [01:08,  1.31it/s]Extractor Predicting: 89it [01:09,  1.32it/s]Extractor Predicting: 90it [01:09,  1.36it/s]Extractor Predicting: 91it [01:10,  1.39it/s]Extractor Predicting: 92it [01:11,  1.41it/s]Extractor Predicting: 93it [01:12,  1.37it/s]Extractor Predicting: 94it [01:12,  1.41it/s]Extractor Predicting: 95it [01:13,  1.40it/s]Extractor Predicting: 96it [01:14,  1.39it/s]Extractor Predicting: 97it [01:14,  1.39it/s]Extractor Predicting: 98it [01:15,  1.35it/s]Extractor Predicting: 99it [01:16,  1.33it/s]Extractor Predicting: 100it [01:17,  1.33it/s]Extractor Predicting: 101it [01:17,  1.38it/s]Extractor Predicting: 102it [01:18,  1.40it/s]Extractor Predicting: 103it [01:19,  1.40it/s]Extractor Predicting: 104it [01:20,  1.38it/s]Extractor Predicting: 105it [01:20,  1.39it/s]Extractor Predicting: 106it [01:21,  1.40it/s]Extractor Predicting: 107it [01:22,  1.39it/s]Extractor Predicting: 108it [01:22,  1.38it/s]Extractor Predicting: 109it [01:23,  1.39it/s]Extractor Predicting: 110it [01:24,  1.39it/s]Extractor Predicting: 111it [01:25,  1.39it/s]Extractor Predicting: 112it [01:25,  1.44it/s]Extractor Predicting: 113it [01:26,  1.45it/s]Extractor Predicting: 114it [01:27,  1.33it/s]Extractor Predicting: 115it [01:28,  1.34it/s]Extractor Predicting: 116it [01:28,  1.35it/s]Extractor Predicting: 117it [01:29,  1.37it/s]Extractor Predicting: 118it [01:30,  1.41it/s]Extractor Predicting: 119it [01:30,  1.40it/s]Extractor Predicting: 120it [01:31,  1.38it/s]Extractor Predicting: 121it [01:32,  1.44it/s]Extractor Predicting: 122it [01:32,  1.43it/s]Extractor Predicting: 123it [01:33,  1.38it/s]Extractor Predicting: 124it [01:34,  1.42it/s]Extractor Predicting: 125it [01:35,  1.41it/s]Extractor Predicting: 126it [01:35,  1.47it/s]Extractor Predicting: 127it [01:36,  1.46it/s]Extractor Predicting: 128it [01:37,  1.45it/s]Extractor Predicting: 129it [01:37,  1.41it/s]Extractor Predicting: 130it [01:38,  1.43it/s]Extractor Predicting: 131it [01:39,  1.47it/s]Extractor Predicting: 132it [01:39,  1.45it/s]Extractor Predicting: 133it [01:40,  1.42it/s]Extractor Predicting: 134it [01:41,  1.44it/s]Extractor Predicting: 135it [01:41,  1.45it/s]Extractor Predicting: 136it [01:42,  1.46it/s]Extractor Predicting: 137it [01:43,  1.44it/s]Extractor Predicting: 138it [01:44,  1.49it/s]Extractor Predicting: 139it [01:44,  1.50it/s]Extractor Predicting: 140it [01:45,  1.51it/s]Extractor Predicting: 141it [01:45,  1.54it/s]Extractor Predicting: 142it [01:46,  1.50it/s]Extractor Predicting: 143it [01:47,  1.47it/s]Extractor Predicting: 144it [01:47,  1.82it/s]Extractor Predicting: 144it [01:47,  1.34it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:14:01,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:14:01,776 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:14:01,776 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:14:01,776 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:14:01,776 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:14:02,396 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:14:02,397 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:14:03,077 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:14:04,094 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:14:04,094 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:14:06,959 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:14:06,964 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:14:06,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:14:06,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:14:06,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:14:07,625 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:14:07,626 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:14:08,197 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:14:08,346 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:14:08,346 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13522
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13622, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.34it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.39it/s]Extractor Predicting: 6it [00:04,  1.36it/s]Extractor Predicting: 7it [00:05,  1.36it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.39it/s]Extractor Predicting: 10it [00:07,  1.42it/s]Extractor Predicting: 11it [00:07,  1.40it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:09,  1.37it/s]Extractor Predicting: 14it [00:10,  1.40it/s]Extractor Predicting: 15it [00:10,  1.39it/s]Extractor Predicting: 16it [00:11,  1.36it/s]Extractor Predicting: 17it [00:12,  1.36it/s]Extractor Predicting: 18it [00:13,  1.35it/s]Extractor Predicting: 19it [00:13,  1.37it/s]Extractor Predicting: 20it [00:14,  1.35it/s]Extractor Predicting: 21it [00:15,  1.37it/s]Extractor Predicting: 22it [00:15,  1.38it/s]Extractor Predicting: 23it [00:16,  1.35it/s]Extractor Predicting: 24it [00:17,  1.34it/s]Extractor Predicting: 25it [00:18,  1.35it/s]Extractor Predicting: 26it [00:18,  1.34it/s]Extractor Predicting: 27it [00:19,  1.34it/s]Extractor Predicting: 28it [00:20,  1.36it/s]Extractor Predicting: 29it [00:21,  1.37it/s]Extractor Predicting: 30it [00:21,  1.33it/s]Extractor Predicting: 31it [00:22,  1.35it/s]Extractor Predicting: 32it [00:23,  1.33it/s]Extractor Predicting: 33it [00:24,  1.34it/s]Extractor Predicting: 34it [00:24,  1.36it/s]Extractor Predicting: 35it [00:25,  1.34it/s]Extractor Predicting: 36it [00:26,  1.39it/s]Extractor Predicting: 37it [00:27,  1.35it/s]Extractor Predicting: 38it [00:27,  1.33it/s]Extractor Predicting: 39it [00:28,  1.33it/s]Extractor Predicting: 40it [00:29,  1.31it/s]Extractor Predicting: 41it [00:30,  1.31it/s]Extractor Predicting: 42it [00:30,  1.32it/s]Extractor Predicting: 43it [00:31,  1.32it/s]Extractor Predicting: 44it [00:32,  1.30it/s]Extractor Predicting: 45it [00:33,  1.31it/s]Extractor Predicting: 46it [00:33,  1.32it/s]Extractor Predicting: 47it [00:34,  1.34it/s]Extractor Predicting: 48it [00:35,  1.34it/s]Extractor Predicting: 49it [00:36,  1.36it/s]Extractor Predicting: 50it [00:36,  1.36it/s]Extractor Predicting: 51it [00:37,  1.40it/s]Extractor Predicting: 52it [00:38,  1.40it/s]Extractor Predicting: 53it [00:39,  1.39it/s]Extractor Predicting: 54it [00:39,  1.39it/s]Extractor Predicting: 55it [00:40,  1.38it/s]Extractor Predicting: 56it [00:41,  1.36it/s]Extractor Predicting: 57it [00:42,  1.31it/s]Extractor Predicting: 58it [00:42,  1.29it/s]Extractor Predicting: 59it [00:43,  1.23it/s]Extractor Predicting: 60it [00:44,  1.27it/s]Extractor Predicting: 61it [00:45,  1.26it/s]Extractor Predicting: 62it [00:46,  1.26it/s]Extractor Predicting: 63it [00:46,  1.33it/s]Extractor Predicting: 64it [00:47,  1.31it/s]Extractor Predicting: 65it [00:48,  1.34it/s]Extractor Predicting: 66it [00:49,  1.33it/s]Extractor Predicting: 67it [00:49,  1.33it/s]Extractor Predicting: 68it [00:50,  1.32it/s]Extractor Predicting: 69it [00:51,  1.31it/s]Extractor Predicting: 70it [00:52,  1.25it/s]Extractor Predicting: 71it [00:52,  1.27it/s]Extractor Predicting: 72it [00:53,  1.27it/s]Extractor Predicting: 73it [00:54,  1.27it/s]Extractor Predicting: 74it [00:55,  1.28it/s]Extractor Predicting: 75it [00:56,  1.26it/s]Extractor Predicting: 76it [00:56,  1.29it/s]Extractor Predicting: 77it [00:57,  1.27it/s]Extractor Predicting: 78it [00:58,  1.28it/s]Extractor Predicting: 79it [00:59,  1.29it/s]Extractor Predicting: 80it [00:59,  1.28it/s]Extractor Predicting: 81it [01:00,  1.29it/s]Extractor Predicting: 82it [01:01,  1.26it/s]Extractor Predicting: 83it [01:02,  1.26it/s]Extractor Predicting: 84it [01:03,  1.29it/s]Extractor Predicting: 85it [01:03,  1.29it/s]Extractor Predicting: 86it [01:04,  1.30it/s]Extractor Predicting: 87it [01:05,  1.28it/s]Extractor Predicting: 88it [01:06,  1.33it/s]Extractor Predicting: 89it [01:06,  1.32it/s]Extractor Predicting: 90it [01:07,  1.31it/s]Extractor Predicting: 91it [01:08,  1.36it/s]Extractor Predicting: 92it [01:09,  1.36it/s]Extractor Predicting: 93it [01:09,  1.38it/s]Extractor Predicting: 94it [01:10,  1.36it/s]Extractor Predicting: 95it [01:11,  1.36it/s]Extractor Predicting: 96it [01:12,  1.35it/s]Extractor Predicting: 97it [01:12,  1.33it/s]Extractor Predicting: 98it [01:13,  1.32it/s]Extractor Predicting: 99it [01:14,  1.33it/s]Extractor Predicting: 100it [01:15,  1.32it/s]Extractor Predicting: 101it [01:15,  1.31it/s]Extractor Predicting: 102it [01:16,  1.27it/s]Extractor Predicting: 103it [01:17,  1.29it/s]Extractor Predicting: 104it [01:18,  1.29it/s]Extractor Predicting: 105it [01:18,  1.32it/s]Extractor Predicting: 106it [01:19,  1.33it/s]Extractor Predicting: 107it [01:20,  1.32it/s]Extractor Predicting: 108it [01:21,  1.32it/s]Extractor Predicting: 109it [01:21,  1.33it/s]Extractor Predicting: 110it [01:22,  1.30it/s]Extractor Predicting: 111it [01:23,  1.32it/s]Extractor Predicting: 112it [01:24,  1.31it/s]Extractor Predicting: 113it [01:24,  1.34it/s]Extractor Predicting: 114it [01:25,  1.32it/s]Extractor Predicting: 115it [01:26,  1.35it/s]Extractor Predicting: 116it [01:27,  1.30it/s]Extractor Predicting: 117it [01:28,  1.28it/s]Extractor Predicting: 118it [01:28,  1.28it/s]Extractor Predicting: 119it [01:29,  1.27it/s]Extractor Predicting: 120it [01:30,  1.27it/s]Extractor Predicting: 121it [01:31,  1.27it/s]Extractor Predicting: 122it [01:31,  1.31it/s]Extractor Predicting: 123it [01:32,  1.30it/s]Extractor Predicting: 124it [01:33,  1.27it/s]Extractor Predicting: 125it [01:34,  1.26it/s]Extractor Predicting: 126it [01:35,  1.23it/s]Extractor Predicting: 127it [01:36,  1.24it/s]Extractor Predicting: 128it [01:36,  1.25it/s]Extractor Predicting: 129it [01:37,  1.15it/s]Extractor Predicting: 130it [01:38,  1.20it/s]Extractor Predicting: 131it [01:39,  1.22it/s]Extractor Predicting: 132it [01:40,  1.23it/s]Extractor Predicting: 133it [01:41,  1.23it/s]Extractor Predicting: 134it [01:41,  1.25it/s]Extractor Predicting: 135it [01:42,  1.28it/s]Extractor Predicting: 136it [01:43,  1.30it/s]Extractor Predicting: 137it [01:43,  1.33it/s]Extractor Predicting: 138it [01:44,  1.32it/s]Extractor Predicting: 139it [01:45,  1.33it/s]Extractor Predicting: 140it [01:46,  1.32it/s]Extractor Predicting: 141it [01:47,  1.27it/s]Extractor Predicting: 142it [01:47,  1.28it/s]Extractor Predicting: 143it [01:48,  1.36it/s]Extractor Predicting: 143it [01:48,  1.32it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:16:03,600 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:16:03,605 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:16:03,605 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:16:03,605 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:16:03,605 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:16:04,229 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:16:04,230 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:16:04,822 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:16:05,835 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:16:05,835 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:16:08,678 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:16:08,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:16:08,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:16:08,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:16:08,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:16:09,335 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:16:09,336 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:16:09,915 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:16:10,068 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:16:10,068 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 464
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 564, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.06it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 2it [00:01,  1.38it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_5_seed_1', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', data_dir='outputs/wrapper/fewrel/unseen_5_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:28<04:15, 28.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:52<03:27, 25.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:16<02:56, 25.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:37<02:20, 23.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [02:10<02:13, 26.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:33<01:42, 25.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:56<01:14, 24.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [03:20<00:48, 24.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:42<00:23, 23.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [04:05<00:00, 23.59s/it]Generating: 100%|██████████| 10/10 [04:05<00:00, 24.59s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 188, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 225, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 268, 'raw': 416}
{'target': 600, 'success': 289, 'raw': 448}
{'target': 600, 'success': 309, 'raw': 480}
{'target': 600, 'success': 337, 'raw': 512}
{'target': 600, 'success': 361, 'raw': 544}
{'target': 600, 'success': 378, 'raw': 576}
{'target': 600, 'success': 401, 'raw': 608}
{'target': 600, 'success': 424, 'raw': 640}
{'target': 600, 'success': 446, 'raw': 672}
{'target': 600, 'success': 465, 'raw': 704}
{'target': 600, 'success': 486, 'raw': 736}
{'target': 600, 'success': 502, 'raw': 768}
{'target': 600, 'success': 528, 'raw': 800}
{'target': 600, 'success': 549, 'raw': 832}
{'target': 600, 'success': 570, 'raw': 864}
{'target': 600, 'success': 589, 'raw': 896}
{'target': 600, 'success': 610, 'raw': 928}
{'prompt': 'Relation : country .', 'success_rate': 0.6573275862068966, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 499, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 547, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7725, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : genre . Context : Later in 2003 , he played in Steven Spielberg 's The Bourne Ultimatum . Head Entity : The Bourne Ultimatum , Tail Entity : director .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 604, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.755, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8579545454545454, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : voice type . Context : Later in the year , the band members decided to cancel their performance of " In My Time " on Broadway , in part to play the first time . Head Entity : In My Time , Tail Entity : vocal .\n']
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 102, 'raw': 160}
{'target': 600, 'success': 118, 'raw': 192}
{'target': 600, 'success': 134, 'raw': 224}
{'target': 600, 'success': 146, 'raw': 256}
{'target': 600, 'success': 160, 'raw': 288}
{'target': 600, 'success': 178, 'raw': 320}
{'target': 600, 'success': 193, 'raw': 352}
{'target': 600, 'success': 215, 'raw': 384}
{'target': 600, 'success': 233, 'raw': 416}
{'target': 600, 'success': 245, 'raw': 448}
{'target': 600, 'success': 262, 'raw': 480}
{'target': 600, 'success': 282, 'raw': 512}
{'target': 600, 'success': 304, 'raw': 544}
{'target': 600, 'success': 318, 'raw': 576}
{'target': 600, 'success': 336, 'raw': 608}
{'target': 600, 'success': 355, 'raw': 640}
{'target': 600, 'success': 375, 'raw': 672}
{'target': 600, 'success': 394, 'raw': 704}
{'target': 600, 'success': 413, 'raw': 736}
{'target': 600, 'success': 431, 'raw': 768}
{'target': 600, 'success': 450, 'raw': 800}
{'target': 600, 'success': 466, 'raw': 832}
{'target': 600, 'success': 480, 'raw': 864}
{'target': 600, 'success': 495, 'raw': 896}
{'target': 600, 'success': 511, 'raw': 928}
{'target': 600, 'success': 529, 'raw': 960}
{'target': 600, 'success': 546, 'raw': 992}
{'target': 600, 'success': 561, 'raw': 1024}
{'target': 600, 'success': 580, 'raw': 1056}
{'target': 600, 'success': 596, 'raw': 1088}
{'target': 600, 'success': 612, 'raw': 1120}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5464285714285714, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : contains administrative territorial entity . Context : The city of Marchec is under the rule of the Marchec Municipal Municipality in Romania . Head Entity : Marchec , Tail Entity : Romanian .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8233695652173914, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8707386363636364, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 12301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_5_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.71s/it]Extractor Estimating: 2it [00:18,  7.73s/it]Extractor Estimating: 3it [00:20,  5.36s/it]Extractor Estimating: 4it [00:21,  3.55s/it]Extractor Estimating: 5it [00:22,  2.56s/it]Extractor Estimating: 6it [00:23,  1.96s/it]Extractor Estimating: 7it [00:23,  1.60s/it]Extractor Estimating: 8it [00:24,  1.33s/it]Extractor Estimating: 9it [00:25,  1.16s/it]Extractor Estimating: 10it [00:26,  1.04s/it]Extractor Estimating: 11it [00:27,  1.02it/s]Extractor Estimating: 12it [00:27,  1.07it/s]Extractor Estimating: 13it [00:28,  1.15it/s]Extractor Estimating: 14it [00:29,  1.21it/s]Extractor Estimating: 15it [00:30,  1.21it/s]Extractor Estimating: 16it [00:30,  1.22it/s]Extractor Estimating: 17it [00:32,  1.18s/it]Extractor Estimating: 18it [00:33,  1.07s/it]Extractor Estimating: 19it [00:34,  1.00s/it]Extractor Estimating: 20it [00:35,  1.07it/s]Extractor Estimating: 21it [00:36,  1.13it/s]Extractor Estimating: 22it [00:37,  1.16it/s]Extractor Estimating: 23it [00:37,  1.20it/s]Extractor Estimating: 24it [00:38,  1.23it/s]Extractor Estimating: 25it [00:39,  1.25it/s]Extractor Estimating: 26it [00:40,  1.22it/s]Extractor Estimating: 27it [00:41,  1.22it/s]Extractor Estimating: 28it [00:41,  1.23it/s]Extractor Estimating: 29it [00:42,  1.24it/s]Extractor Estimating: 30it [00:43,  1.23it/s]Extractor Estimating: 31it [00:44,  1.23it/s]Extractor Estimating: 32it [00:45,  1.25it/s]Extractor Estimating: 33it [00:45,  1.25it/s]Extractor Estimating: 34it [00:46,  1.26it/s]Extractor Estimating: 35it [00:47,  1.23it/s]Extractor Estimating: 36it [00:48,  1.25it/s]Extractor Estimating: 37it [00:49,  1.20it/s]Extractor Estimating: 38it [00:49,  1.21it/s]Extractor Estimating: 39it [00:50,  1.21it/s]Extractor Estimating: 40it [00:51,  1.23it/s]Extractor Estimating: 41it [00:52,  1.18it/s]Extractor Estimating: 42it [00:53,  1.21it/s]Extractor Estimating: 43it [00:54,  1.22it/s]Extractor Estimating: 44it [00:54,  1.25it/s]Extractor Estimating: 45it [00:55,  1.18it/s]Extractor Estimating: 46it [00:56,  1.19it/s]Extractor Estimating: 47it [00:57,  1.23it/s]Extractor Estimating: 48it [00:58,  1.23it/s]Extractor Estimating: 49it [00:58,  1.24it/s]Extractor Estimating: 50it [00:59,  1.26it/s]Extractor Estimating: 51it [01:00,  1.27it/s]Extractor Estimating: 52it [01:01,  1.26it/s]Extractor Estimating: 53it [01:02,  1.27it/s]Extractor Estimating: 54it [01:05,  1.59s/it]Extractor Estimating: 55it [01:06,  1.35s/it]Extractor Estimating: 56it [01:07,  1.21s/it]Extractor Estimating: 57it [01:08,  1.12s/it]Extractor Estimating: 58it [01:08,  1.00s/it]Extractor Estimating: 59it [01:09,  1.06it/s]Extractor Estimating: 60it [01:10,  1.09it/s]Extractor Estimating: 61it [01:11,  1.07it/s]Extractor Estimating: 62it [01:12,  1.15it/s]Extractor Estimating: 63it [01:13,  1.16it/s]Extractor Estimating: 64it [01:13,  1.16it/s]Extractor Estimating: 65it [01:14,  1.17it/s]Extractor Estimating: 66it [01:15,  1.17it/s]Extractor Estimating: 67it [01:16,  1.20it/s]Extractor Estimating: 68it [01:17,  1.23it/s]Extractor Estimating: 69it [01:17,  1.24it/s]Extractor Estimating: 70it [01:18,  1.24it/s]Extractor Estimating: 71it [01:19,  1.24it/s]Extractor Estimating: 72it [01:20,  1.22it/s]Extractor Estimating: 73it [01:21,  1.21it/s]Extractor Estimating: 74it [01:22,  1.23it/s]Extractor Estimating: 75it [01:22,  1.25it/s]Extractor Estimating: 76it [01:23,  1.28it/s]Extractor Estimating: 77it [01:24,  1.26it/s]Extractor Estimating: 78it [01:25,  1.24it/s]Extractor Estimating: 79it [01:25,  1.25it/s]Extractor Estimating: 80it [01:26,  1.28it/s]Extractor Estimating: 81it [01:27,  1.27it/s]Extractor Estimating: 82it [01:28,  1.29it/s]Extractor Estimating: 83it [01:28,  1.33it/s]Extractor Estimating: 84it [01:29,  1.31it/s]Extractor Estimating: 85it [01:30,  1.30it/s]Extractor Estimating: 86it [01:31,  1.26it/s]Extractor Estimating: 87it [01:32,  1.29it/s]Extractor Estimating: 88it [01:32,  1.28it/s]Extractor Estimating: 89it [01:33,  1.30it/s]Extractor Estimating: 90it [01:34,  1.22it/s]Extractor Estimating: 91it [01:35,  1.20it/s]Extractor Estimating: 92it [01:36,  1.22it/s]Extractor Estimating: 93it [01:37,  1.21it/s]Extractor Estimating: 94it [01:37,  1.20it/s]Extractor Estimating: 95it [01:38,  1.20it/s]Extractor Estimating: 96it [01:39,  1.26it/s]Extractor Estimating: 97it [01:40,  1.20it/s]Extractor Estimating: 98it [01:41,  1.23it/s]Extractor Estimating: 99it [01:41,  1.25it/s]Extractor Estimating: 100it [01:42,  1.28it/s]Extractor Estimating: 101it [01:43,  1.25it/s]Extractor Estimating: 102it [01:44,  1.23it/s]Extractor Estimating: 103it [01:45,  1.24it/s]Extractor Estimating: 104it [01:45,  1.23it/s]Extractor Estimating: 105it [01:46,  1.28it/s]Extractor Estimating: 106it [01:47,  1.31it/s]Extractor Estimating: 107it [01:48,  1.30it/s]Extractor Estimating: 108it [01:48,  1.29it/s]Extractor Estimating: 109it [01:49,  1.30it/s]Extractor Estimating: 110it [01:50,  1.26it/s]Extractor Estimating: 111it [01:51,  1.28it/s]Extractor Estimating: 112it [01:52,  1.29it/s]Extractor Estimating: 113it [01:52,  1.32it/s]Extractor Estimating: 114it [01:53,  1.32it/s]Extractor Estimating: 115it [01:54,  1.29it/s]Extractor Estimating: 116it [01:55,  1.22it/s]Extractor Estimating: 117it [01:56,  1.21it/s]Extractor Estimating: 118it [01:56,  1.20it/s]Extractor Estimating: 119it [01:57,  1.16it/s]Extractor Estimating: 120it [01:58,  1.15it/s]Extractor Estimating: 121it [01:59,  1.18it/s]Extractor Estimating: 122it [02:00,  1.17it/s]Extractor Estimating: 123it [02:01,  1.22it/s]Extractor Estimating: 124it [02:01,  1.25it/s]Extractor Estimating: 125it [02:02,  1.22it/s]Extractor Estimating: 126it [02:03,  1.21it/s]Extractor Estimating: 127it [02:04,  1.21it/s]Extractor Estimating: 128it [02:05,  1.24it/s]Extractor Estimating: 129it [02:05,  1.27it/s]Extractor Estimating: 130it [02:06,  1.29it/s]Extractor Estimating: 131it [02:07,  1.28it/s]Extractor Estimating: 132it [02:08,  1.27it/s]Extractor Estimating: 133it [02:09,  1.30it/s]Extractor Estimating: 134it [02:09,  1.29it/s]Extractor Estimating: 135it [02:10,  1.29it/s]Extractor Estimating: 136it [02:11,  1.31it/s]Extractor Estimating: 137it [02:12,  1.26it/s]Extractor Estimating: 138it [02:12,  1.30it/s]Extractor Estimating: 139it [02:13,  1.25it/s]Extractor Estimating: 140it [02:14,  1.29it/s]Extractor Estimating: 141it [02:15,  1.27it/s]Extractor Estimating: 142it [02:16,  1.28it/s]Extractor Estimating: 143it [02:16,  1.27it/s]Extractor Estimating: 144it [02:17,  1.30it/s]Extractor Estimating: 145it [02:18,  1.31it/s]Extractor Estimating: 146it [02:19,  1.29it/s]Extractor Estimating: 147it [02:20,  1.28it/s]Extractor Estimating: 148it [02:20,  1.28it/s]Extractor Estimating: 149it [02:21,  1.27it/s]Extractor Estimating: 150it [02:22,  1.26it/s]Extractor Estimating: 151it [02:23,  1.25it/s]Extractor Estimating: 152it [02:23,  1.26it/s]Extractor Estimating: 153it [02:24,  1.27it/s]Extractor Estimating: 154it [02:25,  1.26it/s]Extractor Estimating: 155it [02:26,  1.25it/s]Extractor Estimating: 156it [02:27,  1.23it/s]Extractor Estimating: 157it [02:28,  1.19it/s]Extractor Estimating: 158it [02:28,  1.24it/s]Extractor Estimating: 159it [02:29,  1.25it/s]Extractor Estimating: 160it [02:30,  1.25it/s]Extractor Estimating: 161it [02:31,  1.27it/s]Extractor Estimating: 162it [02:32,  1.24it/s]Extractor Estimating: 163it [02:32,  1.25it/s]Extractor Estimating: 164it [02:33,  1.22it/s]Extractor Estimating: 165it [02:34,  1.18it/s]Extractor Estimating: 166it [02:35,  1.19it/s]Extractor Estimating: 167it [02:36,  1.18it/s]Extractor Estimating: 168it [02:37,  1.18it/s]Extractor Estimating: 169it [02:38,  1.14it/s]Extractor Estimating: 170it [02:38,  1.18it/s]Extractor Estimating: 171it [02:39,  1.18it/s]Extractor Estimating: 172it [02:40,  1.18it/s]Extractor Estimating: 173it [02:41,  1.17it/s]Extractor Estimating: 174it [02:42,  1.13it/s]Extractor Estimating: 175it [02:43,  1.15it/s]Extractor Estimating: 176it [02:44,  1.13it/s]Extractor Estimating: 177it [02:45,  1.14it/s]Extractor Estimating: 178it [02:45,  1.19it/s]Extractor Estimating: 179it [02:46,  1.24it/s]Extractor Estimating: 180it [02:47,  1.26it/s]Extractor Estimating: 181it [02:48,  1.20it/s]Extractor Estimating: 182it [02:49,  1.19it/s]Extractor Estimating: 183it [02:49,  1.19it/s]Extractor Estimating: 184it [02:50,  1.21it/s]Extractor Estimating: 185it [02:51,  1.24it/s]Extractor Estimating: 186it [02:52,  1.27it/s]Extractor Estimating: 187it [02:53,  1.24it/s]Extractor Estimating: 188it [02:54,  1.13it/s]Extractor Estimating: 189it [02:54,  1.15it/s]Extractor Estimating: 190it [02:55,  1.19it/s]Extractor Estimating: 191it [02:56,  1.20it/s]Extractor Estimating: 192it [02:57,  1.20it/s]Extractor Estimating: 193it [02:58,  1.20it/s]Extractor Estimating: 194it [02:59,  1.20it/s]Extractor Estimating: 195it [02:59,  1.23it/s]Extractor Estimating: 196it [03:00,  1.25it/s]Extractor Estimating: 197it [03:01,  1.23it/s]Extractor Estimating: 198it [03:02,  1.24it/s]Extractor Estimating: 199it [03:02,  1.24it/s]Extractor Estimating: 200it [03:03,  1.24it/s]Extractor Estimating: 201it [03:04,  1.23it/s]Extractor Estimating: 202it [03:05,  1.22it/s]Extractor Estimating: 203it [03:06,  1.26it/s]Extractor Estimating: 204it [03:06,  1.27it/s]Extractor Estimating: 205it [03:07,  1.26it/s]Extractor Estimating: 206it [03:08,  1.29it/s]Extractor Estimating: 207it [03:09,  1.32it/s]Extractor Estimating: 208it [03:10,  1.29it/s]Extractor Estimating: 209it [03:10,  1.27it/s]Extractor Estimating: 210it [03:11,  1.25it/s]Extractor Estimating: 211it [03:12,  1.27it/s]Extractor Estimating: 212it [03:13,  1.27it/s]Extractor Estimating: 213it [03:14,  1.25it/s]Extractor Estimating: 214it [03:14,  1.22it/s]Extractor Estimating: 215it [03:15,  1.22it/s]Extractor Estimating: 216it [03:16,  1.28it/s]Extractor Estimating: 217it [03:17,  1.26it/s]Extractor Estimating: 218it [03:18,  1.22it/s]Extractor Estimating: 219it [03:19,  1.20it/s]Extractor Estimating: 220it [03:19,  1.20it/s]Extractor Estimating: 221it [03:20,  1.21it/s]Extractor Estimating: 222it [03:21,  1.23it/s]Extractor Estimating: 223it [03:22,  1.26it/s]Extractor Estimating: 224it [03:22,  1.27it/s]Extractor Estimating: 225it [03:23,  1.23it/s]Extractor Estimating: 226it [03:24,  1.27it/s]Extractor Estimating: 227it [03:25,  1.27it/s]Extractor Estimating: 228it [03:26,  1.25it/s]Extractor Estimating: 229it [03:26,  1.29it/s]Extractor Estimating: 230it [03:27,  1.27it/s]Extractor Estimating: 231it [03:28,  1.28it/s]Extractor Estimating: 232it [03:29,  1.28it/s]Extractor Estimating: 233it [03:30,  1.29it/s]Extractor Estimating: 234it [03:30,  1.31it/s]Extractor Estimating: 235it [03:31,  1.27it/s]Extractor Estimating: 236it [03:32,  1.31it/s]Extractor Estimating: 237it [03:33,  1.30it/s]Extractor Estimating: 238it [03:33,  1.31it/s]Extractor Estimating: 239it [03:34,  1.31it/s]Extractor Estimating: 240it [03:35,  1.34it/s]Extractor Estimating: 241it [03:36,  1.29it/s]Extractor Estimating: 242it [03:36,  1.31it/s]Extractor Estimating: 243it [03:37,  1.32it/s]Extractor Estimating: 244it [03:38,  1.23it/s]Extractor Estimating: 245it [03:39,  1.30it/s]Extractor Estimating: 246it [03:40,  1.30it/s]Extractor Estimating: 247it [03:40,  1.30it/s]Extractor Estimating: 248it [03:41,  1.28it/s]Extractor Estimating: 249it [03:42,  1.30it/s]Extractor Estimating: 250it [03:43,  1.25it/s]Extractor Estimating: 250it [03:43,  1.12it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 4984 mean pseudo reward: 0.934106971908432
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl'}
train vocab size: 23406
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23506, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_5_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23506, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.578, loss:1258.1040
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.288, loss:1220.5850
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.275, loss:1136.6893
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.314, loss:1135.6959
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.297, loss:1098.5111
>> valid entity prec:0.4991, rec:0.5046, f1:0.5019
>> valid relation prec:0.0198, rec:0.0046, f1:0.0075
>> valid relation with NER prec:0.0198, rec:0.0046, f1:0.0075
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.967, loss:1083.1575
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.290, loss:1076.0579
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.293, loss:1034.3979
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.306, loss:1012.0473
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.300, loss:1060.9423
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5356, rec:0.4049, f1:0.4612
>> valid relation prec:0.0626, rec:0.0077, f1:0.0138
>> valid relation with NER prec:0.0626, rec:0.0077, f1:0.0138
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 60, avg_time 2.966, loss:996.9196
g_step 1200, step 160, avg_time 1.306, loss:979.9522
g_step 1300, step 52, avg_time 1.311, loss:937.1257
g_step 1400, step 152, avg_time 1.307, loss:928.4949
g_step 1500, step 44, avg_time 1.302, loss:935.9917
>> valid entity prec:0.4913, rec:0.4371, f1:0.4626
>> valid relation prec:0.0999, rec:0.0235, f1:0.0381
>> valid relation with NER prec:0.0999, rec:0.0235, f1:0.0381
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 144, avg_time 2.964, loss:884.1680
g_step 1700, step 36, avg_time 1.305, loss:869.2232
g_step 1800, step 136, avg_time 1.305, loss:851.8588
g_step 1900, step 28, avg_time 1.309, loss:839.2801
g_step 2000, step 128, avg_time 1.304, loss:800.8024
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5271, rec:0.4551, f1:0.4885
>> valid relation prec:0.1568, rec:0.0419, f1:0.0661
>> valid relation with NER prec:0.1568, rec:0.0419, f1:0.0661
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 20, avg_time 2.969, loss:821.8060
g_step 2200, step 120, avg_time 1.308, loss:765.0070
g_step 2300, step 12, avg_time 1.312, loss:782.4420
g_step 2400, step 112, avg_time 1.301, loss:717.1394
g_step 2500, step 4, avg_time 1.308, loss:760.7505
>> valid entity prec:0.4912, rec:0.5097, f1:0.5002
>> valid relation prec:0.1030, rec:0.0333, f1:0.0503
>> valid relation with NER prec:0.1030, rec:0.0333, f1:0.0503
g_step 2600, step 104, avg_time 2.978, loss:689.3409
g_step 2700, step 204, avg_time 1.311, loss:756.9746
g_step 2800, step 96, avg_time 1.300, loss:656.2950
g_step 2900, step 196, avg_time 1.306, loss:714.4517
g_step 3000, step 88, avg_time 1.308, loss:650.2491
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5221, rec:0.4465, f1:0.4813
>> valid relation prec:0.1097, rec:0.0307, f1:0.0480
>> valid relation with NER prec:0.1097, rec:0.0307, f1:0.0480
g_step 3100, step 188, avg_time 2.964, loss:652.2580
g_step 3200, step 80, avg_time 1.311, loss:614.5571
g_step 3300, step 180, avg_time 1.299, loss:619.5928
g_step 3400, step 72, avg_time 1.306, loss:604.6579
g_step 3500, step 172, avg_time 1.303, loss:599.5047
>> valid entity prec:0.5226, rec:0.4864, f1:0.5039
>> valid relation prec:0.0888, rec:0.0295, f1:0.0443
>> valid relation with NER prec:0.0888, rec:0.0295, f1:0.0443
new max entity f1 on valid!
g_step 3600, step 64, avg_time 2.973, loss:580.0812
g_step 3700, step 164, avg_time 1.314, loss:549.4519
g_step 3800, step 56, avg_time 1.300, loss:576.5570
g_step 3900, step 156, avg_time 1.309, loss:549.7058
g_step 4000, step 48, avg_time 1.299, loss:547.9589
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4934, rec:0.4978, f1:0.4956
>> valid relation prec:0.1075, rec:0.0410, f1:0.0594
>> valid relation with NER prec:0.1075, rec:0.0410, f1:0.0594
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 148, avg_time 2.978, loss:517.8956
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 12:18:08 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 12:18:08 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_12-18-08_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 12:18:09 - WARNING - datasets.builder -   Using custom data configuration default-5821a9e411a2267b
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5821a9e411a2267b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 12:18:09,706 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:18:09,707 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 12:18:09,708 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:18:09,709 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 12:18:09,717 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:18:09,722 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:18:09,722 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:18:09,722 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:18:09,722 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:18:09,722 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:18:09,722 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 12:18:09,837 >> loading weights file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 12:18:12,919 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 12:18:12,921 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_5_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5821a9e411a2267b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 12:18:12 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1492a4503050> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.87ba/s] 40%|████      | 2/5 [00:00<00:00,  3.73ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.07ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.23ba/s]100%|██████████| 5/5 [00:01<00:00,  4.34ba/s]100%|██████████| 5/5 [00:01<00:00,  4.09ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.18ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.40ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.83ba/s]100%|██████████| 4/4 [00:00<00:00,  4.97ba/s]100%|██████████| 4/4 [00:00<00:00,  4.43ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.17ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.86ba/s] 80%|████████  | 4/5 [00:00<00:00,  9.89ba/s]100%|██████████| 5/5 [00:00<00:00,  9.59ba/s]100%|██████████| 5/5 [00:00<00:00,  9.65ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.14ba/s] 50%|█████     | 2/4 [00:00<00:00,  8.95ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.13ba/s]100%|██████████| 4/4 [00:00<00:00, 10.34ba/s]
[INFO|trainer.py:414] 2023-08-28 12:18:16,326 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 12:18:16,336 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 12:18:16,336 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 12:18:16,336 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 12:18:16,336 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 12:18:16,336 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 12:18:16,336 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 12:18:16,336 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:59,  3.26it/s]  1%|          | 2/390 [00:00<01:55,  3.35it/s]  1%|          | 3/390 [00:00<01:54,  3.39it/s]  1%|          | 4/390 [00:01<01:53,  3.41it/s]  1%|▏         | 5/390 [00:01<01:52,  3.42it/s]  2%|▏         | 6/390 [00:01<01:52,  3.42it/s]  2%|▏         | 7/390 [00:02<01:51,  3.42it/s]  2%|▏         | 8/390 [00:02<01:51,  3.42it/s]  2%|▏         | 9/390 [00:02<01:51,  3.42it/s]  3%|▎         | 10/390 [00:02<01:51,  3.42it/s]  3%|▎         | 11/390 [00:03<01:50,  3.42it/s]  3%|▎         | 12/390 [00:03<01:50,  3.42it/s]  3%|▎         | 13/390 [00:03<01:50,  3.43it/s]  4%|▎         | 14/390 [00:04<01:49,  3.43it/s]  4%|▍         | 15/390 [00:04<01:49,  3.43it/s]  4%|▍         | 16/390 [00:04<01:49,  3.43it/s]  4%|▍         | 17/390 [00:04<01:48,  3.43it/s]  5%|▍         | 18/390 [00:05<01:48,  3.43it/s]  5%|▍         | 19/390 [00:05<01:48,  3.42it/s]  5%|▌         | 20/390 [00:05<01:48,  3.43it/s]  5%|▌         | 21/390 [00:06<01:47,  3.42it/s]  6%|▌         | 22/390 [00:06<01:47,  3.43it/s]  6%|▌         | 23/390 [00:06<01:47,  3.43it/s]  6%|▌         | 24/390 [00:07<01:46,  3.43it/s]  6%|▋         | 25/390 [00:07<01:46,  3.43it/s]  7%|▋         | 26/390 [00:07<01:46,  3.42it/s]  7%|▋         | 27/390 [00:07<01:46,  3.42it/s]  7%|▋         | 28/390 [00:08<01:45,  3.42it/s]  7%|▋         | 29/390 [00:08<01:45,  3.42it/s]  8%|▊         | 30/390 [00:08<01:45,  3.42it/s]  8%|▊         | 31/390 [00:09<01:44,  3.42it/s]  8%|▊         | 32/390 [00:09<01:44,  3.42it/s]  8%|▊         | 33/390 [00:09<01:44,  3.42it/s]  9%|▊         | 34/390 [00:09<01:44,  3.42it/s]  9%|▉         | 35/390 [00:10<01:43,  3.42it/s]  9%|▉         | 36/390 [00:10<01:43,  3.42it/s]  9%|▉         | 37/390 [00:10<01:43,  3.42it/s] 10%|▉         | 38/390 [00:11<01:42,  3.43it/s] 10%|█         | 39/390 [00:11<01:42,  3.42it/s] 10%|█         | 40/390 [00:11<01:42,  3.42it/s] 11%|█         | 41/390 [00:11<01:42,  3.42it/s] 11%|█         | 42/390 [00:12<01:41,  3.42it/s] 11%|█         | 43/390 [00:12<01:41,  3.42it/s] 11%|█▏        | 44/390 [00:12<01:41,  3.42it/s] 12%|█▏        | 45/390 [00:13<01:41,  3.42it/s] 12%|█▏        | 46/390 [00:13<01:40,  3.42it/s] 12%|█▏        | 47/390 [00:13<01:40,  3.42it/s] 12%|█▏        | 48/390 [00:14<01:40,  3.42it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.42it/s] 13%|█▎        | 50/390 [00:14<01:39,  3.42it/s] 13%|█▎        | 51/390 [00:14<01:39,  3.42it/s] 13%|█▎        | 52/390 [00:15<01:39,  3.41it/s] 14%|█▎        | 53/390 [00:15<01:38,  3.41it/s] 14%|█▍        | 54/390 [00:15<01:38,  3.41it/s] 14%|█▍        | 55/390 [00:16<01:38,  3.42it/s] 14%|█▍        | 56/390 [00:16<01:37,  3.42it/s] 15%|█▍        | 57/390 [00:16<01:37,  3.42it/s] 15%|█▍        | 58/390 [00:16<01:37,  3.42it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.42it/s] 15%|█▌        | 60/390 [00:17<01:36,  3.42it/s] 16%|█▌        | 61/390 [00:17<01:36,  3.42it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.42it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.42it/s] 16%|█▋        | 64/390 [00:18<01:35,  3.42it/s] 17%|█▋        | 65/390 [00:19<01:35,  3.42it/s] 17%|█▋        | 66/390 [00:19<01:34,  3.42it/s] 17%|█▋        | 67/390 [00:19<01:34,  3.41it/s] 17%|█▋        | 68/390 [00:19<01:34,  3.42it/s] 18%|█▊        | 69/390 [00:20<01:33,  3.42it/s] 18%|█▊        | 70/390 [00:20<01:33,  3.42it/s] 18%|█▊        | 71/390 [00:20<01:33,  3.42it/s] 18%|█▊        | 72/390 [00:21<01:33,  3.42it/s] 19%|█▊        | 73/390 [00:21<01:32,  3.42it/s] 19%|█▉        | 74/390 [00:21<01:32,  3.42it/s] 19%|█▉        | 75/390 [00:21<01:32,  3.42it/s] 19%|█▉        | 76/390 [00:22<01:31,  3.42it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.42it/s] 20%|██        | 78/390 [00:22<01:31,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 12:18:39,210 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:18:39,210 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 12:18:39,210 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.47it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.95it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.27it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.64it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.19it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.85it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.65it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.14it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.99it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.12it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.14it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.19it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.28it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.33it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.28it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.19it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.98it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.99it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.89it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.07it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.08it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.12it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.24it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.23it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.21it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.03it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.00it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.03it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.96it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.98it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.01it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.11it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.21it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.20it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.07it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.05it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.11it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.01it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.01it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.04it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.08it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.10it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.07it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.04it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.86it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.94it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.92it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.99it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.01it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.09it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.10it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.12it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.15it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.05it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.05it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.95it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.03it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.12it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.12it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.09it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.10it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.12it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.07it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.07it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.97it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.90it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.98it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.01it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.15it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.02it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.05it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.75it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.97it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.92it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.92it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.00it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.07it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.11it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.08it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.03it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.97it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.99it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.90it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.96it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.05it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.00it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:32<01:31,  3.42it/s]
100%|██████████| 436/436 [00:09<00:00, 46.00it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:18:48,695 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 12:18:48,714 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:18:51,006 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:18:51,026 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:18:51,033 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:39<27:18,  5.27s/it] 21%|██        | 80/390 [00:39<19:30,  3.78s/it] 21%|██        | 81/390 [00:40<14:03,  2.73s/it] 21%|██        | 82/390 [00:40<10:15,  2.00s/it] 21%|██▏       | 83/390 [00:40<07:36,  1.49s/it] 22%|██▏       | 84/390 [00:41<05:45,  1.13s/it] 22%|██▏       | 85/390 [00:41<04:27,  1.14it/s] 22%|██▏       | 86/390 [00:41<03:33,  1.42it/s] 22%|██▏       | 87/390 [00:42<02:55,  1.73it/s] 23%|██▎       | 88/390 [00:42<02:28,  2.03it/s] 23%|██▎       | 89/390 [00:42<02:10,  2.31it/s] 23%|██▎       | 90/390 [00:42<01:57,  2.56it/s] 23%|██▎       | 91/390 [00:43<01:48,  2.77it/s] 24%|██▎       | 92/390 [00:43<01:41,  2.93it/s] 24%|██▍       | 93/390 [00:43<01:36,  3.06it/s] 24%|██▍       | 94/390 [00:44<01:33,  3.16it/s] 24%|██▍       | 95/390 [00:44<01:31,  3.24it/s] 25%|██▍       | 96/390 [00:44<01:29,  3.29it/s] 25%|██▍       | 97/390 [00:44<01:28,  3.32it/s] 25%|██▌       | 98/390 [00:45<01:27,  3.35it/s] 25%|██▌       | 99/390 [00:45<01:26,  3.37it/s] 26%|██▌       | 100/390 [00:45<01:25,  3.39it/s] 26%|██▌       | 101/390 [00:46<01:25,  3.39it/s] 26%|██▌       | 102/390 [00:46<01:24,  3.40it/s] 26%|██▋       | 103/390 [00:46<01:24,  3.40it/s] 27%|██▋       | 104/390 [00:47<01:24,  3.40it/s] 27%|██▋       | 105/390 [00:47<01:23,  3.40it/s] 27%|██▋       | 106/390 [00:47<01:23,  3.41it/s] 27%|██▋       | 107/390 [00:47<01:23,  3.40it/s] 28%|██▊       | 108/390 [00:48<01:22,  3.41it/s] 28%|██▊       | 109/390 [00:48<01:22,  3.41it/s] 28%|██▊       | 110/390 [00:48<01:22,  3.41it/s] 28%|██▊       | 111/390 [00:49<01:21,  3.41it/s] 29%|██▊       | 112/390 [00:49<01:21,  3.41it/s] 29%|██▉       | 113/390 [00:49<01:21,  3.41it/s] 29%|██▉       | 114/390 [00:49<01:20,  3.41it/s] 29%|██▉       | 115/390 [00:50<01:21,  3.37it/s] 30%|██▉       | 116/390 [00:50<01:20,  3.38it/s] 30%|███       | 117/390 [00:50<01:20,  3.39it/s] 30%|███       | 118/390 [00:51<01:20,  3.39it/s] 31%|███       | 119/390 [00:51<01:19,  3.40it/s] 31%|███       | 120/390 [00:51<01:19,  3.40it/s] 31%|███       | 121/390 [00:52<01:22,  3.28it/s] 31%|███▏      | 122/390 [00:52<01:20,  3.31it/s] 32%|███▏      | 123/390 [00:52<01:19,  3.34it/s] 32%|███▏      | 124/390 [00:52<01:19,  3.36it/s] 32%|███▏      | 125/390 [00:53<01:18,  3.38it/s] 32%|███▏      | 126/390 [00:53<01:18,  3.36it/s] 33%|███▎      | 127/390 [00:53<01:17,  3.37it/s] 33%|███▎      | 128/390 [00:54<01:17,  3.38it/s] 33%|███▎      | 129/390 [00:54<01:16,  3.39it/s] 33%|███▎      | 130/390 [00:54<01:16,  3.40it/s] 34%|███▎      | 131/390 [00:54<01:16,  3.40it/s] 34%|███▍      | 132/390 [00:55<01:15,  3.40it/s] 34%|███▍      | 133/390 [00:55<01:15,  3.41it/s] 34%|███▍      | 134/390 [00:55<01:15,  3.41it/s] 35%|███▍      | 135/390 [00:56<01:14,  3.41it/s] 35%|███▍      | 136/390 [00:56<01:14,  3.41it/s] 35%|███▌      | 137/390 [00:56<01:14,  3.38it/s] 35%|███▌      | 138/390 [00:57<01:14,  3.38it/s] 36%|███▌      | 139/390 [00:57<01:14,  3.39it/s] 36%|███▌      | 140/390 [00:57<01:13,  3.39it/s] 36%|███▌      | 141/390 [00:57<01:13,  3.40it/s] 36%|███▋      | 142/390 [00:58<01:13,  3.40it/s] 37%|███▋      | 143/390 [00:58<01:12,  3.40it/s] 37%|███▋      | 144/390 [00:58<01:12,  3.40it/s] 37%|███▋      | 145/390 [00:59<01:12,  3.40it/s] 37%|███▋      | 146/390 [00:59<01:11,  3.40it/s] 38%|███▊      | 147/390 [00:59<01:11,  3.40it/s] 38%|███▊      | 148/390 [00:59<01:11,  3.39it/s] 38%|███▊      | 149/390 [01:00<01:11,  3.39it/s] 38%|███▊      | 150/390 [01:00<01:10,  3.39it/s] 39%|███▊      | 151/390 [01:00<01:10,  3.40it/s] 39%|███▉      | 152/390 [01:01<01:10,  3.40it/s] 39%|███▉      | 153/390 [01:01<01:09,  3.40it/s] 39%|███▉      | 154/390 [01:01<01:09,  3.40it/s] 40%|███▉      | 155/390 [01:02<01:09,  3.40it/s] 40%|████      | 156/390 [01:02<01:08,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 12:19:18,743 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:19:18,743 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 12:19:18,743 >>   Batch size = 8
{'eval_loss': 0.9748095870018005, 'eval_runtime': 9.4648, 'eval_samples_per_second': 368.313, 'eval_steps_per_second': 46.066, 'epoch': 0.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.33it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.64it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.08it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.37it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.97it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.67it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.24it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.82it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.86it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.85it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.91it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 45.99it/s][A
 16%|█▌        | 68/436 [00:01<00:08, 45.94it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.06it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.05it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.74it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.64it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.74it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.73it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.80it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.90it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.86it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 45.97it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 45.99it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.87it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.82it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.80it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.77it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.86it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.91it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.83it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.98it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.95it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.81it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.79it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.88it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.75it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.76it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.85it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.89it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.92it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.84it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.89it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.74it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.86it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.77it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.76it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.83it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.87it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.88it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.92it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.83it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.84it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.91it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.80it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.78it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.82it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.86it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.86it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.86it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.79it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.89it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.80it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.78it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.86it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.85it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.84it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.82it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.85it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.88it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.78it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.92it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 45.77it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.84it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.81it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.83it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.82it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.81it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.82it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.85it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.93it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.88it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.86it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.85it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.90it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.82it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:11<01:08,  3.40it/s]
100%|██████████| 436/436 [00:09<00:00, 45.82it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:19:28,281 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 12:19:28,298 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:19:30,629 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:19:30,648 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:19:30,659 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:19<20:28,  5.27s/it] 41%|████      | 158/390 [01:19<14:36,  3.78s/it] 41%|████      | 159/390 [01:19<10:31,  2.73s/it] 41%|████      | 160/390 [01:20<07:40,  2.00s/it] 41%|████▏     | 161/390 [01:20<05:40,  1.49s/it] 42%|████▏     | 162/390 [01:20<04:17,  1.13s/it] 42%|████▏     | 163/390 [01:20<03:19,  1.14it/s] 42%|████▏     | 164/390 [01:21<02:38,  1.42it/s] 42%|████▏     | 165/390 [01:21<02:10,  1.73it/s] 43%|████▎     | 166/390 [01:21<01:50,  2.03it/s] 43%|████▎     | 167/390 [01:22<01:36,  2.31it/s] 43%|████▎     | 168/390 [01:22<01:26,  2.56it/s] 43%|████▎     | 169/390 [01:22<01:20,  2.76it/s] 44%|████▎     | 170/390 [01:23<01:15,  2.93it/s] 44%|████▍     | 171/390 [01:23<01:11,  3.06it/s] 44%|████▍     | 172/390 [01:23<01:09,  3.16it/s] 44%|████▍     | 173/390 [01:23<01:07,  3.23it/s] 45%|████▍     | 174/390 [01:24<01:05,  3.29it/s] 45%|████▍     | 175/390 [01:24<01:04,  3.33it/s] 45%|████▌     | 176/390 [01:24<01:03,  3.35it/s] 45%|████▌     | 177/390 [01:25<01:03,  3.37it/s] 46%|████▌     | 178/390 [01:25<01:02,  3.39it/s] 46%|████▌     | 179/390 [01:25<01:02,  3.39it/s] 46%|████▌     | 180/390 [01:25<01:01,  3.40it/s] 46%|████▋     | 181/390 [01:26<01:01,  3.40it/s] 47%|████▋     | 182/390 [01:26<01:01,  3.41it/s] 47%|████▋     | 183/390 [01:26<01:00,  3.41it/s] 47%|████▋     | 184/390 [01:27<01:00,  3.41it/s] 47%|████▋     | 185/390 [01:27<01:00,  3.39it/s] 48%|████▊     | 186/390 [01:27<01:00,  3.40it/s] 48%|████▊     | 187/390 [01:28<00:59,  3.41it/s] 48%|████▊     | 188/390 [01:28<00:59,  3.41it/s] 48%|████▊     | 189/390 [01:28<00:58,  3.41it/s] 49%|████▊     | 190/390 [01:28<00:58,  3.41it/s] 49%|████▉     | 191/390 [01:29<00:58,  3.41it/s] 49%|████▉     | 192/390 [01:29<00:57,  3.41it/s] 49%|████▉     | 193/390 [01:29<00:57,  3.41it/s] 50%|████▉     | 194/390 [01:30<00:57,  3.42it/s] 50%|█████     | 195/390 [01:30<00:57,  3.41it/s] 50%|█████     | 196/390 [01:30<00:57,  3.40it/s] 51%|█████     | 197/390 [01:30<00:56,  3.40it/s] 51%|█████     | 198/390 [01:31<00:56,  3.40it/s] 51%|█████     | 199/390 [01:31<00:56,  3.41it/s] 51%|█████▏    | 200/390 [01:31<00:55,  3.41it/s] 52%|█████▏    | 201/390 [01:32<00:55,  3.41it/s] 52%|█████▏    | 202/390 [01:32<00:55,  3.41it/s] 52%|█████▏    | 203/390 [01:32<00:54,  3.41it/s] 52%|█████▏    | 204/390 [01:33<00:54,  3.41it/s] 53%|█████▎    | 205/390 [01:33<00:54,  3.41it/s] 53%|█████▎    | 206/390 [01:33<00:53,  3.41it/s] 53%|█████▎    | 207/390 [01:33<00:53,  3.39it/s] 53%|█████▎    | 208/390 [01:34<00:53,  3.40it/s] 54%|█████▎    | 209/390 [01:34<00:53,  3.40it/s] 54%|█████▍    | 210/390 [01:34<00:52,  3.40it/s] 54%|█████▍    | 211/390 [01:35<00:52,  3.40it/s] 54%|█████▍    | 212/390 [01:35<00:52,  3.40it/s] 55%|█████▍    | 213/390 [01:35<00:51,  3.41it/s] 55%|█████▍    | 214/390 [01:35<00:51,  3.41it/s] 55%|█████▌    | 215/390 [01:36<00:51,  3.41it/s] 55%|█████▌    | 216/390 [01:36<00:51,  3.41it/s] 56%|█████▌    | 217/390 [01:36<00:50,  3.41it/s] 56%|█████▌    | 218/390 [01:37<00:50,  3.39it/s] 56%|█████▌    | 219/390 [01:37<00:50,  3.39it/s] 56%|█████▋    | 220/390 [01:37<00:50,  3.40it/s] 57%|█████▋    | 221/390 [01:38<00:49,  3.40it/s] 57%|█████▋    | 222/390 [01:38<00:49,  3.40it/s] 57%|█████▋    | 223/390 [01:38<00:49,  3.40it/s] 57%|█████▋    | 224/390 [01:38<00:48,  3.40it/s] 58%|█████▊    | 225/390 [01:39<00:48,  3.40it/s] 58%|█████▊    | 226/390 [01:39<00:48,  3.40it/s] 58%|█████▊    | 227/390 [01:39<00:47,  3.40it/s] 58%|█████▊    | 228/390 [01:40<00:47,  3.40it/s] 59%|█████▊    | 229/390 [01:40<00:47,  3.39it/s] 59%|█████▉    | 230/390 [01:40<00:47,  3.39it/s] 59%|█████▉    | 231/390 [01:40<00:46,  3.40it/s] 59%|█████▉    | 232/390 [01:41<00:46,  3.40it/s] 60%|█████▉    | 233/390 [01:41<00:46,  3.40it/s] 60%|██████    | 234/390 [01:41<00:45,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 12:19:58,229 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:19:58,229 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 12:19:58,229 >>   Batch size = 8
{'eval_loss': 0.9678109884262085, 'eval_runtime': 9.5063, 'eval_samples_per_second': 366.703, 'eval_steps_per_second': 45.864, 'epoch': 1.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.43it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.79it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.03it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.24it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.86it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.52it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.24it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.83it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.87it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.87it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.91it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 45.98it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.05it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 45.98it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.00it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.75it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.73it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.72it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.74it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.80it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.87it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.86it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 45.96it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 45.92it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.87it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.88it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.86it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.81it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.85it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.89it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.86it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.96it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.83it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.85it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.66it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.75it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.70it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.72it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.74it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.82it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.89it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.84it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.75it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.83it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.91it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.81it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.73it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.80it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.77it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.77it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.86it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.78it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.82it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.89it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.78it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.80it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.66it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.74it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.80it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.88it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.79it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.81it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.84it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.79it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.84it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.75it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.75it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.70it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.80it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.78it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.81it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.75it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 45.75it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.84it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.79it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.77it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.80it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.84it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.82it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.81it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.81it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.81it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.85it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.69it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.83it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.90it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [01:51<00:45,  3.40it/s]
100%|██████████| 436/436 [00:09<00:00, 45.90it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:20:07,759 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 12:20:07,780 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:20:10,149 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:20:10,161 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:20:10,172 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [01:58<13:44,  5.32s/it] 61%|██████    | 236/390 [01:59<09:46,  3.81s/it] 61%|██████    | 237/390 [01:59<07:01,  2.76s/it] 61%|██████    | 238/390 [01:59<05:06,  2.02s/it] 61%|██████▏   | 239/390 [02:00<03:46,  1.50s/it] 62%|██████▏   | 240/390 [02:00<02:50,  1.14s/it] 62%|██████▏   | 241/390 [02:00<02:11,  1.13it/s] 62%|██████▏   | 242/390 [02:00<01:44,  1.42it/s] 62%|██████▏   | 243/390 [02:01<01:25,  1.72it/s] 63%|██████▎   | 244/390 [02:01<01:12,  2.02it/s] 63%|██████▎   | 245/390 [02:01<01:03,  2.30it/s] 63%|██████▎   | 246/390 [02:02<00:56,  2.55it/s] 63%|██████▎   | 247/390 [02:02<00:51,  2.75it/s] 64%|██████▎   | 248/390 [02:02<00:48,  2.92it/s] 64%|██████▍   | 249/390 [02:02<00:46,  3.05it/s] 64%|██████▍   | 250/390 [02:03<00:44,  3.15it/s] 64%|██████▍   | 251/390 [02:03<00:43,  3.23it/s] 65%|██████▍   | 252/390 [02:03<00:42,  3.28it/s] 65%|██████▍   | 253/390 [02:04<00:41,  3.32it/s] 65%|██████▌   | 254/390 [02:04<00:40,  3.35it/s] 65%|██████▌   | 255/390 [02:04<00:40,  3.37it/s] 66%|██████▌   | 256/390 [02:05<00:39,  3.38it/s] 66%|██████▌   | 257/390 [02:05<00:39,  3.39it/s] 66%|██████▌   | 258/390 [02:05<00:38,  3.39it/s] 66%|██████▋   | 259/390 [02:05<00:38,  3.39it/s] 67%|██████▋   | 260/390 [02:06<00:38,  3.40it/s] 67%|██████▋   | 261/390 [02:06<00:37,  3.40it/s] 67%|██████▋   | 262/390 [02:06<00:37,  3.41it/s] 67%|██████▋   | 263/390 [02:07<00:37,  3.41it/s] 68%|██████▊   | 264/390 [02:07<00:36,  3.41it/s] 68%|██████▊   | 265/390 [02:07<00:36,  3.41it/s] 68%|██████▊   | 266/390 [02:07<00:36,  3.41it/s] 68%|██████▊   | 267/390 [02:08<00:36,  3.41it/s] 69%|██████▊   | 268/390 [02:08<00:35,  3.41it/s] 69%|██████▉   | 269/390 [02:08<00:35,  3.38it/s] 69%|██████▉   | 270/390 [02:09<00:35,  3.39it/s] 69%|██████▉   | 271/390 [02:09<00:35,  3.39it/s] 70%|██████▉   | 272/390 [02:09<00:34,  3.40it/s] 70%|███████   | 273/390 [02:10<00:34,  3.40it/s] 70%|███████   | 274/390 [02:10<00:34,  3.40it/s] 71%|███████   | 275/390 [02:10<00:33,  3.41it/s] 71%|███████   | 276/390 [02:10<00:33,  3.41it/s] 71%|███████   | 277/390 [02:11<00:33,  3.41it/s] 71%|███████▏  | 278/390 [02:11<00:32,  3.41it/s] 72%|███████▏  | 279/390 [02:11<00:32,  3.41it/s] 72%|███████▏  | 280/390 [02:12<00:32,  3.40it/s] 72%|███████▏  | 281/390 [02:12<00:32,  3.40it/s] 72%|███████▏  | 282/390 [02:12<00:31,  3.40it/s] 73%|███████▎  | 283/390 [02:12<00:31,  3.40it/s] 73%|███████▎  | 284/390 [02:13<00:31,  3.41it/s] 73%|███████▎  | 285/390 [02:13<00:30,  3.41it/s] 73%|███████▎  | 286/390 [02:13<00:30,  3.41it/s] 74%|███████▎  | 287/390 [02:14<00:30,  3.41it/s] 74%|███████▍  | 288/390 [02:14<00:29,  3.41it/s] 74%|███████▍  | 289/390 [02:14<00:29,  3.40it/s] 74%|███████▍  | 290/390 [02:15<00:29,  3.40it/s] 75%|███████▍  | 291/390 [02:15<00:29,  3.40it/s] 75%|███████▍  | 292/390 [02:15<00:28,  3.40it/s] 75%|███████▌  | 293/390 [02:15<00:28,  3.40it/s] 75%|███████▌  | 294/390 [02:16<00:28,  3.40it/s] 76%|███████▌  | 295/390 [02:16<00:27,  3.40it/s] 76%|███████▌  | 296/390 [02:16<00:27,  3.40it/s] 76%|███████▌  | 297/390 [02:17<00:27,  3.40it/s] 76%|███████▋  | 298/390 [02:17<00:26,  3.41it/s] 77%|███████▋  | 299/390 [02:17<00:26,  3.40it/s] 77%|███████▋  | 300/390 [02:17<00:26,  3.40it/s] 77%|███████▋  | 301/390 [02:18<00:26,  3.41it/s] 77%|███████▋  | 302/390 [02:18<00:25,  3.40it/s] 78%|███████▊  | 303/390 [02:18<00:25,  3.40it/s] 78%|███████▊  | 304/390 [02:19<00:25,  3.40it/s] 78%|███████▊  | 305/390 [02:19<00:24,  3.40it/s] 78%|███████▊  | 306/390 [02:19<00:24,  3.40it/s] 79%|███████▊  | 307/390 [02:20<00:24,  3.40it/s] 79%|███████▉  | 308/390 [02:20<00:24,  3.41it/s] 79%|███████▉  | 309/390 [02:20<00:23,  3.41it/s] 79%|███████▉  | 310/390 [02:20<00:23,  3.40it/s] 80%|███████▉  | 311/390 [02:21<00:23,  3.40it/s] 80%|████████  | 312/390 [02:21<00:22,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 12:20:37,882 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:20:37,882 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 12:20:37,882 >>   Batch size = 8
{'eval_loss': 0.9685143828392029, 'eval_runtime': 9.5136, 'eval_samples_per_second': 366.424, 'eval_steps_per_second': 45.829, 'epoch': 2.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.23it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.65it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.08it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.27it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.84it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.45it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.23it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.82it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.89it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.76it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.88it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 45.90it/s][A
 16%|█▌        | 68/436 [00:01<00:08, 45.92it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.02it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.95it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.91it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.72it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.79it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.80it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.80it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.83it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.89it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 45.91it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 45.88it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.90it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.72it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.74it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.80it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.78it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.86it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.89it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.87it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.88it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.90it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.78it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.80it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.73it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.78it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.74it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.81it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.79it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.86it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.62it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.78it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.81it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.73it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.76it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.64it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.78it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.78it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.86it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.83it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.86it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.89it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.86it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.83it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.79it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.76it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.82it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.84it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.87it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.81it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.80it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.83it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.77it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.78it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.65it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.74it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.79it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.86it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.91it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.83it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 45.81it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.85it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.86it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.74it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.80it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.78it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.86it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.92it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.86it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.79it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.84it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.81it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.81it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.86it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:31<00:22,  3.40it/s]
100%|██████████| 436/436 [00:09<00:00, 45.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:20:47,412 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 12:20:47,431 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:20:49,825 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:20:49,841 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:20:49,849 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:38<06:52,  5.36s/it] 81%|████████  | 314/390 [02:38<04:51,  3.84s/it] 81%|████████  | 315/390 [02:39<03:28,  2.78s/it] 81%|████████  | 316/390 [02:39<02:30,  2.03s/it] 81%|████████▏ | 317/390 [02:39<01:50,  1.51s/it] 82%|████████▏ | 318/390 [02:40<01:22,  1.14s/it] 82%|████████▏ | 319/390 [02:40<01:03,  1.13it/s] 82%|████████▏ | 320/390 [02:40<00:49,  1.41it/s] 82%|████████▏ | 321/390 [02:41<00:40,  1.71it/s] 83%|████████▎ | 322/390 [02:41<00:33,  2.01it/s] 83%|████████▎ | 323/390 [02:41<00:29,  2.30it/s] 83%|████████▎ | 324/390 [02:41<00:25,  2.55it/s] 83%|████████▎ | 325/390 [02:42<00:23,  2.75it/s] 84%|████████▎ | 326/390 [02:42<00:21,  2.92it/s] 84%|████████▍ | 327/390 [02:42<00:20,  3.06it/s] 84%|████████▍ | 328/390 [02:43<00:19,  3.16it/s] 84%|████████▍ | 329/390 [02:43<00:18,  3.23it/s] 85%|████████▍ | 330/390 [02:43<00:18,  3.29it/s] 85%|████████▍ | 331/390 [02:43<00:17,  3.33it/s] 85%|████████▌ | 332/390 [02:44<00:17,  3.35it/s] 85%|████████▌ | 333/390 [02:44<00:16,  3.37it/s] 86%|████████▌ | 334/390 [02:44<00:16,  3.39it/s] 86%|████████▌ | 335/390 [02:45<00:16,  3.40it/s] 86%|████████▌ | 336/390 [02:45<00:15,  3.39it/s] 86%|████████▋ | 337/390 [02:45<00:15,  3.40it/s] 87%|████████▋ | 338/390 [02:45<00:15,  3.40it/s] 87%|████████▋ | 339/390 [02:46<00:15,  3.39it/s] 87%|████████▋ | 340/390 [02:46<00:14,  3.40it/s] 87%|████████▋ | 341/390 [02:46<00:14,  3.41it/s] 88%|████████▊ | 342/390 [02:47<00:14,  3.41it/s] 88%|████████▊ | 343/390 [02:47<00:13,  3.41it/s] 88%|████████▊ | 344/390 [02:47<00:13,  3.41it/s] 88%|████████▊ | 345/390 [02:48<00:13,  3.41it/s] 89%|████████▊ | 346/390 [02:48<00:12,  3.41it/s] 89%|████████▉ | 347/390 [02:48<00:12,  3.40it/s] 89%|████████▉ | 348/390 [02:48<00:12,  3.41it/s] 89%|████████▉ | 349/390 [02:49<00:12,  3.41it/s] 90%|████████▉ | 350/390 [02:49<00:11,  3.41it/s] 90%|█████████ | 351/390 [02:49<00:11,  3.41it/s] 90%|█████████ | 352/390 [02:50<00:11,  3.41it/s] 91%|█████████ | 353/390 [02:50<00:10,  3.41it/s] 91%|█████████ | 354/390 [02:50<00:10,  3.41it/s] 91%|█████████ | 355/390 [02:50<00:10,  3.41it/s] 91%|█████████▏| 356/390 [02:51<00:09,  3.40it/s] 92%|█████████▏| 357/390 [02:51<00:09,  3.40it/s] 92%|█████████▏| 358/390 [02:51<00:09,  3.40it/s] 92%|█████████▏| 359/390 [02:52<00:09,  3.40it/s] 92%|█████████▏| 360/390 [02:52<00:08,  3.41it/s] 93%|█████████▎| 361/390 [02:52<00:08,  3.41it/s] 93%|█████████▎| 362/390 [02:53<00:08,  3.41it/s] 93%|█████████▎| 363/390 [02:53<00:07,  3.41it/s] 93%|█████████▎| 364/390 [02:53<00:07,  3.41it/s] 94%|█████████▎| 365/390 [02:53<00:07,  3.41it/s] 94%|█████████▍| 366/390 [02:54<00:07,  3.33it/s] 94%|█████████▍| 367/390 [02:54<00:06,  3.35it/s] 94%|█████████▍| 368/390 [02:54<00:06,  3.37it/s] 95%|█████████▍| 369/390 [02:55<00:06,  3.36it/s] 95%|█████████▍| 370/390 [02:55<00:05,  3.37it/s] 95%|█████████▌| 371/390 [02:55<00:05,  3.38it/s] 95%|█████████▌| 372/390 [02:55<00:05,  3.39it/s] 96%|█████████▌| 373/390 [02:56<00:05,  3.40it/s] 96%|█████████▌| 374/390 [02:56<00:04,  3.40it/s] 96%|█████████▌| 375/390 [02:56<00:04,  3.40it/s] 96%|█████████▋| 376/390 [02:57<00:04,  3.40it/s] 97%|█████████▋| 377/390 [02:57<00:03,  3.40it/s] 97%|█████████▋| 378/390 [02:57<00:03,  3.41it/s] 97%|█████████▋| 379/390 [02:58<00:03,  3.41it/s] 97%|█████████▋| 380/390 [02:58<00:02,  3.40it/s] 98%|█████████▊| 381/390 [02:58<00:02,  3.40it/s] 98%|█████████▊| 382/390 [02:58<00:02,  3.40it/s] 98%|█████████▊| 383/390 [02:59<00:02,  3.40it/s] 98%|█████████▊| 384/390 [02:59<00:01,  3.40it/s] 99%|█████████▊| 385/390 [02:59<00:01,  3.40it/s] 99%|█████████▉| 386/390 [03:00<00:01,  3.41it/s] 99%|█████████▉| 387/390 [03:00<00:00,  3.41it/s] 99%|█████████▉| 388/390 [03:00<00:00,  3.40it/s]100%|█████████▉| 389/390 [03:00<00:00,  3.41it/s]100%|██████████| 390/390 [03:01<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 12:21:17,620 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:21:17,621 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 12:21:17,621 >>   Batch size = 8
{'eval_loss': 0.9701865315437317, 'eval_runtime': 9.5141, 'eval_samples_per_second': 366.404, 'eval_steps_per_second': 45.827, 'epoch': 3.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.03it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.63it/s][A
  4%|▍         | 18/436 [00:00<00:08, 47.90it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.26it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.89it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.65it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.33it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.04it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.87it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.85it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.90it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 45.88it/s][A
 16%|█▌        | 68/436 [00:01<00:08, 45.95it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 45.96it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.96it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.91it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.89it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.76it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.76it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.81it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.82it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.87it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 45.94it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 45.87it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.87it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.92it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.82it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.83it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.90it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.80it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.85it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.89it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.93it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.82it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.95it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.81it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.82it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.82it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.81it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.87it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.82it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.81it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.88it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.94it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.83it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.81it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.76it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.85it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.81it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.86it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.81it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.78it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.79it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.79it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.74it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.75it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.77it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.82it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.91it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.85it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.92it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.85it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.86it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.79it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.79it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.79it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.88it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.92it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.82it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.92it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.86it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.87it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 45.79it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.86it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.82it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.83it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.88it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.82it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.87it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.84it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.84it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.85it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.89it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.79it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.78it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.86it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:10<00:00,  3.41it/s]
100%|██████████| 436/436 [00:09<00:00, 45.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:21:27,140 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 12:21:27,159 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:21:29,551 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:21:29,565 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:21:29,576 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 12:21:34,481 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 12:21:34,483 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156 (score: 0.9678109884262085).
                                                 100%|██████████| 390/390 [03:19<00:00,  3.41it/s]100%|██████████| 390/390 [03:19<00:00,  1.95it/s]
[INFO|trainer.py:1894] 2023-08-28 12:21:36,297 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 12:21:36,309 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:21:38,759 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:21:38,784 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:21:38,792 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 12:21:38,974 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:38,974 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:38,974 >>   train_loss               =     0.7964
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:38,975 >>   train_runtime            = 0:03:19.95
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:38,975 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:38,975 >>   train_samples_per_second =    125.027
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:38,975 >>   train_steps_per_second   =       1.95
{'eval_loss': 0.973829448223114, 'eval_runtime': 9.5079, 'eval_samples_per_second': 366.644, 'eval_steps_per_second': 45.857, 'epoch': 4.99}
{'train_runtime': 199.9567, 'train_samples_per_second': 125.027, 'train_steps_per_second': 1.95, 'train_loss': 0.7964067508012821, 'epoch': 4.99}
08/28/2023 12:21:39 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 12:21:39,025 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:21:39,026 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 12:21:39,026 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 57.21it/s]  3%|▎         | 12/436 [00:00<00:08, 50.31it/s]  4%|▍         | 18/436 [00:00<00:08, 48.40it/s]  5%|▌         | 23/436 [00:00<00:08, 47.70it/s]  6%|▋         | 28/436 [00:00<00:08, 47.32it/s]  8%|▊         | 33/436 [00:00<00:08, 46.65it/s]  9%|▊         | 38/436 [00:00<00:08, 46.50it/s] 10%|▉         | 43/436 [00:00<00:08, 46.50it/s] 11%|█         | 48/436 [00:01<00:08, 46.41it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.27it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.27it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.22it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.23it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.30it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.27it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.22it/s] 20%|██        | 88/436 [00:01<00:07, 46.17it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.15it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.18it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.09it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.17it/s] 26%|██▌       | 113/436 [00:02<00:07, 46.14it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.10it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.18it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.13it/s] 31%|███       | 133/436 [00:02<00:06, 46.20it/s] 32%|███▏      | 138/436 [00:02<00:06, 46.22it/s] 33%|███▎      | 143/436 [00:03<00:06, 46.11it/s] 34%|███▍      | 148/436 [00:03<00:06, 46.15it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.10it/s] 36%|███▌      | 158/436 [00:03<00:06, 46.20it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.16it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.17it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.00it/s] 41%|████      | 178/436 [00:03<00:05, 45.98it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.07it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.06it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.07it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.06it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.12it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.17it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.14it/s] 50%|█████     | 218/436 [00:04<00:04, 46.17it/s] 51%|█████     | 223/436 [00:04<00:04, 46.06it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.11it/s] 53%|█████▎    | 233/436 [00:05<00:04, 46.08it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.12it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.17it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.07it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.15it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.09it/s] 60%|██████    | 263/436 [00:05<00:03, 46.23it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.05it/s] 63%|██████▎   | 273/436 [00:05<00:03, 46.10it/s] 64%|██████▍   | 278/436 [00:05<00:03, 46.07it/s] 65%|██████▍   | 283/436 [00:06<00:03, 46.05it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.09it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.06it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.01it/s] 69%|██████▉   | 303/436 [00:06<00:02, 45.95it/s] 71%|███████   | 308/436 [00:06<00:02, 46.07it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.05it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.14it/s] 74%|███████▍  | 323/436 [00:06<00:02, 46.09it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.01it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.09it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.07it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.08it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.05it/s] 81%|████████  | 353/436 [00:07<00:01, 45.99it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.01it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.06it/s] 84%|████████▍ | 368/436 [00:07<00:01, 46.15it/s] 86%|████████▌ | 373/436 [00:08<00:01, 46.04it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.06it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.10it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.08it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.09it/s] 91%|█████████▏| 398/436 [00:08<00:00, 46.03it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.00it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.08it/s] 95%|█████████▍| 413/436 [00:08<00:00, 46.13it/s] 96%|█████████▌| 418/436 [00:09<00:00, 46.04it/s] 97%|█████████▋| 423/436 [00:09<00:00, 46.09it/s] 98%|█████████▊| 428/436 [00:09<00:00, 46.00it/s] 99%|█████████▉| 433/436 [00:09<00:00, 45.98it/s]100%|██████████| 436/436 [00:09<00:00, 46.21it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 12:21:48,487 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:48,487 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:48,487 >>   eval_loss               =     0.9678
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:48,487 >>   eval_runtime            = 0:00:09.46
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:48,487 >>   eval_samples            =       3486
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:48,487 >>   eval_samples_per_second =    368.457
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:48,487 >>   eval_steps_per_second   =     46.084
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:21:48,487 >>   perplexity              =     2.6322
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:21:54,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:21:54,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:21:54,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:21:54,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:21:54,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:21:55,593 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:21:55,594 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:21:56,173 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:21:57,239 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:21:57,244 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:22:00,075 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:22:00,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:22:00,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:22:00,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:22:00,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:22:00,717 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:22:00,721 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:22:01,275 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:22:01,425 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:22:01,425 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'manufacturer', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13015
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13115, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.15it/s]Extractor Predicting: 2it [00:01,  1.18it/s]Extractor Predicting: 3it [00:02,  1.20it/s]Extractor Predicting: 4it [00:03,  1.18it/s]Extractor Predicting: 5it [00:04,  1.17it/s]Extractor Predicting: 6it [00:05,  1.17it/s]Extractor Predicting: 7it [00:05,  1.16it/s]Extractor Predicting: 8it [00:06,  1.18it/s]Extractor Predicting: 9it [00:07,  1.18it/s]Extractor Predicting: 10it [00:08,  1.20it/s]Extractor Predicting: 11it [00:09,  1.17it/s]Extractor Predicting: 12it [00:10,  1.18it/s]Extractor Predicting: 13it [00:11,  1.16it/s]Extractor Predicting: 14it [00:11,  1.15it/s]Extractor Predicting: 15it [00:12,  1.17it/s]Extractor Predicting: 16it [00:13,  1.17it/s]Extractor Predicting: 17it [00:14,  1.20it/s]Extractor Predicting: 18it [00:15,  1.21it/s]Extractor Predicting: 19it [00:16,  1.20it/s]Extractor Predicting: 20it [00:16,  1.19it/s]Extractor Predicting: 21it [00:17,  1.20it/s]Extractor Predicting: 22it [00:18,  1.21it/s]Extractor Predicting: 23it [00:19,  1.22it/s]Extractor Predicting: 24it [00:20,  1.18it/s]Extractor Predicting: 25it [00:21,  1.18it/s]Extractor Predicting: 26it [00:22,  1.10it/s]Extractor Predicting: 27it [00:23,  1.13it/s]Extractor Predicting: 28it [00:23,  1.15it/s]Extractor Predicting: 29it [00:24,  1.15it/s]Extractor Predicting: 30it [00:25,  1.13it/s]Extractor Predicting: 31it [00:26,  1.14it/s]Extractor Predicting: 32it [00:27,  1.15it/s]Extractor Predicting: 33it [00:28,  1.15it/s]Extractor Predicting: 34it [00:29,  1.16it/s]Extractor Predicting: 35it [00:29,  1.18it/s]Extractor Predicting: 36it [00:30,  1.19it/s]Extractor Predicting: 37it [00:31,  1.20it/s]Extractor Predicting: 38it [00:32,  1.23it/s]Extractor Predicting: 39it [00:33,  1.23it/s]Extractor Predicting: 40it [00:33,  1.24it/s]Extractor Predicting: 41it [00:34,  1.23it/s]Extractor Predicting: 42it [00:35,  1.22it/s]Extractor Predicting: 43it [00:36,  1.21it/s]Extractor Predicting: 44it [00:37,  1.19it/s]Extractor Predicting: 45it [00:38,  1.23it/s]Extractor Predicting: 46it [00:38,  1.22it/s]Extractor Predicting: 47it [00:39,  1.23it/s]Extractor Predicting: 48it [00:40,  1.22it/s]Extractor Predicting: 49it [00:41,  1.24it/s]Extractor Predicting: 50it [00:42,  1.24it/s]Extractor Predicting: 51it [00:42,  1.23it/s]Extractor Predicting: 52it [00:43,  1.21it/s]Extractor Predicting: 53it [00:44,  1.21it/s]Extractor Predicting: 54it [00:45,  1.20it/s]Extractor Predicting: 55it [00:46,  1.23it/s]Extractor Predicting: 56it [00:46,  1.25it/s]Extractor Predicting: 57it [00:47,  1.21it/s]Extractor Predicting: 58it [00:48,  1.21it/s]Extractor Predicting: 59it [00:49,  1.22it/s]Extractor Predicting: 60it [00:50,  1.20it/s]Extractor Predicting: 61it [00:51,  1.19it/s]Extractor Predicting: 62it [00:52,  1.20it/s]Extractor Predicting: 63it [00:52,  1.21it/s]Extractor Predicting: 64it [00:53,  1.21it/s]Extractor Predicting: 65it [00:54,  1.20it/s]Extractor Predicting: 66it [00:55,  1.21it/s]Extractor Predicting: 67it [00:56,  1.23it/s]Extractor Predicting: 68it [00:56,  1.20it/s]Extractor Predicting: 69it [00:57,  1.20it/s]Extractor Predicting: 70it [00:58,  1.20it/s]Extractor Predicting: 71it [00:59,  1.22it/s]Extractor Predicting: 72it [01:00,  1.23it/s]Extractor Predicting: 73it [01:01,  1.22it/s]Extractor Predicting: 74it [01:01,  1.23it/s]Extractor Predicting: 75it [01:02,  1.20it/s]Extractor Predicting: 76it [01:03,  1.21it/s]Extractor Predicting: 77it [01:04,  1.20it/s]Extractor Predicting: 78it [01:05,  1.21it/s]Extractor Predicting: 79it [01:06,  1.21it/s]Extractor Predicting: 80it [01:06,  1.22it/s]Extractor Predicting: 81it [01:07,  1.21it/s]Extractor Predicting: 82it [01:08,  1.22it/s]Extractor Predicting: 83it [01:09,  1.18it/s]Extractor Predicting: 84it [01:10,  1.20it/s]Extractor Predicting: 85it [01:11,  1.19it/s]Extractor Predicting: 86it [01:11,  1.19it/s]Extractor Predicting: 87it [01:12,  1.12it/s]Extractor Predicting: 88it [01:13,  1.17it/s]Extractor Predicting: 89it [01:14,  1.19it/s]Extractor Predicting: 90it [01:15,  1.24it/s]Extractor Predicting: 91it [01:15,  1.27it/s]Extractor Predicting: 92it [01:16,  1.29it/s]Extractor Predicting: 93it [01:17,  1.26it/s]Extractor Predicting: 94it [01:18,  1.30it/s]Extractor Predicting: 95it [01:19,  1.29it/s]Extractor Predicting: 96it [01:19,  1.28it/s]Extractor Predicting: 97it [01:20,  1.29it/s]Extractor Predicting: 98it [01:21,  1.26it/s]Extractor Predicting: 99it [01:22,  1.23it/s]Extractor Predicting: 100it [01:23,  1.23it/s]Extractor Predicting: 101it [01:23,  1.28it/s]Extractor Predicting: 102it [01:24,  1.30it/s]Extractor Predicting: 103it [01:25,  1.30it/s]Extractor Predicting: 104it [01:26,  1.28it/s]Extractor Predicting: 105it [01:26,  1.29it/s]Extractor Predicting: 106it [01:27,  1.29it/s]Extractor Predicting: 107it [01:28,  1.28it/s]Extractor Predicting: 108it [01:29,  1.28it/s]Extractor Predicting: 109it [01:30,  1.29it/s]Extractor Predicting: 110it [01:30,  1.29it/s]Extractor Predicting: 111it [01:31,  1.28it/s]Extractor Predicting: 112it [01:32,  1.32it/s]Extractor Predicting: 113it [01:33,  1.33it/s]Extractor Predicting: 114it [01:33,  1.32it/s]Extractor Predicting: 115it [01:34,  1.31it/s]Extractor Predicting: 116it [01:35,  1.30it/s]Extractor Predicting: 117it [01:36,  1.31it/s]Extractor Predicting: 118it [01:36,  1.33it/s]Extractor Predicting: 119it [01:37,  1.31it/s]Extractor Predicting: 120it [01:38,  1.29it/s]Extractor Predicting: 121it [01:39,  1.34it/s]Extractor Predicting: 122it [01:39,  1.33it/s]Extractor Predicting: 123it [01:40,  1.28it/s]Extractor Predicting: 124it [01:41,  1.31it/s]Extractor Predicting: 125it [01:42,  1.30it/s]Extractor Predicting: 126it [01:42,  1.35it/s]Extractor Predicting: 127it [01:43,  1.34it/s]Extractor Predicting: 128it [01:44,  1.33it/s]Extractor Predicting: 129it [01:45,  1.30it/s]Extractor Predicting: 130it [01:45,  1.32it/s]Extractor Predicting: 131it [01:46,  1.35it/s]Extractor Predicting: 132it [01:47,  1.32it/s]Extractor Predicting: 133it [01:48,  1.30it/s]Extractor Predicting: 134it [01:48,  1.32it/s]Extractor Predicting: 135it [01:49,  1.33it/s]Extractor Predicting: 136it [01:50,  1.34it/s]Extractor Predicting: 137it [01:51,  1.33it/s]Extractor Predicting: 138it [01:51,  1.37it/s]Extractor Predicting: 139it [01:52,  1.38it/s]Extractor Predicting: 140it [01:53,  1.38it/s]Extractor Predicting: 141it [01:54,  1.41it/s]Extractor Predicting: 142it [01:54,  1.38it/s]Extractor Predicting: 143it [01:55,  1.35it/s]Extractor Predicting: 144it [01:55,  1.65it/s]Extractor Predicting: 144it [01:55,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:04,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:04,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:04,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:04,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:04,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:24:05,407 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:24:05,408 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:24:05,967 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:24:07,021 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:24:07,022 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:09,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:09,884 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:09,884 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:09,884 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:24:09,884 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:24:10,543 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:24:10,544 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:24:11,108 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:24:11,270 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:24:11,270 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5602503912363067,
  "recall": 0.10269650028686174,
  "score": 0.1735757575757576,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13522
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13622, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:03,  1.33it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 6it [00:04,  1.28it/s]Extractor Predicting: 7it [00:05,  1.28it/s]Extractor Predicting: 8it [00:06,  1.30it/s]Extractor Predicting: 9it [00:06,  1.30it/s]Extractor Predicting: 10it [00:07,  1.33it/s]Extractor Predicting: 11it [00:08,  1.30it/s]Extractor Predicting: 12it [00:09,  1.31it/s]Extractor Predicting: 13it [00:10,  1.23it/s]Extractor Predicting: 14it [00:10,  1.27it/s]Extractor Predicting: 15it [00:11,  1.27it/s]Extractor Predicting: 16it [00:12,  1.25it/s]Extractor Predicting: 17it [00:13,  1.26it/s]Extractor Predicting: 18it [00:14,  1.25it/s]Extractor Predicting: 19it [00:14,  1.27it/s]Extractor Predicting: 20it [00:15,  1.26it/s]Extractor Predicting: 21it [00:16,  1.28it/s]Extractor Predicting: 22it [00:17,  1.28it/s]Extractor Predicting: 23it [00:17,  1.26it/s]Extractor Predicting: 24it [00:18,  1.25it/s]Extractor Predicting: 25it [00:19,  1.26it/s]Extractor Predicting: 26it [00:20,  1.25it/s]Extractor Predicting: 27it [00:21,  1.25it/s]Extractor Predicting: 28it [00:21,  1.27it/s]Extractor Predicting: 29it [00:22,  1.27it/s]Extractor Predicting: 30it [00:23,  1.24it/s]Extractor Predicting: 31it [00:24,  1.26it/s]Extractor Predicting: 32it [00:25,  1.24it/s]Extractor Predicting: 33it [00:25,  1.25it/s]Extractor Predicting: 34it [00:26,  1.27it/s]Extractor Predicting: 35it [00:27,  1.26it/s]Extractor Predicting: 36it [00:28,  1.30it/s]Extractor Predicting: 37it [00:29,  1.26it/s]Extractor Predicting: 38it [00:29,  1.24it/s]Extractor Predicting: 39it [00:30,  1.24it/s]Extractor Predicting: 40it [00:31,  1.23it/s]Extractor Predicting: 41it [00:32,  1.23it/s]Extractor Predicting: 42it [00:33,  1.23it/s]Extractor Predicting: 43it [00:33,  1.23it/s]Extractor Predicting: 44it [00:34,  1.21it/s]Extractor Predicting: 45it [00:35,  1.22it/s]Extractor Predicting: 46it [00:36,  1.22it/s]Extractor Predicting: 47it [00:37,  1.24it/s]Extractor Predicting: 48it [00:38,  1.25it/s]Extractor Predicting: 49it [00:38,  1.26it/s]Extractor Predicting: 50it [00:39,  1.26it/s]Extractor Predicting: 51it [00:40,  1.30it/s]Extractor Predicting: 52it [00:41,  1.30it/s]Extractor Predicting: 53it [00:41,  1.30it/s]Extractor Predicting: 54it [00:42,  1.29it/s]Extractor Predicting: 55it [00:43,  1.29it/s]Extractor Predicting: 56it [00:44,  1.27it/s]Extractor Predicting: 57it [00:45,  1.22it/s]Extractor Predicting: 58it [00:46,  1.20it/s]Extractor Predicting: 59it [00:46,  1.21it/s]Extractor Predicting: 60it [00:47,  1.23it/s]Extractor Predicting: 61it [00:48,  1.21it/s]Extractor Predicting: 62it [00:49,  1.20it/s]Extractor Predicting: 63it [00:50,  1.25it/s]Extractor Predicting: 64it [00:50,  1.23it/s]Extractor Predicting: 65it [00:51,  1.25it/s]Extractor Predicting: 66it [00:52,  1.24it/s]Extractor Predicting: 67it [00:53,  1.24it/s]Extractor Predicting: 68it [00:54,  1.22it/s]Extractor Predicting: 69it [00:54,  1.22it/s]Extractor Predicting: 70it [00:55,  1.17it/s]Extractor Predicting: 71it [00:56,  1.19it/s]Extractor Predicting: 72it [00:57,  1.19it/s]Extractor Predicting: 73it [00:58,  1.19it/s]Extractor Predicting: 74it [00:59,  1.19it/s]Extractor Predicting: 75it [01:00,  1.18it/s]Extractor Predicting: 76it [01:00,  1.20it/s]Extractor Predicting: 77it [01:01,  1.19it/s]Extractor Predicting: 78it [01:02,  1.20it/s]Extractor Predicting: 79it [01:03,  1.20it/s]Extractor Predicting: 80it [01:04,  1.20it/s]Extractor Predicting: 81it [01:05,  1.20it/s]Extractor Predicting: 82it [01:05,  1.17it/s]Extractor Predicting: 83it [01:06,  1.18it/s]Extractor Predicting: 84it [01:07,  1.20it/s]Extractor Predicting: 85it [01:08,  1.20it/s]Extractor Predicting: 86it [01:09,  1.21it/s]Extractor Predicting: 87it [01:10,  1.19it/s]Extractor Predicting: 88it [01:10,  1.24it/s]Extractor Predicting: 89it [01:11,  1.23it/s]Extractor Predicting: 90it [01:12,  1.22it/s]Extractor Predicting: 91it [01:13,  1.27it/s]Extractor Predicting: 92it [01:13,  1.27it/s]Extractor Predicting: 93it [01:14,  1.29it/s]Extractor Predicting: 94it [01:15,  1.27it/s]Extractor Predicting: 95it [01:16,  1.26it/s]Extractor Predicting: 96it [01:17,  1.26it/s]Extractor Predicting: 97it [01:17,  1.24it/s]Extractor Predicting: 98it [01:18,  1.23it/s]Extractor Predicting: 99it [01:19,  1.24it/s]Extractor Predicting: 100it [01:20,  1.15it/s]Extractor Predicting: 101it [01:21,  1.16it/s]Extractor Predicting: 102it [01:22,  1.15it/s]Extractor Predicting: 103it [01:23,  1.17it/s]Extractor Predicting: 104it [01:24,  1.18it/s]Extractor Predicting: 105it [01:24,  1.21it/s]Extractor Predicting: 106it [01:25,  1.22it/s]Extractor Predicting: 107it [01:26,  1.22it/s]Extractor Predicting: 108it [01:27,  1.23it/s]Extractor Predicting: 109it [01:28,  1.23it/s]Extractor Predicting: 110it [01:28,  1.22it/s]Extractor Predicting: 111it [01:29,  1.23it/s]Extractor Predicting: 112it [01:30,  1.22it/s]Extractor Predicting: 113it [01:31,  1.25it/s]Extractor Predicting: 114it [01:32,  1.23it/s]Extractor Predicting: 115it [01:32,  1.25it/s]Extractor Predicting: 116it [01:33,  1.20it/s]Extractor Predicting: 117it [01:34,  1.18it/s]Extractor Predicting: 118it [01:35,  1.18it/s]Extractor Predicting: 119it [01:36,  1.17it/s]Extractor Predicting: 120it [01:37,  1.17it/s]Extractor Predicting: 121it [01:38,  1.18it/s]Extractor Predicting: 122it [01:38,  1.21it/s]Extractor Predicting: 123it [01:39,  1.20it/s]Extractor Predicting: 124it [01:40,  1.18it/s]Extractor Predicting: 125it [01:41,  1.17it/s]Extractor Predicting: 126it [01:42,  1.15it/s]Extractor Predicting: 127it [01:43,  1.15it/s]Extractor Predicting: 128it [01:44,  1.16it/s]Extractor Predicting: 129it [01:44,  1.14it/s]Extractor Predicting: 130it [01:45,  1.17it/s]Extractor Predicting: 131it [01:46,  1.18it/s]Extractor Predicting: 132it [01:47,  1.17it/s]Extractor Predicting: 133it [01:48,  1.17it/s]Extractor Predicting: 134it [01:49,  1.19it/s]Extractor Predicting: 135it [01:49,  1.20it/s]Extractor Predicting: 136it [01:50,  1.22it/s]Extractor Predicting: 137it [01:51,  1.25it/s]Extractor Predicting: 138it [01:52,  1.23it/s]Extractor Predicting: 139it [01:53,  1.24it/s]Extractor Predicting: 140it [01:53,  1.22it/s]Extractor Predicting: 141it [01:54,  1.19it/s]Extractor Predicting: 142it [01:55,  1.19it/s]Extractor Predicting: 143it [01:56,  1.28it/s]Extractor Predicting: 143it [01:56,  1.23it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:26:14,473 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:26:14,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:26:14,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:26:14,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:26:14,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:26:15,117 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:26:15,118 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:26:15,701 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:26:16,721 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:26:16,721 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:26:19,607 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:26:19,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:26:19,612 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:26:19,612 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:26:19,612 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:26:20,260 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:26:20,261 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:26:20,836 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:26:20,998 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:26:20,999 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5310734463276836,
  "recall": 0.08240794856808883,
  "score": 0.14267644826713888,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 464
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 564, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.07it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 2it [00:01,  1.39it/s]
[INFO|configuration_utils.py:515] 2023-08-28 12:26:22,819 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:26:22,824 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 12:26:22,829 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:26:22,830 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 12:26:22,839 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 12:26:26,041 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 12:26:26,041 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 12:26:26,063 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:26:26,064 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 12:26:26,068 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:26:26,071 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:26:26,071 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:26:26,071 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:26:26,071 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:26:26,071 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:26:26,071 >> loading file outputs/wrapper/fewrel/unseen_5_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6666666666666666,
  "recall": 0.02564102564102564,
  "score": 0.04938271604938271,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 12:26:26,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:27,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:28,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:28,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:29,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:30,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:31,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:32,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:33,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:34,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:35,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:36,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:37,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:38,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:39,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:40,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:41,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:41,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:42,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:43,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:44,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:45,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:46,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:47,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:48,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:23<03:27, 23.04s/it][WARNING|generation_utils.py:914] 2023-08-28 12:26:49,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:50,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:51,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:52,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:52,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:54,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:55,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:56,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:56,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:57,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:58,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:26:59,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:00,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:01,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:02,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:03,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:04,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:05,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:06,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:07,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:08,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:08,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:43<02:53, 21.64s/it][WARNING|generation_utils.py:914] 2023-08-28 12:27:10,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:10,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:11,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:12,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:13,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:14,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:16,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:17,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:18,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:19,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:19,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:20,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:21,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:22,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:24,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:24,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:25,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:26,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:27,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:28,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:29,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:30,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:31,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:32,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:06<02:36, 22.34s/it][WARNING|generation_utils.py:914] 2023-08-28 12:27:33,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:33,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:35,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:36,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:38,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:39,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:40,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:41,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:42,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:43,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:43,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:44,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:45,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:46,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:47,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:48,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:49,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:50,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:51,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:52,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:53,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:28<02:11, 21.96s/it][WARNING|generation_utils.py:914] 2023-08-28 12:27:54,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:55,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:56,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:57,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:58,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:27:59,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:00,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:00,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:01,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:02,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:03,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:04,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:05,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:06,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:07,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:08,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:09,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:09,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:10,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:11,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:12,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:13,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:14,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:15,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:16,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:17,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:18,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:18,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:20,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:20,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:21,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:22,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:23,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:24,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:25,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:26,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [02:00<02:08, 25.77s/it][WARNING|generation_utils.py:914] 2023-08-28 12:28:27,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:28,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:29,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:30,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:30,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:31,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:32,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:33,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:34,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:36,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:37,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:38,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:39,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:41,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:41,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:42,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:43,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:44,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:45,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:46,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:48,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:49,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:50,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:25<01:41, 25.31s/it][WARNING|generation_utils.py:914] 2023-08-28 12:28:51,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:52,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:53,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:54,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:54,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:56,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:56,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:57,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:58,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:28:59,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:00,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:01,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:02,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:03,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:04,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:06,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:07,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:08,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:09,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:10,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:11,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:46<01:11, 23.96s/it][WARNING|generation_utils.py:914] 2023-08-28 12:29:12,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:13,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:14,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:15,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:16,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:17,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:18,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:19,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:20,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:21,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:22,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:23,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:24,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:25,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:26,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:27,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:28,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:29,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:30,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:31,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:32,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:33,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [03:07<00:46, 23.17s/it][WARNING|generation_utils.py:914] 2023-08-28 12:29:34,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:35,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:36,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:37,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:38,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:39,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:39,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:40,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:41,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:42,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:43,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:44,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:45,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:46,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:47,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:48,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:49,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:50,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:51,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:52,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:53,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:28<00:22, 22.41s/it][WARNING|generation_utils.py:914] 2023-08-28 12:29:54,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:55,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:56,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:57,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:58,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:29:59,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:00,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:01,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:02,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:03,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:04,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:06,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:07,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:08,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:09,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:10,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:11,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:12,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:13,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:14,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:15,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:17,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:30:17,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:52<00:00, 22.88s/it]Generating: 100%|██████████| 10/10 [03:52<00:00, 23.25s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:24,923 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:24,927 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:24,927 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:24,927 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:24,927 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:30:25,546 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:30:25,547 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:30:26,135 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:30:27,219 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:30:27,219 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:30,067 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:30,071 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:30,071 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:30,071 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:30:30,071 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:30:30,738 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:30:30,739 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:30:31,311 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:30:31,476 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:30:31,476 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 536, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : country .', 'success_rate': 0.75625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : followed by .', 'success_rate': 0.8565340909090909, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.8072916666666666, 'errors': {'', '(\'Lost in Translation\', \'genre\', \'\', \'The video for " The Phantom Menace " features Richard Linklater , director of " Lost in Translation " ( 2001 ) , performing a song " When The Dark Throws " in the background .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9047619047619048, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : voice type . Context : Later in the year , the band formed the single " The Way To Die " at the end of 2010 , with singer - songwriter Michael Krieger serving as the lead , alongside songwriter Steve Vai . Head Entity : The Way To Die , Tail Entity : vocal .\n']
['Relation : voice type . Context : Later in the year , the band formed the single " The Way To Die " at the end of 2010 , with singer - songwriter Michael Krieger serving as the lead , alongside songwriter Steve Vai . Head Entity : The Way To Die , Tail Entity : vocal .\n', "Relation : voice type . Context : After he gained steam as a member of his band the Travail - Jax , he decided to sing along to the jazz - influenced songs of the band 's lead singer , Jimi Hendrix . Head Entity : Jimi Hendrix , Tail Entity : saxophonist .\n"]
['Relation : voice type . Context : Later in the year , the band formed the single " The Way To Die " at the end of 2010 , with singer - songwriter Michael Krieger serving as the lead , alongside songwriter Steve Vai . Head Entity : The Way To Die , Tail Entity : vocal .\n', "Relation : voice type . Context : After he gained steam as a member of his band the Travail - Jax , he decided to sing along to the jazz - influenced songs of the band 's lead singer , Jimi Hendrix . Head Entity : Jimi Hendrix , Tail Entity : saxophonist .\n", 'Relation : voice type . Context : This film explores the social implications of the film " The Brothers Karamazov " ( 1960 ) , and the impact of the film on the Soviet Union . Head Entity : The Brothers Karamazov , Tail Entity : voice type .\n']
['Relation : voice type . Context : Later in the year , the band formed the single " The Way To Die " at the end of 2010 , with singer - songwriter Michael Krieger serving as the lead , alongside songwriter Steve Vai . Head Entity : The Way To Die , Tail Entity : vocal .\n', "Relation : voice type . Context : After he gained steam as a member of his band the Travail - Jax , he decided to sing along to the jazz - influenced songs of the band 's lead singer , Jimi Hendrix . Head Entity : Jimi Hendrix , Tail Entity : saxophonist .\n", 'Relation : voice type . Context : This film explores the social implications of the film " The Brothers Karamazov " ( 1960 ) , and the impact of the film on the Soviet Union . Head Entity : The Brothers Karamazov , Tail Entity : voice type .\n', 'Relation : voice type . Context : In 1994 , she appeared on the soundtrack for " The Music of Henry Cavill " . Head Entity : The Music of Henry Cavill , Tail Entity : music .\n']
['Relation : voice type . Context : Later in the year , the band formed the single " The Way To Die " at the end of 2010 , with singer - songwriter Michael Krieger serving as the lead , alongside songwriter Steve Vai . Head Entity : The Way To Die , Tail Entity : vocal .\n', "Relation : voice type . Context : After he gained steam as a member of his band the Travail - Jax , he decided to sing along to the jazz - influenced songs of the band 's lead singer , Jimi Hendrix . Head Entity : Jimi Hendrix , Tail Entity : saxophonist .\n", 'Relation : voice type . Context : This film explores the social implications of the film " The Brothers Karamazov " ( 1960 ) , and the impact of the film on the Soviet Union . Head Entity : The Brothers Karamazov , Tail Entity : voice type .\n', 'Relation : voice type . Context : In 1994 , she appeared on the soundtrack for " The Music of Henry Cavill " . Head Entity : The Music of Henry Cavill , Tail Entity : music .\n', 'Relation : voice type . Context : She also sang the song for her studio album " Gremlins " , which is due out in 2019 on Kazaa Records . Head Entity : Gremlins , Tail Entity : voice type .\n']
{'target': 600, 'success': 13, 'raw': 32}
{'target': 600, 'success': 33, 'raw': 64}
{'target': 600, 'success': 49, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 88, 'raw': 160}
{'target': 600, 'success': 101, 'raw': 192}
{'target': 600, 'success': 116, 'raw': 224}
{'target': 600, 'success': 131, 'raw': 256}
{'target': 600, 'success': 142, 'raw': 288}
{'target': 600, 'success': 160, 'raw': 320}
{'target': 600, 'success': 169, 'raw': 352}
{'target': 600, 'success': 184, 'raw': 384}
{'target': 600, 'success': 206, 'raw': 416}
{'target': 600, 'success': 229, 'raw': 448}
{'target': 600, 'success': 247, 'raw': 480}
{'target': 600, 'success': 268, 'raw': 512}
{'target': 600, 'success': 285, 'raw': 544}
{'target': 600, 'success': 306, 'raw': 576}
{'target': 600, 'success': 326, 'raw': 608}
{'target': 600, 'success': 342, 'raw': 640}
{'target': 600, 'success': 355, 'raw': 672}
{'target': 600, 'success': 375, 'raw': 704}
{'target': 600, 'success': 390, 'raw': 736}
{'target': 600, 'success': 406, 'raw': 768}
{'target': 600, 'success': 423, 'raw': 800}
{'target': 600, 'success': 438, 'raw': 832}
{'target': 600, 'success': 455, 'raw': 864}
{'target': 600, 'success': 471, 'raw': 896}
{'target': 600, 'success': 491, 'raw': 928}
{'target': 600, 'success': 513, 'raw': 960}
{'target': 600, 'success': 532, 'raw': 992}
{'target': 600, 'success': 541, 'raw': 1024}
{'target': 600, 'success': 558, 'raw': 1056}
{'target': 600, 'success': 576, 'raw': 1088}
{'target': 600, 'success': 594, 'raw': 1120}
{'target': 600, 'success': 614, 'raw': 1152}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5329861111111112, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', '(\'The Devil in White\', \'voice type\', \'\', \'In May 2009 , she appeared in the BBC \\\'s musical adaptation of the French literary mystery author Bernard Proust \\\'s novel " The Devil in White " .\')'}}
['Relation : contains administrative territorial entity . Context : The city of Marrero de México has a population of 9,150 at census time and is part of the México - San Lorenzo region , a municipality of the central Spanish state of Guadalajara . Head Entity : municipality of Guadalajara , Tail Entity : Spanish state of Guadalajara .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9002976190476191, 'errors': {'', '(\'"\', \'distributed by\', \'\', \'Among " " , " " , " " , " ..\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8636363636363636, 'errors': {'', "('Aselas', 'field of work', '', 'Karel M. Aselas ( born 7 January 1964 in Amsterdam ) is a Dutch neuroscientist and an inventor , known for his work in the field of genetics .')", 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 10662
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10762, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.16it/s]Extractor Estimating: 2it [00:01,  1.18it/s]Extractor Estimating: 3it [00:02,  1.21it/s]Extractor Estimating: 4it [00:03,  1.25it/s]Extractor Estimating: 5it [00:03,  1.30it/s]Extractor Estimating: 6it [00:04,  1.27it/s]Extractor Estimating: 7it [00:05,  1.29it/s]Extractor Estimating: 8it [00:06,  1.29it/s]Extractor Estimating: 9it [00:07,  1.24it/s]Extractor Estimating: 10it [00:08,  1.22it/s]Extractor Estimating: 11it [00:08,  1.18it/s]Extractor Estimating: 12it [00:09,  1.20it/s]Extractor Estimating: 13it [00:10,  1.19it/s]Extractor Estimating: 14it [00:11,  1.19it/s]Extractor Estimating: 15it [00:12,  1.24it/s]Extractor Estimating: 16it [00:12,  1.26it/s]Extractor Estimating: 17it [00:13,  1.24it/s]Extractor Estimating: 18it [00:14,  1.28it/s]Extractor Estimating: 19it [00:15,  1.26it/s]Extractor Estimating: 20it [00:16,  1.26it/s]Extractor Estimating: 21it [00:16,  1.23it/s]Extractor Estimating: 22it [00:17,  1.22it/s]Extractor Estimating: 23it [00:18,  1.23it/s]Extractor Estimating: 24it [00:19,  1.22it/s]Extractor Estimating: 25it [00:20,  1.22it/s]Extractor Estimating: 26it [00:21,  1.21it/s]Extractor Estimating: 27it [00:21,  1.21it/s]Extractor Estimating: 28it [00:22,  1.17it/s]Extractor Estimating: 29it [00:23,  1.19it/s]Extractor Estimating: 30it [00:24,  1.18it/s]Extractor Estimating: 31it [00:25,  1.18it/s]Extractor Estimating: 32it [00:26,  1.17it/s]Extractor Estimating: 33it [00:27,  1.15it/s]Extractor Estimating: 34it [00:27,  1.18it/s]Extractor Estimating: 35it [00:28,  1.18it/s]Extractor Estimating: 36it [00:29,  1.16it/s]Extractor Estimating: 37it [00:30,  1.14it/s]Extractor Estimating: 38it [00:31,  1.16it/s]Extractor Estimating: 39it [00:32,  1.14it/s]Extractor Estimating: 40it [00:33,  1.13it/s]Extractor Estimating: 41it [00:34,  1.15it/s]Extractor Estimating: 42it [00:34,  1.19it/s]Extractor Estimating: 43it [00:35,  1.23it/s]Extractor Estimating: 44it [00:36,  1.24it/s]Extractor Estimating: 45it [00:37,  1.28it/s]Extractor Estimating: 46it [00:37,  1.24it/s]Extractor Estimating: 47it [00:38,  1.26it/s]Extractor Estimating: 48it [00:39,  1.24it/s]Extractor Estimating: 49it [00:40,  1.22it/s]Extractor Estimating: 50it [00:41,  1.22it/s]Extractor Estimating: 51it [00:42,  1.21it/s]Extractor Estimating: 52it [00:42,  1.19it/s]Extractor Estimating: 53it [00:43,  1.18it/s]Extractor Estimating: 54it [00:44,  1.17it/s]Extractor Estimating: 55it [00:45,  1.13it/s]Extractor Estimating: 56it [00:46,  1.16it/s]Extractor Estimating: 57it [00:47,  1.18it/s]Extractor Estimating: 58it [00:48,  1.17it/s]Extractor Estimating: 59it [00:48,  1.19it/s]Extractor Estimating: 60it [00:49,  1.22it/s]Extractor Estimating: 61it [00:50,  1.23it/s]Extractor Estimating: 62it [00:51,  1.24it/s]Extractor Estimating: 63it [00:52,  1.26it/s]Extractor Estimating: 64it [00:52,  1.21it/s]Extractor Estimating: 65it [00:53,  1.21it/s]Extractor Estimating: 66it [00:54,  1.23it/s]Extractor Estimating: 67it [00:55,  1.22it/s]Extractor Estimating: 68it [00:56,  1.19it/s]Extractor Estimating: 69it [00:57,  1.21it/s]Extractor Estimating: 70it [00:57,  1.22it/s]Extractor Estimating: 71it [00:58,  1.22it/s]Extractor Estimating: 72it [00:59,  1.22it/s]Extractor Estimating: 73it [01:00,  1.17it/s]Extractor Estimating: 74it [01:01,  1.17it/s]Extractor Estimating: 75it [01:02,  1.16it/s]Extractor Estimating: 76it [01:02,  1.25it/s]Extractor Estimating: 77it [01:03,  1.19it/s]Extractor Estimating: 78it [01:04,  1.20it/s]Extractor Estimating: 79it [01:05,  1.09it/s]Extractor Estimating: 80it [01:06,  1.12it/s]Extractor Estimating: 81it [01:07,  1.16it/s]Extractor Estimating: 82it [01:08,  1.22it/s]Extractor Estimating: 83it [01:08,  1.19it/s]Extractor Estimating: 84it [01:09,  1.24it/s]Extractor Estimating: 85it [01:10,  1.18it/s]Extractor Estimating: 86it [01:11,  1.26it/s]Extractor Estimating: 87it [01:12,  1.30it/s]Extractor Estimating: 88it [01:12,  1.22it/s]Extractor Estimating: 89it [01:13,  1.24it/s]Extractor Estimating: 90it [01:14,  1.27it/s]Extractor Estimating: 91it [01:15,  1.29it/s]Extractor Estimating: 92it [01:15,  1.29it/s]Extractor Estimating: 93it [01:16,  1.29it/s]Extractor Estimating: 94it [01:17,  1.30it/s]Extractor Estimating: 95it [01:18,  1.29it/s]Extractor Estimating: 96it [01:19,  1.28it/s]Extractor Estimating: 97it [01:19,  1.27it/s]Extractor Estimating: 98it [01:20,  1.22it/s]Extractor Estimating: 99it [01:21,  1.24it/s]Extractor Estimating: 100it [01:22,  1.22it/s]Extractor Estimating: 101it [01:23,  1.27it/s]Extractor Estimating: 102it [01:23,  1.26it/s]Extractor Estimating: 103it [01:24,  1.26it/s]Extractor Estimating: 104it [01:25,  1.22it/s]Extractor Estimating: 105it [01:26,  1.23it/s]Extractor Estimating: 106it [01:27,  1.24it/s]Extractor Estimating: 107it [01:28,  1.24it/s]Extractor Estimating: 108it [01:28,  1.21it/s]Extractor Estimating: 109it [01:29,  1.25it/s]Extractor Estimating: 110it [01:30,  1.25it/s]Extractor Estimating: 111it [01:31,  1.24it/s]Extractor Estimating: 112it [01:31,  1.27it/s]Extractor Estimating: 113it [01:32,  1.25it/s]Extractor Estimating: 114it [01:33,  1.24it/s]Extractor Estimating: 115it [01:34,  1.22it/s]Extractor Estimating: 116it [01:35,  1.27it/s]Extractor Estimating: 117it [01:36,  1.26it/s]Extractor Estimating: 118it [01:36,  1.28it/s]Extractor Estimating: 119it [01:37,  1.31it/s]Extractor Estimating: 120it [01:38,  1.25it/s]Extractor Estimating: 121it [01:39,  1.26it/s]Extractor Estimating: 122it [01:39,  1.28it/s]Extractor Estimating: 123it [01:40,  1.27it/s]Extractor Estimating: 124it [01:41,  1.29it/s]Extractor Estimating: 125it [01:42,  1.30it/s]Extractor Estimating: 126it [01:42,  1.33it/s]Extractor Estimating: 127it [01:43,  1.25it/s]Extractor Estimating: 128it [01:44,  1.27it/s]Extractor Estimating: 129it [01:45,  1.25it/s]Extractor Estimating: 130it [01:46,  1.24it/s]Extractor Estimating: 131it [01:46,  1.28it/s]Extractor Estimating: 132it [01:47,  1.23it/s]Extractor Estimating: 133it [01:48,  1.24it/s]Extractor Estimating: 134it [01:49,  1.27it/s]Extractor Estimating: 135it [01:50,  1.24it/s]Extractor Estimating: 136it [01:50,  1.28it/s]Extractor Estimating: 137it [01:51,  1.24it/s]Extractor Estimating: 138it [01:52,  1.28it/s]Extractor Estimating: 139it [01:53,  1.28it/s]Extractor Estimating: 140it [01:54,  1.15it/s]Extractor Estimating: 141it [01:55,  1.20it/s]Extractor Estimating: 142it [01:55,  1.24it/s]Extractor Estimating: 143it [01:56,  1.29it/s]Extractor Estimating: 144it [01:57,  1.26it/s]Extractor Estimating: 145it [01:58,  1.28it/s]Extractor Estimating: 146it [01:59,  1.21it/s]Extractor Estimating: 147it [01:59,  1.20it/s]Extractor Estimating: 148it [02:00,  1.24it/s]Extractor Estimating: 149it [02:01,  1.27it/s]Extractor Estimating: 150it [02:02,  1.26it/s]Extractor Estimating: 151it [02:03,  1.27it/s]Extractor Estimating: 152it [02:03,  1.27it/s]Extractor Estimating: 153it [02:04,  1.29it/s]Extractor Estimating: 154it [02:05,  1.29it/s]Extractor Estimating: 155it [02:06,  1.27it/s]Extractor Estimating: 156it [02:07,  1.24it/s]Extractor Estimating: 157it [02:07,  1.24it/s]Extractor Estimating: 158it [02:08,  1.29it/s]Extractor Estimating: 159it [02:09,  1.29it/s]Extractor Estimating: 160it [02:10,  1.26it/s]Extractor Estimating: 161it [02:10,  1.26it/s]Extractor Estimating: 162it [02:11,  1.30it/s]Extractor Estimating: 163it [02:12,  1.29it/s]Extractor Estimating: 164it [02:13,  1.30it/s]Extractor Estimating: 165it [02:14,  1.26it/s]Extractor Estimating: 166it [02:15,  1.18it/s]Extractor Estimating: 167it [02:15,  1.21it/s]Extractor Estimating: 168it [02:16,  1.27it/s]Extractor Estimating: 169it [02:17,  1.27it/s]Extractor Estimating: 170it [02:18,  1.27it/s]Extractor Estimating: 171it [02:18,  1.23it/s]Extractor Estimating: 172it [02:19,  1.22it/s]Extractor Estimating: 173it [02:20,  1.20it/s]Extractor Estimating: 174it [02:21,  1.20it/s]Extractor Estimating: 175it [02:22,  1.20it/s]Extractor Estimating: 176it [02:22,  1.27it/s]Extractor Estimating: 177it [02:23,  1.24it/s]Extractor Estimating: 178it [02:24,  1.27it/s]Extractor Estimating: 179it [02:25,  1.26it/s]Extractor Estimating: 180it [02:26,  1.25it/s]Extractor Estimating: 181it [02:27,  1.24it/s]Extractor Estimating: 182it [02:27,  1.19it/s]Extractor Estimating: 183it [02:28,  1.19it/s]Extractor Estimating: 184it [02:29,  1.24it/s]Extractor Estimating: 185it [02:30,  1.22it/s]Extractor Estimating: 186it [02:31,  1.21it/s]Extractor Estimating: 187it [02:32,  1.21it/s]Extractor Estimating: 188it [02:32,  1.19it/s]Extractor Estimating: 189it [02:33,  1.24it/s]Extractor Estimating: 190it [02:34,  1.27it/s]Extractor Estimating: 191it [02:35,  1.28it/s]Extractor Estimating: 192it [02:35,  1.28it/s]Extractor Estimating: 193it [02:36,  1.27it/s]Extractor Estimating: 194it [02:37,  1.24it/s]Extractor Estimating: 195it [02:38,  1.27it/s]Extractor Estimating: 196it [02:39,  1.22it/s]Extractor Estimating: 197it [02:40,  1.22it/s]Extractor Estimating: 198it [02:40,  1.22it/s]Extractor Estimating: 199it [02:41,  1.24it/s]Extractor Estimating: 200it [02:42,  1.21it/s]Extractor Estimating: 201it [02:43,  1.21it/s]Extractor Estimating: 202it [02:44,  1.27it/s]Extractor Estimating: 203it [02:44,  1.24it/s]Extractor Estimating: 204it [02:45,  1.26it/s]Extractor Estimating: 205it [02:46,  1.30it/s]Extractor Estimating: 206it [02:47,  1.33it/s]Extractor Estimating: 207it [02:47,  1.32it/s]Extractor Estimating: 208it [02:48,  1.29it/s]Extractor Estimating: 209it [02:49,  1.27it/s]Extractor Estimating: 210it [02:50,  1.27it/s]Extractor Estimating: 211it [02:50,  1.30it/s]Extractor Estimating: 212it [02:51,  1.37it/s]Extractor Estimating: 213it [02:52,  1.32it/s]Extractor Estimating: 214it [02:53,  1.17it/s]Extractor Estimating: 215it [02:54,  1.23it/s]Extractor Estimating: 216it [02:54,  1.27it/s]Extractor Estimating: 217it [02:55,  1.28it/s]Extractor Estimating: 218it [02:56,  1.27it/s]Extractor Estimating: 219it [02:57,  1.24it/s]Extractor Estimating: 220it [02:58,  1.23it/s]Extractor Estimating: 221it [02:59,  1.22it/s]Extractor Estimating: 222it [02:59,  1.25it/s]Extractor Estimating: 223it [03:00,  1.24it/s]Extractor Estimating: 224it [03:01,  1.23it/s]Extractor Estimating: 225it [03:02,  1.25it/s]Extractor Estimating: 226it [03:02,  1.28it/s]Extractor Estimating: 227it [03:03,  1.31it/s]Extractor Estimating: 228it [03:04,  1.29it/s]Extractor Estimating: 229it [03:05,  1.28it/s]Extractor Estimating: 230it [03:06,  1.28it/s]Extractor Estimating: 231it [03:06,  1.32it/s]Extractor Estimating: 232it [03:07,  1.37it/s]Extractor Estimating: 233it [03:08,  1.33it/s]Extractor Estimating: 234it [03:08,  1.33it/s]Extractor Estimating: 235it [03:09,  1.31it/s]Extractor Estimating: 236it [03:10,  1.31it/s]Extractor Estimating: 237it [03:11,  1.34it/s]Extractor Estimating: 238it [03:12,  1.32it/s]Extractor Estimating: 239it [03:12,  1.25it/s]Extractor Estimating: 240it [03:13,  1.28it/s]Extractor Estimating: 241it [03:14,  1.27it/s]Extractor Estimating: 242it [03:15,  1.28it/s]Extractor Estimating: 243it [03:15,  1.29it/s]Extractor Estimating: 244it [03:16,  1.30it/s]Extractor Estimating: 245it [03:17,  1.32it/s]Extractor Estimating: 246it [03:18,  1.28it/s]Extractor Estimating: 247it [03:19,  1.28it/s]Extractor Estimating: 248it [03:19,  1.25it/s]Extractor Estimating: 249it [03:20,  1.27it/s]Extractor Estimating: 250it [03:21,  1.30it/s]Extractor Estimating: 250it [03:21,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:03,656 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:03,660 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:03,660 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:03,660 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:03,660 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:34:03,963 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:34:03,964 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:34:04,225 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:34:05,299 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:34:05,299 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:06,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:06,661 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:06,661 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:06,661 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:06,661 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:34:06,981 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:34:06,983 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:34:07,249 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:34:07,417 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:34:07,417 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 14:27:36,955 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 14:27:36,990 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 4991 mean pseudo reward: 0.9323906365272722
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl'}
train vocab size: 21964
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22064, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22064, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.313, loss:664.3003
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.347, loss:586.3322
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.308, loss:547.8102
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.319, loss:570.6231
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.320, loss:544.8494
>> valid entity prec:0.4919, rec:0.5583, f1:0.5230
>> valid relation prec:0.1577, rec:0.0838, f1:0.1094
>> valid relation with NER prec:0.1577, rec:0.0838, f1:0.1094
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.990, loss:539.8415
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.303, loss:513.9322
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.314, loss:546.7708
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.321, loss:539.1545
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.315, loss:560.3186
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5419, rec:0.4303, f1:0.4797
>> valid relation prec:0.2498, rec:0.0929, f1:0.1355
>> valid relation with NER prec:0.2498, rec:0.0929, f1:0.1355
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 60, avg_time 2.972, loss:541.3529
g_step 1200, step 160, avg_time 1.314, loss:561.4234
g_step 1300, step 52, avg_time 1.299, loss:516.0736
g_step 1400, step 152, avg_time 1.318, loss:546.5682
g_step 1500, step 44, avg_time 1.319, loss:519.6440
>> valid entity prec:0.5141, rec:0.4703, f1:0.4912
>> valid relation prec:0.1391, rec:0.0551, f1:0.0789
>> valid relation with NER prec:0.1391, rec:0.0551, f1:0.0789
g_step 1600, step 144, avg_time 2.981, loss:506.6737
g_step 1700, step 36, avg_time 1.312, loss:499.6517
g_step 1800, step 136, avg_time 1.316, loss:493.3748
g_step 1900, step 28, avg_time 1.317, loss:471.8192
g_step 2000, step 128, avg_time 1.315, loss:455.5376
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5395, rec:0.4838, f1:0.5102
>> valid relation prec:0.1289, rec:0.0519, f1:0.0740
>> valid relation with NER prec:0.1289, rec:0.0519, f1:0.0740
g_step 2100, step 20, avg_time 2.980, loss:460.2067
g_step 2200, step 120, avg_time 1.324, loss:413.3562
g_step 2300, step 12, avg_time 1.300, loss:455.6303
g_step 2400, step 112, avg_time 1.314, loss:410.1926
g_step 2500, step 4, avg_time 1.314, loss:439.2999
>> valid entity prec:0.5052, rec:0.4255, f1:0.4620
>> valid relation prec:0.1107, rec:0.0422, f1:0.0611
>> valid relation with NER prec:0.1107, rec:0.0422, f1:0.0611
g_step 2600, step 104, avg_time 2.975, loss:398.5363
g_step 2700, step 204, avg_time 1.323, loss:430.9454
g_step 2800, step 96, avg_time 1.313, loss:387.9125
g_step 2900, step 196, avg_time 1.320, loss:415.2565
g_step 3000, step 88, avg_time 1.318, loss:380.2505
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4924, rec:0.4547, f1:0.4728
>> valid relation prec:0.1354, rec:0.0551, f1:0.0783
>> valid relation with NER prec:0.1354, rec:0.0551, f1:0.0783
g_step 3100, step 188, avg_time 2.978, loss:383.1518
g_step 3200, step 80, avg_time 1.318, loss:360.5927
g_step 3300, step 180, avg_time 1.313, loss:373.4407
g_step 3400, step 72, avg_time 1.313, loss:352.6837
g_step 3500, step 172, avg_time 1.319, loss:347.8317
>> valid entity prec:0.4902, rec:0.5214, f1:0.5053
>> valid relation prec:0.0986, rec:0.0499, f1:0.0663
>> valid relation with NER prec:0.0986, rec:0.0499, f1:0.0663
g_step 3600, step 64, avg_time 2.999, loss:342.5840
g_step 3700, step 164, avg_time 1.317, loss:342.5077
g_step 3800, step 56, avg_time 1.319, loss:333.3022
g_step 3900, step 156, avg_time 1.319, loss:336.5957
g_step 4000, step 48, avg_time 1.316, loss:328.8267
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5092, rec:0.4599, f1:0.4833
>> valid relation prec:0.1006, rec:0.0491, f1:0.0660
>> valid relation with NER prec:0.1006, rec:0.0491, f1:0.0660
g_step 4100, step 148, avg_time 2.983, loss:317.7099
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:27:36 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:27:36 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-27-36_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:27:37 - WARNING - datasets.builder -   Using custom data configuration default-ee92ab5ad14441d3
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ee92ab5ad14441d3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:27:38,205 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:27:38,206 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:27:38,207 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:27:38,208 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:27:38,227 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:27:38,233 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:27:38,233 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:27:38,233 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:27:38,233 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:27:38,233 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:27:38,233 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:27:38,362 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:27:41,518 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:27:41,520 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ee92ab5ad14441d3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.29ba/s] 40%|████      | 2/5 [00:00<00:00,  3.31ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.81ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.08ba/s]100%|██████████| 5/5 [00:01<00:00,  4.24ba/s]100%|██████████| 5/5 [00:01<00:00,  3.85ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.12ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.35ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.42ba/s]100%|██████████| 4/4 [00:00<00:00,  5.57ba/s]100%|██████████| 4/4 [00:00<00:00,  5.04ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  7.99ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.61ba/s]100%|██████████| 5/5 [00:00<00:00,  9.94ba/s]100%|██████████| 5/5 [00:00<00:00,  9.74ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.11ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.59ba/s]100%|██████████| 4/4 [00:00<00:00, 10.83ba/s]
[INFO|trainer.py:414] 2023-08-28 14:27:44,862 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:27:44,873 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:27:44,873 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 14:27:44,873 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:27:44,873 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:27:44,874 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:27:44,874 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:27:44,874 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<02:00,  3.24it/s]  1%|          | 2/390 [00:00<01:55,  3.35it/s]  1%|          | 3/390 [00:00<01:54,  3.39it/s]  1%|          | 4/390 [00:01<01:53,  3.41it/s]  1%|▏         | 5/390 [00:01<01:52,  3.42it/s]  2%|▏         | 6/390 [00:01<01:52,  3.42it/s]  2%|▏         | 7/390 [00:02<01:51,  3.43it/s]  2%|▏         | 8/390 [00:02<01:51,  3.43it/s]  2%|▏         | 9/390 [00:02<01:51,  3.43it/s]  3%|▎         | 10/390 [00:02<01:50,  3.43it/s]  3%|▎         | 11/390 [00:03<01:50,  3.42it/s]  3%|▎         | 12/390 [00:03<01:50,  3.43it/s]  3%|▎         | 13/390 [00:03<01:49,  3.43it/s]  4%|▎         | 14/390 [00:04<01:49,  3.43it/s]  4%|▍         | 15/390 [00:04<01:49,  3.43it/s]  4%|▍         | 16/390 [00:04<01:49,  3.43it/s]  4%|▍         | 17/390 [00:04<01:48,  3.43it/s]  5%|▍         | 18/390 [00:05<01:48,  3.42it/s]  5%|▍         | 19/390 [00:05<01:48,  3.43it/s]  5%|▌         | 20/390 [00:05<01:47,  3.43it/s]  5%|▌         | 21/390 [00:06<01:47,  3.43it/s]  6%|▌         | 22/390 [00:06<01:47,  3.43it/s]  6%|▌         | 23/390 [00:06<01:47,  3.43it/s]  6%|▌         | 24/390 [00:07<01:46,  3.43it/s]  6%|▋         | 25/390 [00:07<01:46,  3.43it/s]  7%|▋         | 26/390 [00:07<01:46,  3.43it/s]  7%|▋         | 27/390 [00:07<01:45,  3.43it/s]  7%|▋         | 28/390 [00:08<01:45,  3.43it/s]  7%|▋         | 29/390 [00:08<01:45,  3.43it/s]  8%|▊         | 30/390 [00:08<01:45,  3.43it/s]  8%|▊         | 31/390 [00:09<01:44,  3.43it/s]  8%|▊         | 32/390 [00:09<01:44,  3.43it/s]  8%|▊         | 33/390 [00:09<01:44,  3.43it/s]  9%|▊         | 34/390 [00:09<01:43,  3.43it/s]  9%|▉         | 35/390 [00:10<01:43,  3.43it/s]  9%|▉         | 36/390 [00:10<01:43,  3.43it/s]  9%|▉         | 37/390 [00:10<01:42,  3.43it/s] 10%|▉         | 38/390 [00:11<01:42,  3.43it/s] 10%|█         | 39/390 [00:11<01:42,  3.43it/s] 10%|█         | 40/390 [00:11<01:42,  3.42it/s] 11%|█         | 41/390 [00:11<01:41,  3.42it/s] 11%|█         | 42/390 [00:12<01:41,  3.42it/s] 11%|█         | 43/390 [00:12<01:41,  3.42it/s] 11%|█▏        | 44/390 [00:12<01:40,  3.43it/s] 12%|█▏        | 45/390 [00:13<01:40,  3.43it/s] 12%|█▏        | 46/390 [00:13<01:40,  3.42it/s] 12%|█▏        | 47/390 [00:13<01:40,  3.43it/s] 12%|█▏        | 48/390 [00:14<01:39,  3.42it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.43it/s] 13%|█▎        | 50/390 [00:14<01:39,  3.42it/s] 13%|█▎        | 51/390 [00:14<01:39,  3.42it/s] 13%|█▎        | 52/390 [00:15<01:38,  3.42it/s] 14%|█▎        | 53/390 [00:15<01:38,  3.41it/s] 14%|█▍        | 54/390 [00:15<01:38,  3.41it/s] 14%|█▍        | 55/390 [00:16<01:38,  3.42it/s] 14%|█▍        | 56/390 [00:16<01:37,  3.42it/s] 15%|█▍        | 57/390 [00:16<01:37,  3.42it/s] 15%|█▍        | 58/390 [00:16<01:36,  3.42it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.42it/s] 15%|█▌        | 60/390 [00:17<01:36,  3.43it/s] 16%|█▌        | 61/390 [00:17<01:36,  3.42it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.42it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.42it/s] 16%|█▋        | 64/390 [00:18<01:35,  3.42it/s] 17%|█▋        | 65/390 [00:18<01:34,  3.42it/s] 17%|█▋        | 66/390 [00:19<01:34,  3.42it/s] 17%|█▋        | 67/390 [00:19<01:34,  3.42it/s] 17%|█▋        | 68/390 [00:19<01:34,  3.42it/s] 18%|█▊        | 69/390 [00:20<01:33,  3.42it/s] 18%|█▊        | 70/390 [00:20<01:33,  3.42it/s] 18%|█▊        | 71/390 [00:20<01:33,  3.42it/s] 18%|█▊        | 72/390 [00:21<01:32,  3.42it/s] 19%|█▊        | 73/390 [00:21<01:32,  3.42it/s] 19%|█▉        | 74/390 [00:21<01:32,  3.42it/s] 19%|█▉        | 75/390 [00:21<01:32,  3.42it/s] 19%|█▉        | 76/390 [00:22<01:31,  3.42it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.42it/s] 20%|██        | 78/390 [00:22<01:31,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 14:28:07,718 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:28:07,718 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 14:28:07,718 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.53it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.13it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.41it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.68it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.13it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.70it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.51it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.00it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.05it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.12it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.19it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.28it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.36it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.35it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.17it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.10it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.98it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.88it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.03it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.10it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.15it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.20it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.20it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.23it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.07it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.01it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.83it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.99it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.09it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.08it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.18it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.21it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.22it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.04it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.03it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.94it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.96it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.09it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.12it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.20it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.21it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.08it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.06it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.99it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.94it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.91it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.10it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.10it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.21it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.16it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.09it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.08it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.93it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.98it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.98it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.88it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.03it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.04it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.13it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.04it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.04it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.95it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.04it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.01it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.02it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.08it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.98it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.10it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.10it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.11it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.99it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.03it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.94it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.94it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.98it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.90it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.07it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.09it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.07it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.01it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.00it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.05it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.00it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.10it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.99it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.05it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:32<01:31,  3.42it/s]
100%|██████████| 436/436 [00:09<00:00, 46.05it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:28:17,192 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 14:28:17,212 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:28:19,437 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:28:19,449 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:28:19,466 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:39<27:27,  5.30s/it] 21%|██        | 80/390 [00:40<19:36,  3.80s/it] 21%|██        | 81/390 [00:40<14:08,  2.75s/it] 21%|██        | 82/390 [00:40<10:18,  2.01s/it] 21%|██▏       | 83/390 [00:40<07:38,  1.49s/it] 22%|██▏       | 84/390 [00:41<05:46,  1.13s/it] 22%|██▏       | 85/390 [00:41<04:28,  1.13it/s] 22%|██▏       | 86/390 [00:41<03:34,  1.42it/s] 22%|██▏       | 87/390 [00:42<02:55,  1.72it/s] 23%|██▎       | 88/390 [00:42<02:29,  2.02it/s] 23%|██▎       | 89/390 [00:42<02:10,  2.31it/s] 23%|██▎       | 90/390 [00:42<01:57,  2.56it/s] 23%|██▎       | 91/390 [00:43<01:48,  2.77it/s] 24%|██▎       | 92/390 [00:43<01:41,  2.93it/s] 24%|██▍       | 93/390 [00:43<01:36,  3.07it/s] 24%|██▍       | 94/390 [00:44<01:33,  3.16it/s] 24%|██▍       | 95/390 [00:44<01:31,  3.24it/s] 25%|██▍       | 96/390 [00:44<01:29,  3.29it/s] 25%|██▍       | 97/390 [00:45<01:28,  3.33it/s] 25%|██▌       | 98/390 [00:45<01:26,  3.36it/s] 25%|██▌       | 99/390 [00:45<01:26,  3.38it/s] 26%|██▌       | 100/390 [00:45<01:25,  3.39it/s] 26%|██▌       | 101/390 [00:46<01:24,  3.40it/s] 26%|██▌       | 102/390 [00:46<01:24,  3.40it/s] 26%|██▋       | 103/390 [00:46<01:24,  3.41it/s] 27%|██▋       | 104/390 [00:47<01:24,  3.40it/s] 27%|██▋       | 105/390 [00:47<01:23,  3.40it/s] 27%|██▋       | 106/390 [00:47<01:23,  3.40it/s] 27%|██▋       | 107/390 [00:47<01:23,  3.41it/s] 28%|██▊       | 108/390 [00:48<01:22,  3.41it/s] 28%|██▊       | 109/390 [00:48<01:22,  3.41it/s] 28%|██▊       | 110/390 [00:48<01:21,  3.42it/s] 28%|██▊       | 111/390 [00:49<01:21,  3.42it/s] 29%|██▊       | 112/390 [00:49<01:21,  3.42it/s] 29%|██▉       | 113/390 [00:49<01:21,  3.42it/s] 29%|██▉       | 114/390 [00:50<01:20,  3.42it/s] 29%|██▉       | 115/390 [00:50<01:20,  3.40it/s] 30%|██▉       | 116/390 [00:50<01:20,  3.41it/s] 30%|███       | 117/390 [00:50<01:20,  3.41it/s] 30%|███       | 118/390 [00:51<01:19,  3.41it/s] 31%|███       | 119/390 [00:51<01:19,  3.41it/s] 31%|███       | 120/390 [00:51<01:19,  3.41it/s] 31%|███       | 121/390 [00:52<01:18,  3.42it/s] 31%|███▏      | 122/390 [00:52<01:21,  3.30it/s] 32%|███▏      | 123/390 [00:52<01:20,  3.34it/s] 32%|███▏      | 124/390 [00:52<01:19,  3.36it/s] 32%|███▏      | 125/390 [00:53<01:18,  3.38it/s] 32%|███▏      | 126/390 [00:53<01:18,  3.37it/s] 33%|███▎      | 127/390 [00:53<01:17,  3.39it/s] 33%|███▎      | 128/390 [00:54<01:17,  3.39it/s] 33%|███▎      | 129/390 [00:54<01:16,  3.40it/s] 33%|███▎      | 130/390 [00:54<01:16,  3.41it/s] 34%|███▎      | 131/390 [00:55<01:15,  3.41it/s] 34%|███▍      | 132/390 [00:55<01:15,  3.41it/s] 34%|███▍      | 133/390 [00:55<01:15,  3.41it/s] 34%|███▍      | 134/390 [00:55<01:15,  3.41it/s] 35%|███▍      | 135/390 [00:56<01:14,  3.41it/s] 35%|███▍      | 136/390 [00:56<01:14,  3.41it/s] 35%|███▌      | 137/390 [00:56<01:14,  3.39it/s] 35%|███▌      | 138/390 [00:57<01:14,  3.40it/s] 36%|███▌      | 139/390 [00:57<01:13,  3.41it/s] 36%|███▌      | 140/390 [00:57<01:13,  3.41it/s] 36%|███▌      | 141/390 [00:57<01:12,  3.41it/s] 36%|███▋      | 142/390 [00:58<01:12,  3.41it/s] 37%|███▋      | 143/390 [00:58<01:12,  3.41it/s] 37%|███▋      | 144/390 [00:58<01:12,  3.41it/s] 37%|███▋      | 145/390 [00:59<01:11,  3.41it/s] 37%|███▋      | 146/390 [00:59<01:11,  3.41it/s] 38%|███▊      | 147/390 [00:59<01:11,  3.41it/s] 38%|███▊      | 148/390 [01:00<01:11,  3.38it/s] 38%|███▊      | 149/390 [01:00<01:11,  3.39it/s] 38%|███▊      | 150/390 [01:00<01:10,  3.39it/s] 39%|███▊      | 151/390 [01:00<01:10,  3.40it/s] 39%|███▉      | 152/390 [01:01<01:09,  3.40it/s] 39%|███▉      | 153/390 [01:01<01:09,  3.40it/s] 39%|███▉      | 154/390 [01:01<01:09,  3.41it/s] 40%|███▉      | 155/390 [01:02<01:08,  3.41it/s] 40%|████      | 156/390 [01:02<01:08,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 14:28:47,295 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:28:47,295 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 14:28:47,295 >>   Batch size = 8
{'eval_loss': 0.9708475470542908, 'eval_runtime': 9.4621, 'eval_samples_per_second': 368.416, 'eval_steps_per_second': 46.078, 'epoch': 0.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.05it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.82it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.10it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.22it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.84it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.51it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.34it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.94it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.88it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.95it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.03it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.00it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.11it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.02it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.08it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.95it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.77it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.75it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.87it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.89it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.97it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.05it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.01it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.05it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.92it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.87it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.80it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.85it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.87it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.97it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.00it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.00it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.97it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.89it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.90it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.89it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.95it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.88it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.99it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.03it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.95it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.92it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.92it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.87it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.86it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.88it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.87it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.99it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.95it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.93it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.90it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.89it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.92it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.91it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.92it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.86it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.88it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.96it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.88it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.80it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.82it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.91it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.81it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.80it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.77it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.94it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.00it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.95it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.88it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.81it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.88it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.82it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.81it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.76it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.83it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.86it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.91it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.95it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.87it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.89it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.93it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.93it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.91it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.87it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.93it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.91it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:11<01:08,  3.41it/s]
100%|██████████| 436/436 [00:09<00:00, 45.91it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:28:56,833 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 14:28:56,857 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:28:59,310 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:28:59,322 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:28:59,330 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:19<21:02,  5.42s/it] 41%|████      | 158/390 [01:20<15:00,  3.88s/it] 41%|████      | 159/390 [01:20<10:47,  2.80s/it] 41%|████      | 160/390 [01:20<07:51,  2.05s/it] 41%|████▏     | 161/390 [01:20<05:48,  1.52s/it] 42%|████▏     | 162/390 [01:21<04:23,  1.15s/it] 42%|████▏     | 163/390 [01:21<03:23,  1.12it/s] 42%|████▏     | 164/390 [01:21<02:41,  1.40it/s] 42%|████▏     | 165/390 [01:22<02:12,  1.70it/s] 43%|████▎     | 166/390 [01:22<01:51,  2.00it/s] 43%|████▎     | 167/390 [01:22<01:37,  2.29it/s] 43%|████▎     | 168/390 [01:22<01:27,  2.54it/s] 43%|████▎     | 169/390 [01:23<01:20,  2.75it/s] 44%|████▎     | 170/390 [01:23<01:15,  2.92it/s] 44%|████▍     | 171/390 [01:23<01:11,  3.06it/s] 44%|████▍     | 172/390 [01:24<01:09,  3.16it/s] 44%|████▍     | 173/390 [01:24<01:07,  3.23it/s] 45%|████▍     | 174/390 [01:24<01:05,  3.29it/s] 45%|████▍     | 175/390 [01:25<01:04,  3.33it/s] 45%|████▌     | 176/390 [01:25<01:03,  3.35it/s] 45%|████▌     | 177/390 [01:25<01:03,  3.36it/s] 46%|████▌     | 178/390 [01:25<01:02,  3.38it/s] 46%|████▌     | 179/390 [01:26<01:02,  3.39it/s] 46%|████▌     | 180/390 [01:26<01:01,  3.40it/s] 46%|████▋     | 181/390 [01:26<01:01,  3.41it/s] 47%|████▋     | 182/390 [01:27<01:00,  3.41it/s] 47%|████▋     | 183/390 [01:27<01:00,  3.41it/s] 47%|████▋     | 184/390 [01:27<01:00,  3.40it/s] 47%|████▋     | 185/390 [01:27<01:00,  3.41it/s] 48%|████▊     | 186/390 [01:28<00:59,  3.41it/s] 48%|████▊     | 187/390 [01:28<00:59,  3.42it/s] 48%|████▊     | 188/390 [01:28<00:59,  3.42it/s] 48%|████▊     | 189/390 [01:29<00:58,  3.42it/s] 49%|████▊     | 190/390 [01:29<00:58,  3.42it/s] 49%|████▉     | 191/390 [01:29<00:58,  3.42it/s] 49%|████▉     | 192/390 [01:29<00:57,  3.42it/s] 49%|████▉     | 193/390 [01:30<00:57,  3.42it/s] 50%|████▉     | 194/390 [01:30<00:57,  3.42it/s] 50%|█████     | 195/390 [01:30<00:57,  3.40it/s] 50%|█████     | 196/390 [01:31<00:57,  3.40it/s] 51%|█████     | 197/390 [01:31<00:56,  3.41it/s] 51%|█████     | 198/390 [01:31<00:56,  3.41it/s] 51%|█████     | 199/390 [01:32<00:55,  3.41it/s] 51%|█████▏    | 200/390 [01:32<00:55,  3.42it/s] 52%|█████▏    | 201/390 [01:32<00:55,  3.42it/s] 52%|█████▏    | 202/390 [01:32<00:54,  3.42it/s] 52%|█████▏    | 203/390 [01:33<00:54,  3.42it/s] 52%|█████▏    | 204/390 [01:33<00:54,  3.42it/s] 53%|█████▎    | 205/390 [01:33<00:54,  3.42it/s] 53%|█████▎    | 206/390 [01:34<00:54,  3.41it/s] 53%|█████▎    | 207/390 [01:34<00:53,  3.41it/s] 53%|█████▎    | 208/390 [01:34<00:53,  3.41it/s] 54%|█████▎    | 209/390 [01:34<00:53,  3.41it/s] 54%|█████▍    | 210/390 [01:35<00:52,  3.41it/s] 54%|█████▍    | 211/390 [01:35<00:52,  3.42it/s] 54%|█████▍    | 212/390 [01:35<00:52,  3.42it/s] 55%|█████▍    | 213/390 [01:36<00:51,  3.42it/s] 55%|█████▍    | 214/390 [01:36<00:51,  3.42it/s] 55%|█████▌    | 215/390 [01:36<00:51,  3.42it/s] 55%|█████▌    | 216/390 [01:37<00:50,  3.42it/s] 56%|█████▌    | 217/390 [01:37<00:50,  3.40it/s] 56%|█████▌    | 218/390 [01:37<00:50,  3.40it/s] 56%|█████▌    | 219/390 [01:37<00:50,  3.40it/s] 56%|█████▋    | 220/390 [01:38<00:49,  3.41it/s] 57%|█████▋    | 221/390 [01:38<00:49,  3.41it/s] 57%|█████▋    | 222/390 [01:38<00:49,  3.41it/s] 57%|█████▋    | 223/390 [01:39<00:49,  3.41it/s] 57%|█████▋    | 224/390 [01:39<00:48,  3.41it/s] 58%|█████▊    | 225/390 [01:39<00:48,  3.39it/s] 58%|█████▊    | 226/390 [01:39<00:48,  3.40it/s] 58%|█████▊    | 227/390 [01:40<00:47,  3.40it/s] 58%|█████▊    | 228/390 [01:40<00:47,  3.40it/s] 59%|█████▊    | 229/390 [01:40<00:47,  3.40it/s] 59%|█████▉    | 230/390 [01:41<00:47,  3.40it/s] 59%|█████▉    | 231/390 [01:41<00:46,  3.41it/s] 59%|█████▉    | 232/390 [01:41<00:46,  3.41it/s] 60%|█████▉    | 233/390 [01:42<00:46,  3.41it/s] 60%|██████    | 234/390 [01:42<00:45,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 14:29:27,230 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:29:27,230 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 14:29:27,230 >>   Batch size = 8
{'eval_loss': 0.9811332821846008, 'eval_runtime': 9.495, 'eval_samples_per_second': 367.139, 'eval_steps_per_second': 45.919, 'epoch': 1.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.62it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.78it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.09it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.27it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.95it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.61it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.40it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.90it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.90it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.93it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.85it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 45.87it/s][A
 16%|█▌        | 68/436 [00:01<00:08, 45.86it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 45.99it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.98it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.87it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.74it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.81it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.86it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.87it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.93it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.89it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 45.99it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 45.93it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.94it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.80it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.89it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.83it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.90it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.94it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.83it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.90it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.96it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.94it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.89it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.87it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.88it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.85it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.87it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.88it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.91it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.87it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.87it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.86it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.96it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.81it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.86it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.79it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.84it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.89it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.95it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.83it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.89it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.89it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.91it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.96it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.78it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.80it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.90it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.91it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.92it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.91it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.89it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.91it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.95it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.79it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.80it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.84it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.84it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.90it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.89it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.89it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.84it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.88it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.74it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.85it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.90it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.88it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.95it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.01it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.90it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.77it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.86it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.78it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.86it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.86it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [01:51<00:45,  3.41it/s]
100%|██████████| 436/436 [00:09<00:00, 45.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:29:36,742 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 14:29:36,760 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:29:39,006 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:29:39,021 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:29:39,028 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [01:58<13:28,  5.21s/it] 61%|██████    | 236/390 [01:59<09:35,  3.74s/it] 61%|██████    | 237/390 [01:59<06:53,  2.70s/it] 61%|██████    | 238/390 [01:59<05:01,  1.98s/it] 61%|██████▏   | 239/390 [02:00<03:42,  1.47s/it] 62%|██████▏   | 240/390 [02:00<02:47,  1.12s/it] 62%|██████▏   | 241/390 [02:00<02:09,  1.15it/s] 62%|██████▏   | 242/390 [02:01<01:43,  1.43it/s] 62%|██████▏   | 243/390 [02:01<01:24,  1.74it/s] 63%|██████▎   | 244/390 [02:01<01:11,  2.04it/s] 63%|██████▎   | 245/390 [02:01<01:02,  2.32it/s] 63%|██████▎   | 246/390 [02:02<00:56,  2.57it/s] 63%|██████▎   | 247/390 [02:02<00:51,  2.76it/s] 64%|██████▎   | 248/390 [02:02<00:48,  2.93it/s] 64%|██████▍   | 249/390 [02:03<00:46,  3.06it/s] 64%|██████▍   | 250/390 [02:03<00:44,  3.16it/s] 64%|██████▍   | 251/390 [02:03<00:43,  3.23it/s] 65%|██████▍   | 252/390 [02:03<00:41,  3.29it/s] 65%|██████▍   | 253/390 [02:04<00:41,  3.32it/s] 65%|██████▌   | 254/390 [02:04<00:40,  3.35it/s] 65%|██████▌   | 255/390 [02:04<00:40,  3.37it/s] 66%|██████▌   | 256/390 [02:05<00:39,  3.38it/s] 66%|██████▌   | 257/390 [02:05<00:39,  3.40it/s] 66%|██████▌   | 258/390 [02:05<00:38,  3.39it/s] 66%|██████▋   | 259/390 [02:06<00:38,  3.40it/s] 67%|██████▋   | 260/390 [02:06<00:38,  3.41it/s] 67%|██████▋   | 261/390 [02:06<00:37,  3.41it/s] 67%|██████▋   | 262/390 [02:06<00:37,  3.41it/s] 67%|██████▋   | 263/390 [02:07<00:37,  3.41it/s] 68%|██████▊   | 264/390 [02:07<00:36,  3.41it/s] 68%|██████▊   | 265/390 [02:07<00:36,  3.41it/s] 68%|██████▊   | 266/390 [02:08<00:36,  3.42it/s] 68%|██████▊   | 267/390 [02:08<00:36,  3.41it/s] 69%|██████▊   | 268/390 [02:08<00:35,  3.42it/s] 69%|██████▉   | 269/390 [02:08<00:35,  3.41it/s] 69%|██████▉   | 270/390 [02:09<00:35,  3.41it/s] 69%|██████▉   | 271/390 [02:09<00:34,  3.41it/s] 70%|██████▉   | 272/390 [02:09<00:34,  3.41it/s] 70%|███████   | 273/390 [02:10<00:34,  3.41it/s] 70%|███████   | 274/390 [02:10<00:33,  3.42it/s] 71%|███████   | 275/390 [02:10<00:33,  3.42it/s] 71%|███████   | 276/390 [02:11<00:33,  3.41it/s] 71%|███████   | 277/390 [02:11<00:33,  3.42it/s] 71%|███████▏  | 278/390 [02:11<00:32,  3.41it/s] 72%|███████▏  | 279/390 [02:11<00:32,  3.42it/s] 72%|███████▏  | 280/390 [02:12<00:32,  3.38it/s] 72%|███████▏  | 281/390 [02:12<00:32,  3.39it/s] 72%|███████▏  | 282/390 [02:12<00:31,  3.40it/s] 73%|███████▎  | 283/390 [02:13<00:31,  3.41it/s] 73%|███████▎  | 284/390 [02:13<00:31,  3.41it/s] 73%|███████▎  | 285/390 [02:13<00:30,  3.41it/s] 73%|███████▎  | 286/390 [02:13<00:30,  3.41it/s] 74%|███████▎  | 287/390 [02:14<00:30,  3.42it/s] 74%|███████▍  | 288/390 [02:14<00:29,  3.41it/s] 74%|███████▍  | 289/390 [02:14<00:29,  3.41it/s] 74%|███████▍  | 290/390 [02:15<00:29,  3.41it/s] 75%|███████▍  | 291/390 [02:15<00:29,  3.38it/s] 75%|███████▍  | 292/390 [02:15<00:28,  3.39it/s] 75%|███████▌  | 293/390 [02:16<00:28,  3.40it/s] 75%|███████▌  | 294/390 [02:16<00:28,  3.40it/s] 76%|███████▌  | 295/390 [02:16<00:27,  3.40it/s] 76%|███████▌  | 296/390 [02:16<00:27,  3.40it/s] 76%|███████▌  | 297/390 [02:17<00:27,  3.40it/s] 76%|███████▋  | 298/390 [02:17<00:27,  3.40it/s] 77%|███████▋  | 299/390 [02:17<00:26,  3.40it/s] 77%|███████▋  | 300/390 [02:18<00:26,  3.40it/s] 77%|███████▋  | 301/390 [02:18<00:26,  3.40it/s] 77%|███████▋  | 302/390 [02:18<00:25,  3.39it/s] 78%|███████▊  | 303/390 [02:18<00:25,  3.39it/s] 78%|███████▊  | 304/390 [02:19<00:25,  3.40it/s] 78%|███████▊  | 305/390 [02:19<00:24,  3.40it/s] 78%|███████▊  | 306/390 [02:19<00:24,  3.40it/s] 79%|███████▊  | 307/390 [02:20<00:24,  3.41it/s] 79%|███████▉  | 308/390 [02:20<00:24,  3.41it/s] 79%|███████▉  | 309/390 [02:20<00:23,  3.41it/s] 79%|███████▉  | 310/390 [02:20<00:23,  3.41it/s] 80%|███████▉  | 311/390 [02:21<00:23,  3.41it/s] 80%|████████  | 312/390 [02:21<00:22,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 14:30:06,518 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:30:06,518 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 14:30:06,518 >>   Batch size = 8
{'eval_loss': 0.9878658056259155, 'eval_runtime': 9.501, 'eval_samples_per_second': 366.909, 'eval_steps_per_second': 45.89, 'epoch': 2.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.49it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.78it/s][A
  4%|▍         | 18/436 [00:00<00:08, 47.85it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.23it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.84it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.49it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.44it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.88it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.91it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.96it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.97it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 45.90it/s][A
 16%|█▌        | 68/436 [00:01<00:08, 45.92it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 45.99it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.97it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.89it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.79it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.80it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.86it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.91it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.88it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.87it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 45.94it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 45.90it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.95it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.81it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.75it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.70it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.78it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.82it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.88it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.84it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.91it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.91it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.82it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.87it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.84it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.53it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.68it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.80it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.76it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.88it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.83it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.87it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.84it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.82it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.79it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.87it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.86it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.86it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.88it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.96it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.85it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.93it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.77it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.89it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.81it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.85it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.88it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.93it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.84it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.85it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.86it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.77it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.67it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.75it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.76it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.83it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.91it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.89it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.87it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.87it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 45.78it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.85it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.84it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.79it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.76it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.86it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.86it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.95it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.83it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.82it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.82it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.87it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.80it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.84it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:31<00:22,  3.41it/s]
100%|██████████| 436/436 [00:09<00:00, 45.84it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:30:16,044 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 14:30:16,068 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:30:18,379 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:30:18,396 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:30:18,407 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:38<06:48,  5.31s/it] 81%|████████  | 314/390 [02:38<04:49,  3.80s/it] 81%|████████  | 315/390 [02:39<03:26,  2.75s/it] 81%|████████  | 316/390 [02:39<02:28,  2.01s/it] 81%|████████▏ | 317/390 [02:39<01:49,  1.50s/it] 82%|████████▏ | 318/390 [02:40<01:21,  1.14s/it] 82%|████████▏ | 319/390 [02:40<01:02,  1.13it/s] 82%|████████▏ | 320/390 [02:40<00:49,  1.42it/s] 82%|████████▏ | 321/390 [02:40<00:40,  1.72it/s] 83%|████████▎ | 322/390 [02:41<00:33,  2.02it/s] 83%|████████▎ | 323/390 [02:41<00:29,  2.30it/s] 83%|████████▎ | 324/390 [02:41<00:25,  2.56it/s] 83%|████████▎ | 325/390 [02:42<00:23,  2.76it/s] 84%|████████▎ | 326/390 [02:42<00:21,  2.93it/s] 84%|████████▍ | 327/390 [02:42<00:20,  3.06it/s] 84%|████████▍ | 328/390 [02:42<00:19,  3.16it/s] 84%|████████▍ | 329/390 [02:43<00:18,  3.24it/s] 85%|████████▍ | 330/390 [02:43<00:18,  3.29it/s] 85%|████████▍ | 331/390 [02:43<00:17,  3.32it/s] 85%|████████▌ | 332/390 [02:44<00:17,  3.35it/s] 85%|████████▌ | 333/390 [02:44<00:16,  3.37it/s] 86%|████████▌ | 334/390 [02:44<00:16,  3.38it/s] 86%|████████▌ | 335/390 [02:45<00:16,  3.40it/s] 86%|████████▌ | 336/390 [02:45<00:15,  3.39it/s] 86%|████████▋ | 337/390 [02:45<00:15,  3.40it/s] 87%|████████▋ | 338/390 [02:45<00:15,  3.41it/s] 87%|████████▋ | 339/390 [02:46<00:14,  3.41it/s] 87%|████████▋ | 340/390 [02:46<00:14,  3.41it/s] 87%|████████▋ | 341/390 [02:46<00:14,  3.41it/s] 88%|████████▊ | 342/390 [02:47<00:14,  3.41it/s] 88%|████████▊ | 343/390 [02:47<00:13,  3.42it/s] 88%|████████▊ | 344/390 [02:47<00:13,  3.42it/s] 88%|████████▊ | 345/390 [02:47<00:13,  3.42it/s] 89%|████████▊ | 346/390 [02:48<00:12,  3.42it/s] 89%|████████▉ | 347/390 [02:48<00:12,  3.39it/s] 89%|████████▉ | 348/390 [02:48<00:12,  3.40it/s] 89%|████████▉ | 349/390 [02:49<00:12,  3.41it/s] 90%|████████▉ | 350/390 [02:49<00:11,  3.41it/s] 90%|█████████ | 351/390 [02:49<00:11,  3.41it/s] 90%|█████████ | 352/390 [02:50<00:11,  3.42it/s] 91%|█████████ | 353/390 [02:50<00:10,  3.42it/s] 91%|█████████ | 354/390 [02:50<00:10,  3.42it/s] 91%|█████████ | 355/390 [02:50<00:10,  3.42it/s] 91%|█████████▏| 356/390 [02:51<00:09,  3.42it/s] 92%|█████████▏| 357/390 [02:51<00:09,  3.40it/s] 92%|█████████▏| 358/390 [02:51<00:09,  3.40it/s] 92%|█████████▏| 359/390 [02:52<00:09,  3.40it/s] 92%|█████████▏| 360/390 [02:52<00:08,  3.41it/s] 93%|█████████▎| 361/390 [02:52<00:08,  3.41it/s] 93%|█████████▎| 362/390 [02:52<00:08,  3.42it/s] 93%|█████████▎| 363/390 [02:53<00:07,  3.41it/s] 93%|█████████▎| 364/390 [02:53<00:07,  3.42it/s] 94%|█████████▎| 365/390 [02:53<00:07,  3.42it/s] 94%|█████████▍| 366/390 [02:54<00:07,  3.42it/s] 94%|█████████▍| 367/390 [02:54<00:06,  3.30it/s] 94%|█████████▍| 368/390 [02:54<00:06,  3.33it/s] 95%|█████████▍| 369/390 [02:55<00:06,  3.34it/s] 95%|█████████▍| 370/390 [02:55<00:05,  3.37it/s] 95%|█████████▌| 371/390 [02:55<00:05,  3.38it/s] 95%|█████████▌| 372/390 [02:55<00:05,  3.39it/s] 96%|█████████▌| 373/390 [02:56<00:05,  3.40it/s] 96%|█████████▌| 374/390 [02:56<00:04,  3.40it/s] 96%|█████████▌| 375/390 [02:56<00:04,  3.40it/s] 96%|█████████▋| 376/390 [02:57<00:04,  3.41it/s] 97%|█████████▋| 377/390 [02:57<00:03,  3.41it/s] 97%|█████████▋| 378/390 [02:57<00:03,  3.40it/s] 97%|█████████▋| 379/390 [02:57<00:03,  3.41it/s] 97%|█████████▋| 380/390 [02:58<00:02,  3.41it/s] 98%|█████████▊| 381/390 [02:58<00:02,  3.41it/s] 98%|█████████▊| 382/390 [02:58<00:02,  3.41it/s] 98%|█████████▊| 383/390 [02:59<00:02,  3.41it/s] 98%|█████████▊| 384/390 [02:59<00:01,  3.41it/s] 99%|█████████▊| 385/390 [02:59<00:01,  3.39it/s] 99%|█████████▉| 386/390 [03:00<00:01,  3.40it/s] 99%|█████████▉| 387/390 [03:00<00:00,  3.40it/s] 99%|█████████▉| 388/390 [03:00<00:00,  3.40it/s]100%|█████████▉| 389/390 [03:00<00:00,  3.41it/s]100%|██████████| 390/390 [03:01<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 14:30:46,069 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:30:46,069 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 14:30:46,069 >>   Batch size = 8
{'eval_loss': 0.9931392669677734, 'eval_runtime': 9.5073, 'eval_samples_per_second': 366.667, 'eval_steps_per_second': 45.86, 'epoch': 3.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.65it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.85it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.19it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.33it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.81it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.42it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.17it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.89it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.70it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.03it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.03it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.00it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.04it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 45.98it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.88it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.93it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.79it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.83it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.90it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.92it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.86it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.93it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 45.94it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 45.91it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.86it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.84it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.82it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.85it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.94it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.89it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.84it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.80it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.88it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.82it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.73it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.80it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.81it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.86it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.91it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.95it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.93it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.84it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.86it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.81it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.88it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.83it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.85it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.85it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.91it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.89it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.87it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.88it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.79it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.75it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.82it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.80it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.79it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.83it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.86it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.87it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.87it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.87it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.88it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.82it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.79it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.80it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.87it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.84it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.86it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.88it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.88it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.86it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.88it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.83it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.84it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.76it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.82it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.88it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.88it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.91it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.87it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.88it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.83it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.84it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.76it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.77it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:10<00:00,  3.41it/s]
100%|██████████| 436/436 [00:09<00:00, 45.77it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:30:55,593 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 14:30:55,613 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:30:57,926 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:30:57,948 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:30:57,958 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:31:03,267 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:31:03,272 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78 (score: 0.9708475470542908).
                                                 100%|██████████| 390/390 [03:20<00:00,  3.41it/s]100%|██████████| 390/390 [03:20<00:00,  1.95it/s]
[INFO|trainer.py:1894] 2023-08-28 14:31:05,063 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 14:31:05,076 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:31:07,482 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:31:07,502 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:31:07,514 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:31:07,708 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:07,709 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:07,709 >>   train_loss               =     0.7058
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:07,709 >>   train_runtime            = 0:03:20.18
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:07,709 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:07,709 >>   train_samples_per_second =    124.884
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:07,709 >>   train_steps_per_second   =      1.948
{'eval_loss': 0.9981870651245117, 'eval_runtime': 9.5047, 'eval_samples_per_second': 366.767, 'eval_steps_per_second': 45.872, 'epoch': 4.99}
{'train_runtime': 200.1861, 'train_samples_per_second': 124.884, 'train_steps_per_second': 1.948, 'train_loss': 0.7057871500651042, 'epoch': 4.99}
08/28/2023 14:31:07 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:31:07,746 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:31:07,747 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 14:31:07,747 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 57.17it/s]  3%|▎         | 12/436 [00:00<00:08, 50.20it/s]  4%|▍         | 18/436 [00:00<00:08, 48.35it/s]  5%|▌         | 23/436 [00:00<00:08, 47.68it/s]  6%|▋         | 28/436 [00:00<00:08, 47.24it/s]  8%|▊         | 33/436 [00:00<00:08, 46.96it/s]  9%|▊         | 38/436 [00:00<00:08, 46.78it/s] 10%|▉         | 43/436 [00:00<00:08, 46.46it/s] 11%|█         | 48/436 [00:01<00:08, 46.43it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.37it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.37it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.31it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.27it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.34it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.31it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.28it/s] 20%|██        | 88/436 [00:01<00:07, 46.21it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.09it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.19it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.15it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.25it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.21it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.19it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.20it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.10it/s] 31%|███       | 133/436 [00:02<00:06, 45.84it/s] 32%|███▏      | 138/436 [00:02<00:06, 45.81it/s] 33%|███▎      | 143/436 [00:03<00:06, 45.95it/s] 34%|███▍      | 148/436 [00:03<00:06, 45.99it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.08it/s] 36%|███▌      | 158/436 [00:03<00:06, 46.23it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.13it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.26it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.19it/s] 41%|████      | 178/436 [00:03<00:05, 46.06it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.01it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.08it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.14it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.17it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.18it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.23it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.14it/s] 50%|█████     | 218/436 [00:04<00:04, 46.16it/s] 51%|█████     | 223/436 [00:04<00:04, 46.04it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.08it/s] 53%|█████▎    | 233/436 [00:05<00:04, 45.97it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.17it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.20it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.26it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.11it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.13it/s] 60%|██████    | 263/436 [00:05<00:03, 46.14it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.07it/s] 63%|██████▎   | 273/436 [00:05<00:03, 46.11it/s] 64%|██████▍   | 278/436 [00:05<00:03, 46.03it/s] 65%|██████▍   | 283/436 [00:06<00:03, 46.04it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.15it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.17it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.19it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.10it/s] 71%|███████   | 308/436 [00:06<00:02, 46.10it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.08it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.04it/s] 74%|███████▍  | 323/436 [00:06<00:02, 46.04it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.03it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.14it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.12it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.18it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.14it/s] 81%|████████  | 353/436 [00:07<00:01, 46.10it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.06it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.03it/s] 84%|████████▍ | 368/436 [00:07<00:01, 45.94it/s] 86%|████████▌ | 373/436 [00:08<00:01, 45.90it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.06it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.13it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.16it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.12it/s] 91%|█████████▏| 398/436 [00:08<00:00, 45.96it/s] 92%|█████████▏| 403/436 [00:08<00:00, 45.99it/s] 94%|█████████▎| 408/436 [00:08<00:00, 45.97it/s] 95%|█████████▍| 413/436 [00:08<00:00, 46.03it/s] 96%|█████████▌| 418/436 [00:09<00:00, 45.97it/s] 97%|█████████▋| 423/436 [00:09<00:00, 46.10it/s] 98%|█████████▊| 428/436 [00:09<00:00, 45.95it/s] 99%|█████████▉| 433/436 [00:09<00:00, 46.05it/s]100%|██████████| 436/436 [00:09<00:00, 46.23it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:31:17,204 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:17,204 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:17,204 >>   eval_loss               =     0.9708
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:17,204 >>   eval_runtime            = 0:00:09.45
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:17,204 >>   eval_samples            =       3486
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:17,204 >>   eval_samples_per_second =    368.614
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:17,204 >>   eval_steps_per_second   =     46.103
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:31:17,204 >>   perplexity              =     2.6402
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:22,245 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:22,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:22,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:22,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:22,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:31:22,553 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:31:22,554 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:31:22,821 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:31:23,883 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:31:23,883 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:25,235 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:25,237 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:25,237 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:25,237 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:25,237 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:31:25,571 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:31:25,578 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:31:25,837 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:31:26,002 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:31:26,003 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'manufacturer', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13015
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13115, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.15it/s]Extractor Predicting: 2it [00:01,  1.09it/s]Extractor Predicting: 3it [00:02,  1.14it/s]Extractor Predicting: 4it [00:03,  1.14it/s]Extractor Predicting: 5it [00:04,  1.14it/s]Extractor Predicting: 6it [00:05,  1.15it/s]Extractor Predicting: 7it [00:06,  1.14it/s]Extractor Predicting: 8it [00:06,  1.16it/s]Extractor Predicting: 9it [00:07,  1.16it/s]Extractor Predicting: 10it [00:08,  1.18it/s]Extractor Predicting: 11it [00:09,  1.15it/s]Extractor Predicting: 12it [00:10,  1.16it/s]Extractor Predicting: 13it [00:11,  1.15it/s]Extractor Predicting: 14it [00:12,  1.14it/s]Extractor Predicting: 15it [00:13,  1.16it/s]Extractor Predicting: 16it [00:13,  1.16it/s]Extractor Predicting: 17it [00:14,  1.19it/s]Extractor Predicting: 18it [00:15,  1.20it/s]Extractor Predicting: 19it [00:16,  1.19it/s]Extractor Predicting: 20it [00:17,  1.18it/s]Extractor Predicting: 21it [00:18,  1.19it/s]Extractor Predicting: 22it [00:18,  1.21it/s]Extractor Predicting: 23it [00:19,  1.21it/s]Extractor Predicting: 24it [00:20,  1.17it/s]Extractor Predicting: 25it [00:21,  1.17it/s]Extractor Predicting: 26it [00:22,  1.15it/s]Extractor Predicting: 27it [00:23,  1.16it/s]Extractor Predicting: 28it [00:24,  1.17it/s]Extractor Predicting: 29it [00:24,  1.15it/s]Extractor Predicting: 30it [00:25,  1.13it/s]Extractor Predicting: 31it [00:26,  1.15it/s]Extractor Predicting: 32it [00:27,  1.14it/s]Extractor Predicting: 33it [00:28,  1.14it/s]Extractor Predicting: 34it [00:29,  1.16it/s]Extractor Predicting: 35it [00:30,  1.17it/s]Extractor Predicting: 36it [00:30,  1.18it/s]Extractor Predicting: 37it [00:31,  1.20it/s]Extractor Predicting: 38it [00:32,  1.22it/s]Extractor Predicting: 39it [00:33,  1.22it/s]Extractor Predicting: 40it [00:34,  1.24it/s]Extractor Predicting: 41it [00:34,  1.23it/s]Extractor Predicting: 42it [00:35,  1.22it/s]Extractor Predicting: 43it [00:36,  1.20it/s]Extractor Predicting: 44it [00:37,  1.19it/s]Extractor Predicting: 45it [00:38,  1.23it/s]Extractor Predicting: 46it [00:39,  1.22it/s]Extractor Predicting: 47it [00:39,  1.22it/s]Extractor Predicting: 48it [00:40,  1.22it/s]Extractor Predicting: 49it [00:41,  1.24it/s]Extractor Predicting: 50it [00:42,  1.23it/s]Extractor Predicting: 51it [00:43,  1.22it/s]Extractor Predicting: 52it [00:44,  1.20it/s]Extractor Predicting: 53it [00:44,  1.20it/s]Extractor Predicting: 54it [00:45,  1.19it/s]Extractor Predicting: 55it [00:46,  1.23it/s]Extractor Predicting: 56it [00:47,  1.24it/s]Extractor Predicting: 57it [00:48,  1.21it/s]Extractor Predicting: 58it [00:48,  1.21it/s]Extractor Predicting: 59it [00:49,  1.22it/s]Extractor Predicting: 60it [00:50,  1.20it/s]Extractor Predicting: 61it [00:51,  1.19it/s]Extractor Predicting: 62it [00:52,  1.20it/s]Extractor Predicting: 63it [00:53,  1.20it/s]Extractor Predicting: 64it [00:53,  1.21it/s]Extractor Predicting: 65it [00:54,  1.20it/s]Extractor Predicting: 66it [00:55,  1.21it/s]Extractor Predicting: 67it [00:56,  1.23it/s]Extractor Predicting: 68it [00:57,  1.20it/s]Extractor Predicting: 69it [00:58,  1.20it/s]Extractor Predicting: 70it [00:58,  1.20it/s]Extractor Predicting: 71it [00:59,  1.13it/s]Extractor Predicting: 72it [01:00,  1.16it/s]Extractor Predicting: 73it [01:01,  1.17it/s]Extractor Predicting: 74it [01:02,  1.19it/s]Extractor Predicting: 75it [01:03,  1.17it/s]Extractor Predicting: 76it [01:04,  1.18it/s]Extractor Predicting: 77it [01:04,  1.18it/s]Extractor Predicting: 78it [01:05,  1.19it/s]Extractor Predicting: 79it [01:06,  1.19it/s]Extractor Predicting: 80it [01:07,  1.21it/s]Extractor Predicting: 81it [01:08,  1.20it/s]Extractor Predicting: 82it [01:09,  1.21it/s]Extractor Predicting: 83it [01:10,  1.17it/s]Extractor Predicting: 84it [01:10,  1.19it/s]Extractor Predicting: 85it [01:11,  1.18it/s]Extractor Predicting: 86it [01:12,  1.18it/s]Extractor Predicting: 87it [01:13,  1.18it/s]Extractor Predicting: 88it [01:14,  1.22it/s]Extractor Predicting: 89it [01:14,  1.22it/s]Extractor Predicting: 90it [01:15,  1.26it/s]Extractor Predicting: 91it [01:16,  1.28it/s]Extractor Predicting: 92it [01:17,  1.30it/s]Extractor Predicting: 93it [01:18,  1.27it/s]Extractor Predicting: 94it [01:18,  1.30it/s]Extractor Predicting: 95it [01:19,  1.29it/s]Extractor Predicting: 96it [01:20,  1.28it/s]Extractor Predicting: 97it [01:21,  1.28it/s]Extractor Predicting: 98it [01:21,  1.24it/s]Extractor Predicting: 99it [01:22,  1.22it/s]Extractor Predicting: 100it [01:23,  1.22it/s]Extractor Predicting: 101it [01:24,  1.27it/s]Extractor Predicting: 102it [01:25,  1.29it/s]Extractor Predicting: 103it [01:25,  1.28it/s]Extractor Predicting: 104it [01:26,  1.27it/s]Extractor Predicting: 105it [01:27,  1.28it/s]Extractor Predicting: 106it [01:28,  1.28it/s]Extractor Predicting: 107it [01:29,  1.28it/s]Extractor Predicting: 108it [01:29,  1.27it/s]Extractor Predicting: 109it [01:30,  1.28it/s]Extractor Predicting: 110it [01:31,  1.27it/s]Extractor Predicting: 111it [01:32,  1.27it/s]Extractor Predicting: 112it [01:32,  1.31it/s]Extractor Predicting: 113it [01:33,  1.33it/s]Extractor Predicting: 114it [01:34,  1.31it/s]Extractor Predicting: 115it [01:35,  1.30it/s]Extractor Predicting: 116it [01:35,  1.29it/s]Extractor Predicting: 117it [01:36,  1.30it/s]Extractor Predicting: 118it [01:37,  1.32it/s]Extractor Predicting: 119it [01:38,  1.30it/s]Extractor Predicting: 120it [01:39,  1.28it/s]Extractor Predicting: 121it [01:39,  1.33it/s]Extractor Predicting: 122it [01:40,  1.32it/s]Extractor Predicting: 123it [01:41,  1.27it/s]Extractor Predicting: 124it [01:42,  1.30it/s]Extractor Predicting: 125it [01:42,  1.30it/s]Extractor Predicting: 126it [01:43,  1.35it/s]Extractor Predicting: 127it [01:44,  1.34it/s]Extractor Predicting: 128it [01:45,  1.33it/s]Extractor Predicting: 129it [01:45,  1.30it/s]Extractor Predicting: 130it [01:46,  1.32it/s]Extractor Predicting: 131it [01:47,  1.34it/s]Extractor Predicting: 132it [01:48,  1.33it/s]Extractor Predicting: 133it [01:48,  1.30it/s]Extractor Predicting: 134it [01:49,  1.32it/s]Extractor Predicting: 135it [01:50,  1.33it/s]Extractor Predicting: 136it [01:51,  1.34it/s]Extractor Predicting: 137it [01:51,  1.32it/s]Extractor Predicting: 138it [01:52,  1.37it/s]Extractor Predicting: 139it [01:53,  1.38it/s]Extractor Predicting: 140it [01:54,  1.38it/s]Extractor Predicting: 141it [01:54,  1.41it/s]Extractor Predicting: 142it [01:55,  1.38it/s]Extractor Predicting: 143it [01:56,  1.35it/s]Extractor Predicting: 144it [01:56,  1.65it/s]Extractor Predicting: 144it [01:56,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:33:30,245 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:33:30,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:33:30,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:33:30,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:33:30,252 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:33:30,862 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:33:30,863 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:33:31,451 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:33:32,488 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:33:32,488 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:33:35,335 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:33:35,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:33:35,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:33:35,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:33:35,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:33:35,999 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:33:36,000 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:33:36,582 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:33:36,737 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:33:36,737 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5440414507772021,
  "recall": 0.15060240963855423,
  "score": 0.23590204448438556,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13522
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13622, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.32it/s]Extractor Predicting: 3it [00:02,  1.28it/s]Extractor Predicting: 4it [00:03,  1.31it/s]Extractor Predicting: 5it [00:03,  1.29it/s]Extractor Predicting: 6it [00:04,  1.26it/s]Extractor Predicting: 7it [00:05,  1.26it/s]Extractor Predicting: 8it [00:06,  1.29it/s]Extractor Predicting: 9it [00:06,  1.29it/s]Extractor Predicting: 10it [00:07,  1.31it/s]Extractor Predicting: 11it [00:08,  1.29it/s]Extractor Predicting: 12it [00:09,  1.29it/s]Extractor Predicting: 13it [00:10,  1.26it/s]Extractor Predicting: 14it [00:10,  1.29it/s]Extractor Predicting: 15it [00:11,  1.28it/s]Extractor Predicting: 16it [00:12,  1.25it/s]Extractor Predicting: 17it [00:13,  1.25it/s]Extractor Predicting: 18it [00:14,  1.25it/s]Extractor Predicting: 19it [00:14,  1.26it/s]Extractor Predicting: 20it [00:15,  1.25it/s]Extractor Predicting: 21it [00:16,  1.26it/s]Extractor Predicting: 22it [00:17,  1.27it/s]Extractor Predicting: 23it [00:18,  1.25it/s]Extractor Predicting: 24it [00:18,  1.24it/s]Extractor Predicting: 25it [00:19,  1.24it/s]Extractor Predicting: 26it [00:20,  1.23it/s]Extractor Predicting: 27it [00:21,  1.24it/s]Extractor Predicting: 28it [00:22,  1.25it/s]Extractor Predicting: 29it [00:22,  1.25it/s]Extractor Predicting: 30it [00:23,  1.22it/s]Extractor Predicting: 31it [00:24,  1.25it/s]Extractor Predicting: 32it [00:25,  1.23it/s]Extractor Predicting: 33it [00:26,  1.23it/s]Extractor Predicting: 34it [00:26,  1.26it/s]Extractor Predicting: 35it [00:27,  1.24it/s]Extractor Predicting: 36it [00:28,  1.28it/s]Extractor Predicting: 37it [00:29,  1.25it/s]Extractor Predicting: 38it [00:30,  1.23it/s]Extractor Predicting: 39it [00:30,  1.23it/s]Extractor Predicting: 40it [00:31,  1.22it/s]Extractor Predicting: 41it [00:32,  1.21it/s]Extractor Predicting: 42it [00:33,  1.22it/s]Extractor Predicting: 43it [00:34,  1.22it/s]Extractor Predicting: 44it [00:35,  1.20it/s]Extractor Predicting: 45it [00:35,  1.22it/s]Extractor Predicting: 46it [00:36,  1.22it/s]Extractor Predicting: 47it [00:37,  1.23it/s]Extractor Predicting: 48it [00:38,  1.24it/s]Extractor Predicting: 49it [00:39,  1.26it/s]Extractor Predicting: 50it [00:39,  1.26it/s]Extractor Predicting: 51it [00:40,  1.29it/s]Extractor Predicting: 52it [00:41,  1.29it/s]Extractor Predicting: 53it [00:42,  1.28it/s]Extractor Predicting: 54it [00:43,  1.28it/s]Extractor Predicting: 55it [00:43,  1.27it/s]Extractor Predicting: 56it [00:44,  1.25it/s]Extractor Predicting: 57it [00:45,  1.21it/s]Extractor Predicting: 58it [00:46,  1.19it/s]Extractor Predicting: 59it [00:47,  1.20it/s]Extractor Predicting: 60it [00:48,  1.22it/s]Extractor Predicting: 61it [00:48,  1.20it/s]Extractor Predicting: 62it [00:49,  1.19it/s]Extractor Predicting: 63it [00:50,  1.24it/s]Extractor Predicting: 64it [00:51,  1.23it/s]Extractor Predicting: 65it [00:52,  1.25it/s]Extractor Predicting: 66it [00:52,  1.23it/s]Extractor Predicting: 67it [00:53,  1.23it/s]Extractor Predicting: 68it [00:54,  1.22it/s]Extractor Predicting: 69it [00:55,  1.21it/s]Extractor Predicting: 70it [00:56,  1.16it/s]Extractor Predicting: 71it [00:57,  1.18it/s]Extractor Predicting: 72it [00:57,  1.18it/s]Extractor Predicting: 73it [00:58,  1.18it/s]Extractor Predicting: 74it [00:59,  1.19it/s]Extractor Predicting: 75it [01:00,  1.17it/s]Extractor Predicting: 76it [01:01,  1.20it/s]Extractor Predicting: 77it [01:02,  1.18it/s]Extractor Predicting: 78it [01:03,  1.18it/s]Extractor Predicting: 79it [01:03,  1.19it/s]Extractor Predicting: 80it [01:04,  1.19it/s]Extractor Predicting: 81it [01:05,  1.13it/s]Extractor Predicting: 82it [01:06,  1.12it/s]Extractor Predicting: 83it [01:07,  1.13it/s]Extractor Predicting: 84it [01:08,  1.16it/s]Extractor Predicting: 85it [01:09,  1.18it/s]Extractor Predicting: 86it [01:09,  1.19it/s]Extractor Predicting: 87it [01:10,  1.17it/s]Extractor Predicting: 88it [01:11,  1.22it/s]Extractor Predicting: 89it [01:12,  1.22it/s]Extractor Predicting: 90it [01:13,  1.21it/s]Extractor Predicting: 91it [01:13,  1.25it/s]Extractor Predicting: 92it [01:14,  1.25it/s]Extractor Predicting: 93it [01:15,  1.27it/s]Extractor Predicting: 94it [01:16,  1.26it/s]Extractor Predicting: 95it [01:17,  1.25it/s]Extractor Predicting: 96it [01:17,  1.24it/s]Extractor Predicting: 97it [01:18,  1.22it/s]Extractor Predicting: 98it [01:19,  1.22it/s]Extractor Predicting: 99it [01:20,  1.22it/s]Extractor Predicting: 100it [01:21,  1.22it/s]Extractor Predicting: 101it [01:22,  1.21it/s]Extractor Predicting: 102it [01:23,  1.18it/s]Extractor Predicting: 103it [01:23,  1.19it/s]Extractor Predicting: 104it [01:24,  1.19it/s]Extractor Predicting: 105it [01:25,  1.22it/s]Extractor Predicting: 106it [01:26,  1.23it/s]Extractor Predicting: 107it [01:27,  1.22it/s]Extractor Predicting: 108it [01:27,  1.23it/s]Extractor Predicting: 109it [01:28,  1.22it/s]Extractor Predicting: 110it [01:29,  1.20it/s]Extractor Predicting: 111it [01:30,  1.22it/s]Extractor Predicting: 112it [01:31,  1.21it/s]Extractor Predicting: 113it [01:31,  1.24it/s]Extractor Predicting: 114it [01:32,  1.22it/s]Extractor Predicting: 115it [01:33,  1.24it/s]Extractor Predicting: 116it [01:34,  1.19it/s]Extractor Predicting: 117it [01:35,  1.17it/s]Extractor Predicting: 118it [01:36,  1.17it/s]Extractor Predicting: 119it [01:37,  1.17it/s]Extractor Predicting: 120it [01:37,  1.17it/s]Extractor Predicting: 121it [01:38,  1.17it/s]Extractor Predicting: 122it [01:39,  1.20it/s]Extractor Predicting: 123it [01:40,  1.19it/s]Extractor Predicting: 124it [01:41,  1.17it/s]Extractor Predicting: 125it [01:42,  1.16it/s]Extractor Predicting: 126it [01:43,  1.13it/s]Extractor Predicting: 127it [01:44,  1.13it/s]Extractor Predicting: 128it [01:44,  1.14it/s]Extractor Predicting: 129it [01:45,  1.13it/s]Extractor Predicting: 130it [01:46,  1.16it/s]Extractor Predicting: 131it [01:47,  1.17it/s]Extractor Predicting: 132it [01:48,  1.16it/s]Extractor Predicting: 133it [01:49,  1.16it/s]Extractor Predicting: 134it [01:50,  1.17it/s]Extractor Predicting: 135it [01:50,  1.19it/s]Extractor Predicting: 136it [01:51,  1.21it/s]Extractor Predicting: 137it [01:52,  1.23it/s]Extractor Predicting: 138it [01:53,  1.22it/s]Extractor Predicting: 139it [01:54,  1.22it/s]Extractor Predicting: 140it [01:54,  1.21it/s]Extractor Predicting: 141it [01:55,  1.17it/s]Extractor Predicting: 142it [01:56,  1.18it/s]Extractor Predicting: 143it [01:57,  1.27it/s]Extractor Predicting: 143it [01:57,  1.22it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:41,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:41,137 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:41,137 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:41,137 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:41,137 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:35:41,764 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:35:41,765 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:35:42,333 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:35:43,366 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:35:43,367 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:46,249 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:46,252 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:46,252 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:46,252 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:46,253 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:35:46,922 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:35:46,923 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:35:47,497 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:35:47,660 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:35:47,660 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.47862232779097386,
  "recall": 0.11776738749269433,
  "score": 0.18902439024390247,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 464
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 564, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.03it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 2it [00:01,  1.36it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:35:49,524 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:35:49,525 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:35:49,529 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:35:49,530 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:35:49,533 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:35:52,700 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:35:52,704 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:35:52,725 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:35:52,726 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:35:52,734 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:35:52,736 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:35:52,736 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:35:52,736 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:35:52,736 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:35:52,736 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:35:52,736 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 1.0,
  "recall": 0.01282051282051282,
  "score": 0.02531645569620253,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:35:52,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:53,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:54,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:55,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:56,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:57,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:58,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:59,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:00,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:01,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:02,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:02,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:03,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:04,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:05,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:06,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:07,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:08,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:09,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:09,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:10,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:11,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:12,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:13,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:21<03:14, 21.64s/it][WARNING|generation_utils.py:914] 2023-08-28 14:36:14,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:15,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:16,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:17,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:18,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:19,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:20,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:21,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:22,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:23,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:24,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:25,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:26,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:27,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:27,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:28,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:29,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:30,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:31,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:32,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:33,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:41<02:44, 20.56s/it][WARNING|generation_utils.py:914] 2023-08-28 14:36:34,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:35,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:36,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:37,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:38,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:39,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:39,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:40,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:41,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:42,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:43,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:44,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:45,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:45,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:46,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:47,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:48,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:49,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:50,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:51,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:51,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:52,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:53,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:01<02:22, 20.42s/it][WARNING|generation_utils.py:914] 2023-08-28 14:36:54,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:55,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:56,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:57,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:58,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:58,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:59,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:00,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:01,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:02,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:03,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:04,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:05,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:06,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:07,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:08,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:09,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:10,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:11,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:12,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:20<01:58, 19.74s/it][WARNING|generation_utils.py:914] 2023-08-28 14:37:13,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:14,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:15,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:15,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:16,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:17,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:18,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:19,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:20,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:21,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:22,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:22,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:23,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:24,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:25,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:26,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:26,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:27,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:28,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:29,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:30,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:31,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:32,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:33,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:34,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:35,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:35,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:36,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:44<01:46, 21.39s/it][WARNING|generation_utils.py:914] 2023-08-28 14:37:37,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:38,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:39,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:40,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:41,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:42,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:42,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:44,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:44,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:45,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:47,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:47,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:48,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:49,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:50,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:51,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:52,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:53,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:54,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:55,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:55,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:56,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:04<01:23, 20.94s/it][WARNING|generation_utils.py:914] 2023-08-28 14:37:57,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:58,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:59,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:00,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:01,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:02,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:03,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:04,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:05,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:05,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:06,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:07,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:08,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:09,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:10,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:11,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:12,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:13,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:14,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:15,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:16,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:24<01:01, 20.39s/it][WARNING|generation_utils.py:914] 2023-08-28 14:38:17,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:18,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:19,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:20,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:21,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:22,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:22,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:24,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:24,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:25,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:26,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:27,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:28,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:29,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:30,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:31,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:32,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:33,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:34,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:35,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:36,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:37,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:45<00:41, 20.74s/it][WARNING|generation_utils.py:914] 2023-08-28 14:38:38,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:39,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:40,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:41,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:42,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:43,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:44,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:45,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:46,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:47,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:48,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:49,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:50,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:51,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:52,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:53,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:54,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:55,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:55,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:56,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:57,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:05<00:20, 20.56s/it][WARNING|generation_utils.py:914] 2023-08-28 14:38:58,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:59,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:00,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:01,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:02,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:03,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:04,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:05,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:06,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:07,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:09,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:09,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:10,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:12,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:13,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:14,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:15,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:16,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:17,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:18,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:39:19,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:27<00:00, 21.00s/it]Generating: 100%|██████████| 10/10 [03:27<00:00, 20.77s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:26,868 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:26,878 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:26,879 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:26,879 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:26,879 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:39:27,510 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:39:27,512 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:39:28,092 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:39:29,168 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:39:29,168 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:32,011 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:32,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:32,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:32,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:32,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:39:32,663 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:39:32,664 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:39:33,213 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:39:33,378 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:39:33,378 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 565, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : country .', 'success_rate': 0.8059895833333334, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : followed by .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 209, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 258, 'raw': 384}
{'target': 600, 'success': 278, 'raw': 416}
{'target': 600, 'success': 300, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 347, 'raw': 512}
{'target': 600, 'success': 367, 'raw': 544}
{'target': 600, 'success': 392, 'raw': 576}
{'target': 600, 'success': 411, 'raw': 608}
{'target': 600, 'success': 431, 'raw': 640}
{'target': 600, 'success': 446, 'raw': 672}
{'target': 600, 'success': 470, 'raw': 704}
{'target': 600, 'success': 494, 'raw': 736}
{'target': 600, 'success': 516, 'raw': 768}
{'target': 600, 'success': 540, 'raw': 800}
{'target': 600, 'success': 564, 'raw': 832}
{'target': 600, 'success': 585, 'raw': 864}
{'target': 600, 'success': 605, 'raw': 896}
{'prompt': 'Relation : voice type .', 'success_rate': 0.6752232142857143, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8877840909090909, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9166666666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8892045454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 9369
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9469, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.39it/s]Extractor Estimating: 2it [00:01,  1.22it/s]Extractor Estimating: 3it [00:02,  1.26it/s]Extractor Estimating: 4it [00:03,  1.31it/s]Extractor Estimating: 5it [00:03,  1.26it/s]Extractor Estimating: 6it [00:04,  1.28it/s]Extractor Estimating: 7it [00:05,  1.29it/s]Extractor Estimating: 8it [00:06,  1.27it/s]Extractor Estimating: 9it [00:07,  1.19it/s]Extractor Estimating: 10it [00:07,  1.25it/s]Extractor Estimating: 11it [00:08,  1.28it/s]Extractor Estimating: 12it [00:09,  1.30it/s]Extractor Estimating: 13it [00:10,  1.28it/s]Extractor Estimating: 14it [00:11,  1.27it/s]Extractor Estimating: 15it [00:11,  1.31it/s]Extractor Estimating: 16it [00:12,  1.32it/s]Extractor Estimating: 17it [00:13,  1.31it/s]Extractor Estimating: 18it [00:13,  1.32it/s]Extractor Estimating: 19it [00:14,  1.30it/s]Extractor Estimating: 20it [00:15,  1.30it/s]Extractor Estimating: 21it [00:16,  1.29it/s]Extractor Estimating: 22it [00:17,  1.28it/s]Extractor Estimating: 23it [00:17,  1.28it/s]Extractor Estimating: 24it [00:18,  1.25it/s]Extractor Estimating: 25it [00:19,  1.21it/s]Extractor Estimating: 26it [00:20,  1.19it/s]Extractor Estimating: 27it [00:21,  1.17it/s]Extractor Estimating: 28it [00:22,  1.22it/s]Extractor Estimating: 29it [00:23,  1.14it/s]Extractor Estimating: 30it [00:23,  1.17it/s]Extractor Estimating: 31it [00:24,  1.15it/s]Extractor Estimating: 32it [00:25,  1.17it/s]Extractor Estimating: 33it [00:26,  1.14it/s]Extractor Estimating: 34it [00:27,  1.17it/s]Extractor Estimating: 35it [00:28,  1.17it/s]Extractor Estimating: 36it [00:29,  1.21it/s]Extractor Estimating: 37it [00:29,  1.20it/s]Extractor Estimating: 38it [00:30,  1.21it/s]Extractor Estimating: 39it [00:31,  1.21it/s]Extractor Estimating: 40it [00:32,  1.22it/s]Extractor Estimating: 41it [00:33,  1.20it/s]Extractor Estimating: 42it [00:34,  1.21it/s]Extractor Estimating: 43it [00:34,  1.19it/s]Extractor Estimating: 44it [00:35,  1.17it/s]Extractor Estimating: 45it [00:36,  1.19it/s]Extractor Estimating: 46it [00:37,  1.22it/s]Extractor Estimating: 47it [00:38,  1.22it/s]Extractor Estimating: 48it [00:38,  1.22it/s]Extractor Estimating: 49it [00:39,  1.20it/s]Extractor Estimating: 50it [00:40,  1.18it/s]Extractor Estimating: 51it [00:41,  1.20it/s]Extractor Estimating: 52it [00:42,  1.17it/s]Extractor Estimating: 53it [00:43,  1.22it/s]Extractor Estimating: 54it [00:43,  1.24it/s]Extractor Estimating: 55it [00:44,  1.27it/s]Extractor Estimating: 56it [00:45,  1.26it/s]Extractor Estimating: 57it [00:46,  1.29it/s]Extractor Estimating: 58it [00:47,  1.29it/s]Extractor Estimating: 59it [00:47,  1.27it/s]Extractor Estimating: 60it [00:48,  1.29it/s]Extractor Estimating: 61it [00:49,  1.27it/s]Extractor Estimating: 62it [00:50,  1.29it/s]Extractor Estimating: 63it [00:50,  1.27it/s]Extractor Estimating: 64it [00:51,  1.29it/s]Extractor Estimating: 65it [00:52,  1.28it/s]Extractor Estimating: 66it [00:53,  1.29it/s]Extractor Estimating: 67it [00:54,  1.30it/s]Extractor Estimating: 68it [00:54,  1.29it/s]Extractor Estimating: 69it [00:55,  1.30it/s]Extractor Estimating: 70it [00:56,  1.29it/s]Extractor Estimating: 71it [00:57,  1.29it/s]Extractor Estimating: 72it [00:57,  1.32it/s]Extractor Estimating: 73it [00:58,  1.32it/s]Extractor Estimating: 74it [00:59,  1.29it/s]Extractor Estimating: 75it [01:00,  1.19it/s]Extractor Estimating: 76it [01:01,  1.25it/s]Extractor Estimating: 77it [01:01,  1.25it/s]Extractor Estimating: 78it [01:02,  1.28it/s]Extractor Estimating: 79it [01:03,  1.31it/s]Extractor Estimating: 80it [01:04,  1.33it/s]Extractor Estimating: 81it [01:04,  1.34it/s]Extractor Estimating: 82it [01:05,  1.35it/s]Extractor Estimating: 83it [01:06,  1.40it/s]Extractor Estimating: 84it [01:06,  1.40it/s]Extractor Estimating: 85it [01:07,  1.42it/s]Extractor Estimating: 86it [01:08,  1.40it/s]Extractor Estimating: 87it [01:09,  1.41it/s]Extractor Estimating: 88it [01:09,  1.38it/s]Extractor Estimating: 89it [01:10,  1.35it/s]Extractor Estimating: 90it [01:11,  1.35it/s]Extractor Estimating: 91it [01:12,  1.35it/s]Extractor Estimating: 92it [01:13,  1.24it/s]Extractor Estimating: 93it [01:13,  1.24it/s]Extractor Estimating: 94it [01:14,  1.29it/s]Extractor Estimating: 95it [01:15,  1.25it/s]Extractor Estimating: 96it [01:16,  1.27it/s]Extractor Estimating: 97it [01:16,  1.29it/s]Extractor Estimating: 98it [01:17,  1.33it/s]Extractor Estimating: 99it [01:18,  1.25it/s]Extractor Estimating: 100it [01:19,  1.30it/s]Extractor Estimating: 101it [01:19,  1.30it/s]Extractor Estimating: 102it [01:20,  1.30it/s]Extractor Estimating: 103it [01:21,  1.30it/s]Extractor Estimating: 104it [01:22,  1.28it/s]Extractor Estimating: 105it [01:23,  1.26it/s]Extractor Estimating: 106it [01:23,  1.28it/s]Extractor Estimating: 107it [01:24,  1.30it/s]Extractor Estimating: 108it [01:25,  1.26it/s]Extractor Estimating: 109it [01:26,  1.29it/s]Extractor Estimating: 110it [01:27,  1.27it/s]Extractor Estimating: 111it [01:27,  1.26it/s]Extractor Estimating: 112it [01:28,  1.27it/s]Extractor Estimating: 113it [01:29,  1.29it/s]Extractor Estimating: 114it [01:30,  1.34it/s]Extractor Estimating: 115it [01:30,  1.33it/s]Extractor Estimating: 116it [01:31,  1.34it/s]Extractor Estimating: 117it [01:32,  1.31it/s]Extractor Estimating: 118it [01:33,  1.29it/s]Extractor Estimating: 119it [01:33,  1.27it/s]Extractor Estimating: 120it [01:34,  1.28it/s]Extractor Estimating: 121it [01:35,  1.30it/s]Extractor Estimating: 122it [01:36,  1.30it/s]Extractor Estimating: 123it [01:36,  1.32it/s]Extractor Estimating: 124it [01:37,  1.30it/s]Extractor Estimating: 125it [01:38,  1.28it/s]Extractor Estimating: 126it [01:39,  1.25it/s]Extractor Estimating: 127it [01:40,  1.29it/s]Extractor Estimating: 128it [01:40,  1.33it/s]Extractor Estimating: 129it [01:41,  1.35it/s]Extractor Estimating: 130it [01:42,  1.39it/s]Extractor Estimating: 131it [01:42,  1.37it/s]Extractor Estimating: 132it [01:43,  1.32it/s]Extractor Estimating: 133it [01:44,  1.36it/s]Extractor Estimating: 134it [01:45,  1.37it/s]Extractor Estimating: 135it [01:45,  1.37it/s]Extractor Estimating: 136it [01:46,  1.32it/s]Extractor Estimating: 137it [01:47,  1.29it/s]Extractor Estimating: 138it [01:48,  1.28it/s]Extractor Estimating: 139it [01:49,  1.33it/s]Extractor Estimating: 140it [01:49,  1.36it/s]Extractor Estimating: 141it [01:50,  1.36it/s]Extractor Estimating: 142it [01:51,  1.33it/s]Extractor Estimating: 143it [01:51,  1.35it/s]Extractor Estimating: 144it [01:52,  1.31it/s]Extractor Estimating: 145it [01:53,  1.37it/s]Extractor Estimating: 146it [01:54,  1.35it/s]Extractor Estimating: 147it [01:54,  1.38it/s]Extractor Estimating: 148it [01:55,  1.35it/s]Extractor Estimating: 149it [01:56,  1.35it/s]Extractor Estimating: 150it [01:57,  1.38it/s]Extractor Estimating: 151it [01:57,  1.32it/s]Extractor Estimating: 152it [01:58,  1.33it/s]Extractor Estimating: 153it [01:59,  1.30it/s]Extractor Estimating: 154it [02:00,  1.29it/s]Extractor Estimating: 155it [02:01,  1.28it/s]Extractor Estimating: 156it [02:01,  1.29it/s]Extractor Estimating: 157it [02:02,  1.30it/s]Extractor Estimating: 158it [02:03,  1.25it/s]Extractor Estimating: 159it [02:04,  1.28it/s]Extractor Estimating: 160it [02:05,  1.23it/s]Extractor Estimating: 161it [02:05,  1.24it/s]Extractor Estimating: 162it [02:06,  1.23it/s]Extractor Estimating: 163it [02:07,  1.25it/s]Extractor Estimating: 164it [02:08,  1.26it/s]Extractor Estimating: 165it [02:08,  1.30it/s]Extractor Estimating: 166it [02:09,  1.27it/s]Extractor Estimating: 167it [02:10,  1.30it/s]Extractor Estimating: 168it [02:11,  1.25it/s]Extractor Estimating: 169it [02:12,  1.32it/s]Extractor Estimating: 170it [02:12,  1.25it/s]Extractor Estimating: 171it [02:13,  1.25it/s]Extractor Estimating: 172it [02:14,  1.28it/s]Extractor Estimating: 173it [02:15,  1.26it/s]Extractor Estimating: 174it [02:16,  1.24it/s]Extractor Estimating: 175it [02:16,  1.23it/s]Extractor Estimating: 176it [02:17,  1.27it/s]Extractor Estimating: 177it [02:18,  1.26it/s]Extractor Estimating: 178it [02:19,  1.30it/s]Extractor Estimating: 179it [02:19,  1.31it/s]Extractor Estimating: 180it [02:20,  1.25it/s]Extractor Estimating: 181it [02:21,  1.23it/s]Extractor Estimating: 182it [02:22,  1.24it/s]Extractor Estimating: 183it [02:23,  1.23it/s]Extractor Estimating: 184it [02:24,  1.25it/s]Extractor Estimating: 185it [02:24,  1.22it/s]Extractor Estimating: 186it [02:25,  1.23it/s]Extractor Estimating: 187it [02:26,  1.24it/s]Extractor Estimating: 188it [02:27,  1.25it/s]Extractor Estimating: 189it [02:28,  1.26it/s]Extractor Estimating: 190it [02:28,  1.24it/s]Extractor Estimating: 191it [02:29,  1.22it/s]Extractor Estimating: 192it [02:30,  1.23it/s]Extractor Estimating: 193it [02:31,  1.24it/s]Extractor Estimating: 194it [02:32,  1.25it/s]Extractor Estimating: 195it [02:32,  1.24it/s]Extractor Estimating: 196it [02:33,  1.27it/s]Extractor Estimating: 197it [02:34,  1.31it/s]Extractor Estimating: 198it [02:35,  1.28it/s]Extractor Estimating: 199it [02:36,  1.28it/s]Extractor Estimating: 200it [02:36,  1.24it/s]Extractor Estimating: 201it [02:37,  1.25it/s]Extractor Estimating: 202it [02:38,  1.26it/s]Extractor Estimating: 203it [02:39,  1.29it/s]Extractor Estimating: 204it [02:39,  1.32it/s]Extractor Estimating: 205it [02:40,  1.34it/s]Extractor Estimating: 206it [02:41,  1.35it/s]Extractor Estimating: 207it [02:42,  1.32it/s]Extractor Estimating: 208it [02:42,  1.37it/s]Extractor Estimating: 209it [02:43,  1.36it/s]Extractor Estimating: 210it [02:44,  1.35it/s]Extractor Estimating: 211it [02:45,  1.35it/s]Extractor Estimating: 212it [02:45,  1.31it/s]Extractor Estimating: 213it [02:46,  1.30it/s]Extractor Estimating: 214it [02:47,  1.33it/s]Extractor Estimating: 215it [02:48,  1.33it/s]Extractor Estimating: 216it [02:48,  1.35it/s]Extractor Estimating: 217it [02:49,  1.36it/s]Extractor Estimating: 218it [02:50,  1.36it/s]Extractor Estimating: 219it [02:51,  1.34it/s]Extractor Estimating: 220it [02:51,  1.33it/s]Extractor Estimating: 221it [02:52,  1.31it/s]Extractor Estimating: 222it [02:53,  1.34it/s]Extractor Estimating: 223it [02:54,  1.37it/s]Extractor Estimating: 224it [02:54,  1.36it/s]Extractor Estimating: 225it [02:55,  1.33it/s]Extractor Estimating: 226it [02:56,  1.32it/s]Extractor Estimating: 227it [02:57,  1.36it/s]Extractor Estimating: 228it [02:57,  1.42it/s]Extractor Estimating: 229it [02:58,  1.41it/s]Extractor Estimating: 230it [02:59,  1.45it/s]Extractor Estimating: 231it [02:59,  1.43it/s]Extractor Estimating: 232it [03:00,  1.39it/s]Extractor Estimating: 233it [03:01,  1.35it/s]Extractor Estimating: 234it [03:02,  1.31it/s]Extractor Estimating: 235it [03:02,  1.30it/s]Extractor Estimating: 236it [03:03,  1.36it/s]Extractor Estimating: 237it [03:04,  1.34it/s]Extractor Estimating: 238it [03:04,  1.39it/s]Extractor Estimating: 239it [03:05,  1.44it/s]Extractor Estimating: 240it [03:06,  1.34it/s]Extractor Estimating: 241it [03:07,  1.25it/s]Extractor Estimating: 242it [03:08,  1.27it/s]Extractor Estimating: 243it [03:08,  1.30it/s]Extractor Estimating: 244it [03:09,  1.32it/s]Extractor Estimating: 245it [03:10,  1.29it/s]Extractor Estimating: 246it [03:11,  1.31it/s]Extractor Estimating: 247it [03:11,  1.37it/s]Extractor Estimating: 248it [03:12,  1.35it/s]Extractor Estimating: 249it [03:13,  1.36it/s]Extractor Estimating: 250it [03:14,  1.35it/s]Extractor Estimating: 250it [03:14,  1.29it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:59,728 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:59,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:59,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:59,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:59,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:43:00,348 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:43:00,349 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:43:00,933 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:43:01,989 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:43:01,989 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:04,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:04,834 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:04,834 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:04,835 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:04,835 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:43:05,496 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:43:05,497 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:43:06,072 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:43:06,238 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:43:06,238 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 16:37:48,080 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 16:37:48,095 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 4996 mean pseudo reward: 0.9472476963374856
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl'}
train vocab size: 20227
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20327, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20327, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.352, loss:619.4267
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.321, loss:542.8939
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.326, loss:544.9494
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.338, loss:557.3975
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.328, loss:516.2743
>> valid entity prec:0.5359, rec:0.4841, f1:0.5087
>> valid relation prec:0.2097, rec:0.0783, f1:0.1140
>> valid relation with NER prec:0.2097, rec:0.0783, f1:0.1140
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.997, loss:535.8020
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.319, loss:509.2801
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.319, loss:520.3841
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.333, loss:522.7188
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.323, loss:556.4126
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5204, rec:0.5035, f1:0.5118
>> valid relation prec:0.1432, rec:0.0743, f1:0.0978
>> valid relation with NER prec:0.1432, rec:0.0743, f1:0.0978
new max entity f1 on valid!
g_step 1100, step 55, avg_time 2.988, loss:544.5669
g_step 1200, step 155, avg_time 1.317, loss:537.1537
g_step 1300, step 46, avg_time 1.327, loss:498.6692
g_step 1400, step 146, avg_time 1.342, loss:504.2994
g_step 1500, step 37, avg_time 1.319, loss:481.1781
>> valid entity prec:0.5549, rec:0.4356, f1:0.4881
>> valid relation prec:0.1133, rec:0.0373, f1:0.0561
>> valid relation with NER prec:0.1133, rec:0.0373, f1:0.0561
g_step 1600, step 137, avg_time 2.989, loss:484.0411
g_step 1700, step 28, avg_time 1.330, loss:485.1133
g_step 1800, step 128, avg_time 1.345, loss:465.2028
g_step 1900, step 19, avg_time 1.306, loss:459.6185
g_step 2000, step 119, avg_time 1.319, loss:443.7309
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5131, rec:0.4848, f1:0.4986
>> valid relation prec:0.1290, rec:0.0542, f1:0.0763
>> valid relation with NER prec:0.1290, rec:0.0542, f1:0.0763
g_step 2100, step 10, avg_time 3.004, loss:430.9599
g_step 2200, step 110, avg_time 1.335, loss:406.9787
g_step 2300, step 1, avg_time 1.322, loss:422.8441
g_step 2400, step 101, avg_time 1.317, loss:376.8653
g_step 2500, step 201, avg_time 1.343, loss:422.7023
>> valid entity prec:0.5389, rec:0.4481, f1:0.4893
>> valid relation prec:0.1504, rec:0.0608, f1:0.0866
>> valid relation with NER prec:0.1504, rec:0.0608, f1:0.0866
g_step 2600, step 92, avg_time 2.983, loss:384.1410
g_step 2700, step 192, avg_time 1.339, loss:386.3725
g_step 2800, step 83, avg_time 1.322, loss:356.5560
g_step 2900, step 183, avg_time 1.323, loss:372.2638
g_step 3000, step 74, avg_time 1.337, loss:366.2461
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5443, rec:0.4906, f1:0.5161
>> valid relation prec:0.2771, rec:0.1262, f1:0.1734
>> valid relation with NER prec:0.2771, rec:0.1262, f1:0.1734
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 174, avg_time 2.989, loss:364.3858
g_step 3200, step 65, avg_time 1.314, loss:354.8409
g_step 3300, step 165, avg_time 1.335, loss:336.0372
g_step 3400, step 56, avg_time 1.346, loss:330.7989
g_step 3500, step 156, avg_time 1.318, loss:337.6942
>> valid entity prec:0.5397, rec:0.4622, f1:0.4979
>> valid relation prec:0.1724, rec:0.0734, f1:0.1030
>> valid relation with NER prec:0.1724, rec:0.0734, f1:0.1030
g_step 3600, step 47, avg_time 2.972, loss:329.0812
g_step 3700, step 147, avg_time 1.312, loss:326.1492
g_step 3800, step 38, avg_time 1.341, loss:322.1743
g_step 3900, step 138, avg_time 1.339, loss:308.2850
g_step 4000, step 29, avg_time 1.314, loss:312.1029
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5351, rec:0.4827, f1:0.5075
>> valid relation prec:0.2106, rec:0.0915, f1:0.1276
>> valid relation with NER prec:0.2106, rec:0.0915, f1:0.1276
g_step 4100, step 129, avg_time 2.996, loss:302.9655
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 16:37:48 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 16:37:48 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_16-37-48_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 16:37:49 - WARNING - datasets.builder -   Using custom data configuration default-4828a22b4d76acf9
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-4828a22b4d76acf9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 16:37:49,563 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:37:49,564 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:37:49,564 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:37:49,565 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:37:49,619 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:37:49,635 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:37:49,635 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:37:49,635 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:37:49,635 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:37:49,635 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:37:49,635 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 16:37:49,780 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:37:53,085 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 16:37:53,106 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-4828a22b4d76acf9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.92ba/s] 40%|████      | 2/5 [00:00<00:01,  2.99ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.62ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.00ba/s]100%|██████████| 5/5 [00:01<00:00,  4.22ba/s]100%|██████████| 5/5 [00:01<00:00,  3.84ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.02ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.32ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.42ba/s]100%|██████████| 4/4 [00:00<00:00,  5.56ba/s]100%|██████████| 4/4 [00:00<00:00,  5.02ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.77ba/s] 40%|████      | 2/5 [00:00<00:00,  6.67ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.85ba/s] 80%|████████  | 4/5 [00:00<00:00,  8.40ba/s]100%|██████████| 5/5 [00:00<00:00,  8.81ba/s]100%|██████████| 5/5 [00:00<00:00,  7.96ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.26ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.87ba/s]100%|██████████| 4/4 [00:00<00:00, 10.01ba/s]
[INFO|trainer.py:414] 2023-08-28 16:37:56,954 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 16:37:56,971 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 16:37:56,971 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 16:37:56,971 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 16:37:56,971 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 16:37:56,972 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 16:37:56,972 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 16:37:56,972 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<02:00,  3.22it/s]  1%|          | 2/390 [00:00<01:56,  3.34it/s]  1%|          | 3/390 [00:00<01:54,  3.38it/s]  1%|          | 4/390 [00:01<01:53,  3.40it/s]  1%|▏         | 5/390 [00:01<01:52,  3.41it/s]  2%|▏         | 6/390 [00:01<01:52,  3.42it/s]  2%|▏         | 7/390 [00:02<01:51,  3.42it/s]  2%|▏         | 8/390 [00:02<01:51,  3.43it/s]  2%|▏         | 9/390 [00:02<01:51,  3.40it/s]  3%|▎         | 10/390 [00:02<01:51,  3.41it/s]  3%|▎         | 11/390 [00:03<01:50,  3.42it/s]  3%|▎         | 12/390 [00:03<01:50,  3.42it/s]  3%|▎         | 13/390 [00:03<01:50,  3.42it/s]  4%|▎         | 14/390 [00:04<01:49,  3.42it/s]  4%|▍         | 15/390 [00:04<01:49,  3.43it/s]  4%|▍         | 16/390 [00:04<01:49,  3.43it/s]  4%|▍         | 17/390 [00:04<01:48,  3.43it/s]  5%|▍         | 18/390 [00:05<01:48,  3.43it/s]  5%|▍         | 19/390 [00:05<01:48,  3.43it/s]  5%|▌         | 20/390 [00:05<01:48,  3.41it/s]  5%|▌         | 21/390 [00:06<01:47,  3.42it/s]  6%|▌         | 22/390 [00:06<01:47,  3.42it/s]  6%|▌         | 23/390 [00:06<01:47,  3.42it/s]  6%|▌         | 24/390 [00:07<01:46,  3.42it/s]  6%|▋         | 25/390 [00:07<01:46,  3.42it/s]  7%|▋         | 26/390 [00:07<01:46,  3.43it/s]  7%|▋         | 27/390 [00:07<01:45,  3.43it/s]  7%|▋         | 28/390 [00:08<01:45,  3.43it/s]  7%|▋         | 29/390 [00:08<01:45,  3.43it/s]  8%|▊         | 30/390 [00:08<01:45,  3.43it/s]  8%|▊         | 31/390 [00:09<01:46,  3.39it/s]  8%|▊         | 32/390 [00:09<01:45,  3.40it/s]  8%|▊         | 33/390 [00:09<01:44,  3.41it/s]  9%|▊         | 34/390 [00:09<01:44,  3.42it/s]  9%|▉         | 35/390 [00:10<01:43,  3.42it/s]  9%|▉         | 36/390 [00:10<01:43,  3.42it/s]  9%|▉         | 37/390 [00:10<01:43,  3.42it/s] 10%|▉         | 38/390 [00:11<01:42,  3.42it/s] 10%|█         | 39/390 [00:11<01:42,  3.43it/s] 10%|█         | 40/390 [00:11<01:42,  3.43it/s] 11%|█         | 41/390 [00:11<01:41,  3.43it/s] 11%|█         | 42/390 [00:12<01:41,  3.42it/s] 11%|█         | 43/390 [00:12<01:41,  3.42it/s] 11%|█▏        | 44/390 [00:12<01:41,  3.43it/s] 12%|█▏        | 45/390 [00:13<01:40,  3.42it/s] 12%|█▏        | 46/390 [00:13<01:40,  3.43it/s] 12%|█▏        | 47/390 [00:13<01:40,  3.43it/s] 12%|█▏        | 48/390 [00:14<01:39,  3.42it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.43it/s] 13%|█▎        | 50/390 [00:14<01:39,  3.43it/s] 13%|█▎        | 51/390 [00:14<01:38,  3.43it/s] 13%|█▎        | 52/390 [00:15<01:38,  3.43it/s] 14%|█▎        | 53/390 [00:15<01:43,  3.25it/s] 14%|█▍        | 54/390 [00:15<01:41,  3.30it/s] 14%|█▍        | 55/390 [00:16<01:40,  3.33it/s] 14%|█▍        | 56/390 [00:16<01:39,  3.36it/s] 15%|█▍        | 57/390 [00:16<01:38,  3.37it/s] 15%|█▍        | 58/390 [00:17<01:38,  3.38it/s] 15%|█▌        | 59/390 [00:19<05:26,  1.01it/s] 15%|█▌        | 60/390 [00:19<04:16,  1.28it/s] 16%|█▌        | 61/390 [00:20<03:52,  1.41it/s] 16%|█▌        | 62/390 [00:20<03:11,  1.72it/s] 16%|█▌        | 63/390 [00:21<02:42,  2.02it/s] 16%|█▋        | 64/390 [00:21<02:21,  2.30it/s] 17%|█▋        | 65/390 [00:21<02:07,  2.55it/s] 17%|█▋        | 66/390 [00:21<01:57,  2.76it/s] 17%|█▋        | 67/390 [00:22<01:50,  2.93it/s] 17%|█▋        | 68/390 [00:22<01:45,  3.06it/s] 18%|█▊        | 69/390 [00:22<01:41,  3.16it/s] 18%|█▊        | 70/390 [00:23<01:39,  3.23it/s] 18%|█▊        | 71/390 [00:23<01:40,  3.18it/s] 18%|█▊        | 72/390 [00:23<01:37,  3.25it/s] 19%|█▊        | 73/390 [00:23<01:36,  3.30it/s] 19%|█▉        | 74/390 [00:24<01:34,  3.33it/s] 19%|█▉        | 75/390 [00:24<01:33,  3.36it/s] 19%|█▉        | 76/390 [00:24<01:33,  3.37it/s] 20%|█▉        | 77/390 [00:25<01:32,  3.39it/s] 20%|██        | 78/390 [00:25<01:31,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 16:38:22,490 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:38:22,490 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 16:38:22,490 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.28it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.14it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.40it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.72it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.40it/s][A
  8%|▊         | 33/436 [00:00<00:09, 40.42it/s][A
  9%|▊         | 38/436 [00:00<00:09, 42.18it/s][A
 10%|▉         | 43/436 [00:00<00:09, 43.51it/s][A
 11%|█         | 48/436 [00:01<00:08, 44.33it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.06it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.27it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 45.67it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.11it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 45.77it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.83it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.94it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.05it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 46.24it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.28it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.34it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.42it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.32it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.18it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.13it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.12it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.15it/s][A
 32%|███▏      | 138/436 [00:03<00:06, 46.29it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.37it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.46it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.38it/s][A
 36%|███▌      | 158/436 [00:03<00:05, 46.36it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.22it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.19it/s][A
 40%|███▉      | 173/436 [00:03<00:06, 42.33it/s][A
 41%|████      | 178/436 [00:03<00:05, 43.42it/s][A
 42%|████▏     | 183/436 [00:04<00:05, 44.30it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.03it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.43it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.79it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.92it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.08it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.81it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.85it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.96it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.10it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.19it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.33it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.43it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.47it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.25it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.15it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.95it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.17it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.17it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.21it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.29it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.32it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.43it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.38it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.12it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.15it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.54it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.80it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.89it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.00it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.15it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.06it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.01it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.98it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.85it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.99it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.14it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 46.21it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.28it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.28it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.34it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.30it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.10it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.20it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.10it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.13it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.26it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.21it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.28it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.27it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.11it/s][A
                                                 [A                                                
100%|██████████| 436/436 [00:09<00:00, 46.11it/s][A 20%|██        | 78/390 [00:35<01:31,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:38:32,020 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 16:38:32,041 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:38:37,549 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:38:37,585 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:38:37,596 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:50<39:20,  7.59s/it] 21%|██        | 80/390 [00:50<27:56,  5.41s/it] 21%|██        | 81/390 [00:50<19:56,  3.87s/it] 21%|██        | 82/390 [00:50<14:22,  2.80s/it] 21%|██▏       | 83/390 [00:51<10:28,  2.05s/it] 22%|██▏       | 84/390 [00:51<07:45,  1.52s/it] 22%|██▏       | 85/390 [00:51<05:51,  1.15s/it] 22%|██▏       | 86/390 [00:52<04:31,  1.12it/s] 22%|██▏       | 87/390 [00:52<03:36,  1.40it/s] 23%|██▎       | 88/390 [00:52<02:57,  1.70it/s] 23%|██▎       | 89/390 [00:53<02:29,  2.01it/s] 23%|██▎       | 90/390 [00:53<02:10,  2.29it/s] 23%|██▎       | 91/390 [00:53<01:57,  2.55it/s] 24%|██▎       | 92/390 [00:53<01:48,  2.76it/s] 24%|██▍       | 93/390 [00:54<01:41,  2.92it/s] 24%|██▍       | 94/390 [00:54<01:36,  3.06it/s] 24%|██▍       | 95/390 [00:54<01:33,  3.16it/s] 25%|██▍       | 96/390 [00:55<01:30,  3.24it/s] 25%|██▍       | 97/390 [00:55<01:29,  3.29it/s] 25%|██▌       | 98/390 [00:55<01:27,  3.33it/s] 25%|██▌       | 99/390 [00:55<01:26,  3.36it/s] 26%|██▌       | 100/390 [00:56<01:25,  3.38it/s] 26%|██▌       | 101/390 [00:56<01:25,  3.39it/s] 26%|██▌       | 102/390 [00:56<01:24,  3.40it/s] 26%|██▋       | 103/390 [00:57<01:24,  3.41it/s] 27%|██▋       | 104/390 [00:57<01:25,  3.34it/s] 27%|██▋       | 105/390 [00:57<01:24,  3.36it/s] 27%|██▋       | 106/390 [00:58<01:24,  3.38it/s] 27%|██▋       | 107/390 [00:58<01:23,  3.39it/s] 28%|██▊       | 108/390 [00:58<01:23,  3.40it/s] 28%|██▊       | 109/390 [00:58<01:22,  3.40it/s] 28%|██▊       | 110/390 [00:59<01:22,  3.41it/s] 28%|██▊       | 111/390 [00:59<01:21,  3.41it/s] 29%|██▊       | 112/390 [00:59<01:21,  3.41it/s] 29%|██▉       | 113/390 [01:00<01:21,  3.41it/s] 29%|██▉       | 114/390 [01:00<01:20,  3.41it/s] 29%|██▉       | 115/390 [01:00<01:21,  3.35it/s] 30%|██▉       | 116/390 [01:00<01:21,  3.37it/s] 30%|███       | 117/390 [01:01<01:20,  3.39it/s] 30%|███       | 118/390 [01:01<01:20,  3.40it/s] 31%|███       | 119/390 [01:01<01:19,  3.40it/s] 31%|███       | 120/390 [01:02<01:19,  3.41it/s] 31%|███       | 121/390 [01:02<01:18,  3.41it/s] 31%|███▏      | 122/390 [01:02<01:18,  3.41it/s] 32%|███▏      | 123/390 [01:02<01:18,  3.42it/s] 32%|███▏      | 124/390 [01:03<01:17,  3.42it/s] 32%|███▏      | 125/390 [01:03<01:17,  3.42it/s] 32%|███▏      | 126/390 [01:03<01:17,  3.40it/s] 33%|███▎      | 127/390 [01:04<01:17,  3.40it/s] 33%|███▎      | 128/390 [01:04<01:16,  3.41it/s] 33%|███▎      | 129/390 [01:04<01:16,  3.41it/s] 33%|███▎      | 130/390 [01:05<01:16,  3.41it/s] 34%|███▎      | 131/390 [01:05<01:15,  3.41it/s] 34%|███▍      | 132/390 [01:05<01:15,  3.41it/s] 34%|███▍      | 133/390 [01:05<01:15,  3.41it/s] 34%|███▍      | 134/390 [01:06<01:15,  3.41it/s] 35%|███▍      | 135/390 [01:06<01:14,  3.41it/s] 35%|███▍      | 136/390 [01:06<01:14,  3.41it/s] 35%|███▌      | 137/390 [01:07<01:17,  3.25it/s] 35%|███▌      | 138/390 [01:07<01:16,  3.30it/s] 36%|███▌      | 139/390 [01:07<01:15,  3.33it/s] 36%|███▌      | 140/390 [01:08<01:14,  3.35it/s] 36%|███▌      | 141/390 [01:08<01:13,  3.37it/s] 36%|███▋      | 142/390 [01:08<01:13,  3.39it/s] 37%|███▋      | 143/390 [01:08<01:12,  3.39it/s] 37%|███▋      | 144/390 [01:09<01:12,  3.40it/s] 37%|███▋      | 145/390 [01:09<01:11,  3.41it/s] 37%|███▋      | 146/390 [01:09<01:11,  3.41it/s] 38%|███▊      | 147/390 [01:10<01:11,  3.41it/s] 38%|███▊      | 148/390 [01:10<01:17,  3.14it/s] 38%|███▊      | 149/390 [01:10<01:14,  3.21it/s] 38%|███▊      | 150/390 [01:11<01:13,  3.27it/s] 39%|███▊      | 151/390 [01:11<01:12,  3.31it/s] 39%|███▉      | 152/390 [01:11<01:11,  3.33it/s] 39%|███▉      | 153/390 [01:11<01:10,  3.35it/s] 39%|███▉      | 154/390 [01:12<01:10,  3.37it/s] 40%|███▉      | 155/390 [01:12<01:09,  3.38it/s] 40%|████      | 156/390 [01:12<01:08,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 16:39:09,836 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:39:09,836 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 16:39:09,836 >>   Batch size = 8
{'eval_loss': 0.9860376715660095, 'eval_runtime': 9.514, 'eval_samples_per_second': 366.406, 'eval_steps_per_second': 45.827, 'epoch': 0.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.27it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.22it/s][A
  4%|▍         | 18/436 [00:01<00:29, 14.18it/s][A
  5%|▌         | 23/436 [00:01<00:21, 18.77it/s][A
  6%|▋         | 28/436 [00:01<00:17, 23.45it/s][A
  8%|▊         | 33/436 [00:01<00:14, 27.91it/s][A
  9%|▊         | 38/436 [00:01<00:12, 31.96it/s][A
 10%|▉         | 43/436 [00:01<00:11, 35.42it/s][A
 11%|█         | 48/436 [00:01<00:10, 38.13it/s][A
 12%|█▏        | 53/436 [00:01<00:09, 40.25it/s][A
 13%|█▎        | 58/436 [00:01<00:09, 41.65it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 42.86it/s][A
 16%|█▌        | 68/436 [00:02<00:08, 43.81it/s][A
 17%|█▋        | 73/436 [00:02<00:08, 44.54it/s][A
 18%|█▊        | 78/436 [00:02<00:07, 45.06it/s][A
 19%|█▉        | 83/436 [00:02<00:07, 45.41it/s][A
 20%|██        | 88/436 [00:02<00:07, 45.75it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.86it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.93it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.91it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.87it/s][A
 26%|██▌       | 113/436 [00:03<00:07, 45.99it/s][A
 27%|██▋       | 118/436 [00:03<00:06, 46.07it/s][A
 28%|██▊       | 123/436 [00:03<00:06, 46.09it/s][A
 29%|██▉       | 128/436 [00:03<00:09, 32.67it/s][A
 31%|███       | 133/436 [00:03<00:08, 35.84it/s][A
 32%|███▏      | 138/436 [00:03<00:07, 38.46it/s][A
 33%|███▎      | 143/436 [00:03<00:07, 40.53it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 42.13it/s][A
 35%|███▌      | 153/436 [00:04<00:06, 43.40it/s][A
 36%|███▌      | 158/436 [00:04<00:06, 44.21it/s][A
 37%|███▋      | 163/436 [00:04<00:06, 44.80it/s][A
 39%|███▊      | 168/436 [00:04<00:05, 44.83it/s][A
 40%|███▉      | 173/436 [00:04<00:05, 45.17it/s][A
 41%|████      | 178/436 [00:04<00:05, 45.46it/s][A
 42%|████▏     | 183/436 [00:04<00:05, 45.48it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.79it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.91it/s][A
 45%|████▌     | 198/436 [00:05<00:05, 46.13it/s][A
 47%|████▋     | 203/436 [00:05<00:05, 46.16it/s][A
 48%|████▊     | 208/436 [00:05<00:04, 46.02it/s][A
 49%|████▉     | 213/436 [00:05<00:04, 46.02it/s][A
 50%|█████     | 218/436 [00:05<00:09, 23.02it/s][A
 51%|█████     | 223/436 [00:05<00:07, 27.11it/s][A
 52%|█████▏    | 228/436 [00:06<00:06, 30.99it/s][A
 53%|█████▎    | 233/436 [00:06<00:05, 34.39it/s][A
 55%|█████▍    | 238/436 [00:06<00:05, 37.29it/s][A
 56%|█████▌    | 243/436 [00:06<00:04, 39.60it/s][A
 57%|█████▋    | 248/436 [00:06<00:04, 41.42it/s][A
 58%|█████▊    | 253/436 [00:06<00:04, 42.68it/s][A
 59%|█████▉    | 258/436 [00:06<00:04, 43.47it/s][A
 60%|██████    | 263/436 [00:06<00:03, 44.13it/s][A
 61%|██████▏   | 268/436 [00:06<00:03, 44.71it/s][A
 63%|██████▎   | 273/436 [00:07<00:03, 45.25it/s][A
 64%|██████▍   | 278/436 [00:07<00:03, 45.48it/s][A
 65%|██████▍   | 283/436 [00:07<00:03, 45.76it/s][A
 66%|██████▌   | 288/436 [00:07<00:03, 45.96it/s][A
 67%|██████▋   | 293/436 [00:07<00:03, 45.85it/s][A
 68%|██████▊   | 298/436 [00:07<00:03, 45.83it/s][A
 69%|██████▉   | 303/436 [00:07<00:02, 45.86it/s][A
 71%|███████   | 308/436 [00:07<00:02, 45.84it/s][A
 72%|███████▏  | 313/436 [00:07<00:02, 45.88it/s][A
 73%|███████▎  | 318/436 [00:08<00:02, 46.09it/s][A
 74%|███████▍  | 323/436 [00:08<00:02, 46.08it/s][A
 75%|███████▌  | 328/436 [00:08<00:02, 46.15it/s][A
 76%|███████▋  | 333/436 [00:08<00:02, 46.25it/s][A
 78%|███████▊  | 338/436 [00:08<00:02, 39.01it/s][A
 79%|███████▊  | 343/436 [00:08<00:02, 40.88it/s][A
 80%|███████▉  | 348/436 [00:08<00:02, 42.35it/s][A
 81%|████████  | 353/436 [00:08<00:01, 43.52it/s][A
 82%|████████▏ | 358/436 [00:08<00:01, 44.30it/s][A
 83%|████████▎ | 363/436 [00:09<00:01, 44.79it/s][A
 84%|████████▍ | 368/436 [00:09<00:01, 45.27it/s][A
 86%|████████▌ | 373/436 [00:09<00:01, 45.61it/s][A
 87%|████████▋ | 378/436 [00:09<00:01, 45.34it/s][A
 88%|████████▊ | 383/436 [00:09<00:01, 45.56it/s][A
 89%|████████▉ | 388/436 [00:09<00:01, 45.75it/s][A
 90%|█████████ | 393/436 [00:09<00:00, 45.78it/s][A
 91%|█████████▏| 398/436 [00:09<00:00, 46.02it/s][A
 92%|█████████▏| 403/436 [00:09<00:00, 46.04it/s][A
 94%|█████████▎| 408/436 [00:10<00:00, 46.10it/s][A
 95%|█████████▍| 413/436 [00:10<00:00, 46.13it/s][A
 96%|█████████▌| 418/436 [00:10<00:00, 45.99it/s][A
 97%|█████████▋| 423/436 [00:10<00:00, 45.74it/s][A
 98%|█████████▊| 428/436 [00:10<00:00, 45.89it/s][A
 99%|█████████▉| 433/436 [00:10<00:00, 45.23it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:10<00:00, 45.23it/s][A 40%|████      | 156/390 [01:23<01:08,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:39:20,978 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 16:39:21,060 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:39:26,589 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:39:26,642 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:39:26,673 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:40<32:33,  8.38s/it] 41%|████      | 158/390 [01:40<23:04,  5.97s/it] 41%|████      | 159/390 [01:40<16:25,  4.26s/it] 41%|████      | 160/390 [01:40<11:46,  3.07s/it] 41%|████▏     | 161/390 [01:41<08:32,  2.24s/it] 42%|████▏     | 162/390 [01:41<06:17,  1.65s/it] 42%|████▏     | 163/390 [01:41<04:42,  1.25s/it] 42%|████▏     | 164/390 [01:42<03:36,  1.04it/s] 42%|████▏     | 165/390 [01:42<02:50,  1.32it/s] 43%|████▎     | 166/390 [01:42<02:18,  1.62it/s] 43%|████▎     | 167/390 [01:43<01:56,  1.92it/s] 43%|████▎     | 168/390 [01:43<01:40,  2.21it/s] 43%|████▎     | 169/390 [01:43<01:29,  2.46it/s] 44%|████▎     | 170/390 [01:43<01:21,  2.69it/s] 44%|████▍     | 171/390 [01:44<01:16,  2.88it/s] 44%|████▍     | 172/390 [01:44<01:12,  3.02it/s] 44%|████▍     | 173/390 [01:44<01:09,  3.13it/s] 45%|████▍     | 174/390 [01:45<01:07,  3.21it/s] 45%|████▍     | 175/390 [01:45<01:05,  3.27it/s] 45%|████▌     | 176/390 [01:45<01:04,  3.32it/s] 45%|████▌     | 177/390 [01:45<01:03,  3.35it/s] 46%|████▌     | 178/390 [01:46<01:02,  3.37it/s] 46%|████▌     | 179/390 [01:46<01:02,  3.38it/s] 46%|████▌     | 180/390 [01:46<01:04,  3.26it/s] 46%|████▋     | 181/390 [01:47<01:03,  3.31it/s] 47%|████▋     | 182/390 [01:47<01:02,  3.34it/s] 47%|████▋     | 183/390 [01:47<01:01,  3.36it/s] 47%|████▋     | 184/390 [01:48<01:01,  3.38it/s] 47%|████▋     | 185/390 [01:48<01:00,  3.39it/s] 48%|████▊     | 186/390 [01:48<01:00,  3.40it/s] 48%|████▊     | 187/390 [01:48<00:59,  3.40it/s] 48%|████▊     | 188/390 [01:49<00:59,  3.41it/s] 48%|████▊     | 189/390 [01:49<00:58,  3.41it/s] 49%|████▊     | 190/390 [01:49<00:58,  3.42it/s] 49%|████▉     | 191/390 [01:50<00:58,  3.41it/s] 49%|████▉     | 192/390 [01:50<00:58,  3.37it/s] 49%|████▉     | 193/390 [01:50<00:58,  3.38it/s] 50%|████▉     | 194/390 [01:50<00:57,  3.39it/s] 50%|█████     | 195/390 [01:51<00:57,  3.40it/s] 50%|█████     | 196/390 [01:51<00:56,  3.41it/s] 51%|█████     | 197/390 [01:51<00:56,  3.41it/s] 51%|█████     | 198/390 [01:52<00:56,  3.41it/s] 51%|█████     | 199/390 [01:52<00:57,  3.30it/s] 51%|█████▏    | 200/390 [01:52<00:56,  3.34it/s] 52%|█████▏    | 201/390 [01:53<00:56,  3.36it/s] 52%|█████▏    | 202/390 [01:53<00:55,  3.38it/s] 52%|█████▏    | 203/390 [01:53<00:55,  3.39it/s] 52%|█████▏    | 204/390 [01:53<00:54,  3.40it/s] 53%|█████▎    | 205/390 [01:54<00:54,  3.40it/s] 53%|█████▎    | 206/390 [01:54<00:54,  3.41it/s] 53%|█████▎    | 207/390 [01:54<00:53,  3.41it/s] 53%|█████▎    | 208/390 [01:55<00:53,  3.41it/s] 54%|█████▎    | 209/390 [01:55<00:52,  3.42it/s] 54%|█████▍    | 210/390 [01:55<00:53,  3.38it/s] 54%|█████▍    | 211/390 [01:55<00:52,  3.39it/s] 54%|█████▍    | 212/390 [01:56<00:52,  3.40it/s] 55%|█████▍    | 213/390 [01:56<00:52,  3.40it/s] 55%|█████▍    | 214/390 [01:56<00:51,  3.41it/s] 55%|█████▌    | 215/390 [01:57<00:51,  3.41it/s] 55%|█████▌    | 216/390 [01:57<00:50,  3.41it/s] 56%|█████▌    | 217/390 [01:57<00:50,  3.41it/s] 56%|█████▌    | 218/390 [01:58<00:50,  3.41it/s] 56%|█████▌    | 219/390 [01:58<00:50,  3.41it/s] 56%|█████▋    | 220/390 [01:58<00:49,  3.41it/s] 57%|█████▋    | 221/390 [01:58<00:52,  3.20it/s] 57%|█████▋    | 222/390 [01:59<00:51,  3.26it/s] 57%|█████▋    | 223/390 [01:59<00:50,  3.31it/s] 57%|█████▋    | 224/390 [01:59<00:49,  3.34it/s] 58%|█████▊    | 225/390 [02:00<00:49,  3.36it/s] 58%|█████▊    | 226/390 [02:00<00:48,  3.38it/s] 58%|█████▊    | 227/390 [02:00<00:48,  3.39it/s] 58%|█████▊    | 228/390 [02:01<00:47,  3.39it/s] 59%|█████▊    | 229/390 [02:01<00:47,  3.40it/s] 59%|█████▉    | 230/390 [02:01<00:46,  3.41it/s] 59%|█████▉    | 231/390 [02:01<00:46,  3.41it/s] 59%|█████▉    | 232/390 [02:02<00:49,  3.19it/s] 60%|█████▉    | 233/390 [02:02<00:48,  3.26it/s] 60%|██████    | 234/390 [02:02<00:47,  3.30it/s][INFO|trainer.py:2140] 2023-08-28 16:39:59,885 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:39:59,885 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 16:39:59,885 >>   Batch size = 8
{'eval_loss': 0.998138427734375, 'eval_runtime': 10.6853, 'eval_samples_per_second': 326.242, 'eval_steps_per_second': 40.804, 'epoch': 1.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.19it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.95it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.13it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.44it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.03it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.65it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.32it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.92it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.84it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.06it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.14it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.10it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.18it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.29it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.21it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.07it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.00it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.90it/s][A
 22%|██▏       | 98/436 [00:02<00:09, 36.35it/s][A
 24%|██▎       | 103/436 [00:02<00:08, 38.91it/s][A
 25%|██▍       | 108/436 [00:02<00:08, 40.82it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 42.28it/s][A
 27%|██▋       | 118/436 [00:02<00:07, 43.38it/s][A
 28%|██▊       | 123/436 [00:02<00:07, 44.24it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 44.85it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.29it/s][A
 32%|███▏      | 138/436 [00:03<00:06, 45.19it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.42it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.70it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.65it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.81it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.96it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.91it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.07it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.98it/s][A
 42%|████▏     | 183/436 [00:04<00:05, 45.93it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.87it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.85it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.99it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.03it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.09it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.08it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.12it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.01it/s][A
 52%|█████▏    | 228/436 [00:05<00:04, 45.99it/s][A
 53%|█████▎    | 233/436 [00:05<00:14, 14.23it/s][A
 55%|█████▍    | 238/436 [00:06<00:11, 17.97it/s][A
 56%|█████▌    | 243/436 [00:06<00:08, 22.00it/s][A
 57%|█████▋    | 248/436 [00:06<00:07, 26.14it/s][A
 58%|█████▊    | 253/436 [00:06<00:06, 30.05it/s][A
 59%|█████▉    | 258/436 [00:06<00:05, 33.60it/s][A
 60%|██████    | 263/436 [00:06<00:04, 36.63it/s][A
 61%|██████▏   | 268/436 [00:06<00:04, 39.00it/s][A
 63%|██████▎   | 273/436 [00:06<00:04, 40.65it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 42.13it/s][A
 65%|██████▍   | 283/436 [00:07<00:03, 43.19it/s][A
 66%|██████▌   | 288/436 [00:07<00:03, 44.04it/s][A
 67%|██████▋   | 293/436 [00:07<00:03, 44.67it/s][A
 68%|██████▊   | 298/436 [00:07<00:03, 45.10it/s][A
 69%|██████▉   | 303/436 [00:07<00:02, 45.44it/s][A
 71%|███████   | 308/436 [00:07<00:02, 45.77it/s][A
 72%|███████▏  | 313/436 [00:07<00:02, 45.68it/s][A
 73%|███████▎  | 318/436 [00:07<00:02, 45.76it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.82it/s][A
 75%|███████▌  | 328/436 [00:08<00:02, 45.84it/s][A
 76%|███████▋  | 333/436 [00:08<00:02, 45.83it/s][A
 78%|███████▊  | 338/436 [00:08<00:03, 24.99it/s][A
 79%|███████▊  | 343/436 [00:08<00:03, 28.98it/s][A
 80%|███████▉  | 348/436 [00:08<00:02, 32.66it/s][A
 81%|████████  | 353/436 [00:08<00:02, 35.84it/s][A
 82%|████████▏ | 358/436 [00:08<00:02, 38.41it/s][A
 83%|████████▎ | 363/436 [00:09<00:01, 40.50it/s][A
 84%|████████▍ | 368/436 [00:09<00:01, 42.04it/s][A
 86%|████████▌ | 373/436 [00:09<00:01, 43.30it/s][A
 87%|████████▋ | 378/436 [00:09<00:01, 43.66it/s][A
 88%|████████▊ | 383/436 [00:09<00:01, 44.36it/s][A
 89%|████████▉ | 388/436 [00:09<00:01, 44.88it/s][A
 90%|█████████ | 393/436 [00:09<00:00, 45.26it/s][A
 91%|█████████▏| 398/436 [00:09<00:00, 45.56it/s][A
 92%|█████████▏| 403/436 [00:09<00:00, 45.76it/s][A
 94%|█████████▎| 408/436 [00:10<00:00, 45.93it/s][A
 95%|█████████▍| 413/436 [00:10<00:00, 46.06it/s][A
 96%|█████████▌| 418/436 [00:10<00:00, 45.97it/s][A
 97%|█████████▋| 423/436 [00:10<00:00, 45.81it/s][A
 98%|█████████▊| 428/436 [00:10<00:00, 45.71it/s][A
 99%|█████████▉| 433/436 [00:10<00:00, 45.99it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:10<00:00, 45.99it/s][A 60%|██████    | 234/390 [02:13<00:47,  3.30it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:40:11,078 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 16:40:11,557 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:40:16,928 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:40:16,947 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:40:16,952 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:31<23:00,  8.91s/it] 61%|██████    | 236/390 [02:32<16:14,  6.33s/it] 61%|██████    | 237/390 [02:32<11:31,  4.52s/it] 61%|██████    | 238/390 [02:32<08:14,  3.25s/it] 61%|██████▏   | 239/390 [02:33<05:56,  2.36s/it] 62%|██████▏   | 240/390 [02:33<04:21,  1.74s/it] 62%|██████▏   | 241/390 [02:33<03:14,  1.31s/it] 62%|██████▏   | 242/390 [02:33<02:28,  1.00s/it] 62%|██████▏   | 243/390 [02:34<01:56,  1.27it/s] 63%|██████▎   | 244/390 [02:34<01:33,  1.56it/s] 63%|██████▎   | 245/390 [02:34<01:17,  1.87it/s] 63%|██████▎   | 246/390 [02:35<01:06,  2.16it/s] 63%|██████▎   | 247/390 [02:35<01:00,  2.38it/s] 64%|██████▎   | 248/390 [02:35<00:54,  2.62it/s] 64%|██████▍   | 249/390 [02:35<00:50,  2.82it/s] 64%|██████▍   | 250/390 [02:36<00:47,  2.97it/s] 64%|██████▍   | 251/390 [02:36<00:44,  3.10it/s] 65%|██████▍   | 252/390 [02:36<00:43,  3.19it/s] 65%|██████▍   | 253/390 [02:37<00:42,  3.26it/s] 65%|██████▌   | 254/390 [02:37<00:41,  3.30it/s] 65%|██████▌   | 255/390 [02:37<00:40,  3.34it/s] 66%|██████▌   | 256/390 [02:38<00:39,  3.37it/s] 66%|██████▌   | 257/390 [02:38<00:39,  3.38it/s] 66%|██████▌   | 258/390 [02:38<00:39,  3.36it/s] 66%|██████▋   | 259/390 [02:38<00:38,  3.38it/s] 67%|██████▋   | 260/390 [02:39<00:38,  3.39it/s] 67%|██████▋   | 261/390 [02:39<00:37,  3.40it/s] 67%|██████▋   | 262/390 [02:39<00:37,  3.41it/s] 67%|██████▋   | 263/390 [02:40<00:37,  3.41it/s] 68%|██████▊   | 264/390 [02:40<00:36,  3.41it/s] 68%|██████▊   | 265/390 [02:40<00:36,  3.41it/s] 68%|██████▊   | 266/390 [02:40<00:36,  3.42it/s] 68%|██████▊   | 267/390 [02:41<00:35,  3.42it/s] 69%|██████▊   | 268/390 [02:41<00:35,  3.42it/s] 69%|██████▉   | 269/390 [02:41<00:36,  3.33it/s] 69%|██████▉   | 270/390 [02:42<00:35,  3.36it/s] 69%|██████▉   | 271/390 [02:42<00:35,  3.37it/s] 70%|██████▉   | 272/390 [02:42<00:34,  3.39it/s] 70%|███████   | 273/390 [02:43<00:34,  3.40it/s] 70%|███████   | 274/390 [02:43<00:34,  3.40it/s] 71%|███████   | 275/390 [02:43<00:33,  3.41it/s] 71%|███████   | 276/390 [02:43<00:33,  3.41it/s] 71%|███████   | 277/390 [02:44<00:33,  3.41it/s] 71%|███████▏  | 278/390 [02:44<00:32,  3.41it/s] 72%|███████▏  | 279/390 [02:44<00:32,  3.41it/s] 72%|███████▏  | 280/390 [02:45<00:33,  3.33it/s] 72%|███████▏  | 281/390 [02:45<00:32,  3.36it/s] 72%|███████▏  | 282/390 [02:45<00:31,  3.38it/s] 73%|███████▎  | 283/390 [02:45<00:31,  3.39it/s] 73%|███████▎  | 284/390 [02:46<00:31,  3.40it/s] 73%|███████▎  | 285/390 [02:46<00:30,  3.41it/s] 73%|███████▎  | 286/390 [02:46<00:30,  3.40it/s] 74%|███████▎  | 287/390 [02:47<00:30,  3.41it/s] 74%|███████▍  | 288/390 [02:47<00:29,  3.41it/s] 74%|███████▍  | 289/390 [02:47<00:29,  3.41it/s] 74%|███████▍  | 290/390 [02:48<00:29,  3.41it/s] 75%|███████▍  | 291/390 [02:48<00:29,  3.33it/s] 75%|███████▍  | 292/390 [02:48<00:29,  3.36it/s] 75%|███████▌  | 293/390 [02:48<00:28,  3.37it/s] 75%|███████▌  | 294/390 [02:49<00:28,  3.39it/s] 76%|███████▌  | 295/390 [02:49<00:27,  3.40it/s] 76%|███████▌  | 296/390 [02:49<00:27,  3.40it/s] 76%|███████▌  | 297/390 [02:50<00:27,  3.41it/s] 76%|███████▋  | 298/390 [02:50<00:28,  3.28it/s] 77%|███████▋  | 299/390 [02:50<00:34,  2.67it/s] 77%|███████▋  | 300/390 [02:51<00:31,  2.85it/s] 77%|███████▋  | 301/390 [02:51<00:29,  3.00it/s] 77%|███████▋  | 302/390 [02:51<00:28,  3.11it/s] 78%|███████▊  | 303/390 [02:52<00:27,  3.20it/s] 78%|███████▊  | 304/390 [02:52<00:26,  3.26it/s] 78%|███████▊  | 305/390 [02:52<00:25,  3.31it/s] 78%|███████▊  | 306/390 [02:53<00:25,  3.34it/s] 79%|███████▊  | 307/390 [02:53<00:24,  3.36it/s] 79%|███████▉  | 308/390 [02:53<00:24,  3.38it/s] 79%|███████▉  | 309/390 [02:54<00:27,  2.92it/s] 79%|███████▉  | 310/390 [02:54<00:26,  3.05it/s] 80%|███████▉  | 311/390 [02:54<00:25,  3.15it/s] 80%|████████  | 312/390 [02:54<00:24,  3.23it/s][INFO|trainer.py:2140] 2023-08-28 16:40:51,968 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:40:51,968 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 16:40:51,968 >>   Batch size = 8
{'eval_loss': 1.0120190382003784, 'eval_runtime': 10.6736, 'eval_samples_per_second': 326.6, 'eval_steps_per_second': 40.848, 'epoch': 2.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.43it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.08it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.26it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.62it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.20it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.86it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.72it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.27it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.32it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.30it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.19it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.24it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.33it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.30it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.30it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.41it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.60it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.81it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.84it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.80it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.95it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.02it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.09it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.05it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.93it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.90it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.92it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.81it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.92it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.83it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.94it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.08it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.11it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.13it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.07it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.18it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.00it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.12it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.11it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.10it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.24it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.11it/s][A
 50%|█████     | 218/436 [00:05<00:04, 46.17it/s][A
 51%|█████     | 223/436 [00:05<00:09, 22.46it/s][A
 52%|█████▏    | 228/436 [00:05<00:07, 26.57it/s][A
 53%|█████▎    | 233/436 [00:05<00:06, 30.46it/s][A
 55%|█████▍    | 238/436 [00:05<00:05, 33.97it/s][A
 56%|█████▌    | 243/436 [00:05<00:05, 36.96it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 39.35it/s][A
 58%|█████▊    | 253/436 [00:05<00:04, 41.20it/s][A
 59%|█████▉    | 258/436 [00:05<00:04, 42.45it/s][A
 60%|██████    | 263/436 [00:06<00:04, 43.14it/s][A
 61%|██████▏   | 268/436 [00:06<00:03, 44.01it/s][A
 63%|██████▎   | 273/436 [00:06<00:03, 44.57it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.20it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.45it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.74it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.93it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.97it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.94it/s][A
 71%|███████   | 308/436 [00:07<00:02, 45.84it/s][A
 72%|███████▏  | 313/436 [00:07<00:02, 45.91it/s][A
 73%|███████▎  | 318/436 [00:07<00:02, 45.96it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 46.08it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.07it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.14it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.26it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.18it/s][A
 80%|███████▉  | 348/436 [00:08<00:02, 36.06it/s][A
 81%|████████  | 353/436 [00:08<00:02, 38.58it/s][A
 82%|████████▏ | 358/436 [00:08<00:01, 40.62it/s][A
 83%|████████▎ | 363/436 [00:08<00:01, 42.20it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 43.34it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 44.20it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 44.79it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.25it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.12it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.43it/s][A
 91%|█████████▏| 398/436 [00:09<00:00, 45.64it/s][A
 92%|█████████▏| 403/436 [00:09<00:00, 45.77it/s][A
 94%|█████████▎| 408/436 [00:09<00:00, 46.00it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 46.08it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.20it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.14it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.07it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.94it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 45.94it/s][A 80%|████████  | 312/390 [03:04<00:24,  3.23it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:41:02,505 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 16:41:03,416 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:41:10,306 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:41:10,360 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:41:10,385 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:25<12:11,  9.49s/it] 81%|████████  | 314/390 [03:26<08:31,  6.73s/it] 81%|████████  | 315/390 [03:26<06:00,  4.80s/it] 81%|████████  | 316/390 [03:26<04:15,  3.45s/it] 81%|████████▏ | 317/390 [03:27<03:02,  2.50s/it] 82%|████████▏ | 318/390 [03:27<02:12,  1.84s/it] 82%|████████▏ | 319/390 [03:27<01:37,  1.37s/it] 82%|████████▏ | 320/390 [03:27<01:13,  1.05s/it] 82%|████████▏ | 321/390 [03:28<00:56,  1.22it/s] 83%|████████▎ | 322/390 [03:28<00:45,  1.51it/s] 83%|████████▎ | 323/390 [03:28<00:36,  1.81it/s] 83%|████████▎ | 324/390 [03:29<00:31,  2.11it/s] 83%|████████▎ | 325/390 [03:29<00:28,  2.31it/s] 84%|████████▎ | 326/390 [03:29<00:24,  2.56it/s] 84%|████████▍ | 327/390 [03:29<00:22,  2.77it/s] 84%|████████▍ | 328/390 [03:30<00:21,  2.94it/s] 84%|████████▍ | 329/390 [03:30<00:19,  3.07it/s] 85%|████████▍ | 330/390 [03:30<00:18,  3.17it/s] 85%|████████▍ | 331/390 [03:31<00:18,  3.24it/s] 85%|████████▌ | 332/390 [03:31<00:17,  3.29it/s] 85%|████████▌ | 333/390 [03:31<00:17,  3.33it/s] 86%|████████▌ | 334/390 [03:32<00:16,  3.36it/s] 86%|████████▌ | 335/390 [03:32<00:16,  3.38it/s] 86%|████████▌ | 336/390 [03:32<00:16,  3.33it/s] 86%|████████▋ | 337/390 [03:32<00:15,  3.36it/s] 87%|████████▋ | 338/390 [03:33<00:15,  3.38it/s] 87%|████████▋ | 339/390 [03:33<00:15,  3.39it/s] 87%|████████▋ | 340/390 [03:33<00:14,  3.40it/s] 87%|████████▋ | 341/390 [03:34<00:14,  3.41it/s] 88%|████████▊ | 342/390 [03:34<00:14,  3.41it/s] 88%|████████▊ | 343/390 [03:34<00:13,  3.41it/s] 88%|████████▊ | 344/390 [03:34<00:13,  3.41it/s] 88%|████████▊ | 345/390 [03:35<00:13,  3.41it/s] 89%|████████▊ | 346/390 [03:35<00:12,  3.41it/s] 89%|████████▉ | 347/390 [03:35<00:12,  3.35it/s] 89%|████████▉ | 348/390 [03:36<00:12,  3.37it/s] 89%|████████▉ | 349/390 [03:36<00:12,  3.39it/s] 90%|████████▉ | 350/390 [03:36<00:11,  3.40it/s] 90%|█████████ | 351/390 [03:37<00:11,  3.40it/s] 90%|█████████ | 352/390 [03:37<00:11,  3.41it/s] 91%|█████████ | 353/390 [03:37<00:10,  3.41it/s] 91%|█████████ | 354/390 [03:37<00:10,  3.41it/s] 91%|█████████ | 355/390 [03:38<00:10,  3.42it/s] 91%|█████████▏| 356/390 [03:38<00:09,  3.41it/s] 92%|█████████▏| 357/390 [03:38<00:09,  3.41it/s] 92%|█████████▏| 358/390 [03:39<00:09,  3.23it/s] 92%|█████████▏| 359/390 [03:39<00:09,  3.28it/s] 92%|█████████▏| 360/390 [03:39<00:09,  3.32it/s] 93%|█████████▎| 361/390 [03:40<00:08,  3.35it/s] 93%|█████████▎| 362/390 [03:40<00:08,  3.37it/s] 93%|█████████▎| 363/390 [03:40<00:07,  3.38it/s] 93%|█████████▎| 364/390 [03:40<00:07,  3.39it/s] 94%|█████████▎| 365/390 [03:41<00:07,  3.40it/s] 94%|█████████▍| 366/390 [03:41<00:07,  3.40it/s] 94%|█████████▍| 367/390 [03:41<00:06,  3.41it/s] 94%|█████████▍| 368/390 [03:42<00:06,  3.41it/s] 95%|█████████▍| 369/390 [03:42<00:06,  3.38it/s] 95%|█████████▍| 370/390 [03:42<00:05,  3.39it/s] 95%|█████████▌| 371/390 [03:42<00:05,  3.40it/s] 95%|█████████▌| 372/390 [03:43<00:05,  3.40it/s] 96%|█████████▌| 373/390 [03:43<00:04,  3.40it/s] 96%|█████████▌| 374/390 [03:43<00:04,  3.41it/s] 96%|█████████▌| 375/390 [03:44<00:04,  3.41it/s] 96%|█████████▋| 376/390 [03:44<00:04,  3.41it/s] 97%|█████████▋| 377/390 [03:44<00:03,  3.41it/s] 97%|█████████▋| 378/390 [03:45<00:03,  3.41it/s] 97%|█████████▋| 379/390 [03:45<00:03,  3.41it/s] 97%|█████████▋| 380/390 [03:45<00:02,  3.40it/s] 98%|█████████▊| 381/390 [03:45<00:02,  3.40it/s] 98%|█████████▊| 382/390 [03:46<00:02,  3.41it/s] 98%|█████████▊| 383/390 [03:46<00:02,  3.41it/s] 98%|█████████▊| 384/390 [03:46<00:01,  3.41it/s] 99%|█████████▊| 385/390 [03:47<00:01,  3.41it/s] 99%|█████████▉| 386/390 [03:47<00:01,  3.41it/s] 99%|█████████▉| 387/390 [03:47<00:00,  3.40it/s] 99%|█████████▉| 388/390 [03:47<00:00,  3.40it/s]100%|█████████▉| 389/390 [03:48<00:00,  3.41it/s]100%|██████████| 390/390 [03:48<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 16:41:45,521 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:41:45,521 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 16:41:45,521 >>   Batch size = 8
{'eval_loss': 1.0208591222763062, 'eval_runtime': 9.9393, 'eval_samples_per_second': 350.728, 'eval_steps_per_second': 43.866, 'epoch': 3.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:08, 48.72it/s][A
  3%|▎         | 11/436 [00:00<00:08, 48.72it/s][A
  4%|▎         | 16/436 [00:00<00:08, 47.55it/s][A
  5%|▍         | 21/436 [00:00<00:08, 47.04it/s][A
  6%|▌         | 26/436 [00:00<00:08, 46.78it/s][A
  7%|▋         | 31/436 [00:00<00:08, 46.48it/s][A
  8%|▊         | 36/436 [00:00<00:08, 46.34it/s][A
  9%|▉         | 41/436 [00:00<00:08, 46.18it/s][A
 11%|█         | 46/436 [00:00<00:08, 46.05it/s][A
 12%|█▏        | 51/436 [00:01<00:08, 46.05it/s][A
 13%|█▎        | 56/436 [00:01<00:08, 46.06it/s][A
 14%|█▍        | 61/436 [00:01<00:08, 46.04it/s][A
 15%|█▌        | 66/436 [00:01<00:08, 46.21it/s][A
 16%|█▋        | 71/436 [00:01<00:07, 46.15it/s][A
 17%|█▋        | 76/436 [00:01<00:07, 46.18it/s][A
 19%|█▊        | 81/436 [00:01<00:07, 46.26it/s][A
 20%|█▉        | 86/436 [00:01<00:07, 45.10it/s][A
 21%|██        | 91/436 [00:01<00:07, 45.56it/s][A
 22%|██▏       | 96/436 [00:02<00:07, 44.11it/s][A
 23%|██▎       | 101/436 [00:02<00:08, 38.94it/s][A
 24%|██▍       | 106/436 [00:02<00:08, 39.43it/s][A
 25%|██▌       | 111/436 [00:02<00:07, 41.22it/s][A
 27%|██▋       | 116/436 [00:02<00:07, 42.57it/s][A
 28%|██▊       | 121/436 [00:02<00:07, 43.71it/s][A
 29%|██▉       | 126/436 [00:02<00:06, 44.39it/s][A
 30%|███       | 131/436 [00:02<00:06, 44.99it/s][A
 31%|███       | 136/436 [00:03<00:06, 45.43it/s][A
 32%|███▏      | 141/436 [00:03<00:06, 44.62it/s][A
 33%|███▎      | 146/436 [00:03<00:06, 44.88it/s][A
 35%|███▍      | 151/436 [00:03<00:06, 45.27it/s][A
 36%|███▌      | 156/436 [00:03<00:06, 45.48it/s][A
 37%|███▋      | 161/436 [00:03<00:06, 45.68it/s][A
 38%|███▊      | 166/436 [00:03<00:05, 45.94it/s][A
 39%|███▉      | 171/436 [00:03<00:05, 45.90it/s][A
 40%|████      | 176/436 [00:03<00:05, 46.05it/s][A
 42%|████▏     | 181/436 [00:04<00:05, 46.08it/s][A
 43%|████▎     | 186/436 [00:04<00:05, 46.01it/s][A
 44%|████▍     | 191/436 [00:04<00:05, 45.96it/s][A
 45%|████▍     | 196/436 [00:04<00:05, 46.01it/s][A
 46%|████▌     | 201/436 [00:04<00:05, 45.98it/s][A
 47%|████▋     | 206/436 [00:04<00:05, 45.95it/s][A
 48%|████▊     | 211/436 [00:04<00:04, 46.07it/s][A
 50%|████▉     | 216/436 [00:04<00:04, 46.04it/s][A
 51%|█████     | 221/436 [00:04<00:04, 46.20it/s][A
 52%|█████▏    | 226/436 [00:04<00:04, 46.19it/s][A
 53%|█████▎    | 231/436 [00:05<00:04, 46.02it/s][A
 54%|█████▍    | 236/436 [00:05<00:04, 46.06it/s][A
 55%|█████▌    | 241/436 [00:05<00:04, 45.96it/s][A
 56%|█████▋    | 246/436 [00:05<00:04, 45.90it/s][A
 58%|█████▊    | 251/436 [00:05<00:04, 46.04it/s][A
 59%|█████▊    | 256/436 [00:05<00:03, 46.02it/s][A
 60%|█████▉    | 261/436 [00:05<00:03, 46.02it/s][A
 61%|██████    | 266/436 [00:05<00:03, 46.16it/s][A
 62%|██████▏   | 271/436 [00:05<00:03, 46.16it/s][A
 63%|██████▎   | 276/436 [00:06<00:03, 46.07it/s][A
 64%|██████▍   | 281/436 [00:06<00:03, 46.05it/s][A
 66%|██████▌   | 286/436 [00:06<00:03, 45.70it/s][A
 67%|██████▋   | 291/436 [00:06<00:03, 44.92it/s][A
 68%|██████▊   | 296/436 [00:06<00:03, 45.32it/s][A
 69%|██████▉   | 301/436 [00:06<00:02, 45.55it/s][A
 70%|███████   | 306/436 [00:06<00:02, 45.74it/s][A
 71%|███████▏  | 311/436 [00:06<00:02, 45.86it/s][A
 72%|███████▏  | 316/436 [00:06<00:02, 45.90it/s][A
 74%|███████▎  | 321/436 [00:07<00:02, 45.87it/s][A
 75%|███████▍  | 326/436 [00:07<00:02, 45.88it/s][A
 76%|███████▌  | 331/436 [00:07<00:02, 45.84it/s][A
 77%|███████▋  | 336/436 [00:07<00:02, 45.85it/s][A
 78%|███████▊  | 341/436 [00:07<00:02, 45.98it/s][A
 79%|███████▉  | 346/436 [00:07<00:01, 45.84it/s][A
 81%|████████  | 351/436 [00:07<00:01, 46.03it/s][A
 82%|████████▏ | 356/436 [00:07<00:01, 46.07it/s][A
 83%|████████▎ | 361/436 [00:07<00:01, 46.07it/s][A
 84%|████████▍ | 366/436 [00:08<00:01, 46.05it/s][A
 85%|████████▌ | 371/436 [00:08<00:01, 45.54it/s][A
 86%|████████▌ | 376/436 [00:08<00:01, 45.86it/s][A
 87%|████████▋ | 381/436 [00:08<00:01, 45.95it/s][A
 89%|████████▊ | 386/436 [00:08<00:01, 45.98it/s][A
 90%|████████▉ | 391/436 [00:08<00:00, 46.05it/s][A
 91%|█████████ | 396/436 [00:08<00:00, 46.04it/s][A
 92%|█████████▏| 401/436 [00:08<00:00, 46.06it/s][A
 93%|█████████▎| 406/436 [00:08<00:00, 46.12it/s][A
 94%|█████████▍| 411/436 [00:09<00:00, 46.01it/s][A
 95%|█████████▌| 416/436 [00:09<00:00, 45.98it/s][A
 97%|█████████▋| 421/436 [00:09<00:00, 45.96it/s][A
 98%|█████████▊| 426/436 [00:09<00:00, 46.04it/s][A
 99%|█████████▉| 431/436 [00:09<00:00, 45.55it/s][A
100%|██████████| 436/436 [00:09<00:00, 45.75it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 45.75it/s][A100%|██████████| 390/390 [03:58<00:00,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:41:55,152 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 16:41:55,256 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:41:58,092 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:41:58,233 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:41:58,310 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 16:42:06,059 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 16:42:06,061 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78 (score: 0.9860376715660095).
                                                 100%|██████████| 390/390 [04:13<00:00,  3.41it/s]100%|██████████| 390/390 [04:13<00:00,  1.54it/s]
[INFO|trainer.py:1894] 2023-08-28 16:42:10,150 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 16:42:10,177 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:42:16,216 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:42:16,255 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:42:16,265 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:42:16,497 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:16,497 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:16,497 >>   train_loss               =     0.6244
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:16,498 >>   train_runtime            = 0:04:13.16
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:16,498 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:16,498 >>   train_samples_per_second =      98.75
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:16,498 >>   train_steps_per_second   =      1.541
{'eval_loss': 1.0244542360305786, 'eval_runtime': 9.5849, 'eval_samples_per_second': 363.696, 'eval_steps_per_second': 45.488, 'epoch': 4.99}
{'train_runtime': 253.1644, 'train_samples_per_second': 98.75, 'train_steps_per_second': 1.541, 'train_loss': 0.6244245089017428, 'epoch': 4.99}
08/28/2023 16:42:16 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 16:42:16,528 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:42:16,528 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 16:42:16,528 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 57.47it/s]  3%|▎         | 12/436 [00:00<00:08, 50.47it/s]  4%|▍         | 18/436 [00:00<00:08, 48.70it/s]  5%|▌         | 23/436 [00:00<00:08, 47.95it/s]  6%|▋         | 28/436 [00:00<00:08, 47.40it/s]  8%|▊         | 33/436 [00:00<00:08, 47.24it/s]  9%|▊         | 38/436 [00:00<00:08, 47.00it/s] 10%|▉         | 43/436 [00:00<00:08, 46.84it/s] 11%|█         | 48/436 [00:01<00:08, 46.75it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.58it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.65it/s] 14%|█▍        | 63/436 [00:01<00:07, 46.65it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.64it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.59it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.51it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.51it/s] 20%|██        | 88/436 [00:01<00:07, 46.55it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.51it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.41it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.44it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.53it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.47it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.42it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.53it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.35it/s] 31%|███       | 133/436 [00:02<00:06, 45.86it/s] 32%|███▏      | 138/436 [00:02<00:06, 46.13it/s] 33%|███▎      | 143/436 [00:03<00:06, 46.13it/s] 34%|███▍      | 148/436 [00:03<00:06, 46.22it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.33it/s] 36%|███▌      | 158/436 [00:03<00:05, 46.37it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.35it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.44it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.31it/s] 41%|████      | 178/436 [00:03<00:05, 46.19it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.23it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.32it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.28it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.23it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.19it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.28it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.26it/s] 50%|█████     | 218/436 [00:04<00:04, 46.27it/s] 51%|█████     | 223/436 [00:04<00:04, 46.36it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.30it/s] 53%|█████▎    | 233/436 [00:04<00:04, 46.39it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.30it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.38it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.33it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.26it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.31it/s] 60%|██████    | 263/436 [00:05<00:03, 46.35it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.39it/s] 63%|██████▎   | 273/436 [00:05<00:03, 46.33it/s] 64%|██████▍   | 278/436 [00:05<00:03, 46.35it/s] 65%|██████▍   | 283/436 [00:06<00:03, 46.41it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.33it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.38it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.24it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.27it/s] 71%|███████   | 308/436 [00:06<00:02, 46.23it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.29it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.39it/s] 74%|███████▍  | 323/436 [00:06<00:02, 46.27it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.18it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.33it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.32it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.38it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.33it/s] 81%|████████  | 353/436 [00:07<00:01, 46.25it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.20it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.28it/s] 84%|████████▍ | 368/436 [00:07<00:01, 46.32it/s] 86%|████████▌ | 373/436 [00:08<00:01, 46.33it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.30it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.29it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.21it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.29it/s] 91%|█████████▏| 398/436 [00:08<00:00, 46.25it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.28it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.29it/s] 95%|█████████▍| 413/436 [00:08<00:00, 46.27it/s] 96%|█████████▌| 418/436 [00:08<00:00, 46.24it/s] 97%|█████████▋| 423/436 [00:09<00:00, 45.54it/s] 98%|█████████▊| 428/436 [00:09<00:00, 45.83it/s] 99%|█████████▉| 433/436 [00:09<00:00, 45.91it/s]100%|██████████| 436/436 [00:09<00:00, 46.42it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:42:25,945 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:25,945 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:25,945 >>   eval_loss               =      0.986
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:25,945 >>   eval_runtime            = 0:00:09.41
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:25,945 >>   eval_samples            =       3486
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:25,945 >>   eval_samples_per_second =    370.181
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:25,946 >>   eval_steps_per_second   =     46.299
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:42:25,946 >>   perplexity              =     2.6806
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:34,173 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:34,181 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:34,181 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:34,181 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:34,181 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:42:34,811 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:42:34,812 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:42:35,387 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:42:36,414 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:42:36,414 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:39,574 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:39,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:39,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:39,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:39,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:42:40,217 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:42:40,218 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:42:40,776 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:42:40,929 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:42:40,929 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'manufacturer', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13015
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13115, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.12it/s]Extractor Predicting: 2it [00:01,  1.16it/s]Extractor Predicting: 3it [00:02,  1.19it/s]Extractor Predicting: 4it [00:03,  1.17it/s]Extractor Predicting: 5it [00:04,  1.14it/s]Extractor Predicting: 6it [00:05,  1.15it/s]Extractor Predicting: 7it [00:06,  1.14it/s]Extractor Predicting: 8it [00:06,  1.17it/s]Extractor Predicting: 9it [00:07,  1.16it/s]Extractor Predicting: 10it [00:08,  1.19it/s]Extractor Predicting: 11it [00:09,  1.16it/s]Extractor Predicting: 12it [00:10,  1.17it/s]Extractor Predicting: 13it [00:11,  1.16it/s]Extractor Predicting: 14it [00:12,  1.15it/s]Extractor Predicting: 15it [00:12,  1.17it/s]Extractor Predicting: 16it [00:13,  1.17it/s]Extractor Predicting: 17it [00:14,  1.20it/s]Extractor Predicting: 18it [00:15,  1.21it/s]Extractor Predicting: 19it [00:16,  1.20it/s]Extractor Predicting: 20it [00:17,  1.18it/s]Extractor Predicting: 21it [00:17,  1.19it/s]Extractor Predicting: 22it [00:18,  1.21it/s]Extractor Predicting: 23it [00:19,  1.21it/s]Extractor Predicting: 24it [00:20,  1.18it/s]Extractor Predicting: 25it [00:21,  1.17it/s]Extractor Predicting: 26it [00:22,  1.14it/s]Extractor Predicting: 27it [00:23,  1.16it/s]Extractor Predicting: 28it [00:23,  1.17it/s]Extractor Predicting: 29it [00:24,  1.16it/s]Extractor Predicting: 30it [00:25,  1.11it/s]Extractor Predicting: 31it [00:26,  1.13it/s]Extractor Predicting: 32it [00:27,  1.14it/s]Extractor Predicting: 33it [00:28,  1.14it/s]Extractor Predicting: 34it [00:29,  1.15it/s]Extractor Predicting: 35it [00:30,  1.16it/s]Extractor Predicting: 36it [00:30,  1.18it/s]Extractor Predicting: 37it [00:31,  1.19it/s]Extractor Predicting: 38it [00:32,  1.22it/s]Extractor Predicting: 39it [00:33,  1.22it/s]Extractor Predicting: 40it [00:34,  1.24it/s]Extractor Predicting: 41it [00:34,  1.23it/s]Extractor Predicting: 42it [00:35,  1.22it/s]Extractor Predicting: 43it [00:36,  1.21it/s]Extractor Predicting: 44it [00:37,  1.19it/s]Extractor Predicting: 45it [00:38,  1.23it/s]Extractor Predicting: 46it [00:39,  1.22it/s]Extractor Predicting: 47it [00:39,  1.23it/s]Extractor Predicting: 48it [00:40,  1.23it/s]Extractor Predicting: 49it [00:41,  1.25it/s]Extractor Predicting: 50it [00:42,  1.24it/s]Extractor Predicting: 51it [00:43,  1.15it/s]Extractor Predicting: 52it [00:44,  1.16it/s]Extractor Predicting: 53it [00:44,  1.17it/s]Extractor Predicting: 54it [00:45,  1.16it/s]Extractor Predicting: 55it [00:46,  1.20it/s]Extractor Predicting: 56it [00:47,  1.22it/s]Extractor Predicting: 57it [00:48,  1.20it/s]Extractor Predicting: 58it [00:49,  1.18it/s]Extractor Predicting: 59it [00:49,  1.20it/s]Extractor Predicting: 60it [00:50,  1.19it/s]Extractor Predicting: 61it [00:51,  1.18it/s]Extractor Predicting: 62it [00:52,  1.19it/s]Extractor Predicting: 63it [00:53,  1.20it/s]Extractor Predicting: 64it [00:54,  1.20it/s]Extractor Predicting: 65it [00:54,  1.20it/s]Extractor Predicting: 66it [00:55,  1.21it/s]Extractor Predicting: 67it [00:56,  1.23it/s]Extractor Predicting: 68it [00:57,  1.20it/s]Extractor Predicting: 69it [00:58,  1.20it/s]Extractor Predicting: 70it [00:59,  1.20it/s]Extractor Predicting: 71it [00:59,  1.22it/s]Extractor Predicting: 72it [01:00,  1.23it/s]Extractor Predicting: 73it [01:01,  1.22it/s]Extractor Predicting: 74it [01:02,  1.23it/s]Extractor Predicting: 75it [01:03,  1.20it/s]Extractor Predicting: 76it [01:03,  1.21it/s]Extractor Predicting: 77it [01:04,  1.20it/s]Extractor Predicting: 78it [01:05,  1.21it/s]Extractor Predicting: 79it [01:06,  1.21it/s]Extractor Predicting: 80it [01:07,  1.22it/s]Extractor Predicting: 81it [01:08,  1.21it/s]Extractor Predicting: 82it [01:08,  1.22it/s]Extractor Predicting: 83it [01:09,  1.18it/s]Extractor Predicting: 84it [01:10,  1.20it/s]Extractor Predicting: 85it [01:11,  1.19it/s]Extractor Predicting: 86it [01:12,  1.20it/s]Extractor Predicting: 87it [01:13,  1.19it/s]Extractor Predicting: 88it [01:13,  1.22it/s]Extractor Predicting: 89it [01:14,  1.23it/s]Extractor Predicting: 90it [01:15,  1.26it/s]Extractor Predicting: 91it [01:16,  1.29it/s]Extractor Predicting: 92it [01:16,  1.30it/s]Extractor Predicting: 93it [01:17,  1.27it/s]Extractor Predicting: 94it [01:18,  1.30it/s]Extractor Predicting: 95it [01:19,  1.28it/s]Extractor Predicting: 96it [01:20,  1.28it/s]Extractor Predicting: 97it [01:20,  1.28it/s]Extractor Predicting: 98it [01:21,  1.25it/s]Extractor Predicting: 99it [01:22,  1.23it/s]Extractor Predicting: 100it [01:23,  1.23it/s]Extractor Predicting: 101it [01:24,  1.28it/s]Extractor Predicting: 102it [01:24,  1.30it/s]Extractor Predicting: 103it [01:25,  1.29it/s]Extractor Predicting: 104it [01:26,  1.28it/s]Extractor Predicting: 105it [01:27,  1.29it/s]Extractor Predicting: 106it [01:27,  1.29it/s]Extractor Predicting: 107it [01:28,  1.28it/s]Extractor Predicting: 108it [01:29,  1.28it/s]Extractor Predicting: 109it [01:30,  1.29it/s]Extractor Predicting: 110it [01:31,  1.29it/s]Extractor Predicting: 111it [01:31,  1.29it/s]Extractor Predicting: 112it [01:32,  1.33it/s]Extractor Predicting: 113it [01:33,  1.34it/s]Extractor Predicting: 114it [01:34,  1.32it/s]Extractor Predicting: 115it [01:34,  1.32it/s]Extractor Predicting: 116it [01:35,  1.29it/s]Extractor Predicting: 117it [01:36,  1.31it/s]Extractor Predicting: 118it [01:37,  1.33it/s]Extractor Predicting: 119it [01:37,  1.31it/s]Extractor Predicting: 120it [01:38,  1.27it/s]Extractor Predicting: 121it [01:39,  1.24it/s]Extractor Predicting: 122it [01:40,  1.25it/s]Extractor Predicting: 123it [01:41,  1.19it/s]Extractor Predicting: 124it [01:41,  1.24it/s]Extractor Predicting: 125it [01:42,  1.26it/s]Extractor Predicting: 126it [01:43,  1.26it/s]Extractor Predicting: 127it [01:44,  1.27it/s]Extractor Predicting: 128it [01:45,  1.28it/s]Extractor Predicting: 129it [01:45,  1.27it/s]Extractor Predicting: 130it [01:46,  1.24it/s]Extractor Predicting: 131it [01:47,  1.29it/s]Extractor Predicting: 132it [01:48,  1.29it/s]Extractor Predicting: 133it [01:49,  1.28it/s]Extractor Predicting: 134it [01:49,  1.19it/s]Extractor Predicting: 135it [01:50,  1.23it/s]Extractor Predicting: 136it [01:51,  1.27it/s]Extractor Predicting: 137it [01:52,  1.28it/s]Extractor Predicting: 138it [01:53,  1.28it/s]Extractor Predicting: 139it [01:53,  1.32it/s]Extractor Predicting: 140it [01:54,  1.34it/s]Extractor Predicting: 141it [01:55,  1.38it/s]Extractor Predicting: 142it [01:55,  1.36it/s]Extractor Predicting: 143it [01:56,  1.28it/s]Extractor Predicting: 144it [01:57,  1.58it/s]Extractor Predicting: 144it [01:57,  1.23it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:44:47,118 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:44:47,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:44:47,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:44:47,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:44:47,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:44:47,778 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:44:47,779 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:44:48,429 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:44:49,487 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:44:49,487 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:44:52,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:44:52,609 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:44:52,609 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:44:52,609 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:44:52,609 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:44:53,590 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:44:53,591 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:44:54,302 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:44:54,524 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:44:54,525 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5509259259259259,
  "recall": 0.1706827309236948,
  "score": 0.2606219886114761,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13522
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13622, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.30it/s]Extractor Predicting: 3it [00:02,  1.27it/s]Extractor Predicting: 4it [00:03,  1.31it/s]Extractor Predicting: 5it [00:03,  1.29it/s]Extractor Predicting: 6it [00:04,  1.26it/s]Extractor Predicting: 7it [00:05,  1.26it/s]Extractor Predicting: 8it [00:06,  1.29it/s]Extractor Predicting: 9it [00:07,  1.29it/s]Extractor Predicting: 10it [00:07,  1.31it/s]Extractor Predicting: 11it [00:08,  1.28it/s]Extractor Predicting: 12it [00:09,  1.29it/s]Extractor Predicting: 13it [00:10,  1.27it/s]Extractor Predicting: 14it [00:10,  1.29it/s]Extractor Predicting: 15it [00:11,  1.29it/s]Extractor Predicting: 16it [00:12,  1.26it/s]Extractor Predicting: 17it [00:13,  1.26it/s]Extractor Predicting: 18it [00:14,  1.21it/s]Extractor Predicting: 19it [00:14,  1.23it/s]Extractor Predicting: 20it [00:15,  1.23it/s]Extractor Predicting: 21it [00:16,  1.26it/s]Extractor Predicting: 22it [00:17,  1.21it/s]Extractor Predicting: 23it [00:18,  1.21it/s]Extractor Predicting: 24it [00:19,  1.21it/s]Extractor Predicting: 25it [00:19,  1.23it/s]Extractor Predicting: 26it [00:21,  1.07it/s]Extractor Predicting: 27it [00:21,  1.12it/s]Extractor Predicting: 28it [00:22,  1.17it/s]Extractor Predicting: 29it [00:23,  1.20it/s]Extractor Predicting: 30it [00:24,  1.10it/s]Extractor Predicting: 31it [00:25,  1.16it/s]Extractor Predicting: 32it [00:26,  1.17it/s]Extractor Predicting: 33it [00:26,  1.20it/s]Extractor Predicting: 34it [00:27,  1.17it/s]Extractor Predicting: 35it [00:28,  1.19it/s]Extractor Predicting: 36it [00:29,  1.25it/s]Extractor Predicting: 37it [00:30,  1.23it/s]Extractor Predicting: 38it [00:30,  1.22it/s]Extractor Predicting: 39it [00:31,  1.19it/s]Extractor Predicting: 40it [00:32,  1.19it/s]Extractor Predicting: 41it [00:33,  1.20it/s]Extractor Predicting: 42it [00:34,  1.22it/s]Extractor Predicting: 43it [00:35,  1.22it/s]Extractor Predicting: 44it [00:36,  1.15it/s]Extractor Predicting: 45it [00:36,  1.18it/s]Extractor Predicting: 46it [00:37,  1.19it/s]Extractor Predicting: 47it [00:38,  1.21it/s]Extractor Predicting: 48it [00:39,  1.22it/s]Extractor Predicting: 49it [00:40,  1.25it/s]Extractor Predicting: 50it [00:40,  1.25it/s]Extractor Predicting: 51it [00:41,  1.28it/s]Extractor Predicting: 52it [00:42,  1.28it/s]Extractor Predicting: 53it [00:46,  1.83s/it]Extractor Predicting: 54it [00:47,  1.52s/it]Extractor Predicting: 55it [00:48,  1.30s/it]Extractor Predicting: 56it [00:49,  1.16s/it]Extractor Predicting: 57it [00:50,  1.08s/it]Extractor Predicting: 58it [00:50,  1.01s/it]Extractor Predicting: 59it [00:51,  1.05it/s]Extractor Predicting: 60it [00:52,  1.11it/s]Extractor Predicting: 61it [00:53,  1.12it/s]Extractor Predicting: 62it [00:54,  1.13it/s]Extractor Predicting: 63it [00:54,  1.20it/s]Extractor Predicting: 64it [00:55,  1.19it/s]Extractor Predicting: 65it [00:56,  1.22it/s]Extractor Predicting: 66it [00:57,  1.22it/s]Extractor Predicting: 67it [00:58,  1.22it/s]Extractor Predicting: 68it [00:59,  1.21it/s]Extractor Predicting: 69it [00:59,  1.21it/s]Extractor Predicting: 70it [01:00,  1.16it/s]Extractor Predicting: 71it [01:01,  1.18it/s]Extractor Predicting: 72it [01:02,  1.17it/s]Extractor Predicting: 73it [01:03,  1.17it/s]Extractor Predicting: 74it [01:04,  1.19it/s]Extractor Predicting: 75it [01:05,  1.17it/s]Extractor Predicting: 76it [01:05,  1.18it/s]Extractor Predicting: 77it [01:06,  1.17it/s]Extractor Predicting: 78it [01:07,  1.18it/s]Extractor Predicting: 79it [01:08,  1.20it/s]Extractor Predicting: 80it [01:09,  1.16it/s]Extractor Predicting: 81it [01:10,  1.18it/s]Extractor Predicting: 82it [01:11,  1.16it/s]Extractor Predicting: 83it [01:11,  1.16it/s]Extractor Predicting: 84it [01:12,  1.18it/s]Extractor Predicting: 85it [01:13,  1.19it/s]Extractor Predicting: 86it [01:14,  1.20it/s]Extractor Predicting: 87it [01:15,  1.19it/s]Extractor Predicting: 88it [01:15,  1.21it/s]Extractor Predicting: 89it [01:16,  1.22it/s]Extractor Predicting: 90it [01:17,  1.21it/s]Extractor Predicting: 91it [01:18,  1.26it/s]Extractor Predicting: 92it [01:19,  1.21it/s]Extractor Predicting: 93it [01:19,  1.25it/s]Extractor Predicting: 94it [01:20,  1.25it/s]Extractor Predicting: 95it [01:21,  1.25it/s]Extractor Predicting: 96it [01:22,  1.24it/s]Extractor Predicting: 97it [01:23,  1.23it/s]Extractor Predicting: 98it [01:24,  1.22it/s]Extractor Predicting: 99it [01:24,  1.23it/s]Extractor Predicting: 100it [01:25,  1.21it/s]Extractor Predicting: 101it [01:26,  1.21it/s]Extractor Predicting: 102it [01:27,  1.18it/s]Extractor Predicting: 103it [01:28,  1.20it/s]Extractor Predicting: 104it [01:29,  1.20it/s]Extractor Predicting: 105it [01:29,  1.23it/s]Extractor Predicting: 106it [01:30,  1.24it/s]Extractor Predicting: 107it [01:31,  1.24it/s]Extractor Predicting: 108it [01:32,  1.24it/s]Extractor Predicting: 109it [01:33,  1.24it/s]Extractor Predicting: 110it [01:33,  1.22it/s]Extractor Predicting: 111it [01:34,  1.24it/s]Extractor Predicting: 112it [01:35,  1.23it/s]Extractor Predicting: 113it [01:36,  1.25it/s]Extractor Predicting: 114it [01:37,  1.24it/s]Extractor Predicting: 115it [01:37,  1.26it/s]Extractor Predicting: 116it [01:38,  1.21it/s]Extractor Predicting: 117it [01:39,  1.12it/s]Extractor Predicting: 118it [01:40,  1.13it/s]Extractor Predicting: 119it [01:41,  1.14it/s]Extractor Predicting: 120it [01:42,  1.15it/s]Extractor Predicting: 121it [01:43,  1.16it/s]Extractor Predicting: 122it [01:44,  1.19it/s]Extractor Predicting: 123it [01:44,  1.19it/s]Extractor Predicting: 124it [01:45,  1.17it/s]Extractor Predicting: 125it [01:46,  1.16it/s]Extractor Predicting: 126it [01:47,  1.13it/s]Extractor Predicting: 127it [01:48,  1.14it/s]Extractor Predicting: 128it [01:49,  1.15it/s]Extractor Predicting: 129it [01:50,  1.14it/s]Extractor Predicting: 130it [01:50,  1.17it/s]Extractor Predicting: 131it [01:51,  1.18it/s]Extractor Predicting: 132it [01:52,  1.17it/s]Extractor Predicting: 133it [01:53,  1.16it/s]Extractor Predicting: 134it [01:54,  1.17it/s]Extractor Predicting: 135it [01:55,  1.19it/s]Extractor Predicting: 136it [01:56,  1.21it/s]Extractor Predicting: 137it [01:56,  1.23it/s]Extractor Predicting: 138it [01:57,  1.22it/s]Extractor Predicting: 139it [01:58,  1.23it/s]Extractor Predicting: 140it [01:59,  1.22it/s]Extractor Predicting: 141it [02:00,  1.19it/s]Extractor Predicting: 142it [02:00,  1.18it/s]Extractor Predicting: 143it [02:03,  1.39s/it]Extractor Predicting: 143it [02:03,  1.16it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:17,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:17,350 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:17,350 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:17,351 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:17,351 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:47:17,710 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:47:17,711 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:47:17,978 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:47:19,064 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:47:19,064 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:20,570 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:20,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:20,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:20,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:20,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:47:21,016 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:47:21,018 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:47:21,310 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:47:21,477 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:47:21,477 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4111328125,
  "recall": 0.12302746931618937,
  "score": 0.18938371569950518,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 464
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 564, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.06it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 2it [00:01,  1.38it/s]
[INFO|configuration_utils.py:515] 2023-08-28 16:47:23,310 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:47:23,311 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:47:23,320 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:47:23,321 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 16:47:23,323 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:47:28,842 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 16:47:28,842 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 16:47:28,976 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:47:28,977 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:47:29,005 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:47:29,011 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:47:29,012 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:47:29,012 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:47:29,012 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:47:29,012 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:47:29,012 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7,
  "recall": 0.08974358974358974,
  "score": 0.1590909090909091,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 16:47:29,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:30,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:31,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:32,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:33,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:33,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:35,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:36,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:37,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:38,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:38,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:39,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:40,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:42,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:43,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:43,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:44,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:45,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:46,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:47,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:48,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:49,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:49,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:21<03:11, 21.22s/it][WARNING|generation_utils.py:914] 2023-08-28 16:47:50,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:51,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:52,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:53,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:54,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:55,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:56,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:57,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:58,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:47:59,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:00,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:01,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:02,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:03,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:03,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:04,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:05,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:06,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:08,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:08,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:40<02:39, 19.91s/it][WARNING|generation_utils.py:914] 2023-08-28 16:48:09,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:10,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:11,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:12,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:13,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:13,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:14,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:15,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:16,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:17,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:18,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:19,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:20,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:21,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:21,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:22,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:23,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:24,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:25,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:26,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:27,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:28,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:59<02:17, 19.58s/it][WARNING|generation_utils.py:914] 2023-08-28 16:48:28,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:30,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:30,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:31,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:32,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:33,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:34,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:35,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:36,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:36,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:37,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:38,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:39,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:40,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:41,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:42,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:43,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:43,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:44,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:45,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:46,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:17<01:54, 19.09s/it][WARNING|generation_utils.py:914] 2023-08-28 16:48:47,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:48,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:49,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:49,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:50,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:51,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:52,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:53,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:54,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:55,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:56,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:57,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:58,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:58,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:48:59,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:00,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:01,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:02,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:03,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:04,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:04,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:05,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:06,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:07,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:39<01:39, 19.88s/it][WARNING|generation_utils.py:914] 2023-08-28 16:49:08,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:09,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:10,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:11,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:12,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:12,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:13,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:14,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:15,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:16,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:17,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:18,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:19,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:20,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:21,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:22,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:23,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:24,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:24,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:25,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:27,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:28,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:59<01:20, 20.15s/it][WARNING|generation_utils.py:914] 2023-08-28 16:49:29,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:30,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:31,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:31,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:33,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:34,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:35,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:36,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:37,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:38,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:39,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:40,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:41,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:42,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:43,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:44,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:45,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:46,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:46,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:47,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:48,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:20<01:00, 20.24s/it][WARNING|generation_utils.py:914] 2023-08-28 16:49:49,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:50,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:51,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:52,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:53,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:54,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:55,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:56,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:57,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:58,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:49:59,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:00,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:01,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:02,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:02,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:03,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:04,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:05,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:06,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:07,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:08,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:40<00:40, 20.20s/it][WARNING|generation_utils.py:914] 2023-08-28 16:50:09,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:10,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:11,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:12,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:13,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:14,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:15,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:16,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:17,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:18,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:18,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:19,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:20,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:21,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:22,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:23,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:24,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:25,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:26,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:27,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:28,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:59<00:19, 19.88s/it][WARNING|generation_utils.py:914] 2023-08-28 16:50:28,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:29,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:30,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:31,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:32,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:33,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:34,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:35,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:36,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:37,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:38,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:39,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:40,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:41,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:41,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:42,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:43,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:45,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:46,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:47,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:50:48,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:19<00:00, 20.03s/it]Generating: 100%|██████████| 10/10 [03:19<00:00, 19.98s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:50:54,059 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:50:54,075 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:50:54,075 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:50:54,075 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:50:54,075 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:50:54,446 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:50:54,447 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:50:54,709 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:50:55,780 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:50:55,780 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:50:57,086 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:50:57,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:50:57,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:50:57,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:50:57,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:50:57,410 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:50:57,411 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:50:57,691 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:50:57,847 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:50:57,847 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : country .', 'success_rate': 0.8478260869565217, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : followed by .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : genre .', 'success_rate': 0.8536931818181818, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9196428571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : voice type .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8764204545454546, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9211309523809523, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 8028
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8128, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.20it/s]Extractor Estimating: 2it [00:01,  1.23it/s]Extractor Estimating: 3it [00:02,  1.16it/s]Extractor Estimating: 4it [00:03,  1.25it/s]Extractor Estimating: 5it [00:03,  1.30it/s]Extractor Estimating: 6it [00:04,  1.28it/s]Extractor Estimating: 7it [00:05,  1.32it/s]Extractor Estimating: 8it [00:06,  1.38it/s]Extractor Estimating: 9it [00:06,  1.33it/s]Extractor Estimating: 10it [00:07,  1.28it/s]Extractor Estimating: 11it [00:08,  1.27it/s]Extractor Estimating: 12it [00:09,  1.33it/s]Extractor Estimating: 13it [00:10,  1.28it/s]Extractor Estimating: 14it [00:10,  1.30it/s]Extractor Estimating: 15it [00:11,  1.26it/s]Extractor Estimating: 16it [00:12,  1.31it/s]Extractor Estimating: 17it [00:13,  1.30it/s]Extractor Estimating: 18it [00:14,  1.25it/s]Extractor Estimating: 19it [00:14,  1.27it/s]Extractor Estimating: 20it [00:15,  1.25it/s]Extractor Estimating: 21it [00:16,  1.26it/s]Extractor Estimating: 22it [00:17,  1.25it/s]Extractor Estimating: 23it [00:17,  1.30it/s]Extractor Estimating: 24it [00:18,  1.30it/s]Extractor Estimating: 25it [00:19,  1.33it/s]Extractor Estimating: 26it [00:20,  1.29it/s]Extractor Estimating: 27it [00:21,  1.28it/s]Extractor Estimating: 28it [00:21,  1.23it/s]Extractor Estimating: 29it [00:22,  1.25it/s]Extractor Estimating: 30it [00:23,  1.24it/s]Extractor Estimating: 31it [00:24,  1.25it/s]Extractor Estimating: 32it [00:25,  1.25it/s]Extractor Estimating: 33it [00:25,  1.26it/s]Extractor Estimating: 34it [00:26,  1.27it/s]Extractor Estimating: 35it [00:27,  1.25it/s]Extractor Estimating: 36it [00:28,  1.23it/s]Extractor Estimating: 37it [00:29,  1.20it/s]Extractor Estimating: 38it [00:30,  1.17it/s]Extractor Estimating: 39it [00:30,  1.17it/s]Extractor Estimating: 40it [00:31,  1.19it/s]Extractor Estimating: 41it [00:32,  1.27it/s]Extractor Estimating: 42it [00:33,  1.29it/s]Extractor Estimating: 43it [00:33,  1.31it/s]Extractor Estimating: 44it [00:34,  1.31it/s]Extractor Estimating: 45it [00:35,  1.27it/s]Extractor Estimating: 46it [00:36,  1.24it/s]Extractor Estimating: 47it [00:37,  1.25it/s]Extractor Estimating: 48it [00:37,  1.28it/s]Extractor Estimating: 49it [00:38,  1.31it/s]Extractor Estimating: 50it [00:39,  1.28it/s]Extractor Estimating: 51it [00:40,  1.31it/s]Extractor Estimating: 52it [00:40,  1.32it/s]Extractor Estimating: 53it [00:41,  1.32it/s]Extractor Estimating: 54it [00:42,  1.29it/s]Extractor Estimating: 55it [00:43,  1.29it/s]Extractor Estimating: 56it [00:44,  1.29it/s]Extractor Estimating: 57it [00:44,  1.29it/s]Extractor Estimating: 58it [00:45,  1.23it/s]Extractor Estimating: 59it [00:46,  1.25it/s]Extractor Estimating: 60it [00:47,  1.32it/s]Extractor Estimating: 61it [00:47,  1.31it/s]Extractor Estimating: 62it [00:48,  1.28it/s]Extractor Estimating: 63it [00:49,  1.34it/s]Extractor Estimating: 64it [00:50,  1.34it/s]Extractor Estimating: 65it [00:50,  1.37it/s]Extractor Estimating: 66it [00:51,  1.36it/s]Extractor Estimating: 67it [00:52,  1.41it/s]Extractor Estimating: 68it [00:53,  1.37it/s]Extractor Estimating: 69it [00:53,  1.35it/s]Extractor Estimating: 70it [00:54,  1.32it/s]Extractor Estimating: 71it [00:55,  1.28it/s]Extractor Estimating: 72it [00:56,  1.32it/s]Extractor Estimating: 73it [00:56,  1.32it/s]Extractor Estimating: 74it [00:57,  1.35it/s]Extractor Estimating: 75it [00:58,  1.34it/s]Extractor Estimating: 76it [00:59,  1.35it/s]Extractor Estimating: 77it [00:59,  1.37it/s]Extractor Estimating: 78it [01:00,  1.38it/s]Extractor Estimating: 79it [01:01,  1.40it/s]Extractor Estimating: 80it [01:01,  1.41it/s]Extractor Estimating: 81it [01:02,  1.43it/s]Extractor Estimating: 82it [01:03,  1.35it/s]Extractor Estimating: 83it [01:04,  1.35it/s]Extractor Estimating: 84it [01:04,  1.36it/s]Extractor Estimating: 85it [01:05,  1.44it/s]Extractor Estimating: 86it [01:06,  1.32it/s]Extractor Estimating: 87it [01:07,  1.33it/s]Extractor Estimating: 88it [01:07,  1.29it/s]Extractor Estimating: 89it [01:08,  1.33it/s]Extractor Estimating: 90it [01:13,  2.11s/it]Extractor Estimating: 91it [01:15,  1.84s/it]Extractor Estimating: 92it [01:15,  1.54s/it]Extractor Estimating: 93it [01:16,  1.29s/it]Extractor Estimating: 94it [01:17,  1.10s/it]Extractor Estimating: 95it [01:17,  1.03it/s]Extractor Estimating: 96it [01:18,  1.14it/s]Extractor Estimating: 97it [01:19,  1.18it/s]Extractor Estimating: 98it [01:20,  1.24it/s]Extractor Estimating: 99it [01:20,  1.29it/s]Extractor Estimating: 100it [01:21,  1.36it/s]Extractor Estimating: 101it [01:22,  1.32it/s]Extractor Estimating: 102it [01:23,  1.32it/s]Extractor Estimating: 103it [01:23,  1.31it/s]Extractor Estimating: 104it [01:24,  1.30it/s]Extractor Estimating: 105it [01:25,  1.33it/s]Extractor Estimating: 106it [01:26,  1.32it/s]Extractor Estimating: 107it [01:26,  1.34it/s]Extractor Estimating: 108it [01:27,  1.30it/s]Extractor Estimating: 109it [01:28,  1.30it/s]Extractor Estimating: 110it [01:29,  1.32it/s]Extractor Estimating: 111it [01:29,  1.28it/s]Extractor Estimating: 112it [01:30,  1.29it/s]Extractor Estimating: 113it [01:31,  1.31it/s]Extractor Estimating: 114it [01:32,  1.28it/s]Extractor Estimating: 115it [01:33,  1.29it/s]Extractor Estimating: 116it [01:33,  1.27it/s]Extractor Estimating: 117it [01:34,  1.32it/s]Extractor Estimating: 118it [01:35,  1.33it/s]Extractor Estimating: 119it [01:35,  1.34it/s]Extractor Estimating: 120it [01:36,  1.33it/s]Extractor Estimating: 121it [01:37,  1.32it/s]Extractor Estimating: 122it [01:38,  1.34it/s]Extractor Estimating: 123it [01:39,  1.31it/s]Extractor Estimating: 124it [01:39,  1.29it/s]Extractor Estimating: 125it [01:40,  1.28it/s]Extractor Estimating: 126it [01:41,  1.30it/s]Extractor Estimating: 127it [01:42,  1.33it/s]Extractor Estimating: 128it [01:42,  1.36it/s]Extractor Estimating: 129it [01:43,  1.42it/s]Extractor Estimating: 130it [01:44,  1.39it/s]Extractor Estimating: 131it [01:44,  1.40it/s]Extractor Estimating: 132it [01:45,  1.42it/s]Extractor Estimating: 133it [01:46,  1.49it/s]Extractor Estimating: 134it [01:46,  1.46it/s]Extractor Estimating: 135it [01:47,  1.45it/s]Extractor Estimating: 136it [01:48,  1.47it/s]Extractor Estimating: 137it [01:49,  1.39it/s]Extractor Estimating: 138it [01:49,  1.47it/s]Extractor Estimating: 139it [01:50,  1.46it/s]Extractor Estimating: 140it [01:51,  1.46it/s]Extractor Estimating: 141it [01:51,  1.43it/s]Extractor Estimating: 142it [01:52,  1.25it/s]Extractor Estimating: 143it [01:53,  1.31it/s]Extractor Estimating: 144it [01:54,  1.32it/s]Extractor Estimating: 145it [01:54,  1.39it/s]Extractor Estimating: 146it [01:55,  1.43it/s]Extractor Estimating: 147it [01:56,  1.42it/s]Extractor Estimating: 148it [01:56,  1.44it/s]Extractor Estimating: 149it [01:57,  1.44it/s]Extractor Estimating: 150it [01:58,  1.44it/s]Extractor Estimating: 151it [01:58,  1.43it/s]Extractor Estimating: 152it [01:59,  1.39it/s]Extractor Estimating: 153it [02:00,  1.37it/s]Extractor Estimating: 154it [02:01,  1.38it/s]Extractor Estimating: 155it [02:01,  1.38it/s]Extractor Estimating: 156it [02:02,  1.37it/s]Extractor Estimating: 157it [02:03,  1.31it/s]Extractor Estimating: 158it [02:04,  1.28it/s]Extractor Estimating: 159it [02:05,  1.32it/s]Extractor Estimating: 160it [02:05,  1.30it/s]Extractor Estimating: 161it [02:06,  1.30it/s]Extractor Estimating: 162it [02:07,  1.34it/s]Extractor Estimating: 163it [02:08,  1.34it/s]Extractor Estimating: 164it [02:08,  1.37it/s]Extractor Estimating: 165it [02:09,  1.33it/s]Extractor Estimating: 166it [02:10,  1.35it/s]Extractor Estimating: 167it [02:10,  1.38it/s]Extractor Estimating: 168it [02:11,  1.38it/s]Extractor Estimating: 169it [02:12,  1.38it/s]Extractor Estimating: 170it [02:13,  1.36it/s]Extractor Estimating: 171it [02:13,  1.36it/s]Extractor Estimating: 172it [02:14,  1.36it/s]Extractor Estimating: 173it [02:15,  1.36it/s]Extractor Estimating: 174it [02:16,  1.40it/s]Extractor Estimating: 175it [02:16,  1.35it/s]Extractor Estimating: 176it [02:17,  1.38it/s]Extractor Estimating: 177it [02:18,  1.32it/s]Extractor Estimating: 178it [02:19,  1.22it/s]Extractor Estimating: 179it [02:20,  1.26it/s]Extractor Estimating: 180it [02:20,  1.31it/s]Extractor Estimating: 181it [02:21,  1.26it/s]Extractor Estimating: 182it [02:22,  1.27it/s]Extractor Estimating: 183it [02:23,  1.29it/s]Extractor Estimating: 184it [02:23,  1.32it/s]Extractor Estimating: 185it [02:24,  1.34it/s]Extractor Estimating: 186it [02:25,  1.29it/s]Extractor Estimating: 187it [02:26,  1.30it/s]Extractor Estimating: 188it [02:27,  1.26it/s]Extractor Estimating: 189it [02:27,  1.26it/s]Extractor Estimating: 190it [02:28,  1.20it/s]Extractor Estimating: 191it [02:29,  1.26it/s]Extractor Estimating: 192it [02:30,  1.28it/s]Extractor Estimating: 193it [02:31,  1.26it/s]Extractor Estimating: 194it [02:31,  1.29it/s]Extractor Estimating: 195it [02:32,  1.23it/s]Extractor Estimating: 196it [02:33,  1.26it/s]Extractor Estimating: 197it [02:34,  1.26it/s]Extractor Estimating: 198it [02:34,  1.27it/s]Extractor Estimating: 199it [02:35,  1.28it/s]Extractor Estimating: 200it [02:36,  1.29it/s]Extractor Estimating: 201it [02:37,  1.30it/s]Extractor Estimating: 202it [02:37,  1.32it/s]Extractor Estimating: 203it [02:38,  1.33it/s]Extractor Estimating: 204it [02:39,  1.31it/s]Extractor Estimating: 205it [02:40,  1.35it/s]Extractor Estimating: 206it [02:40,  1.35it/s]Extractor Estimating: 207it [02:41,  1.39it/s]Extractor Estimating: 208it [02:42,  1.40it/s]Extractor Estimating: 209it [02:43,  1.39it/s]Extractor Estimating: 210it [02:43,  1.38it/s]Extractor Estimating: 211it [02:44,  1.36it/s]Extractor Estimating: 212it [02:45,  1.38it/s]Extractor Estimating: 213it [02:45,  1.38it/s]Extractor Estimating: 214it [02:46,  1.38it/s]Extractor Estimating: 215it [02:47,  1.40it/s]Extractor Estimating: 216it [02:48,  1.38it/s]Extractor Estimating: 217it [02:48,  1.41it/s]Extractor Estimating: 218it [02:49,  1.37it/s]Extractor Estimating: 219it [02:50,  1.40it/s]Extractor Estimating: 220it [02:51,  1.37it/s]Extractor Estimating: 221it [02:51,  1.36it/s]Extractor Estimating: 222it [02:52,  1.38it/s]Extractor Estimating: 223it [02:53,  1.41it/s]Extractor Estimating: 224it [02:53,  1.41it/s]Extractor Estimating: 225it [02:54,  1.42it/s]Extractor Estimating: 226it [02:55,  1.42it/s]Extractor Estimating: 227it [02:55,  1.40it/s]Extractor Estimating: 228it [02:56,  1.47it/s]Extractor Estimating: 229it [02:57,  1.52it/s]Extractor Estimating: 230it [02:57,  1.47it/s]Extractor Estimating: 231it [02:58,  1.50it/s]Extractor Estimating: 232it [02:59,  1.54it/s]Extractor Estimating: 233it [02:59,  1.49it/s]Extractor Estimating: 234it [03:00,  1.47it/s]Extractor Estimating: 235it [03:01,  1.44it/s]Extractor Estimating: 236it [03:01,  1.47it/s]Extractor Estimating: 237it [03:02,  1.46it/s]Extractor Estimating: 238it [03:03,  1.49it/s]Extractor Estimating: 239it [03:03,  1.51it/s]Extractor Estimating: 240it [03:04,  1.52it/s]Extractor Estimating: 241it [03:05,  1.48it/s]Extractor Estimating: 242it [03:06,  1.46it/s]Extractor Estimating: 243it [03:06,  1.49it/s]Extractor Estimating: 244it [03:07,  1.53it/s]Extractor Estimating: 245it [03:07,  1.51it/s]Extractor Estimating: 246it [03:08,  1.50it/s]Extractor Estimating: 247it [03:09,  1.50it/s]Extractor Estimating: 248it [03:09,  1.51it/s]Extractor Estimating: 249it [03:10,  1.44it/s]Extractor Estimating: 250it [03:11,  1.37it/s]Extractor Estimating: 250it [03:11,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:54:22,629 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:54:22,635 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:54:22,635 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:54:22,636 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:54:22,636 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:54:22,978 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:54:22,979 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:54:23,682 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:54:24,754 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:54:24,754 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:54:27,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:54:27,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:54:27,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:54:27,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:54:27,631 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:54:28,322 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:54:28,323 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:54:28,956 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:54:29,134 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:54:29,134 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 18:48:18,809 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 18:48:18,844 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 4996 mean pseudo reward: 0.9470286806664014
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl'}
train vocab size: 18135
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18235, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=18235, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.304, loss:624.0623
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.404, loss:528.5845
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.309, loss:532.5459
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.307, loss:498.5789
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.302, loss:456.5538
>> valid entity prec:0.5348, rec:0.5004, f1:0.5170
>> valid relation prec:0.2033, rec:0.0886, f1:0.1235
>> valid relation with NER prec:0.2033, rec:0.0886, f1:0.1235
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.979, loss:487.4508
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.317, loss:453.2767
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.305, loss:483.4338
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.306, loss:469.3239
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.328, loss:472.7869
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4910, rec:0.5668, f1:0.5262
>> valid relation prec:0.1073, rec:0.0579, f1:0.0752
>> valid relation with NER prec:0.1073, rec:0.0579, f1:0.0752
new max entity f1 on valid!
g_step 1100, step 55, avg_time 2.968, loss:465.1722
g_step 1200, step 155, avg_time 1.317, loss:462.5129
g_step 1300, step 46, avg_time 1.328, loss:465.5881
g_step 1400, step 146, avg_time 1.310, loss:425.6643
g_step 1500, step 37, avg_time 1.306, loss:439.8913
>> valid entity prec:0.5406, rec:0.4322, f1:0.4804
>> valid relation prec:0.1838, rec:0.0694, f1:0.1008
>> valid relation with NER prec:0.1838, rec:0.0694, f1:0.1008
g_step 1600, step 137, avg_time 2.974, loss:428.8741
g_step 1700, step 28, avg_time 1.340, loss:404.4562
g_step 1800, step 128, avg_time 1.310, loss:384.0961
g_step 1900, step 19, avg_time 1.291, loss:394.5667
g_step 2000, step 119, avg_time 1.330, loss:381.0613
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5326, rec:0.4971, f1:0.5143
>> valid relation prec:0.1826, rec:0.0752, f1:0.1065
>> valid relation with NER prec:0.1826, rec:0.0752, f1:0.1065
g_step 2100, step 10, avg_time 2.955, loss:370.6579
g_step 2200, step 110, avg_time 1.318, loss:342.9905
g_step 2300, step 1, avg_time 1.301, loss:385.0296
g_step 2400, step 101, avg_time 1.316, loss:355.1412
g_step 2500, step 201, avg_time 1.302, loss:366.7887
>> valid entity prec:0.4962, rec:0.5150, f1:0.5054
>> valid relation prec:0.1897, rec:0.0858, f1:0.1181
>> valid relation with NER prec:0.1897, rec:0.0858, f1:0.1181
g_step 2600, step 92, avg_time 2.988, loss:324.8108
g_step 2700, step 192, avg_time 1.305, loss:364.5334
g_step 2800, step 83, avg_time 1.305, loss:325.4316
g_step 2900, step 183, avg_time 1.326, loss:331.5246
g_step 3000, step 74, avg_time 1.297, loss:311.7918
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5194, rec:0.4903, f1:0.5045
>> valid relation prec:0.1895, rec:0.0932, f1:0.1250
>> valid relation with NER prec:0.1895, rec:0.0932, f1:0.1250
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 174, avg_time 2.986, loss:320.1367
g_step 3200, step 65, avg_time 1.308, loss:300.2639
g_step 3300, step 165, avg_time 1.323, loss:293.8522
g_step 3400, step 56, avg_time 1.308, loss:298.2144
g_step 3500, step 156, avg_time 1.297, loss:287.6446
>> valid entity prec:0.5175, rec:0.4664, f1:0.4906
>> valid relation prec:0.1499, rec:0.0760, f1:0.1009
>> valid relation with NER prec:0.1499, rec:0.0760, f1:0.1009
g_step 3600, step 47, avg_time 3.007, loss:290.3624
g_step 3700, step 147, avg_time 1.313, loss:282.0072
g_step 3800, step 38, avg_time 1.296, loss:278.0036
g_step 3900, step 138, avg_time 1.321, loss:287.7252
g_step 4000, step 29, avg_time 1.315, loss:291.3376
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5356, rec:0.4577, f1:0.4936
>> valid relation prec:0.1958, rec:0.0835, f1:0.1171
>> valid relation with NER prec:0.1958, rec:0.0835, f1:0.1171
g_step 4100, step 129, avg_time 2.987, loss:267.6732
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 18:48:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 18:48:18 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_18-48-18_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 18:48:19 - WARNING - datasets.builder -   Using custom data configuration default-81221469100b8428
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-81221469100b8428/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 18:48:21,111 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:48:21,112 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:48:21,113 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:48:21,114 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:48:21,148 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:48:21,168 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:48:21,168 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:48:21,168 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:48:21,168 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:48:21,168 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:48:21,168 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 18:48:21,359 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:48:24,938 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 18:48:25,146 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-81221469100b8428/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.08ba/s] 40%|████      | 2/5 [00:00<00:00,  3.15ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.78ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.20ba/s]100%|██████████| 5/5 [00:01<00:00,  3.79ba/s]100%|██████████| 5/5 [00:01<00:00,  3.59ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.57ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.55ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.19ba/s]100%|██████████| 4/4 [00:01<00:00,  4.31ba/s]100%|██████████| 4/4 [00:01<00:00,  3.37ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  7.92ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.55ba/s]100%|██████████| 5/5 [00:00<00:00,  9.95ba/s]100%|██████████| 5/5 [00:00<00:00,  9.73ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.67ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  7.87ba/s]100%|██████████| 4/4 [00:00<00:00,  8.70ba/s]
[INFO|trainer.py:414] 2023-08-28 18:48:30,775 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 18:48:30,783 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 18:48:30,783 >>   Num examples = 4999
[INFO|trainer.py:1149] 2023-08-28 18:48:30,783 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 18:48:30,783 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 18:48:30,783 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 18:48:30,783 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 18:48:30,783 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:56,  3.33it/s]  1%|          | 2/390 [00:00<01:56,  3.32it/s]  1%|          | 3/390 [00:00<01:54,  3.37it/s]  1%|          | 4/390 [00:01<01:53,  3.40it/s]  1%|▏         | 5/390 [00:01<01:52,  3.41it/s]  2%|▏         | 6/390 [00:01<01:52,  3.42it/s]  2%|▏         | 7/390 [00:02<01:51,  3.43it/s]  2%|▏         | 8/390 [00:02<01:51,  3.43it/s]  2%|▏         | 9/390 [00:02<01:50,  3.43it/s]  3%|▎         | 10/390 [00:02<01:50,  3.43it/s]  3%|▎         | 11/390 [00:03<01:50,  3.43it/s]  3%|▎         | 12/390 [00:03<01:50,  3.43it/s]  3%|▎         | 13/390 [00:03<01:49,  3.44it/s]  4%|▎         | 14/390 [00:04<01:49,  3.44it/s]  4%|▍         | 15/390 [00:04<01:49,  3.44it/s]  4%|▍         | 16/390 [00:04<01:48,  3.43it/s]  4%|▍         | 17/390 [00:04<01:48,  3.43it/s]  5%|▍         | 18/390 [00:05<01:48,  3.43it/s]  5%|▍         | 19/390 [00:05<01:48,  3.44it/s]  5%|▌         | 20/390 [00:05<01:47,  3.43it/s]  5%|▌         | 21/390 [00:06<01:47,  3.43it/s]  6%|▌         | 22/390 [00:06<01:47,  3.43it/s]  6%|▌         | 23/390 [00:06<01:47,  3.42it/s]  6%|▌         | 24/390 [00:07<01:46,  3.42it/s]  6%|▋         | 25/390 [00:07<01:46,  3.43it/s]  7%|▋         | 26/390 [00:07<01:46,  3.43it/s]  7%|▋         | 27/390 [00:07<01:45,  3.43it/s]  7%|▋         | 28/390 [00:08<01:45,  3.43it/s]  7%|▋         | 29/390 [00:08<01:45,  3.43it/s]  8%|▊         | 30/390 [00:08<01:44,  3.43it/s]  8%|▊         | 31/390 [00:09<01:44,  3.43it/s]  8%|▊         | 32/390 [00:09<01:44,  3.43it/s]  8%|▊         | 33/390 [00:09<01:44,  3.43it/s]  9%|▊         | 34/390 [00:09<01:43,  3.43it/s]  9%|▉         | 35/390 [00:10<01:43,  3.43it/s]  9%|▉         | 36/390 [00:10<01:43,  3.43it/s]  9%|▉         | 37/390 [00:10<01:43,  3.43it/s] 10%|▉         | 38/390 [00:11<01:42,  3.43it/s] 10%|█         | 39/390 [00:11<01:42,  3.42it/s] 10%|█         | 40/390 [00:11<01:42,  3.43it/s] 11%|█         | 41/390 [00:11<01:41,  3.43it/s] 11%|█         | 42/390 [00:12<01:41,  3.43it/s] 11%|█         | 43/390 [00:12<01:41,  3.42it/s] 11%|█▏        | 44/390 [00:12<01:41,  3.43it/s] 12%|█▏        | 45/390 [00:13<01:40,  3.43it/s] 12%|█▏        | 46/390 [00:13<01:40,  3.43it/s] 12%|█▏        | 47/390 [00:13<01:40,  3.43it/s] 12%|█▏        | 48/390 [00:14<01:39,  3.43it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.43it/s] 13%|█▎        | 50/390 [00:14<01:39,  3.43it/s] 13%|█▎        | 51/390 [00:14<01:38,  3.43it/s] 13%|█▎        | 52/390 [00:15<01:38,  3.43it/s] 14%|█▎        | 53/390 [00:15<01:38,  3.43it/s] 14%|█▍        | 54/390 [00:15<01:38,  3.42it/s] 14%|█▍        | 55/390 [00:16<01:37,  3.42it/s] 14%|█▍        | 56/390 [00:16<01:37,  3.42it/s] 15%|█▍        | 57/390 [00:16<01:37,  3.42it/s] 15%|█▍        | 58/390 [00:16<01:36,  3.42it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.42it/s] 15%|█▌        | 60/390 [00:17<01:36,  3.43it/s] 16%|█▌        | 61/390 [00:17<01:36,  3.42it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.43it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.42it/s] 16%|█▋        | 64/390 [00:18<01:35,  3.42it/s] 17%|█▋        | 65/390 [00:18<01:35,  3.41it/s] 17%|█▋        | 66/390 [00:19<01:34,  3.42it/s] 17%|█▋        | 67/390 [00:19<01:34,  3.42it/s] 17%|█▋        | 68/390 [00:19<01:34,  3.42it/s] 18%|█▊        | 69/390 [00:20<01:33,  3.42it/s] 18%|█▊        | 70/390 [00:20<01:33,  3.42it/s] 18%|█▊        | 71/390 [00:20<01:33,  3.42it/s] 18%|█▊        | 72/390 [00:21<01:32,  3.43it/s] 19%|█▊        | 73/390 [00:21<01:32,  3.43it/s] 19%|█▉        | 74/390 [00:21<01:32,  3.43it/s] 19%|█▉        | 75/390 [00:21<01:31,  3.43it/s] 19%|█▉        | 76/390 [00:22<01:31,  3.43it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.42it/s] 20%|██        | 78/390 [00:22<01:31,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 18:48:53,614 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:48:53,614 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 18:48:53,614 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 57.20it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.19it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.57it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.77it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.40it/s][A
  8%|▊         | 33/436 [00:00<00:08, 47.14it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.88it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.42it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.28it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.29it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.23it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.31it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.46it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.29it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.40it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.29it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.05it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.11it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.01it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.17it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.18it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.18it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.19it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.18it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.18it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.15it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.14it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.10it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.11it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.14it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.20it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.38it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.40it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.32it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.30it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.20it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.15it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.17it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.01it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.18it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.22it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.35it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.16it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.22it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.16it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.13it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.18it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.21it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.34it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.36it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.15it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.29it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.26it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.32it/s][A
 64%|██████▍   | 278/436 [00:05<00:03, 46.24it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.16it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.33it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.32it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.38it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.37it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.35it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.42it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.34it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.22it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.21it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.33it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.38it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.38it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.42it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.36it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.37it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.43it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.24it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.30it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.33it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.24it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.37it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.37it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.30it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.34it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.24it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.14it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.07it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.12it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.03it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.10it/s][A
                                                 [A                                                
100%|██████████| 436/436 [00:09<00:00, 46.10it/s][A 20%|██        | 78/390 [00:32<01:31,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:49:03,649 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 18:49:04,713 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:49:16,113 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:49:16,734 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:49:17,009 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [01:05<1:07:37, 13.05s/it] 21%|██        | 80/390 [01:05<47:44,  9.24s/it]   21%|██        | 81/390 [01:06<33:45,  6.55s/it] 21%|██        | 82/390 [01:06<24:00,  4.68s/it] 21%|██▏       | 83/390 [01:06<17:11,  3.36s/it] 22%|██▏       | 84/390 [01:07<12:26,  2.44s/it] 22%|██▏       | 85/390 [01:08<10:33,  2.08s/it] 22%|██▏       | 86/390 [01:08<07:48,  1.54s/it] 22%|██▏       | 87/390 [01:09<06:02,  1.20s/it] 23%|██▎       | 88/390 [01:09<04:39,  1.08it/s] 23%|██▎       | 89/390 [01:09<03:41,  1.36it/s] 23%|██▎       | 90/390 [01:09<03:00,  1.66it/s] 23%|██▎       | 91/390 [01:10<02:32,  1.97it/s] 24%|██▎       | 92/390 [01:10<02:12,  2.25it/s] 24%|██▍       | 93/390 [01:10<01:58,  2.51it/s] 24%|██▍       | 94/390 [01:11<01:48,  2.73it/s] 24%|██▍       | 95/390 [01:11<01:41,  2.91it/s] 25%|██▍       | 96/390 [01:11<01:36,  3.05it/s] 25%|██▍       | 97/390 [01:11<01:33,  3.12it/s] 25%|██▌       | 98/390 [01:12<01:31,  3.21it/s] 25%|██▌       | 99/390 [01:12<01:28,  3.27it/s] 26%|██▌       | 100/390 [01:12<01:27,  3.32it/s] 26%|██▌       | 101/390 [01:13<01:26,  3.35it/s] 26%|██▌       | 102/390 [01:13<01:25,  3.37it/s] 26%|██▋       | 103/390 [01:13<01:24,  3.38it/s] 27%|██▋       | 104/390 [01:13<01:24,  3.39it/s] 27%|██▋       | 105/390 [01:14<01:23,  3.40it/s] 27%|██▋       | 106/390 [01:14<01:23,  3.41it/s] 27%|██▋       | 107/390 [01:14<01:22,  3.41it/s] 28%|██▊       | 108/390 [01:15<01:50,  2.55it/s] 28%|██▊       | 109/390 [01:15<01:41,  2.76it/s] 28%|██▊       | 110/390 [01:16<01:35,  2.94it/s] 28%|██▊       | 111/390 [01:16<01:30,  3.07it/s] 29%|██▊       | 112/390 [01:16<01:27,  3.17it/s] 29%|██▉       | 113/390 [01:16<01:25,  3.25it/s] 29%|██▉       | 114/390 [01:17<01:23,  3.30it/s] 29%|██▉       | 115/390 [01:17<01:22,  3.34it/s] 30%|██▉       | 116/390 [01:17<01:21,  3.37it/s] 30%|███       | 117/390 [01:18<01:20,  3.39it/s] 30%|███       | 118/390 [01:18<01:25,  3.17it/s] 31%|███       | 119/390 [01:18<01:23,  3.24it/s] 31%|███       | 120/390 [01:19<01:21,  3.30it/s] 31%|███       | 121/390 [01:19<01:20,  3.34it/s] 31%|███▏      | 122/390 [01:19<01:19,  3.36it/s] 32%|███▏      | 123/390 [01:19<01:18,  3.39it/s] 32%|███▏      | 124/390 [01:20<01:18,  3.40it/s] 32%|███▏      | 125/390 [01:20<01:17,  3.41it/s] 32%|███▏      | 126/390 [01:20<01:17,  3.42it/s] 33%|███▎      | 127/390 [01:21<01:16,  3.42it/s] 33%|███▎      | 128/390 [01:21<01:16,  3.43it/s] 33%|███▎      | 129/390 [01:21<01:17,  3.37it/s] 33%|███▎      | 130/390 [01:21<01:16,  3.38it/s] 34%|███▎      | 131/390 [01:22<01:16,  3.40it/s] 34%|███▍      | 132/390 [01:22<01:15,  3.41it/s] 34%|███▍      | 133/390 [01:22<01:15,  3.41it/s] 34%|███▍      | 134/390 [01:23<01:14,  3.42it/s] 35%|███▍      | 135/390 [01:23<01:14,  3.42it/s] 35%|███▍      | 136/390 [01:23<01:14,  3.42it/s] 35%|███▌      | 137/390 [01:24<01:13,  3.42it/s] 35%|███▌      | 138/390 [01:24<01:13,  3.42it/s] 36%|███▌      | 139/390 [01:24<01:13,  3.42it/s] 36%|███▌      | 140/390 [01:24<01:13,  3.42it/s] 36%|███▌      | 141/390 [01:25<01:12,  3.43it/s] 36%|███▋      | 142/390 [01:25<01:12,  3.43it/s] 37%|███▋      | 143/390 [01:25<01:12,  3.43it/s] 37%|███▋      | 144/390 [01:26<01:11,  3.43it/s] 37%|███▋      | 145/390 [01:26<01:11,  3.43it/s] 37%|███▋      | 146/390 [01:26<01:11,  3.43it/s] 38%|███▊      | 147/390 [01:26<01:12,  3.36it/s] 38%|███▊      | 148/390 [01:27<01:11,  3.38it/s] 38%|███▊      | 149/390 [01:27<01:10,  3.40it/s] 38%|███▊      | 150/390 [01:27<01:10,  3.41it/s] 39%|███▊      | 151/390 [01:28<01:10,  3.41it/s] 39%|███▉      | 152/390 [01:28<01:09,  3.41it/s] 39%|███▉      | 153/390 [01:28<01:09,  3.41it/s] 39%|███▉      | 154/390 [01:29<01:09,  3.42it/s] 40%|███▉      | 155/390 [01:29<01:08,  3.42it/s] 40%|████      | 156/390 [01:29<01:08,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 18:50:00,429 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:50:00,429 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 18:50:00,429 >>   Batch size = 8
{'eval_loss': 1.0086047649383545, 'eval_runtime': 9.4242, 'eval_samples_per_second': 369.898, 'eval_steps_per_second': 46.264, 'epoch': 0.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 59.58it/s][A
  3%|▎         | 12/436 [00:00<00:08, 51.09it/s][A
  4%|▍         | 18/436 [00:00<00:08, 49.02it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.94it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.48it/s][A
  8%|▊         | 33/436 [00:00<00:08, 47.27it/s][A
  9%|▊         | 38/436 [00:00<00:08, 47.03it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.58it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.43it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.40it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.48it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.49it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.53it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.39it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.39it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.46it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.36it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.31it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.26it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.32it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.38it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.47it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.53it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.32it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.50it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.42it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.26it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.29it/s][A
 34%|███▍      | 148/436 [00:03<00:07, 37.12it/s][A
 35%|███▌      | 153/436 [00:03<00:07, 39.58it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 41.44it/s][A
 37%|███▋      | 163/436 [00:03<00:06, 42.83it/s][A
 39%|███▊      | 168/436 [00:03<00:06, 43.95it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 44.64it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.27it/s][A
 42%|████▏     | 183/436 [00:04<00:05, 45.67it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.46it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.73it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.84it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.98it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.09it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.07it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.15it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.28it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.18it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.07it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.14it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.18it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.23it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.33it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.42it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.47it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.54it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.43it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.30it/s][A
 65%|██████▍   | 283/436 [00:06<00:04, 35.86it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 38.55it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 40.67it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 42.24it/s][A
 69%|██████▉   | 303/436 [00:06<00:03, 43.35it/s][A
 71%|███████   | 308/436 [00:06<00:02, 44.03it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 44.82it/s][A
 73%|███████▎  | 318/436 [00:07<00:02, 45.28it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.37it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.66it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.87it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.11it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.21it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.26it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.34it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.34it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.35it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 46.17it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.15it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.26it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.24it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.39it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.08it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.18it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.28it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.26it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 46.25it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 36.38it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 38.92it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 40.96it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 42.46it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 42.46it/s][A 40%|████      | 156/390 [01:39<01:08,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:50:10,564 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 18:50:10,713 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:50:19,303 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:50:19,536 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:50:19,642 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [02:01<38:26,  9.90s/it] 41%|████      | 158/390 [02:02<27:08,  7.02s/it] 41%|████      | 159/390 [02:02<19:15,  5.00s/it] 41%|████      | 160/390 [02:02<13:45,  3.59s/it] 41%|████▏     | 161/390 [02:03<09:55,  2.60s/it] 42%|████▏     | 162/390 [02:03<07:14,  1.91s/it] 42%|████▏     | 163/390 [02:03<05:22,  1.42s/it] 42%|████▏     | 164/390 [02:03<04:04,  1.08s/it] 42%|████▏     | 165/390 [02:04<03:10,  1.18it/s] 43%|████▎     | 166/390 [02:04<02:32,  1.47it/s] 43%|████▎     | 167/390 [02:04<02:05,  1.78it/s] 43%|████▎     | 168/390 [02:05<01:46,  2.08it/s] 43%|████▎     | 169/390 [02:05<01:37,  2.27it/s] 44%|████▎     | 170/390 [02:05<01:27,  2.52it/s] 44%|████▍     | 171/390 [02:06<01:19,  2.74it/s] 44%|████▍     | 172/390 [02:06<01:14,  2.92it/s] 44%|████▍     | 173/390 [02:06<01:11,  3.05it/s] 45%|████▍     | 174/390 [02:06<01:08,  3.14it/s] 45%|████▍     | 175/390 [02:07<01:06,  3.22it/s] 45%|████▌     | 176/390 [02:07<01:05,  3.27it/s] 45%|████▌     | 177/390 [02:07<01:05,  3.27it/s] 46%|████▌     | 178/390 [02:08<01:17,  2.75it/s] 46%|████▌     | 179/390 [02:08<01:13,  2.87it/s] 46%|████▌     | 180/390 [02:08<01:09,  3.02it/s] 46%|████▋     | 181/390 [02:09<01:06,  3.13it/s] 47%|████▋     | 182/390 [02:09<01:04,  3.21it/s] 47%|████▋     | 183/390 [02:09<01:03,  3.27it/s] 47%|████▋     | 184/390 [02:10<01:02,  3.32it/s] 47%|████▋     | 185/390 [02:10<01:01,  3.35it/s] 48%|████▊     | 186/390 [02:10<01:00,  3.37it/s] 48%|████▊     | 187/390 [02:10<00:59,  3.39it/s] 48%|████▊     | 188/390 [02:11<00:59,  3.40it/s] 48%|████▊     | 189/390 [02:11<00:59,  3.41it/s] 49%|████▊     | 190/390 [02:11<01:04,  3.10it/s] 49%|████▉     | 191/390 [02:12<01:02,  3.19it/s] 49%|████▉     | 192/390 [02:12<01:00,  3.25it/s] 49%|████▉     | 193/390 [02:12<00:59,  3.30it/s] 50%|████▉     | 194/390 [02:13<00:58,  3.34it/s] 50%|█████     | 195/390 [02:13<00:57,  3.36it/s] 50%|█████     | 196/390 [02:13<00:57,  3.38it/s] 51%|█████     | 197/390 [02:13<00:56,  3.39it/s] 51%|█████     | 198/390 [02:14<00:56,  3.40it/s] 51%|█████     | 199/390 [02:14<00:56,  3.41it/s] 51%|█████▏    | 200/390 [02:14<00:55,  3.40it/s] 52%|█████▏    | 201/390 [02:15<00:55,  3.40it/s] 52%|█████▏    | 202/390 [02:15<00:55,  3.41it/s] 52%|█████▏    | 203/390 [02:15<00:54,  3.41it/s] 52%|█████▏    | 204/390 [02:16<00:54,  3.41it/s] 53%|█████▎    | 205/390 [02:16<00:54,  3.41it/s] 53%|█████▎    | 206/390 [02:16<00:53,  3.41it/s] 53%|█████▎    | 207/390 [02:16<00:53,  3.42it/s] 53%|█████▎    | 208/390 [02:17<00:53,  3.41it/s] 54%|█████▎    | 209/390 [02:17<00:52,  3.42it/s] 54%|█████▍    | 210/390 [02:17<00:52,  3.41it/s] 54%|█████▍    | 211/390 [02:18<00:55,  3.23it/s] 54%|█████▍    | 212/390 [02:18<00:54,  3.29it/s] 55%|█████▍    | 213/390 [02:18<00:53,  3.32it/s] 55%|█████▍    | 214/390 [02:19<00:52,  3.34it/s] 55%|█████▌    | 215/390 [02:19<00:51,  3.37it/s] 55%|█████▌    | 216/390 [02:19<00:51,  3.38it/s] 56%|█████▌    | 217/390 [02:19<00:50,  3.39it/s] 56%|█████▌    | 218/390 [02:20<00:50,  3.40it/s] 56%|█████▌    | 219/390 [02:20<00:50,  3.41it/s] 56%|█████▋    | 220/390 [02:20<00:49,  3.41it/s] 57%|█████▋    | 221/390 [02:21<00:49,  3.41it/s] 57%|█████▋    | 222/390 [02:21<00:49,  3.40it/s] 57%|█████▋    | 223/390 [02:21<00:49,  3.41it/s] 57%|█████▋    | 224/390 [02:21<00:48,  3.41it/s] 58%|█████▊    | 225/390 [02:22<00:48,  3.41it/s] 58%|█████▊    | 226/390 [02:22<00:48,  3.42it/s] 58%|█████▊    | 227/390 [02:22<00:47,  3.42it/s] 58%|█████▊    | 228/390 [02:23<00:47,  3.41it/s] 59%|█████▊    | 229/390 [02:23<00:47,  3.42it/s] 59%|█████▉    | 230/390 [02:23<00:46,  3.42it/s] 59%|█████▉    | 231/390 [02:24<00:46,  3.42it/s] 59%|█████▉    | 232/390 [02:24<00:46,  3.42it/s] 60%|█████▉    | 233/390 [02:24<00:48,  3.26it/s] 60%|██████    | 234/390 [02:24<00:47,  3.30it/s][INFO|trainer.py:2140] 2023-08-28 18:50:55,775 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:50:55,775 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 18:50:55,775 >>   Batch size = 8
{'eval_loss': 1.0231056213378906, 'eval_runtime': 9.6995, 'eval_samples_per_second': 359.401, 'eval_steps_per_second': 44.951, 'epoch': 1.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 57.29it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.06it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.49it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.77it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.34it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.94it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.79it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.36it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.25it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.20it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.31it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.28it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.39it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.36it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.40it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.39it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.16it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.19it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.00it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.15it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.25it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.26it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.43it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.40it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.23it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.23it/s][A
 32%|███▏      | 138/436 [00:03<00:07, 41.20it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 42.76it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 43.71it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 44.59it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.12it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.54it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.89it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.93it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.35it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.38it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.74it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.97it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.04it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.25it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.31it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.39it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.32it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.95it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.89it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.98it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.14it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.13it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.27it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.34it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.32it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.28it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.07it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.03it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.22it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.54it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.72it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.93it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.04it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.20it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.27it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.13it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.90it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.98it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.12it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.19it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.23it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.27it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.18it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.28it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.19it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.16it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.00it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.12it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.03it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.99it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.10it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.09it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.15it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.06it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.09it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.10it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 44.32it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 44.83it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.33it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.63it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 45.63it/s][A 60%|██████    | 234/390 [02:34<00:47,  3.30it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:51:05,309 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 18:51:05,339 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:51:10,922 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:51:11,729 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:51:12,150 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:57<26:00, 10.07s/it] 61%|██████    | 236/390 [02:58<18:21,  7.16s/it] 61%|██████    | 237/390 [02:58<12:59,  5.10s/it] 61%|██████    | 238/390 [02:58<09:15,  3.65s/it] 61%|██████▏   | 239/390 [02:59<06:39,  2.65s/it] 62%|██████▏   | 240/390 [02:59<04:50,  1.94s/it] 62%|██████▏   | 241/390 [02:59<03:35,  1.45s/it] 62%|██████▏   | 242/390 [02:59<02:42,  1.10s/it] 62%|██████▏   | 243/390 [03:00<02:05,  1.17it/s] 63%|██████▎   | 244/390 [03:00<01:40,  1.45it/s] 63%|██████▎   | 245/390 [03:00<01:22,  1.76it/s] 63%|██████▎   | 246/390 [03:01<01:09,  2.06it/s] 63%|██████▎   | 247/390 [03:01<01:01,  2.33it/s] 64%|██████▎   | 248/390 [03:01<00:55,  2.58it/s] 64%|██████▍   | 249/390 [03:01<00:50,  2.78it/s] 64%|██████▍   | 250/390 [03:02<00:47,  2.95it/s] 64%|██████▍   | 251/390 [03:02<00:45,  3.08it/s] 65%|██████▍   | 252/390 [03:02<00:43,  3.18it/s] 65%|██████▍   | 253/390 [03:03<00:42,  3.25it/s] 65%|██████▌   | 254/390 [03:03<00:41,  3.30it/s] 65%|██████▌   | 255/390 [03:03<00:40,  3.34it/s] 66%|██████▌   | 256/390 [03:03<00:39,  3.36it/s] 66%|██████▌   | 257/390 [03:04<00:39,  3.38it/s] 66%|██████▌   | 258/390 [03:04<00:39,  3.37it/s] 66%|██████▋   | 259/390 [03:04<00:38,  3.39it/s] 67%|██████▋   | 260/390 [03:05<00:38,  3.40it/s] 67%|██████▋   | 261/390 [03:05<00:37,  3.41it/s] 67%|██████▋   | 262/390 [03:05<00:37,  3.41it/s] 67%|██████▋   | 263/390 [03:06<00:37,  3.41it/s] 68%|██████▊   | 264/390 [03:06<00:36,  3.42it/s] 68%|██████▊   | 265/390 [03:06<00:36,  3.42it/s] 68%|██████▊   | 266/390 [03:06<00:36,  3.42it/s] 68%|██████▊   | 267/390 [03:07<00:35,  3.42it/s] 69%|██████▊   | 268/390 [03:07<00:35,  3.42it/s] 69%|██████▉   | 269/390 [03:07<00:35,  3.42it/s] 69%|██████▉   | 270/390 [03:08<00:35,  3.42it/s] 69%|██████▉   | 271/390 [03:08<00:34,  3.40it/s] 70%|██████▉   | 272/390 [03:08<00:34,  3.41it/s] 70%|███████   | 273/390 [03:08<00:34,  3.41it/s] 70%|███████   | 274/390 [03:09<00:34,  3.34it/s] 71%|███████   | 275/390 [03:09<00:34,  3.36it/s] 71%|███████   | 276/390 [03:09<00:33,  3.38it/s] 71%|███████   | 277/390 [03:10<00:33,  3.39it/s] 71%|███████▏  | 278/390 [03:10<00:32,  3.40it/s] 72%|███████▏  | 279/390 [03:10<00:32,  3.40it/s] 72%|███████▏  | 280/390 [03:11<00:32,  3.41it/s] 72%|███████▏  | 281/390 [03:11<00:31,  3.41it/s] 72%|███████▏  | 282/390 [03:11<00:31,  3.41it/s] 73%|███████▎  | 283/390 [03:11<00:31,  3.41it/s] 73%|███████▎  | 284/390 [03:12<00:31,  3.41it/s] 73%|███████▎  | 285/390 [03:12<00:31,  3.28it/s] 73%|███████▎  | 286/390 [03:12<00:31,  3.32it/s] 74%|███████▎  | 287/390 [03:13<00:30,  3.36it/s] 74%|███████▍  | 288/390 [03:13<00:30,  3.37it/s] 74%|███████▍  | 289/390 [03:13<00:29,  3.38it/s] 74%|███████▍  | 290/390 [03:14<00:29,  3.39it/s] 75%|███████▍  | 291/390 [03:14<00:29,  3.40it/s] 75%|███████▍  | 292/390 [03:14<00:28,  3.40it/s] 75%|███████▌  | 293/390 [03:14<00:28,  3.41it/s] 75%|███████▌  | 294/390 [03:15<00:28,  3.41it/s] 76%|███████▌  | 295/390 [03:15<00:27,  3.42it/s] 76%|███████▌  | 296/390 [03:15<00:29,  3.16it/s] 76%|███████▌  | 297/390 [03:16<00:28,  3.23it/s] 76%|███████▋  | 298/390 [03:16<00:28,  3.28it/s] 77%|███████▋  | 299/390 [03:16<00:27,  3.32it/s] 77%|███████▋  | 300/390 [03:17<00:26,  3.35it/s] 77%|███████▋  | 301/390 [03:17<00:26,  3.37it/s] 77%|███████▋  | 302/390 [03:17<00:26,  3.38it/s] 78%|███████▊  | 303/390 [03:17<00:25,  3.39it/s] 78%|███████▊  | 304/390 [03:18<00:25,  3.40it/s] 78%|███████▊  | 305/390 [03:18<00:24,  3.40it/s] 78%|███████▊  | 306/390 [03:18<00:26,  3.18it/s] 79%|███████▊  | 307/390 [03:19<00:25,  3.25it/s] 79%|███████▉  | 308/390 [03:19<00:24,  3.30it/s] 79%|███████▉  | 309/390 [03:19<00:24,  3.33it/s] 79%|███████▉  | 310/390 [03:20<00:23,  3.36it/s] 80%|███████▉  | 311/390 [03:20<00:23,  3.38it/s] 80%|████████  | 312/390 [03:20<00:23,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 18:51:51,436 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:51:51,436 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 18:51:51,436 >>   Batch size = 8
{'eval_loss': 1.0378541946411133, 'eval_runtime': 9.4997, 'eval_samples_per_second': 366.959, 'eval_steps_per_second': 45.896, 'epoch': 2.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.82it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.14it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.42it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.57it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.20it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.99it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.75it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.19it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.33it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.70it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.91it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.01it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.24it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.17it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.27it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.19it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.76it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.92it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.99it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.12it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.19it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.27it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.24it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.27it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.09it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.83it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.85it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.96it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.99it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.13it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.22it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.31it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.28it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.19it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.03it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.90it/s][A
 43%|████▎     | 188/436 [00:04<00:06, 40.73it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 42.29it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 43.39it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 44.28it/s][A
 48%|████▊     | 208/436 [00:04<00:05, 44.87it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.22it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.54it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.78it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.55it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.60it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.83it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.98it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.03it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.12it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.12it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.20it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.11it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.96it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.76it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.86it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.07it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.08it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.15it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.19it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.14it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.12it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.99it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.92it/s][A
 75%|███████▌  | 328/436 [00:07<00:03, 34.43it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 37.30it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 39.55it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 41.38it/s][A
 80%|███████▉  | 348/436 [00:07<00:02, 42.76it/s][A
 81%|████████  | 353/436 [00:07<00:01, 43.78it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 44.52it/s][A
 83%|████████▎ | 363/436 [00:08<00:01, 45.00it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 44.87it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.22it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.54it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.71it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.92it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.02it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.12it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.12it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.01it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 45.93it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.87it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.95it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.04it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.95it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 45.95it/s][A 80%|████████  | 312/390 [03:30<00:23,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:52:01,141 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 18:52:01,273 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:52:05,171 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:52:05,327 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:52:05,372 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:46<10:20,  8.06s/it] 81%|████████  | 314/390 [03:47<07:16,  5.74s/it] 81%|████████  | 315/390 [03:47<05:07,  4.11s/it] 81%|████████  | 316/390 [03:47<03:39,  2.96s/it] 81%|████████▏ | 317/390 [03:47<02:37,  2.16s/it] 82%|████████▏ | 318/390 [03:48<01:55,  1.60s/it] 82%|████████▏ | 319/390 [03:48<01:25,  1.21s/it] 82%|████████▏ | 320/390 [03:48<01:05,  1.07it/s] 82%|████████▏ | 321/390 [03:49<00:51,  1.35it/s] 83%|████████▎ | 322/390 [03:49<00:41,  1.65it/s] 83%|████████▎ | 323/390 [03:49<00:34,  1.95it/s] 83%|████████▎ | 324/390 [03:50<00:29,  2.24it/s] 83%|████████▎ | 325/390 [03:50<00:27,  2.40it/s] 84%|████████▎ | 326/390 [03:50<00:24,  2.63it/s] 84%|████████▍ | 327/390 [03:50<00:22,  2.83it/s] 84%|████████▍ | 328/390 [03:51<00:20,  2.99it/s] 84%|████████▍ | 329/390 [03:51<00:19,  3.11it/s] 85%|████████▍ | 330/390 [03:51<00:18,  3.20it/s] 85%|████████▍ | 331/390 [03:52<00:18,  3.27it/s] 85%|████████▌ | 332/390 [03:52<00:17,  3.32it/s] 85%|████████▌ | 333/390 [03:52<00:17,  3.35it/s] 86%|████████▌ | 334/390 [03:52<00:16,  3.37it/s] 86%|████████▌ | 335/390 [03:53<00:16,  3.39it/s] 86%|████████▌ | 336/390 [03:53<00:17,  3.16it/s] 86%|████████▋ | 337/390 [03:53<00:16,  3.24it/s] 87%|████████▋ | 338/390 [03:54<00:15,  3.30it/s] 87%|████████▋ | 339/390 [03:54<00:15,  3.34it/s] 87%|████████▋ | 340/390 [03:54<00:14,  3.36it/s] 87%|████████▋ | 341/390 [03:55<00:14,  3.38it/s] 88%|████████▊ | 342/390 [03:55<00:14,  3.39it/s] 88%|████████▊ | 343/390 [03:55<00:13,  3.40it/s] 88%|████████▊ | 344/390 [03:55<00:13,  3.41it/s] 88%|████████▊ | 345/390 [03:56<00:13,  3.41it/s] 89%|████████▊ | 346/390 [03:56<00:12,  3.41it/s] 89%|████████▉ | 347/390 [03:56<00:12,  3.40it/s] 89%|████████▉ | 348/390 [03:57<00:12,  3.40it/s] 89%|████████▉ | 349/390 [03:57<00:12,  3.41it/s] 90%|████████▉ | 350/390 [03:57<00:11,  3.41it/s] 90%|█████████ | 351/390 [03:58<00:11,  3.41it/s] 90%|█████████ | 352/390 [03:58<00:11,  3.41it/s] 91%|█████████ | 353/390 [03:58<00:10,  3.41it/s] 91%|█████████ | 354/390 [03:58<00:10,  3.41it/s] 91%|█████████ | 355/390 [03:59<00:10,  3.42it/s] 91%|█████████▏| 356/390 [03:59<00:09,  3.42it/s] 92%|█████████▏| 357/390 [03:59<00:09,  3.42it/s] 92%|█████████▏| 358/390 [04:00<00:09,  3.38it/s] 92%|█████████▏| 359/390 [04:00<00:09,  3.39it/s] 92%|█████████▏| 360/390 [04:00<00:08,  3.40it/s] 93%|█████████▎| 361/390 [04:00<00:08,  3.40it/s] 93%|█████████▎| 362/390 [04:01<00:08,  3.41it/s] 93%|█████████▎| 363/390 [04:01<00:07,  3.41it/s] 93%|█████████▎| 364/390 [04:01<00:07,  3.41it/s] 94%|█████████▎| 365/390 [04:02<00:07,  3.41it/s] 94%|█████████▍| 366/390 [04:02<00:07,  3.41it/s] 94%|█████████▍| 367/390 [04:02<00:06,  3.42it/s] 94%|█████████▍| 368/390 [04:03<00:06,  3.42it/s] 95%|█████████▍| 369/390 [04:03<00:06,  3.32it/s] 95%|█████████▍| 370/390 [04:03<00:05,  3.34it/s] 95%|█████████▌| 371/390 [04:03<00:05,  3.36it/s] 95%|█████████▌| 372/390 [04:04<00:05,  3.38it/s] 96%|█████████▌| 373/390 [04:04<00:05,  3.39it/s] 96%|█████████▌| 374/390 [04:04<00:04,  3.40it/s] 96%|█████████▌| 375/390 [04:05<00:04,  3.40it/s] 96%|█████████▋| 376/390 [04:05<00:04,  3.40it/s] 97%|█████████▋| 377/390 [04:05<00:03,  3.41it/s] 97%|█████████▋| 378/390 [04:05<00:03,  3.41it/s] 97%|█████████▋| 379/390 [04:06<00:03,  3.41it/s] 97%|█████████▋| 380/390 [04:06<00:02,  3.40it/s] 98%|█████████▊| 381/390 [04:06<00:02,  3.41it/s] 98%|█████████▊| 382/390 [04:07<00:02,  3.41it/s] 98%|█████████▊| 383/390 [04:07<00:02,  3.41it/s] 98%|█████████▊| 384/390 [04:07<00:01,  3.42it/s] 99%|█████████▊| 385/390 [04:08<00:01,  3.39it/s] 99%|█████████▉| 386/390 [04:08<00:01,  3.40it/s] 99%|█████████▉| 387/390 [04:08<00:00,  3.27it/s] 99%|█████████▉| 388/390 [04:08<00:00,  3.31it/s]100%|█████████▉| 389/390 [04:09<00:00,  3.35it/s]100%|██████████| 390/390 [04:09<00:00,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 18:52:40,333 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:52:40,333 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 18:52:40,333 >>   Batch size = 8
{'eval_loss': 1.0449923276901245, 'eval_runtime': 9.6286, 'eval_samples_per_second': 362.046, 'eval_steps_per_second': 45.282, 'epoch': 3.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.32it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.18it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.37it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.65it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.17it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.73it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.46it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.17it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.12it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.21it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.28it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.26it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.24it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.11it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.98it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.03it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.95it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.81it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.98it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.07it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.22it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.12it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.10it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.10it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.04it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.05it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.02it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.08it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.17it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.25it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.33it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.15it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.19it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.96it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.49it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.66it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.86it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.02it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.09it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.01it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.07it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.14it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.98it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.05it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.04it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.12it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.07it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.04it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.19it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.94it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.06it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.01it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.99it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.98it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.00it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.05it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.16it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.24it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.22it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.11it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.07it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.03it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 41.74it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 43.01it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 43.88it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 44.54it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.10it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.49it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.74it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.87it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.71it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.77it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 45.77it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.88it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.06it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.03it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.20it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.21it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.08it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.03it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.87it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.98it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.99it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.12it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.13it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.10it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 46.10it/s][A100%|██████████| 390/390 [04:19<00:00,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:52:49,881 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 18:52:49,955 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:52:54,908 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:52:54,943 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:52:54,951 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:53:02,420 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:53:02,423 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78 (score: 1.0086047649383545).
                                                 100%|██████████| 390/390 [04:37<00:00,  3.37it/s]100%|██████████| 390/390 [04:37<00:00,  1.41it/s]
[INFO|trainer.py:1894] 2023-08-28 18:53:08,033 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 18:53:08,094 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:53:14,141 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:53:14,156 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:53:14,165 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:53:14,365 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:14,365 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:14,365 >>   train_loss               =     0.5105
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:14,365 >>   train_runtime            = 0:04:37.07
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:14,365 >>   train_samples            =       4999
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:14,365 >>   train_samples_per_second =      90.21
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:14,365 >>   train_steps_per_second   =      1.408
{'eval_loss': 1.050726294517517, 'eval_runtime': 9.4961, 'eval_samples_per_second': 367.099, 'eval_steps_per_second': 45.914, 'epoch': 4.99}
{'train_runtime': 277.0752, 'train_samples_per_second': 90.21, 'train_steps_per_second': 1.408, 'train_loss': 0.5105273124499199, 'epoch': 4.99}
08/28/2023 18:53:14 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:53:14,490 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:53:14,490 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 18:53:14,490 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 57.62it/s]  3%|▎         | 12/436 [00:00<00:08, 50.70it/s]  4%|▍         | 18/436 [00:00<00:08, 48.87it/s]  5%|▌         | 23/436 [00:00<00:08, 48.16it/s]  6%|▋         | 28/436 [00:00<00:08, 47.55it/s]  8%|▊         | 33/436 [00:00<00:08, 47.31it/s]  9%|▊         | 38/436 [00:00<00:08, 47.18it/s] 10%|▉         | 43/436 [00:00<00:08, 46.90it/s] 11%|█         | 48/436 [00:01<00:08, 46.57it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.48it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.51it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.57it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.52it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.60it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.60it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.44it/s] 20%|██        | 88/436 [00:01<00:07, 46.45it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.28it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.30it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.34it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.43it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.45it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.40it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.43it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.37it/s] 31%|███       | 133/436 [00:02<00:07, 39.14it/s] 32%|███▏      | 138/436 [00:03<00:07, 41.12it/s] 33%|███▎      | 143/436 [00:03<00:06, 42.64it/s] 34%|███▍      | 148/436 [00:03<00:06, 43.78it/s] 35%|███▌      | 153/436 [00:03<00:06, 44.50it/s] 36%|███▌      | 158/436 [00:03<00:06, 45.09it/s] 37%|███▋      | 163/436 [00:03<00:05, 45.52it/s] 39%|███▊      | 168/436 [00:03<00:05, 45.81it/s] 40%|███▉      | 173/436 [00:03<00:05, 45.44it/s] 41%|████      | 178/436 [00:03<00:05, 45.61it/s] 42%|████▏     | 183/436 [00:03<00:05, 45.79it/s] 43%|████▎     | 188/436 [00:04<00:05, 45.86it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.07it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.13it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.20it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.24it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.14it/s] 50%|█████     | 218/436 [00:04<00:04, 46.05it/s] 51%|█████     | 223/436 [00:04<00:04, 46.07it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.21it/s] 53%|█████▎    | 233/436 [00:05<00:04, 46.24it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.33it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.45it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.51it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.47it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.31it/s] 60%|██████    | 263/436 [00:05<00:03, 46.12it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.13it/s] 63%|██████▎   | 273/436 [00:05<00:04, 40.46it/s] 64%|██████▍   | 278/436 [00:06<00:03, 42.03it/s] 65%|██████▍   | 283/436 [00:06<00:03, 43.33it/s] 66%|██████▌   | 288/436 [00:06<00:03, 44.29it/s] 67%|██████▋   | 293/436 [00:06<00:03, 44.88it/s] 68%|██████▊   | 298/436 [00:06<00:03, 45.46it/s] 69%|██████▉   | 303/436 [00:06<00:02, 45.71it/s] 71%|███████   | 308/436 [00:06<00:02, 46.00it/s] 72%|███████▏  | 313/436 [00:06<00:02, 45.63it/s] 73%|███████▎  | 318/436 [00:06<00:02, 45.76it/s] 74%|███████▍  | 323/436 [00:07<00:02, 45.75it/s] 75%|███████▌  | 328/436 [00:07<00:02, 45.98it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.20it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.35it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.42it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.43it/s] 81%|████████  | 353/436 [00:07<00:01, 46.22it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.14it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.00it/s] 84%|████████▍ | 368/436 [00:08<00:01, 46.08it/s] 86%|████████▌ | 373/436 [00:08<00:01, 46.18it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.21it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.35it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.38it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.48it/s] 91%|█████████▏| 398/436 [00:08<00:00, 46.45it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.22it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.17it/s] 95%|█████████▍| 413/436 [00:09<00:00, 29.64it/s] 96%|█████████▌| 418/436 [00:09<00:00, 33.28it/s] 97%|█████████▋| 423/436 [00:09<00:00, 36.35it/s] 98%|█████████▊| 428/436 [00:09<00:00, 38.87it/s] 99%|█████████▉| 433/436 [00:09<00:00, 40.91it/s]100%|██████████| 436/436 [00:09<00:00, 44.92it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:53:24,224 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:24,224 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:24,224 >>   eval_loss               =     1.0086
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:24,224 >>   eval_runtime            = 0:00:09.73
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:24,224 >>   eval_samples            =       3486
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:24,224 >>   eval_samples_per_second =    358.157
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:24,224 >>   eval_steps_per_second   =     44.795
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:24,224 >>   perplexity              =     2.7418
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:35,221 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:35,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:35,247 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:35,247 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:35,247 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:53:35,550 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:53:35,551 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:53:35,822 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:53:36,847 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:53:36,847 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:38,206 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:38,208 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:38,208 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:38,208 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:53:38,208 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:53:38,520 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:53:38,523 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:53:38,778 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:53:38,930 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:53:38,930 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/checkpoint-156
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'manufacturer', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13015
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13115, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.13it/s]Extractor Predicting: 2it [00:01,  1.16it/s]Extractor Predicting: 3it [00:02,  1.19it/s]Extractor Predicting: 4it [00:03,  1.17it/s]Extractor Predicting: 5it [00:04,  1.17it/s]Extractor Predicting: 6it [00:05,  1.17it/s]Extractor Predicting: 7it [00:06,  1.13it/s]Extractor Predicting: 8it [00:06,  1.15it/s]Extractor Predicting: 9it [00:07,  1.16it/s]Extractor Predicting: 10it [00:08,  1.19it/s]Extractor Predicting: 11it [00:09,  1.15it/s]Extractor Predicting: 12it [00:10,  1.16it/s]Extractor Predicting: 13it [00:11,  1.15it/s]Extractor Predicting: 14it [00:12,  1.15it/s]Extractor Predicting: 15it [00:12,  1.15it/s]Extractor Predicting: 16it [00:13,  1.15it/s]Extractor Predicting: 17it [00:14,  1.18it/s]Extractor Predicting: 18it [00:15,  1.19it/s]Extractor Predicting: 19it [00:16,  1.13it/s]Extractor Predicting: 20it [00:17,  1.14it/s]Extractor Predicting: 21it [00:18,  1.16it/s]Extractor Predicting: 22it [00:18,  1.19it/s]Extractor Predicting: 23it [00:19,  1.19it/s]Extractor Predicting: 24it [00:20,  1.16it/s]Extractor Predicting: 25it [00:21,  1.16it/s]Extractor Predicting: 26it [00:22,  1.15it/s]Extractor Predicting: 27it [00:23,  1.16it/s]Extractor Predicting: 28it [00:24,  1.17it/s]Extractor Predicting: 29it [00:24,  1.16it/s]Extractor Predicting: 30it [00:25,  1.13it/s]Extractor Predicting: 31it [00:26,  1.14it/s]Extractor Predicting: 32it [00:27,  1.14it/s]Extractor Predicting: 33it [00:28,  1.14it/s]Extractor Predicting: 34it [00:29,  1.16it/s]Extractor Predicting: 35it [00:30,  1.17it/s]Extractor Predicting: 36it [00:31,  1.18it/s]Extractor Predicting: 37it [00:31,  1.19it/s]Extractor Predicting: 38it [00:32,  1.21it/s]Extractor Predicting: 39it [00:33,  1.21it/s]Extractor Predicting: 40it [00:34,  1.23it/s]Extractor Predicting: 41it [00:35,  1.22it/s]Extractor Predicting: 42it [00:35,  1.22it/s]Extractor Predicting: 43it [00:36,  1.19it/s]Extractor Predicting: 44it [00:37,  1.18it/s]Extractor Predicting: 45it [00:38,  1.23it/s]Extractor Predicting: 46it [00:39,  1.22it/s]Extractor Predicting: 47it [00:40,  1.23it/s]Extractor Predicting: 48it [00:40,  1.22it/s]Extractor Predicting: 49it [00:41,  1.25it/s]Extractor Predicting: 50it [00:42,  1.24it/s]Extractor Predicting: 51it [00:43,  1.22it/s]Extractor Predicting: 52it [00:44,  1.21it/s]Extractor Predicting: 53it [00:44,  1.21it/s]Extractor Predicting: 54it [00:45,  1.20it/s]Extractor Predicting: 55it [00:46,  1.23it/s]Extractor Predicting: 56it [00:47,  1.24it/s]Extractor Predicting: 57it [00:48,  1.22it/s]Extractor Predicting: 58it [00:49,  1.21it/s]Extractor Predicting: 59it [00:49,  1.23it/s]Extractor Predicting: 60it [00:50,  1.21it/s]Extractor Predicting: 61it [00:51,  1.20it/s]Extractor Predicting: 62it [00:52,  1.21it/s]Extractor Predicting: 63it [00:53,  1.21it/s]Extractor Predicting: 64it [00:54,  1.21it/s]Extractor Predicting: 65it [00:54,  1.21it/s]Extractor Predicting: 66it [00:55,  1.22it/s]Extractor Predicting: 67it [00:56,  1.24it/s]Extractor Predicting: 68it [00:57,  1.21it/s]Extractor Predicting: 69it [00:58,  1.21it/s]Extractor Predicting: 70it [00:58,  1.21it/s]Extractor Predicting: 71it [00:59,  1.23it/s]Extractor Predicting: 72it [01:00,  1.23it/s]Extractor Predicting: 73it [01:01,  1.23it/s]Extractor Predicting: 74it [01:02,  1.24it/s]Extractor Predicting: 75it [01:03,  1.21it/s]Extractor Predicting: 76it [01:03,  1.21it/s]Extractor Predicting: 77it [01:04,  1.21it/s]Extractor Predicting: 78it [01:05,  1.21it/s]Extractor Predicting: 79it [01:06,  1.22it/s]Extractor Predicting: 80it [01:07,  1.22it/s]Extractor Predicting: 81it [01:07,  1.22it/s]Extractor Predicting: 82it [01:08,  1.22it/s]Extractor Predicting: 83it [01:09,  1.19it/s]Extractor Predicting: 84it [01:10,  1.20it/s]Extractor Predicting: 85it [01:11,  1.20it/s]Extractor Predicting: 86it [01:12,  1.20it/s]Extractor Predicting: 87it [01:12,  1.20it/s]Extractor Predicting: 88it [01:13,  1.22it/s]Extractor Predicting: 89it [01:14,  1.23it/s]Extractor Predicting: 90it [01:15,  1.26it/s]Extractor Predicting: 91it [01:16,  1.29it/s]Extractor Predicting: 92it [01:16,  1.30it/s]Extractor Predicting: 93it [01:17,  1.27it/s]Extractor Predicting: 94it [01:18,  1.30it/s]Extractor Predicting: 95it [01:19,  1.30it/s]Extractor Predicting: 96it [01:19,  1.29it/s]Extractor Predicting: 97it [01:20,  1.29it/s]Extractor Predicting: 98it [01:21,  1.26it/s]Extractor Predicting: 99it [01:22,  1.24it/s]Extractor Predicting: 100it [01:23,  1.23it/s]Extractor Predicting: 101it [01:23,  1.27it/s]Extractor Predicting: 102it [01:24,  1.30it/s]Extractor Predicting: 103it [01:25,  1.29it/s]Extractor Predicting: 104it [01:26,  1.28it/s]Extractor Predicting: 105it [01:26,  1.29it/s]Extractor Predicting: 106it [01:27,  1.29it/s]Extractor Predicting: 107it [01:28,  1.29it/s]Extractor Predicting: 108it [01:29,  1.27it/s]Extractor Predicting: 109it [01:30,  1.29it/s]Extractor Predicting: 110it [01:30,  1.28it/s]Extractor Predicting: 111it [01:31,  1.20it/s]Extractor Predicting: 112it [01:32,  1.26it/s]Extractor Predicting: 113it [01:33,  1.29it/s]Extractor Predicting: 114it [01:34,  1.28it/s]Extractor Predicting: 115it [01:34,  1.28it/s]Extractor Predicting: 116it [01:35,  1.28it/s]Extractor Predicting: 117it [01:36,  1.29it/s]Extractor Predicting: 118it [01:37,  1.32it/s]Extractor Predicting: 119it [01:37,  1.30it/s]Extractor Predicting: 120it [01:38,  1.28it/s]Extractor Predicting: 121it [01:39,  1.34it/s]Extractor Predicting: 122it [01:40,  1.32it/s]Extractor Predicting: 123it [01:41,  1.27it/s]Extractor Predicting: 124it [01:41,  1.30it/s]Extractor Predicting: 125it [01:42,  1.30it/s]Extractor Predicting: 126it [01:43,  1.35it/s]Extractor Predicting: 127it [01:43,  1.34it/s]Extractor Predicting: 128it [01:44,  1.33it/s]Extractor Predicting: 129it [01:45,  1.30it/s]Extractor Predicting: 130it [01:46,  1.32it/s]Extractor Predicting: 131it [01:46,  1.34it/s]Extractor Predicting: 132it [01:47,  1.33it/s]Extractor Predicting: 133it [01:48,  1.30it/s]Extractor Predicting: 134it [01:49,  1.32it/s]Extractor Predicting: 135it [01:50,  1.32it/s]Extractor Predicting: 136it [01:50,  1.33it/s]Extractor Predicting: 137it [01:51,  1.32it/s]Extractor Predicting: 138it [01:52,  1.36it/s]Extractor Predicting: 139it [01:52,  1.37it/s]Extractor Predicting: 140it [01:53,  1.37it/s]Extractor Predicting: 141it [01:54,  1.40it/s]Extractor Predicting: 142it [01:55,  1.37it/s]Extractor Predicting: 143it [01:55,  1.34it/s]Extractor Predicting: 144it [01:56,  1.63it/s]Extractor Predicting: 144it [01:56,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:44,746 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:44,748 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:44,748 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:44,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:44,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:55:45,357 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:55:45,363 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:55:45,938 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:55:46,960 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:55:46,960 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:49,854 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:49,867 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:49,867 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:49,867 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:49,867 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:55:50,494 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:55:50,496 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:55:51,088 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:55:51,241 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:55:51,241 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5727650727650727,
  "recall": 0.15806081468732072,
  "score": 0.2477517985611511,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13522
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13622, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.29it/s]Extractor Predicting: 2it [00:01,  1.31it/s]Extractor Predicting: 3it [00:02,  1.29it/s]Extractor Predicting: 4it [00:03,  1.32it/s]Extractor Predicting: 5it [00:03,  1.30it/s]Extractor Predicting: 6it [00:04,  1.27it/s]Extractor Predicting: 7it [00:05,  1.27it/s]Extractor Predicting: 8it [00:06,  1.29it/s]Extractor Predicting: 9it [00:06,  1.29it/s]Extractor Predicting: 10it [00:07,  1.31it/s]Extractor Predicting: 11it [00:08,  1.28it/s]Extractor Predicting: 12it [00:09,  1.29it/s]Extractor Predicting: 13it [00:10,  1.27it/s]Extractor Predicting: 14it [00:10,  1.30it/s]Extractor Predicting: 15it [00:11,  1.29it/s]Extractor Predicting: 16it [00:12,  1.26it/s]Extractor Predicting: 17it [00:13,  1.26it/s]Extractor Predicting: 18it [00:14,  1.25it/s]Extractor Predicting: 19it [00:14,  1.27it/s]Extractor Predicting: 20it [00:15,  1.25it/s]Extractor Predicting: 21it [00:16,  1.27it/s]Extractor Predicting: 22it [00:17,  1.28it/s]Extractor Predicting: 23it [00:18,  1.26it/s]Extractor Predicting: 24it [00:18,  1.24it/s]Extractor Predicting: 25it [00:19,  1.25it/s]Extractor Predicting: 26it [00:20,  1.25it/s]Extractor Predicting: 27it [00:21,  1.24it/s]Extractor Predicting: 28it [00:22,  1.26it/s]Extractor Predicting: 29it [00:22,  1.26it/s]Extractor Predicting: 30it [00:23,  1.23it/s]Extractor Predicting: 31it [00:24,  1.25it/s]Extractor Predicting: 32it [00:25,  1.23it/s]Extractor Predicting: 33it [00:26,  1.17it/s]Extractor Predicting: 34it [00:26,  1.22it/s]Extractor Predicting: 35it [00:27,  1.22it/s]Extractor Predicting: 36it [00:28,  1.24it/s]Extractor Predicting: 37it [00:29,  1.22it/s]Extractor Predicting: 38it [00:30,  1.22it/s]Extractor Predicting: 39it [00:31,  1.22it/s]Extractor Predicting: 40it [00:31,  1.21it/s]Extractor Predicting: 41it [00:32,  1.21it/s]Extractor Predicting: 42it [00:33,  1.23it/s]Extractor Predicting: 43it [00:34,  1.22it/s]Extractor Predicting: 44it [00:35,  1.21it/s]Extractor Predicting: 45it [00:35,  1.23it/s]Extractor Predicting: 46it [00:36,  1.23it/s]Extractor Predicting: 47it [00:37,  1.24it/s]Extractor Predicting: 48it [00:38,  1.24it/s]Extractor Predicting: 49it [00:39,  1.25it/s]Extractor Predicting: 50it [00:39,  1.26it/s]Extractor Predicting: 51it [00:40,  1.29it/s]Extractor Predicting: 52it [00:41,  1.30it/s]Extractor Predicting: 53it [00:42,  1.29it/s]Extractor Predicting: 54it [00:43,  1.29it/s]Extractor Predicting: 55it [00:43,  1.29it/s]Extractor Predicting: 56it [00:44,  1.26it/s]Extractor Predicting: 57it [00:45,  1.22it/s]Extractor Predicting: 58it [00:46,  1.20it/s]Extractor Predicting: 59it [00:47,  1.21it/s]Extractor Predicting: 60it [00:47,  1.23it/s]Extractor Predicting: 61it [00:48,  1.21it/s]Extractor Predicting: 62it [00:49,  1.19it/s]Extractor Predicting: 63it [00:50,  1.24it/s]Extractor Predicting: 64it [00:51,  1.22it/s]Extractor Predicting: 65it [00:52,  1.24it/s]Extractor Predicting: 66it [00:52,  1.24it/s]Extractor Predicting: 67it [00:53,  1.24it/s]Extractor Predicting: 68it [00:54,  1.22it/s]Extractor Predicting: 69it [00:55,  1.22it/s]Extractor Predicting: 70it [00:56,  1.17it/s]Extractor Predicting: 71it [00:57,  1.19it/s]Extractor Predicting: 72it [00:57,  1.19it/s]Extractor Predicting: 73it [00:58,  1.18it/s]Extractor Predicting: 74it [00:59,  1.19it/s]Extractor Predicting: 75it [01:00,  1.18it/s]Extractor Predicting: 76it [01:01,  1.21it/s]Extractor Predicting: 77it [01:02,  1.19it/s]Extractor Predicting: 78it [01:02,  1.19it/s]Extractor Predicting: 79it [01:03,  1.20it/s]Extractor Predicting: 80it [01:04,  1.20it/s]Extractor Predicting: 81it [01:05,  1.20it/s]Extractor Predicting: 82it [01:06,  1.18it/s]Extractor Predicting: 83it [01:07,  1.18it/s]Extractor Predicting: 84it [01:07,  1.21it/s]Extractor Predicting: 85it [01:08,  1.20it/s]Extractor Predicting: 86it [01:09,  1.21it/s]Extractor Predicting: 87it [01:10,  1.19it/s]Extractor Predicting: 88it [01:11,  1.24it/s]Extractor Predicting: 89it [01:12,  1.23it/s]Extractor Predicting: 90it [01:12,  1.22it/s]Extractor Predicting: 91it [01:13,  1.27it/s]Extractor Predicting: 92it [01:14,  1.27it/s]Extractor Predicting: 93it [01:15,  1.29it/s]Extractor Predicting: 94it [01:15,  1.27it/s]Extractor Predicting: 95it [01:16,  1.27it/s]Extractor Predicting: 96it [01:17,  1.26it/s]Extractor Predicting: 97it [01:18,  1.24it/s]Extractor Predicting: 98it [01:19,  1.24it/s]Extractor Predicting: 99it [01:20,  1.23it/s]Extractor Predicting: 100it [01:20,  1.22it/s]Extractor Predicting: 101it [01:21,  1.22it/s]Extractor Predicting: 102it [01:22,  1.18it/s]Extractor Predicting: 103it [01:23,  1.20it/s]Extractor Predicting: 104it [01:24,  1.21it/s]Extractor Predicting: 105it [01:24,  1.23it/s]Extractor Predicting: 106it [01:25,  1.16it/s]Extractor Predicting: 107it [01:26,  1.17it/s]Extractor Predicting: 108it [01:27,  1.19it/s]Extractor Predicting: 109it [01:28,  1.21it/s]Extractor Predicting: 110it [01:29,  1.19it/s]Extractor Predicting: 111it [01:30,  1.21it/s]Extractor Predicting: 112it [01:30,  1.20it/s]Extractor Predicting: 113it [01:31,  1.24it/s]Extractor Predicting: 114it [01:32,  1.22it/s]Extractor Predicting: 115it [01:33,  1.24it/s]Extractor Predicting: 116it [01:34,  1.19it/s]Extractor Predicting: 117it [01:35,  1.18it/s]Extractor Predicting: 118it [01:35,  1.17it/s]Extractor Predicting: 119it [01:36,  1.16it/s]Extractor Predicting: 120it [01:37,  1.17it/s]Extractor Predicting: 121it [01:38,  1.17it/s]Extractor Predicting: 122it [01:39,  1.20it/s]Extractor Predicting: 123it [01:40,  1.19it/s]Extractor Predicting: 124it [01:41,  1.17it/s]Extractor Predicting: 125it [01:41,  1.16it/s]Extractor Predicting: 126it [01:42,  1.14it/s]Extractor Predicting: 127it [01:43,  1.14it/s]Extractor Predicting: 128it [01:44,  1.15it/s]Extractor Predicting: 129it [01:45,  1.14it/s]Extractor Predicting: 130it [01:46,  1.15it/s]Extractor Predicting: 131it [01:47,  1.16it/s]Extractor Predicting: 132it [01:48,  1.15it/s]Extractor Predicting: 133it [01:48,  1.16it/s]Extractor Predicting: 134it [01:49,  1.18it/s]Extractor Predicting: 135it [01:50,  1.19it/s]Extractor Predicting: 136it [01:51,  1.21it/s]Extractor Predicting: 137it [01:52,  1.24it/s]Extractor Predicting: 138it [01:52,  1.22it/s]Extractor Predicting: 139it [01:53,  1.23it/s]Extractor Predicting: 140it [01:54,  1.22it/s]Extractor Predicting: 141it [01:55,  1.18it/s]Extractor Predicting: 142it [01:56,  1.19it/s]Extractor Predicting: 143it [01:56,  1.27it/s]Extractor Predicting: 143it [01:56,  1.22it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:57:55,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:57:55,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:57:55,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:57:55,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:57:55,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:57:55,641 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:57:55,642 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:57:56,413 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:57:57,525 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:57:57,525 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:57:59,049 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:57:59,183 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:57:59,183 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:57:59,183 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:57:59,183 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:57:59,561 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:57:59,563 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:58:00,285 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:58:00,474 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:58:00,474 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4481236203090508,
  "recall": 0.11864406779661017,
  "score": 0.18761552680221813,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 464
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 564, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.05it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 2it [00:01,  1.38it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:58:02,625 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:58:02,626 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:58:02,637 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:58:02,637 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:58:02,639 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:58:09,934 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:58:09,938 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:58:09,963 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:58:09,964 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:58:09,974 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:58:09,978 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:58:09,978 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:58:09,978 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:58:09,978 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:58:09,978 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:58:09,978 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5714285714285714,
  "recall": 0.05128205128205128,
  "score": 0.09411764705882351,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:58:10,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:11,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:12,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:13,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:13,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:14,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:15,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:16,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:17,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:18,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:18,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:19,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:20,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:21,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:22,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:23,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:24,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:24,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:25,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:26,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:27,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:28,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:18<02:49, 18.88s/it][WARNING|generation_utils.py:914] 2023-08-28 18:58:29,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:30,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:31,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:32,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:33,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:34,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:34,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:35,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:36,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:37,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:38,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:39,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:40,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:41,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:42,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:43,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:44,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:45,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:46,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:47,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:37<02:30, 18.87s/it][WARNING|generation_utils.py:914] 2023-08-28 18:58:48,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:48,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:49,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:50,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:51,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:52,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:52,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:53,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:54,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:55,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:56,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:57,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:58,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:58,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:58:59,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:00,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:01,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:02,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:03,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:03,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:04,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:55<02:08, 18.41s/it][WARNING|generation_utils.py:914] 2023-08-28 18:59:05,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:06,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:07,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:08,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:09,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:10,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:11,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:12,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:13,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:14,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:15,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:16,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:17,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:19,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:19,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:20,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:21,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:22,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:23,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:24,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:14<01:52, 18.75s/it][WARNING|generation_utils.py:914] 2023-08-28 18:59:25,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:26,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:27,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:27,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:28,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:29,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:30,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:31,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:32,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:32,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:33,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:34,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:35,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:36,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:37,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:38,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:39,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:40,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:41,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:41,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:42,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:43,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:34<01:34, 18.95s/it][WARNING|generation_utils.py:914] 2023-08-28 18:59:44,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:45,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:46,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:47,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:47,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:49,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:49,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:50,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:51,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:52,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:53,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:54,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:55,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:56,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:56,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:57,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:58,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:59:59,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:00,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:01,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:02,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:52<01:15, 18.89s/it][WARNING|generation_utils.py:914] 2023-08-28 19:00:03,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:04,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:04,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:05,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:06,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:07,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:08,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:09,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:10,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:11,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:11,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:12,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:13,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:14,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:15,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:16,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:17,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:17,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:18,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:19,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:10<00:55, 18.35s/it][WARNING|generation_utils.py:914] 2023-08-28 19:00:20,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:21,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:22,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:23,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:24,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:25,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:25,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:26,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:27,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:28,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:29,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:30,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:31,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:32,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:33,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:34,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:35,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:36,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:37,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:37,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:38,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:29<00:37, 18.64s/it][WARNING|generation_utils.py:914] 2023-08-28 19:00:39,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:40,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:41,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:42,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:43,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:44,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:45,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:46,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:47,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:48,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:49,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:50,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:50,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:51,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:52,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:53,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:54,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:55,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:56,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:57,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:00:58,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:48<00:18, 18.92s/it][WARNING|generation_utils.py:914] 2023-08-28 19:00:59,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:00,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:01,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:02,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:02,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:03,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:04,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:05,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:06,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:07,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:09,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:10,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:11,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:12,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:12,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:13,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:14,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:15,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:16,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:17,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:01:18,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:09<00:00, 19.40s/it]Generating: 100%|██████████| 10/10 [03:09<00:00, 18.95s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:25,392 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:25,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:25,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:25,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:25,398 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:01:25,801 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:01:25,802 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:01:26,491 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:01:27,547 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:01:27,547 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:29,292 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:29,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:29,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:29,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:29,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:01:29,615 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:01:29,616 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:01:29,880 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:01:30,034 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:01:30,034 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country . Context : Later in 2008 , the band became a part of a reunion tour of Europe that included performances in Austria , Czech Republic , France , Germany , the UK , and Austria . Head Entity : tour , Tail Entity : European .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : country .', 'success_rate': 0.8877840909090909, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : followed by .', 'success_rate': 0.9515625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : genre .', 'success_rate': 0.8988095238095238, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.94375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : voice type .', 'success_rate': 0.8636363636363636, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 6816
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6916, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.15it/s]Extractor Estimating: 2it [00:01,  1.23it/s]Extractor Estimating: 3it [00:02,  1.26it/s]Extractor Estimating: 4it [00:03,  1.33it/s]Extractor Estimating: 5it [00:03,  1.35it/s]Extractor Estimating: 6it [00:04,  1.32it/s]Extractor Estimating: 7it [00:05,  1.35it/s]Extractor Estimating: 8it [00:06,  1.34it/s]Extractor Estimating: 9it [00:06,  1.32it/s]Extractor Estimating: 10it [00:07,  1.31it/s]Extractor Estimating: 11it [00:08,  1.38it/s]Extractor Estimating: 12it [00:09,  1.36it/s]Extractor Estimating: 13it [00:09,  1.25it/s]Extractor Estimating: 14it [00:10,  1.20it/s]Extractor Estimating: 15it [00:11,  1.21it/s]Extractor Estimating: 16it [00:12,  1.28it/s]Extractor Estimating: 17it [00:13,  1.32it/s]Extractor Estimating: 18it [00:13,  1.34it/s]Extractor Estimating: 19it [00:14,  1.36it/s]Extractor Estimating: 20it [00:15,  1.35it/s]Extractor Estimating: 21it [00:15,  1.35it/s]Extractor Estimating: 22it [00:16,  1.30it/s]Extractor Estimating: 23it [00:17,  1.30it/s]Extractor Estimating: 24it [00:18,  1.29it/s]Extractor Estimating: 25it [00:19,  1.32it/s]Extractor Estimating: 26it [00:19,  1.32it/s]Extractor Estimating: 27it [00:20,  1.30it/s]Extractor Estimating: 28it [00:21,  1.27it/s]Extractor Estimating: 29it [00:22,  1.28it/s]Extractor Estimating: 30it [00:23,  1.25it/s]Extractor Estimating: 31it [00:23,  1.21it/s]Extractor Estimating: 32it [00:24,  1.20it/s]Extractor Estimating: 33it [00:25,  1.21it/s]Extractor Estimating: 34it [00:26,  1.24it/s]Extractor Estimating: 35it [00:27,  1.22it/s]Extractor Estimating: 36it [00:28,  1.23it/s]Extractor Estimating: 37it [00:28,  1.24it/s]Extractor Estimating: 38it [00:29,  1.24it/s]Extractor Estimating: 39it [00:30,  1.29it/s]Extractor Estimating: 40it [00:31,  1.27it/s]Extractor Estimating: 41it [00:32,  1.23it/s]Extractor Estimating: 42it [00:32,  1.25it/s]Extractor Estimating: 43it [00:33,  1.23it/s]Extractor Estimating: 44it [00:34,  1.26it/s]Extractor Estimating: 45it [00:35,  1.25it/s]Extractor Estimating: 46it [00:36,  1.23it/s]Extractor Estimating: 47it [00:36,  1.22it/s]Extractor Estimating: 48it [00:37,  1.22it/s]Extractor Estimating: 49it [00:38,  1.23it/s]Extractor Estimating: 50it [00:39,  1.25it/s]Extractor Estimating: 51it [00:40,  1.28it/s]Extractor Estimating: 52it [00:40,  1.32it/s]Extractor Estimating: 53it [00:41,  1.35it/s]Extractor Estimating: 54it [00:42,  1.38it/s]Extractor Estimating: 55it [00:42,  1.33it/s]Extractor Estimating: 56it [00:43,  1.38it/s]Extractor Estimating: 57it [00:44,  1.37it/s]Extractor Estimating: 58it [00:45,  1.33it/s]Extractor Estimating: 59it [00:45,  1.33it/s]Extractor Estimating: 60it [00:46,  1.31it/s]Extractor Estimating: 61it [00:47,  1.32it/s]Extractor Estimating: 62it [00:48,  1.33it/s]Extractor Estimating: 63it [00:48,  1.32it/s]Extractor Estimating: 64it [00:49,  1.31it/s]Extractor Estimating: 65it [00:50,  1.33it/s]Extractor Estimating: 66it [00:51,  1.36it/s]Extractor Estimating: 67it [00:51,  1.36it/s]Extractor Estimating: 68it [00:52,  1.38it/s]Extractor Estimating: 69it [00:53,  1.37it/s]Extractor Estimating: 70it [00:53,  1.42it/s]Extractor Estimating: 71it [00:54,  1.40it/s]Extractor Estimating: 72it [00:55,  1.42it/s]Extractor Estimating: 73it [00:56,  1.33it/s]Extractor Estimating: 74it [00:56,  1.36it/s]Extractor Estimating: 75it [00:57,  1.30it/s]Extractor Estimating: 76it [00:58,  1.36it/s]Extractor Estimating: 77it [00:59,  1.34it/s]Extractor Estimating: 78it [00:59,  1.38it/s]Extractor Estimating: 79it [01:00,  1.38it/s]Extractor Estimating: 80it [01:01,  1.42it/s]Extractor Estimating: 81it [01:01,  1.46it/s]Extractor Estimating: 82it [01:02,  1.37it/s]Extractor Estimating: 83it [01:03,  1.44it/s]Extractor Estimating: 84it [01:03,  1.50it/s]Extractor Estimating: 85it [01:04,  1.43it/s]Extractor Estimating: 86it [01:05,  1.42it/s]Extractor Estimating: 87it [01:06,  1.49it/s]Extractor Estimating: 88it [01:06,  1.53it/s]Extractor Estimating: 89it [01:07,  1.48it/s]Extractor Estimating: 90it [01:08,  1.45it/s]Extractor Estimating: 91it [01:08,  1.43it/s]Extractor Estimating: 92it [01:09,  1.48it/s]Extractor Estimating: 93it [01:10,  1.47it/s]Extractor Estimating: 94it [01:10,  1.43it/s]Extractor Estimating: 95it [01:11,  1.47it/s]Extractor Estimating: 96it [01:12,  1.49it/s]Extractor Estimating: 97it [01:12,  1.43it/s]Extractor Estimating: 98it [01:13,  1.48it/s]Extractor Estimating: 99it [01:14,  1.45it/s]Extractor Estimating: 100it [01:14,  1.47it/s]Extractor Estimating: 101it [01:15,  1.40it/s]Extractor Estimating: 102it [01:16,  1.33it/s]Extractor Estimating: 103it [01:17,  1.37it/s]Extractor Estimating: 104it [01:17,  1.39it/s]Extractor Estimating: 105it [01:18,  1.37it/s]Extractor Estimating: 106it [01:19,  1.36it/s]Extractor Estimating: 107it [01:20,  1.38it/s]Extractor Estimating: 108it [01:20,  1.38it/s]Extractor Estimating: 109it [01:21,  1.37it/s]Extractor Estimating: 110it [01:22,  1.37it/s]Extractor Estimating: 111it [01:23,  1.33it/s]Extractor Estimating: 112it [01:23,  1.36it/s]Extractor Estimating: 113it [01:24,  1.38it/s]Extractor Estimating: 114it [01:25,  1.32it/s]Extractor Estimating: 115it [01:26,  1.33it/s]Extractor Estimating: 116it [01:26,  1.32it/s]Extractor Estimating: 117it [01:27,  1.31it/s]Extractor Estimating: 118it [01:28,  1.33it/s]Extractor Estimating: 119it [01:29,  1.35it/s]Extractor Estimating: 120it [01:29,  1.33it/s]Extractor Estimating: 121it [01:30,  1.32it/s]Extractor Estimating: 122it [01:31,  1.32it/s]Extractor Estimating: 123it [01:32,  1.30it/s]Extractor Estimating: 124it [01:32,  1.30it/s]Extractor Estimating: 125it [01:33,  1.31it/s]Extractor Estimating: 126it [01:34,  1.35it/s]Extractor Estimating: 127it [01:35,  1.39it/s]Extractor Estimating: 128it [01:35,  1.44it/s]Extractor Estimating: 129it [01:36,  1.45it/s]Extractor Estimating: 130it [01:37,  1.42it/s]Extractor Estimating: 131it [01:37,  1.42it/s]Extractor Estimating: 132it [01:38,  1.43it/s]Extractor Estimating: 133it [01:39,  1.43it/s]Extractor Estimating: 134it [01:39,  1.44it/s]Extractor Estimating: 135it [01:40,  1.46it/s]Extractor Estimating: 136it [01:41,  1.42it/s]Extractor Estimating: 137it [01:41,  1.49it/s]Extractor Estimating: 138it [01:42,  1.43it/s]Extractor Estimating: 139it [01:43,  1.50it/s]Extractor Estimating: 140it [01:43,  1.52it/s]Extractor Estimating: 141it [01:44,  1.51it/s]Extractor Estimating: 142it [01:45,  1.53it/s]Extractor Estimating: 143it [01:45,  1.49it/s]Extractor Estimating: 144it [01:46,  1.50it/s]Extractor Estimating: 145it [01:47,  1.50it/s]Extractor Estimating: 146it [01:47,  1.54it/s]Extractor Estimating: 147it [01:48,  1.53it/s]Extractor Estimating: 148it [01:49,  1.47it/s]Extractor Estimating: 149it [01:49,  1.46it/s]Extractor Estimating: 150it [01:50,  1.46it/s]Extractor Estimating: 151it [01:51,  1.40it/s]Extractor Estimating: 152it [01:52,  1.41it/s]Extractor Estimating: 153it [01:52,  1.38it/s]Extractor Estimating: 154it [01:53,  1.40it/s]Extractor Estimating: 155it [01:54,  1.31it/s]Extractor Estimating: 156it [01:55,  1.32it/s]Extractor Estimating: 157it [01:56,  1.24it/s]Extractor Estimating: 158it [01:56,  1.30it/s]Extractor Estimating: 159it [01:57,  1.31it/s]Extractor Estimating: 160it [01:58,  1.36it/s]Extractor Estimating: 161it [01:58,  1.34it/s]Extractor Estimating: 162it [01:59,  1.38it/s]Extractor Estimating: 163it [02:00,  1.35it/s]Extractor Estimating: 164it [02:01,  1.37it/s]Extractor Estimating: 165it [02:01,  1.38it/s]Extractor Estimating: 166it [02:02,  1.37it/s]Extractor Estimating: 167it [02:03,  1.36it/s]Extractor Estimating: 168it [02:04,  1.34it/s]Extractor Estimating: 169it [02:04,  1.36it/s]Extractor Estimating: 170it [02:05,  1.36it/s]Extractor Estimating: 171it [02:06,  1.37it/s]Extractor Estimating: 172it [02:06,  1.37it/s]Extractor Estimating: 173it [02:07,  1.40it/s]Extractor Estimating: 174it [02:08,  1.41it/s]Extractor Estimating: 175it [02:09,  1.39it/s]Extractor Estimating: 176it [02:09,  1.38it/s]Extractor Estimating: 177it [02:10,  1.32it/s]Extractor Estimating: 178it [02:11,  1.32it/s]Extractor Estimating: 179it [02:12,  1.34it/s]Extractor Estimating: 180it [02:12,  1.37it/s]Extractor Estimating: 181it [02:13,  1.35it/s]Extractor Estimating: 182it [02:14,  1.29it/s]Extractor Estimating: 183it [02:15,  1.29it/s]Extractor Estimating: 184it [02:15,  1.30it/s]Extractor Estimating: 185it [02:16,  1.30it/s]Extractor Estimating: 186it [02:17,  1.30it/s]Extractor Estimating: 187it [02:18,  1.32it/s]Extractor Estimating: 188it [02:19,  1.31it/s]Extractor Estimating: 189it [02:19,  1.28it/s]Extractor Estimating: 190it [02:20,  1.27it/s]Extractor Estimating: 191it [02:21,  1.28it/s]Extractor Estimating: 192it [02:22,  1.28it/s]Extractor Estimating: 193it [02:22,  1.32it/s]Extractor Estimating: 194it [02:23,  1.35it/s]Extractor Estimating: 195it [02:24,  1.35it/s]Extractor Estimating: 196it [02:25,  1.33it/s]Extractor Estimating: 197it [02:25,  1.37it/s]Extractor Estimating: 198it [02:26,  1.34it/s]Extractor Estimating: 199it [02:27,  1.32it/s]Extractor Estimating: 200it [02:28,  1.36it/s]Extractor Estimating: 201it [02:28,  1.33it/s]Extractor Estimating: 202it [02:29,  1.36it/s]Extractor Estimating: 203it [02:30,  1.35it/s]Extractor Estimating: 204it [02:31,  1.35it/s]Extractor Estimating: 205it [02:31,  1.36it/s]Extractor Estimating: 206it [02:32,  1.37it/s]Extractor Estimating: 207it [02:33,  1.40it/s]Extractor Estimating: 208it [02:33,  1.37it/s]Extractor Estimating: 209it [02:34,  1.35it/s]Extractor Estimating: 210it [02:35,  1.32it/s]Extractor Estimating: 211it [02:36,  1.31it/s]Extractor Estimating: 212it [02:37,  1.30it/s]Extractor Estimating: 213it [02:37,  1.32it/s]Extractor Estimating: 214it [02:38,  1.37it/s]Extractor Estimating: 215it [02:39,  1.37it/s]Extractor Estimating: 216it [02:39,  1.39it/s]Extractor Estimating: 217it [02:40,  1.37it/s]Extractor Estimating: 218it [02:41,  1.41it/s]Extractor Estimating: 219it [02:41,  1.42it/s]Extractor Estimating: 220it [02:42,  1.40it/s]Extractor Estimating: 221it [02:43,  1.41it/s]Extractor Estimating: 222it [02:44,  1.41it/s]Extractor Estimating: 223it [02:45,  1.28it/s]Extractor Estimating: 224it [02:45,  1.33it/s]Extractor Estimating: 225it [02:46,  1.36it/s]Extractor Estimating: 226it [02:47,  1.32it/s]Extractor Estimating: 227it [02:47,  1.36it/s]Extractor Estimating: 228it [02:48,  1.40it/s]Extractor Estimating: 229it [02:49,  1.38it/s]Extractor Estimating: 230it [02:50,  1.40it/s]Extractor Estimating: 231it [02:50,  1.42it/s]Extractor Estimating: 232it [02:51,  1.44it/s]Extractor Estimating: 233it [02:52,  1.43it/s]Extractor Estimating: 234it [02:52,  1.48it/s]Extractor Estimating: 235it [02:53,  1.43it/s]Extractor Estimating: 236it [02:54,  1.45it/s]Extractor Estimating: 237it [02:54,  1.42it/s]Extractor Estimating: 238it [02:55,  1.44it/s]Extractor Estimating: 239it [02:56,  1.40it/s]Extractor Estimating: 240it [02:56,  1.45it/s]Extractor Estimating: 241it [02:57,  1.50it/s]Extractor Estimating: 242it [02:58,  1.54it/s]Extractor Estimating: 243it [02:58,  1.49it/s]Extractor Estimating: 244it [02:59,  1.50it/s]Extractor Estimating: 245it [03:00,  1.51it/s]Extractor Estimating: 246it [03:00,  1.56it/s]Extractor Estimating: 247it [03:01,  1.50it/s]Extractor Estimating: 248it [03:02,  1.52it/s]Extractor Estimating: 249it [03:02,  1.51it/s]Extractor Estimating: 250it [03:03,  1.45it/s]Extractor Estimating: 250it [03:03,  1.36it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:46,593 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:46,600 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:46,600 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:46,600 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:46,600 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:04:47,198 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:04:47,199 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:04:47,859 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:04:48,875 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:04:48,875 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:51,798 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:51,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:51,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:51,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:04:51,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:04:52,420 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:04:52,421 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:04:53,000 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:04:53,147 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:04:53,147 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 20:58:23,308 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 20:58:23,456 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 4999 mean pseudo reward: 0.9379663782042448
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl'}
train vocab size: 15706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15806, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.313, loss:581.2581
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.314, loss:569.4952
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.299, loss:523.6502
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.322, loss:530.5253
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.287, loss:498.3714
>> valid entity prec:0.5322, rec:0.4932, f1:0.5120
>> valid relation prec:0.4168, rec:0.1337, f1:0.2024
>> valid relation with NER prec:0.4168, rec:0.1337, f1:0.2024
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.978, loss:499.3894
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.325, loss:481.7319
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.305, loss:489.5535
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.287, loss:470.3952
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.319, loss:470.5158
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4808, rec:0.4781, f1:0.4794
>> valid relation prec:0.3482, rec:0.1073, f1:0.1640
>> valid relation with NER prec:0.3482, rec:0.1073, f1:0.1640
g_step 1100, step 55, avg_time 2.992, loss:452.0248
g_step 1200, step 155, avg_time 1.314, loss:452.5088
g_step 1300, step 46, avg_time 1.319, loss:427.0556
g_step 1400, step 146, avg_time 1.303, loss:443.4861
g_step 1500, step 37, avg_time 1.296, loss:431.6854
>> valid entity prec:0.5204, rec:0.4694, f1:0.4936
>> valid relation prec:0.3291, rec:0.0964, f1:0.1491
>> valid relation with NER prec:0.3291, rec:0.0964, f1:0.1491
g_step 1600, step 137, avg_time 2.991, loss:403.2453
g_step 1700, step 28, avg_time 1.297, loss:418.4862
g_step 1800, step 128, avg_time 1.302, loss:385.7794
g_step 1900, step 19, avg_time 1.314, loss:409.8191
g_step 2000, step 119, avg_time 1.317, loss:403.0569
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5114, rec:0.4364, f1:0.4709
>> valid relation prec:0.3175, rec:0.0746, f1:0.1208
>> valid relation with NER prec:0.3175, rec:0.0746, f1:0.1208
g_step 2100, step 10, avg_time 2.976, loss:369.1614
g_step 2200, step 110, avg_time 1.304, loss:363.1977
g_step 2300, step 1, avg_time 1.309, loss:378.6811
g_step 2400, step 101, avg_time 1.307, loss:331.3069
g_step 2500, step 201, avg_time 1.321, loss:354.1813
>> valid entity prec:0.4864, rec:0.4942, f1:0.4903
>> valid relation prec:0.2855, rec:0.1477, f1:0.1947
>> valid relation with NER prec:0.2855, rec:0.1477, f1:0.1947
g_step 2600, step 92, avg_time 2.963, loss:328.8247
g_step 2700, step 192, avg_time 1.312, loss:341.7141
g_step 2800, step 83, avg_time 1.314, loss:317.7227
g_step 2900, step 183, avg_time 1.313, loss:342.0688
g_step 3000, step 74, avg_time 1.301, loss:292.7085
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4864, rec:0.5033, f1:0.4947
>> valid relation prec:0.3080, rec:0.1466, f1:0.1986
>> valid relation with NER prec:0.3080, rec:0.1466, f1:0.1986
g_step 3100, step 174, avg_time 2.980, loss:340.0172
g_step 3200, step 65, avg_time 1.311, loss:299.6389
g_step 3300, step 165, avg_time 1.316, loss:329.6458
g_step 3400, step 56, avg_time 1.300, loss:304.8637
g_step 3500, step 156, avg_time 1.311, loss:306.2004
>> valid entity prec:0.5127, rec:0.4838, f1:0.4978
>> valid relation prec:0.3431, rec:0.1776, f1:0.2340
>> valid relation with NER prec:0.3431, rec:0.1776, f1:0.2340
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 47, avg_time 2.961, loss:278.7753
g_step 3700, step 147, avg_time 1.325, loss:302.1242
g_step 3800, step 38, avg_time 1.312, loss:293.9723
g_step 3900, step 138, avg_time 1.319, loss:270.6429
g_step 4000, step 29, avg_time 1.295, loss:285.7954
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5038, rec:0.4408, f1:0.4702
>> valid relation prec:0.3233, rec:0.1388, f1:0.1943
>> valid relation with NER prec:0.3233, rec:0.1388, f1:0.1943
g_step 4100, step 129, avg_time 2.978, loss:256.6014
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 20:58:23 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 20:58:23 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_20-58-23_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 20:58:24 - WARNING - datasets.builder -   Using custom data configuration default-4b1082affa39e468
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-4b1082affa39e468/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 20:58:25,163 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:58:25,164 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:58:25,164 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:58:25,165 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:58:25,174 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:58:25,177 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:58:25,177 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:58:25,177 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:58:25,177 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:58:25,177 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:58:25,178 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 20:58:25,300 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:58:28,553 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 20:58:28,579 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-4b1082affa39e468/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.48ba/s] 40%|████      | 2/5 [00:00<00:00,  3.56ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.15ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.49ba/s]100%|██████████| 5/5 [00:01<00:00,  4.70ba/s]100%|██████████| 5/5 [00:01<00:00,  4.23ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.20ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.89ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.18ba/s]100%|██████████| 4/4 [00:00<00:00,  5.35ba/s]100%|██████████| 4/4 [00:00<00:00,  4.68ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.92ba/s] 40%|████      | 2/5 [00:00<00:00,  6.88ba/s] 80%|████████  | 4/5 [00:00<00:00,  8.54ba/s]100%|██████████| 5/5 [00:00<00:00,  8.78ba/s]100%|██████████| 5/5 [00:00<00:00,  8.12ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.50ba/s] 50%|█████     | 2/4 [00:00<00:00,  9.23ba/s]100%|██████████| 4/4 [00:00<00:00, 11.85ba/s]100%|██████████| 4/4 [00:00<00:00, 11.12ba/s]
[INFO|trainer.py:414] 2023-08-28 20:58:32,720 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 20:58:32,744 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 20:58:32,745 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 20:58:32,745 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 20:58:32,745 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 20:58:32,745 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 20:58:32,745 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 20:58:32,745 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<02:02,  3.18it/s]  1%|          | 2/390 [00:00<01:56,  3.33it/s]  1%|          | 3/390 [00:00<01:54,  3.37it/s]  1%|          | 4/390 [00:01<01:53,  3.39it/s]  1%|▏         | 5/390 [00:01<01:53,  3.39it/s]  2%|▏         | 6/390 [00:01<01:52,  3.40it/s]  2%|▏         | 7/390 [00:02<01:52,  3.42it/s]  2%|▏         | 8/390 [00:02<01:54,  3.34it/s]  2%|▏         | 9/390 [00:02<01:53,  3.37it/s]  3%|▎         | 10/390 [00:02<01:52,  3.39it/s]  3%|▎         | 11/390 [00:03<01:51,  3.41it/s]  3%|▎         | 12/390 [00:03<01:50,  3.42it/s]  3%|▎         | 13/390 [00:03<01:50,  3.42it/s]  4%|▎         | 14/390 [00:04<01:49,  3.43it/s]  4%|▍         | 15/390 [00:04<01:49,  3.43it/s]  4%|▍         | 16/390 [00:04<01:48,  3.43it/s]  4%|▍         | 17/390 [00:04<01:48,  3.43it/s]  5%|▍         | 18/390 [00:05<01:48,  3.43it/s]  5%|▍         | 19/390 [00:05<01:48,  3.43it/s]  5%|▌         | 20/390 [00:05<01:48,  3.42it/s]  5%|▌         | 21/390 [00:06<01:47,  3.43it/s]  6%|▌         | 22/390 [00:06<01:47,  3.43it/s]  6%|▌         | 23/390 [00:06<01:47,  3.42it/s]  6%|▌         | 24/390 [00:07<01:47,  3.42it/s]  6%|▋         | 25/390 [00:07<01:46,  3.42it/s]  7%|▋         | 26/390 [00:07<01:46,  3.42it/s]  7%|▋         | 27/390 [00:07<01:45,  3.43it/s]  7%|▋         | 28/390 [00:08<01:45,  3.43it/s]  7%|▋         | 29/390 [00:08<01:45,  3.43it/s]  8%|▊         | 30/390 [00:08<01:46,  3.39it/s]  8%|▊         | 31/390 [00:09<01:45,  3.40it/s]  8%|▊         | 32/390 [00:09<01:45,  3.40it/s]  8%|▊         | 33/390 [00:09<01:44,  3.41it/s]  9%|▊         | 34/390 [00:09<01:44,  3.41it/s]  9%|▉         | 35/390 [00:10<01:43,  3.42it/s]  9%|▉         | 36/390 [00:10<01:43,  3.42it/s]  9%|▉         | 37/390 [00:10<01:43,  3.42it/s] 10%|▉         | 38/390 [00:11<01:43,  3.39it/s] 10%|█         | 39/390 [00:11<01:43,  3.40it/s] 10%|█         | 40/390 [00:11<01:42,  3.41it/s] 11%|█         | 41/390 [00:12<01:42,  3.41it/s] 11%|█         | 42/390 [00:12<01:41,  3.42it/s] 11%|█         | 43/390 [00:12<01:41,  3.42it/s] 11%|█▏        | 44/390 [00:12<01:41,  3.42it/s] 12%|█▏        | 45/390 [00:13<01:40,  3.42it/s] 12%|█▏        | 46/390 [00:13<01:40,  3.42it/s] 12%|█▏        | 47/390 [00:13<01:40,  3.42it/s] 12%|█▏        | 48/390 [00:14<01:40,  3.42it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.41it/s] 13%|█▎        | 50/390 [00:14<01:39,  3.41it/s] 13%|█▎        | 51/390 [00:14<01:39,  3.42it/s] 13%|█▎        | 52/390 [00:15<01:38,  3.42it/s] 14%|█▎        | 53/390 [00:15<01:38,  3.42it/s] 14%|█▍        | 54/390 [00:15<01:38,  3.42it/s] 14%|█▍        | 55/390 [00:16<01:37,  3.42it/s] 14%|█▍        | 56/390 [00:16<01:37,  3.42it/s] 15%|█▍        | 57/390 [00:16<01:37,  3.42it/s] 15%|█▍        | 58/390 [00:17<01:37,  3.42it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.42it/s] 15%|█▌        | 60/390 [00:17<01:36,  3.42it/s] 16%|█▌        | 61/390 [00:17<01:36,  3.42it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.42it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.42it/s] 16%|█▋        | 64/390 [00:18<01:35,  3.42it/s] 17%|█▋        | 65/390 [00:19<01:35,  3.42it/s] 17%|█▋        | 66/390 [00:19<01:35,  3.41it/s] 17%|█▋        | 67/390 [00:19<01:34,  3.41it/s] 17%|█▋        | 68/390 [00:19<01:34,  3.41it/s] 18%|█▊        | 69/390 [00:20<01:34,  3.41it/s] 18%|█▊        | 70/390 [00:20<01:33,  3.42it/s] 18%|█▊        | 71/390 [00:20<01:33,  3.42it/s] 18%|█▊        | 72/390 [00:21<01:32,  3.42it/s] 19%|█▊        | 73/390 [00:21<01:35,  3.31it/s] 19%|█▉        | 74/390 [00:21<01:34,  3.34it/s] 19%|█▉        | 75/390 [00:22<01:33,  3.37it/s] 19%|█▉        | 76/390 [00:22<01:32,  3.38it/s] 20%|█▉        | 77/390 [00:22<01:32,  3.39it/s] 20%|██        | 78/390 [00:22<01:31,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 20:58:55,685 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:58:55,685 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 20:58:55,685 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.72it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.90it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.45it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.64it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.23it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.86it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.50it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.13it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.17it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.23it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.35it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.38it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.29it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.30it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.31it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.20it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.90it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.97it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.14it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.11it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.25it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.21it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.25it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.29it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.07it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.05it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.11it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.16it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.14it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.15it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.13it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.14it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.26it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.19it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.13it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.02it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.08it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.20it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.15it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.20it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.20it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.13it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.15it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.06it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.11it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.05it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.07it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.19it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.19it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.23it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.12it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.06it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.97it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.06it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.08it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.14it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.08it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.12it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 38.94it/s][A
 69%|██████▉   | 303/436 [00:06<00:03, 41.03it/s][A
 71%|███████   | 308/436 [00:06<00:03, 42.47it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 43.52it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 44.34it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 44.85it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.29it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.54it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.44it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.62it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.86it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.88it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.03it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.07it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 46.12it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.21it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.17it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.81it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.95it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.03it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.06it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.17it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.07it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.10it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.14it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.04it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.99it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.88it/s][A
                                                 [A                                                
100%|██████████| 436/436 [00:09<00:00, 45.88it/s][A 20%|██        | 78/390 [00:32<01:31,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:59:05,228 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 20:59:05,265 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:59:09,529 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:59:09,661 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:59:09,679 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:48<40:12,  7.76s/it] 21%|██        | 80/390 [00:48<28:30,  5.52s/it] 21%|██        | 81/390 [00:48<20:20,  3.95s/it] 21%|██        | 82/390 [00:48<14:38,  2.85s/it] 21%|██▏       | 83/390 [00:49<10:39,  2.08s/it] 22%|██▏       | 84/390 [00:49<07:53,  1.55s/it] 22%|██▏       | 85/390 [00:49<05:57,  1.17s/it] 22%|██▏       | 86/390 [00:50<04:35,  1.10it/s] 22%|██▏       | 87/390 [00:50<03:39,  1.38it/s] 23%|██▎       | 88/390 [00:50<03:08,  1.60it/s] 23%|██▎       | 89/390 [00:51<02:38,  1.90it/s] 23%|██▎       | 90/390 [00:51<02:17,  2.18it/s] 23%|██▎       | 91/390 [00:51<02:02,  2.45it/s] 24%|██▎       | 92/390 [00:51<01:51,  2.68it/s] 24%|██▍       | 93/390 [00:52<01:43,  2.86it/s] 24%|██▍       | 94/390 [00:52<01:38,  3.01it/s] 24%|██▍       | 95/390 [00:52<01:34,  3.12it/s] 25%|██▍       | 96/390 [00:53<01:31,  3.20it/s] 25%|██▍       | 97/390 [00:53<01:29,  3.27it/s] 25%|██▌       | 98/390 [00:53<01:28,  3.31it/s] 25%|██▌       | 99/390 [00:54<01:27,  3.34it/s] 26%|██▌       | 100/390 [00:54<01:26,  3.36it/s] 26%|██▌       | 101/390 [00:54<01:27,  3.31it/s] 26%|██▌       | 102/390 [00:54<01:26,  3.34it/s] 26%|██▋       | 103/390 [00:55<01:25,  3.37it/s] 27%|██▋       | 104/390 [00:55<01:24,  3.38it/s] 27%|██▋       | 105/390 [00:55<01:24,  3.39it/s] 27%|██▋       | 106/390 [00:56<01:23,  3.40it/s] 27%|██▋       | 107/390 [00:56<01:23,  3.40it/s] 28%|██▊       | 108/390 [00:56<01:22,  3.41it/s] 28%|██▊       | 109/390 [00:56<01:22,  3.41it/s] 28%|██▊       | 110/390 [00:57<01:22,  3.41it/s] 28%|██▊       | 111/390 [00:57<01:21,  3.41it/s] 29%|██▊       | 112/390 [00:57<01:22,  3.38it/s] 29%|██▉       | 113/390 [00:58<01:21,  3.39it/s] 29%|██▉       | 114/390 [00:58<01:21,  3.40it/s] 29%|██▉       | 115/390 [00:58<01:20,  3.40it/s] 30%|██▉       | 116/390 [00:59<01:20,  3.41it/s] 30%|███       | 117/390 [00:59<01:20,  3.41it/s] 30%|███       | 118/390 [00:59<01:19,  3.41it/s] 31%|███       | 119/390 [00:59<01:19,  3.41it/s] 31%|███       | 120/390 [01:00<01:19,  3.41it/s] 31%|███       | 121/390 [01:00<01:18,  3.41it/s] 31%|███▏      | 122/390 [01:00<01:18,  3.41it/s] 32%|███▏      | 123/390 [01:01<01:19,  3.34it/s] 32%|███▏      | 124/390 [01:01<01:19,  3.36it/s] 32%|███▏      | 125/390 [01:01<01:18,  3.38it/s] 32%|███▏      | 126/390 [01:01<01:17,  3.39it/s] 33%|███▎      | 127/390 [01:02<01:17,  3.39it/s] 33%|███▎      | 128/390 [01:02<01:17,  3.40it/s] 33%|███▎      | 129/390 [01:02<01:16,  3.40it/s] 33%|███▎      | 130/390 [01:03<01:16,  3.40it/s] 34%|███▎      | 131/390 [01:03<01:16,  3.41it/s] 34%|███▍      | 132/390 [01:03<01:15,  3.41it/s] 34%|███▍      | 133/390 [01:04<01:15,  3.41it/s] 34%|███▍      | 134/390 [01:04<01:15,  3.41it/s] 35%|███▍      | 135/390 [01:04<01:14,  3.41it/s] 35%|███▍      | 136/390 [01:04<01:14,  3.41it/s] 35%|███▌      | 137/390 [01:05<01:14,  3.41it/s] 35%|███▌      | 138/390 [01:05<01:17,  3.25it/s] 36%|███▌      | 139/390 [01:05<01:16,  3.30it/s] 36%|███▌      | 140/390 [01:06<01:15,  3.33it/s] 36%|███▌      | 141/390 [01:06<01:14,  3.36it/s] 36%|███▋      | 142/390 [01:06<01:13,  3.37it/s] 37%|███▋      | 143/390 [01:07<01:12,  3.39it/s] 37%|███▋      | 144/390 [01:07<01:12,  3.39it/s] 37%|███▋      | 145/390 [01:07<01:12,  3.40it/s] 37%|███▋      | 146/390 [01:07<01:11,  3.40it/s] 38%|███▊      | 147/390 [01:08<01:11,  3.41it/s] 38%|███▊      | 148/390 [01:08<01:10,  3.41it/s] 38%|███▊      | 149/390 [01:08<01:12,  3.31it/s] 38%|███▊      | 150/390 [01:09<01:11,  3.34it/s] 39%|███▊      | 151/390 [01:09<01:11,  3.36it/s] 39%|███▉      | 152/390 [01:09<01:10,  3.37it/s] 39%|███▉      | 153/390 [01:09<01:09,  3.39it/s] 39%|███▉      | 154/390 [01:10<01:09,  3.40it/s] 40%|███▉      | 155/390 [01:10<01:09,  3.40it/s] 40%|████      | 156/390 [01:10<01:08,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 20:59:43,649 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:59:43,649 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 20:59:43,649 >>   Batch size = 8
{'eval_loss': 1.0592458248138428, 'eval_runtime': 9.5158, 'eval_samples_per_second': 366.34, 'eval_steps_per_second': 45.819, 'epoch': 0.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.55it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.00it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.32it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.42it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.09it/s][A
  8%|▊         | 33/436 [00:00<00:09, 40.39it/s][A
  9%|▊         | 38/436 [00:00<00:09, 42.05it/s][A
 10%|▉         | 43/436 [00:00<00:09, 43.35it/s][A
 11%|█         | 48/436 [00:01<00:08, 44.14it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 44.68it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.21it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 45.47it/s][A
 16%|█▌        | 68/436 [00:01<00:08, 45.80it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 45.55it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.72it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.88it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.92it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.99it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.04it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.04it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.18it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.04it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 45.98it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.03it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.95it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.97it/s][A
 32%|███▏      | 138/436 [00:03<00:06, 46.11it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.11it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.19it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.19it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.07it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.00it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.95it/s][A
 40%|███▉      | 173/436 [00:03<00:06, 43.48it/s][A
 41%|████      | 178/436 [00:03<00:05, 44.30it/s][A
 42%|████▏     | 183/436 [00:04<00:05, 44.88it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.28it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.54it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.74it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.84it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.90it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.73it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.85it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.84it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.91it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.00it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.05it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.13it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.12it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.01it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.96it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.94it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.98it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.97it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.99it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.95it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.06it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.13it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.09it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.94it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.99it/s][A
 72%|███████▏  | 313/436 [00:06<00:03, 36.62it/s][A
 73%|███████▎  | 318/436 [00:07<00:03, 39.08it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 40.98it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 42.45it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 43.54it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 44.20it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 44.80it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.25it/s][A
 81%|████████  | 353/436 [00:07<00:01, 44.93it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.23it/s][A
 83%|████████▎ | 363/436 [00:08<00:01, 45.54it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 45.63it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.82it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.96it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.05it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.15it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.03it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.73it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.80it/s][A
 94%|█████████▎| 408/436 [00:09<00:00, 45.89it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 45.88it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.97it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.97it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.04it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.04it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 46.04it/s][A 40%|████      | 156/390 [01:20<01:08,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:59:53,503 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 20:59:53,571 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:00:00,228 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:00:00,243 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:00:00,255 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:39<33:47,  8.70s/it] 41%|████      | 158/390 [01:39<23:54,  6.18s/it] 41%|████      | 159/390 [01:39<17:00,  4.42s/it] 41%|████      | 160/390 [01:40<12:11,  3.18s/it] 41%|████▏     | 161/390 [01:40<08:49,  2.31s/it] 42%|████▏     | 162/390 [01:40<06:29,  1.71s/it] 42%|████▏     | 163/390 [01:40<04:50,  1.28s/it] 42%|████▏     | 164/390 [01:41<03:42,  1.02it/s] 42%|████▏     | 165/390 [01:41<02:54,  1.29it/s] 43%|████▎     | 166/390 [01:41<02:21,  1.58it/s] 43%|████▎     | 167/390 [01:42<01:58,  1.89it/s] 43%|████▎     | 168/390 [01:42<01:41,  2.18it/s] 43%|████▎     | 169/390 [01:42<01:38,  2.24it/s] 44%|████▎     | 170/390 [01:43<01:28,  2.50it/s] 44%|████▍     | 171/390 [01:43<01:20,  2.72it/s] 44%|████▍     | 172/390 [01:43<01:15,  2.90it/s] 44%|████▍     | 173/390 [01:43<01:11,  3.04it/s] 45%|████▍     | 174/390 [01:44<01:08,  3.14it/s] 45%|████▍     | 175/390 [01:44<01:06,  3.22it/s] 45%|████▌     | 176/390 [01:44<01:05,  3.28it/s] 45%|████▌     | 177/390 [01:45<01:04,  3.32it/s] 46%|████▌     | 178/390 [01:45<01:03,  3.35it/s] 46%|████▌     | 179/390 [01:45<01:02,  3.36it/s] 46%|████▌     | 180/390 [01:46<01:02,  3.38it/s] 46%|████▋     | 181/390 [01:46<01:01,  3.39it/s] 47%|████▋     | 182/390 [01:46<01:01,  3.40it/s] 47%|████▋     | 183/390 [01:46<01:00,  3.40it/s] 47%|████▋     | 184/390 [01:47<01:00,  3.41it/s] 47%|████▋     | 185/390 [01:47<01:00,  3.41it/s] 48%|████▊     | 186/390 [01:47<00:59,  3.41it/s] 48%|████▊     | 187/390 [01:48<00:59,  3.41it/s] 48%|████▊     | 188/390 [01:48<00:59,  3.42it/s] 48%|████▊     | 189/390 [01:48<00:58,  3.41it/s] 49%|████▊     | 190/390 [01:48<00:59,  3.34it/s] 49%|████▉     | 191/390 [01:49<00:59,  3.36it/s] 49%|████▉     | 192/390 [01:49<00:59,  3.35it/s] 49%|████▉     | 193/390 [01:49<00:58,  3.37it/s] 50%|████▉     | 194/390 [01:50<00:57,  3.39it/s] 50%|█████     | 195/390 [01:50<00:57,  3.40it/s] 50%|█████     | 196/390 [01:51<01:15,  2.59it/s] 51%|█████     | 197/390 [01:51<01:09,  2.79it/s] 51%|█████     | 198/390 [01:51<01:05,  2.95it/s] 51%|█████     | 199/390 [01:51<01:02,  3.08it/s] 51%|█████▏    | 200/390 [01:52<01:00,  3.15it/s] 52%|█████▏    | 201/390 [01:52<00:58,  3.23it/s] 52%|█████▏    | 202/390 [01:52<00:57,  3.28it/s] 52%|█████▏    | 203/390 [01:53<00:56,  3.32it/s] 52%|█████▏    | 204/390 [01:53<00:55,  3.35it/s] 53%|█████▎    | 205/390 [01:53<00:54,  3.37it/s] 53%|█████▎    | 206/390 [01:53<00:54,  3.38it/s] 53%|█████▎    | 207/390 [01:54<00:53,  3.39it/s] 53%|█████▎    | 208/390 [01:54<00:53,  3.40it/s] 54%|█████▎    | 209/390 [01:54<00:53,  3.40it/s] 54%|█████▍    | 210/390 [01:55<00:52,  3.41it/s] 54%|█████▍    | 211/390 [01:55<00:54,  3.31it/s] 54%|█████▍    | 212/390 [01:55<00:53,  3.34it/s] 55%|█████▍    | 213/390 [01:56<00:52,  3.36it/s] 55%|█████▍    | 214/390 [01:56<00:52,  3.37it/s] 55%|█████▌    | 215/390 [01:56<00:51,  3.39it/s] 55%|█████▌    | 216/390 [01:56<00:51,  3.40it/s] 56%|█████▌    | 217/390 [01:57<00:50,  3.40it/s] 56%|█████▌    | 218/390 [01:57<00:50,  3.40it/s] 56%|█████▌    | 219/390 [01:57<00:50,  3.41it/s] 56%|█████▋    | 220/390 [01:58<00:50,  3.40it/s] 57%|█████▋    | 221/390 [01:58<00:49,  3.40it/s] 57%|█████▋    | 222/390 [01:58<00:50,  3.33it/s] 57%|█████▋    | 223/390 [01:59<00:49,  3.35it/s] 57%|█████▋    | 224/390 [01:59<00:49,  3.37it/s] 58%|█████▊    | 225/390 [01:59<00:48,  3.38it/s] 58%|█████▊    | 226/390 [01:59<00:48,  3.39it/s] 58%|█████▊    | 227/390 [02:00<00:48,  3.40it/s] 58%|█████▊    | 228/390 [02:00<00:47,  3.40it/s] 59%|█████▊    | 229/390 [02:00<00:47,  3.40it/s] 59%|█████▉    | 230/390 [02:01<00:46,  3.41it/s] 59%|█████▉    | 231/390 [02:01<00:46,  3.41it/s] 59%|█████▉    | 232/390 [02:01<00:46,  3.41it/s] 60%|█████▉    | 233/390 [02:01<00:46,  3.39it/s] 60%|██████    | 234/390 [02:02<00:45,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 21:00:35,056 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:00:35,056 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 21:00:35,056 >>   Batch size = 8
{'eval_loss': 1.078075647354126, 'eval_runtime': 9.6317, 'eval_samples_per_second': 361.928, 'eval_steps_per_second': 45.267, 'epoch': 1.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.59it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.04it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.07it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.40it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.02it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.75it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.64it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.19it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.07it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.16it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.22it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.12it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.17it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.12it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.18it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.16it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.98it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.95it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.98it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.92it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.08it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.00it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.12it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.17it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.13it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.92it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.99it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.01it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.01it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.16it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.10it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.03it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.13it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.10it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.99it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.95it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.03it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.94it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.03it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.10it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.08it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.10it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.07it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.09it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.82it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.02it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.99it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.05it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.03it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.91it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.94it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.87it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.96it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.88it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.05it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.07it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.05it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.02it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.02it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.10it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.11it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.01it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.98it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 45.93it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.99it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.01it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.09it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.02it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.10it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.02it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.04it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.04it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.04it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.97it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.00it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.93it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.99it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.08it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.08it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.08it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.95it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.89it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.99it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.91it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.00it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 36.71it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 36.71it/s][A 60%|██████    | 234/390 [02:11<00:45,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:00:44,635 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 21:00:44,725 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:00:49,554 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:00:49,573 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:00:49,583 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:27<20:12,  7.82s/it] 61%|██████    | 236/390 [02:27<14:17,  5.57s/it] 61%|██████    | 237/390 [02:28<10:09,  3.98s/it] 61%|██████    | 238/390 [02:28<07:17,  2.88s/it] 61%|██████▏   | 239/390 [02:28<05:17,  2.10s/it] 62%|██████▏   | 240/390 [02:29<03:53,  1.56s/it] 62%|██████▏   | 241/390 [02:29<02:55,  1.18s/it] 62%|██████▏   | 242/390 [02:29<02:15,  1.10it/s] 62%|██████▏   | 243/390 [02:29<01:46,  1.38it/s] 63%|██████▎   | 244/390 [02:30<01:27,  1.68it/s] 63%|██████▎   | 245/390 [02:30<01:13,  1.98it/s] 63%|██████▎   | 246/390 [02:30<01:03,  2.27it/s] 63%|██████▎   | 247/390 [02:31<00:56,  2.52it/s] 64%|██████▎   | 248/390 [02:31<00:51,  2.73it/s] 64%|██████▍   | 249/390 [02:31<00:48,  2.91it/s] 64%|██████▍   | 250/390 [02:32<00:45,  3.05it/s] 64%|██████▍   | 251/390 [02:32<00:44,  3.15it/s] 65%|██████▍   | 252/390 [02:32<00:42,  3.23it/s] 65%|██████▍   | 253/390 [02:32<00:41,  3.28it/s] 65%|██████▌   | 254/390 [02:33<00:40,  3.32it/s] 65%|██████▌   | 255/390 [02:33<00:40,  3.35it/s] 66%|██████▌   | 256/390 [02:33<00:39,  3.37it/s] 66%|██████▌   | 257/390 [02:34<00:39,  3.39it/s] 66%|██████▌   | 258/390 [02:34<00:40,  3.26it/s] 66%|██████▋   | 259/390 [02:34<00:39,  3.31it/s] 67%|██████▋   | 260/390 [02:35<00:38,  3.34it/s] 67%|██████▋   | 261/390 [02:35<00:38,  3.37it/s] 67%|██████▋   | 262/390 [02:35<00:37,  3.38it/s] 67%|██████▋   | 263/390 [02:35<00:37,  3.39it/s] 68%|██████▊   | 264/390 [02:36<00:37,  3.40it/s] 68%|██████▊   | 265/390 [02:36<00:36,  3.40it/s] 68%|██████▊   | 266/390 [02:36<00:36,  3.41it/s] 68%|██████▊   | 267/390 [02:37<00:36,  3.41it/s] 69%|██████▊   | 268/390 [02:37<00:35,  3.42it/s] 69%|██████▉   | 269/390 [02:37<00:36,  3.28it/s] 69%|██████▉   | 270/390 [02:37<00:36,  3.32it/s] 69%|██████▉   | 271/390 [02:38<00:35,  3.35it/s] 70%|██████▉   | 272/390 [02:38<00:35,  3.37it/s] 70%|███████   | 273/390 [02:38<00:34,  3.38it/s] 70%|███████   | 274/390 [02:39<00:34,  3.39it/s] 71%|███████   | 275/390 [02:39<00:33,  3.40it/s] 71%|███████   | 276/390 [02:39<00:33,  3.41it/s] 71%|███████   | 277/390 [02:40<00:33,  3.41it/s] 71%|███████▏  | 278/390 [02:40<00:32,  3.41it/s] 72%|███████▏  | 279/390 [02:40<00:32,  3.41it/s] 72%|███████▏  | 280/390 [02:40<00:32,  3.42it/s] 72%|███████▏  | 281/390 [02:41<00:31,  3.41it/s] 72%|███████▏  | 282/390 [02:41<00:31,  3.42it/s] 73%|███████▎  | 283/390 [02:41<00:31,  3.42it/s] 73%|███████▎  | 284/390 [02:42<00:31,  3.41it/s] 73%|███████▎  | 285/390 [02:42<00:30,  3.42it/s] 73%|███████▎  | 286/390 [02:42<00:31,  3.35it/s] 74%|███████▎  | 287/390 [02:42<00:30,  3.37it/s] 74%|███████▍  | 288/390 [02:43<00:30,  3.38it/s] 74%|███████▍  | 289/390 [02:43<00:29,  3.39it/s] 74%|███████▍  | 290/390 [02:43<00:29,  3.40it/s] 75%|███████▍  | 291/390 [02:44<00:29,  3.40it/s] 75%|███████▍  | 292/390 [02:44<00:28,  3.41it/s] 75%|███████▌  | 293/390 [02:44<00:28,  3.41it/s] 75%|███████▌  | 294/390 [02:45<00:28,  3.41it/s] 76%|███████▌  | 295/390 [02:45<00:27,  3.41it/s] 76%|███████▌  | 296/390 [02:45<00:27,  3.41it/s] 76%|███████▌  | 297/390 [02:45<00:28,  3.27it/s] 76%|███████▋  | 298/390 [02:46<00:27,  3.31it/s] 77%|███████▋  | 299/390 [02:46<00:27,  3.34it/s] 77%|███████▋  | 300/390 [02:46<00:26,  3.37it/s] 77%|███████▋  | 301/390 [02:47<00:26,  3.38it/s] 77%|███████▋  | 302/390 [02:47<00:25,  3.39it/s] 78%|███████▊  | 303/390 [02:47<00:25,  3.39it/s] 78%|███████▊  | 304/390 [02:47<00:25,  3.40it/s] 78%|███████▊  | 305/390 [02:48<00:25,  3.40it/s] 78%|███████▊  | 306/390 [02:48<00:24,  3.41it/s] 79%|███████▊  | 307/390 [02:48<00:24,  3.41it/s] 79%|███████▉  | 308/390 [02:49<00:24,  3.37it/s] 79%|███████▉  | 309/390 [02:49<00:23,  3.38it/s] 79%|███████▉  | 310/390 [02:49<00:23,  3.38it/s] 80%|███████▉  | 311/390 [02:50<00:23,  3.39it/s] 80%|████████  | 312/390 [02:50<00:22,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 21:01:23,153 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:01:23,153 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 21:01:23,153 >>   Batch size = 8
{'eval_loss': 1.095947504043579, 'eval_runtime': 9.5613, 'eval_samples_per_second': 364.596, 'eval_steps_per_second': 45.601, 'epoch': 2.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.86it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.02it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.37it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.31it/s][A
  6%|▋         | 28/436 [00:00<00:10, 40.77it/s][A
  8%|▊         | 33/436 [00:00<00:09, 42.31it/s][A
  9%|▊         | 38/436 [00:00<00:09, 43.48it/s][A
 10%|▉         | 43/436 [00:00<00:08, 44.34it/s][A
 11%|█         | 48/436 [00:01<00:08, 44.88it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.19it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.55it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 45.80it/s][A
 16%|█▌        | 68/436 [00:01<00:08, 45.97it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 45.50it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.51it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.68it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.84it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 46.01it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.02it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.14it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.14it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.07it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.01it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 45.91it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.89it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.88it/s][A
 32%|███▏      | 138/436 [00:03<00:06, 46.04it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.08it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.14it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.22it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.97it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.02it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.91it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.92it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.93it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.88it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.02it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.05it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.18it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.08it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.86it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.61it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.70it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.77it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.85it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.92it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.98it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.11it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.02it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.94it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.94it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.89it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.94it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.95it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.02it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.00it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.09it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.08it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.00it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.93it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.94it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.81it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.91it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.97it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.97it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.98it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.00it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.83it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.94it/s][A
 81%|████████  | 353/436 [00:07<00:01, 42.83it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 43.84it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 44.57it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 44.97it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.29it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.67it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.73it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.90it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.52it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.59it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.76it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.94it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 45.98it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.01it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.13it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.00it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.05it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 46.05it/s][A 80%|████████  | 312/390 [02:59<00:22,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:01:32,766 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 21:01:32,863 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:01:38,010 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:01:38,034 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:01:38,110 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:15<10:04,  7.85s/it] 81%|████████  | 314/390 [03:16<07:04,  5.59s/it] 81%|████████  | 315/390 [03:16<04:59,  4.00s/it] 81%|████████  | 316/390 [03:16<03:33,  2.89s/it] 81%|████████▏ | 317/390 [03:17<02:33,  2.11s/it] 82%|████████▏ | 318/390 [03:17<01:52,  1.56s/it] 82%|████████▏ | 319/390 [03:17<01:23,  1.18s/it] 82%|████████▏ | 320/390 [03:17<01:04,  1.09it/s] 82%|████████▏ | 321/390 [03:18<00:50,  1.37it/s] 83%|████████▎ | 322/390 [03:18<00:40,  1.68it/s] 83%|████████▎ | 323/390 [03:18<00:33,  1.98it/s] 83%|████████▎ | 324/390 [03:19<00:29,  2.27it/s] 83%|████████▎ | 325/390 [03:19<00:26,  2.47it/s] 84%|████████▎ | 326/390 [03:19<00:23,  2.69it/s] 84%|████████▍ | 327/390 [03:19<00:21,  2.88it/s] 84%|████████▍ | 328/390 [03:20<00:20,  3.02it/s] 84%|████████▍ | 329/390 [03:20<00:19,  3.13it/s] 85%|████████▍ | 330/390 [03:20<00:18,  3.21it/s] 85%|████████▍ | 331/390 [03:21<00:18,  3.27it/s] 85%|████████▌ | 332/390 [03:21<00:17,  3.31it/s] 85%|████████▌ | 333/390 [03:21<00:17,  3.34it/s] 86%|████████▌ | 334/390 [03:22<00:16,  3.37it/s] 86%|████████▌ | 335/390 [03:22<00:16,  3.38it/s] 86%|████████▌ | 336/390 [03:22<00:16,  3.34it/s] 86%|████████▋ | 337/390 [03:22<00:15,  3.36it/s] 87%|████████▋ | 338/390 [03:23<00:15,  3.38it/s] 87%|████████▋ | 339/390 [03:23<00:15,  3.39it/s] 87%|████████▋ | 340/390 [03:23<00:14,  3.40it/s] 87%|████████▋ | 341/390 [03:24<00:14,  3.41it/s] 88%|████████▊ | 342/390 [03:24<00:14,  3.41it/s] 88%|████████▊ | 343/390 [03:24<00:13,  3.41it/s] 88%|████████▊ | 344/390 [03:24<00:13,  3.42it/s] 88%|████████▊ | 345/390 [03:25<00:13,  3.42it/s] 89%|████████▊ | 346/390 [03:25<00:12,  3.42it/s] 89%|████████▉ | 347/390 [03:25<00:12,  3.41it/s] 89%|████████▉ | 348/390 [03:26<00:12,  3.41it/s] 89%|████████▉ | 349/390 [03:26<00:12,  3.41it/s] 90%|████████▉ | 350/390 [03:26<00:11,  3.41it/s] 90%|█████████ | 351/390 [03:26<00:11,  3.41it/s] 90%|█████████ | 352/390 [03:27<00:11,  3.41it/s] 91%|█████████ | 353/390 [03:27<00:10,  3.41it/s] 91%|█████████ | 354/390 [03:27<00:10,  3.41it/s] 91%|█████████ | 355/390 [03:28<00:10,  3.41it/s] 91%|█████████▏| 356/390 [03:28<00:09,  3.41it/s] 92%|█████████▏| 357/390 [03:28<00:09,  3.41it/s] 92%|█████████▏| 358/390 [03:29<00:09,  3.36it/s] 92%|█████████▏| 359/390 [03:29<00:09,  3.38it/s] 92%|█████████▏| 360/390 [03:29<00:08,  3.39it/s] 93%|█████████▎| 361/390 [03:29<00:08,  3.40it/s] 93%|█████████▎| 362/390 [03:30<00:08,  3.40it/s] 93%|█████████▎| 363/390 [03:30<00:07,  3.41it/s] 93%|█████████▎| 364/390 [03:30<00:07,  3.41it/s] 94%|█████████▎| 365/390 [03:31<00:07,  3.41it/s] 94%|█████████▍| 366/390 [03:31<00:07,  3.41it/s] 94%|█████████▍| 367/390 [03:31<00:06,  3.42it/s] 94%|█████████▍| 368/390 [03:31<00:06,  3.42it/s] 95%|█████████▍| 369/390 [03:32<00:06,  3.31it/s] 95%|█████████▍| 370/390 [03:32<00:05,  3.34it/s] 95%|█████████▌| 371/390 [03:32<00:05,  3.36it/s] 95%|█████████▌| 372/390 [03:33<00:05,  3.38it/s] 96%|█████████▌| 373/390 [03:33<00:05,  3.39it/s] 96%|█████████▌| 374/390 [03:33<00:04,  3.40it/s] 96%|█████████▌| 375/390 [03:34<00:04,  3.41it/s] 96%|█████████▋| 376/390 [03:34<00:04,  3.41it/s] 97%|█████████▋| 377/390 [03:34<00:03,  3.41it/s] 97%|█████████▋| 378/390 [03:34<00:03,  3.41it/s] 97%|█████████▋| 379/390 [03:35<00:03,  3.41it/s] 97%|█████████▋| 380/390 [03:35<00:02,  3.39it/s] 98%|█████████▊| 381/390 [03:35<00:02,  3.39it/s] 98%|█████████▊| 382/390 [03:36<00:02,  3.40it/s] 98%|█████████▊| 383/390 [03:36<00:02,  3.41it/s] 98%|█████████▊| 384/390 [03:36<00:01,  3.41it/s] 99%|█████████▊| 385/390 [03:37<00:01,  3.41it/s] 99%|█████████▉| 386/390 [03:37<00:01,  3.42it/s] 99%|█████████▉| 387/390 [03:37<00:00,  3.41it/s] 99%|█████████▉| 388/390 [03:37<00:00,  3.41it/s]100%|█████████▉| 389/390 [03:38<00:00,  3.42it/s]100%|██████████| 390/390 [03:38<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 21:02:11,223 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:02:11,223 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 21:02:11,223 >>   Batch size = 8
{'eval_loss': 1.1054445505142212, 'eval_runtime': 9.5513, 'eval_samples_per_second': 364.978, 'eval_steps_per_second': 45.648, 'epoch': 3.99}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.62it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.90it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.20it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.41it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.07it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.87it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.45it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.14it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.11it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.12it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.14it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.12it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.24it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.18it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.19it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.10it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.92it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.92it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.91it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.03it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.01it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.09it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.15it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.01it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.95it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.94it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.91it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.93it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.00it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.02it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.08it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.17it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.11it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.04it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.06it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.93it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.01it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.91it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.94it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.10it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.98it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.08it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.99it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.01it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.97it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.90it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.91it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.94it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.05it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.01it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.96it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.06it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.03it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.01it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.98it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.01it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.87it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.04it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.94it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.01it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.06it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.99it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.95it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 45.91it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.88it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.93it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.05it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.93it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.96it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.01it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.01it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.91it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.81it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.84it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.82it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.99it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.95it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.93it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.03it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.03it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.04it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.98it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.97it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.89it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.98it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.99it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 45.99it/s][A100%|██████████| 390/390 [03:47<00:00,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:02:20,730 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 21:02:20,793 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:02:25,712 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:02:25,740 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:02:25,750 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:02:36,575 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:02:36,579 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78 (score: 1.0592458248138428).
                                                 100%|██████████| 390/390 [04:09<00:00,  3.41it/s]100%|██████████| 390/390 [04:09<00:00,  1.57it/s]
[INFO|trainer.py:1894] 2023-08-28 21:02:41,853 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 21:02:41,870 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:02:46,582 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:02:46,627 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:02:46,649 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:02:47,116 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:47,117 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:47,117 >>   train_loss               =      0.391
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:47,117 >>   train_runtime            = 0:04:09.09
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:47,117 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:47,117 >>   train_samples_per_second =    100.362
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:47,117 >>   train_steps_per_second   =      1.566
{'eval_loss': 1.1116993427276611, 'eval_runtime': 9.4758, 'eval_samples_per_second': 367.884, 'eval_steps_per_second': 46.012, 'epoch': 4.99}
{'train_runtime': 249.0982, 'train_samples_per_second': 100.362, 'train_steps_per_second': 1.566, 'train_loss': 0.39102513239933895, 'epoch': 4.99}
08/28/2023 21:02:47 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:02:47,414 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:02:47,414 >>   Num examples = 3486
[INFO|trainer.py:2145] 2023-08-28 21:02:47,414 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 57.69it/s]  3%|▎         | 12/436 [00:00<00:08, 50.55it/s]  4%|▍         | 18/436 [00:00<00:08, 48.78it/s]  5%|▌         | 23/436 [00:00<00:08, 48.00it/s]  6%|▋         | 28/436 [00:00<00:08, 47.23it/s]  8%|▊         | 33/436 [00:00<00:08, 46.78it/s]  9%|▊         | 38/436 [00:00<00:08, 46.76it/s] 10%|▉         | 43/436 [00:00<00:08, 46.49it/s] 11%|█         | 48/436 [00:01<00:08, 46.40it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.57it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.58it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.57it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.58it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.55it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.55it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.55it/s] 20%|██        | 88/436 [00:01<00:07, 46.47it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.40it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.48it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.54it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.45it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.46it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.48it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.42it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.52it/s] 31%|███       | 133/436 [00:02<00:06, 46.44it/s] 32%|███▏      | 138/436 [00:02<00:06, 46.37it/s] 33%|███▎      | 143/436 [00:03<00:06, 45.63it/s] 34%|███▍      | 148/436 [00:03<00:06, 45.94it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.10it/s] 36%|███▌      | 158/436 [00:03<00:06, 46.14it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.32it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.38it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.35it/s] 41%|████      | 178/436 [00:03<00:05, 46.40it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.25it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.37it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.42it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.41it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.39it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.36it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.42it/s] 50%|█████     | 218/436 [00:04<00:04, 46.42it/s] 51%|█████     | 223/436 [00:04<00:04, 46.36it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.32it/s] 53%|█████▎    | 233/436 [00:04<00:04, 46.34it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.37it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.38it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.46it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.39it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.35it/s] 60%|██████    | 263/436 [00:05<00:03, 46.39it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.35it/s] 63%|██████▎   | 273/436 [00:05<00:03, 46.30it/s] 64%|██████▍   | 278/436 [00:05<00:03, 46.37it/s] 65%|██████▍   | 283/436 [00:06<00:03, 45.35it/s] 66%|██████▌   | 288/436 [00:06<00:03, 45.69it/s] 67%|██████▋   | 293/436 [00:06<00:03, 45.95it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.07it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.13it/s] 71%|███████   | 308/436 [00:06<00:02, 46.13it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.23it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.19it/s] 74%|███████▍  | 323/436 [00:06<00:02, 46.26it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.24it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.27it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.29it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.39it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.36it/s] 81%|████████  | 353/436 [00:07<00:01, 46.38it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.27it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.32it/s] 84%|████████▍ | 368/436 [00:07<00:01, 46.29it/s] 86%|████████▌ | 373/436 [00:08<00:01, 46.29it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.28it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.32it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.20it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.24it/s] 91%|█████████▏| 398/436 [00:08<00:00, 46.37it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.34it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.37it/s] 95%|█████████▍| 413/436 [00:08<00:00, 46.33it/s] 96%|█████████▌| 418/436 [00:09<00:00, 46.28it/s] 97%|█████████▋| 423/436 [00:09<00:00, 42.18it/s] 98%|█████████▊| 428/436 [00:09<00:00, 43.45it/s] 99%|█████████▉| 433/436 [00:09<00:00, 44.31it/s]100%|██████████| 436/436 [00:09<00:00, 46.25it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:02:56,867 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:56,867 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:56,867 >>   eval_loss               =     1.0592
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:56,867 >>   eval_runtime            = 0:00:09.45
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:56,867 >>   eval_samples            =       3486
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:56,867 >>   eval_samples_per_second =      368.8
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:56,867 >>   eval_steps_per_second   =     46.126
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:02:56,867 >>   perplexity              =     2.8842
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:03,936 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:03,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:03,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:03,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:03,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:03:04,718 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:03:04,720 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:03:05,295 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:03:06,329 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:03:06,329 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:09,334 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:09,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:09,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:09,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:09,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:03:09,989 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:03:09,990 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:03:10,611 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:03:10,764 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:03:10,764 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/generator/iter5/model/checkpoint-390
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'manufacturer', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13015
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13115, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.09it/s]Extractor Predicting: 2it [00:01,  1.15it/s]Extractor Predicting: 3it [00:02,  1.18it/s]Extractor Predicting: 4it [00:03,  1.16it/s]Extractor Predicting: 5it [00:04,  1.16it/s]Extractor Predicting: 6it [00:05,  1.16it/s]Extractor Predicting: 7it [00:06,  1.15it/s]Extractor Predicting: 8it [00:06,  1.17it/s]Extractor Predicting: 9it [00:07,  1.17it/s]Extractor Predicting: 10it [00:08,  1.20it/s]Extractor Predicting: 11it [00:09,  1.16it/s]Extractor Predicting: 12it [00:10,  1.16it/s]Extractor Predicting: 13it [00:11,  1.15it/s]Extractor Predicting: 14it [00:12,  1.15it/s]Extractor Predicting: 15it [00:12,  1.16it/s]Extractor Predicting: 16it [00:13,  1.16it/s]Extractor Predicting: 17it [00:14,  1.19it/s]Extractor Predicting: 18it [00:15,  1.20it/s]Extractor Predicting: 19it [00:16,  1.19it/s]Extractor Predicting: 20it [00:17,  1.16it/s]Extractor Predicting: 21it [00:17,  1.18it/s]Extractor Predicting: 22it [00:18,  1.20it/s]Extractor Predicting: 23it [00:19,  1.20it/s]Extractor Predicting: 24it [00:20,  1.16it/s]Extractor Predicting: 25it [00:21,  1.16it/s]Extractor Predicting: 26it [00:22,  1.15it/s]Extractor Predicting: 27it [00:23,  1.17it/s]Extractor Predicting: 28it [00:23,  1.17it/s]Extractor Predicting: 29it [00:24,  1.16it/s]Extractor Predicting: 30it [00:25,  1.14it/s]Extractor Predicting: 31it [00:26,  1.15it/s]Extractor Predicting: 32it [00:27,  1.15it/s]Extractor Predicting: 33it [00:28,  1.15it/s]Extractor Predicting: 34it [00:29,  1.17it/s]Extractor Predicting: 35it [00:30,  1.18it/s]Extractor Predicting: 36it [00:30,  1.19it/s]Extractor Predicting: 37it [00:31,  1.20it/s]Extractor Predicting: 38it [00:32,  1.23it/s]Extractor Predicting: 39it [00:33,  1.16it/s]Extractor Predicting: 40it [00:34,  1.19it/s]Extractor Predicting: 41it [00:35,  1.19it/s]Extractor Predicting: 42it [00:35,  1.20it/s]Extractor Predicting: 43it [00:36,  1.19it/s]Extractor Predicting: 44it [00:37,  1.16it/s]Extractor Predicting: 45it [00:38,  1.21it/s]Extractor Predicting: 46it [00:39,  1.20it/s]Extractor Predicting: 47it [00:39,  1.22it/s]Extractor Predicting: 48it [00:40,  1.18it/s]Extractor Predicting: 49it [00:41,  1.21it/s]Extractor Predicting: 50it [00:42,  1.21it/s]Extractor Predicting: 51it [00:43,  1.21it/s]Extractor Predicting: 52it [00:44,  1.20it/s]Extractor Predicting: 53it [00:45,  1.20it/s]Extractor Predicting: 54it [00:45,  1.18it/s]Extractor Predicting: 55it [00:46,  1.22it/s]Extractor Predicting: 56it [00:47,  1.23it/s]Extractor Predicting: 57it [00:48,  1.20it/s]Extractor Predicting: 58it [00:49,  1.21it/s]Extractor Predicting: 59it [00:49,  1.22it/s]Extractor Predicting: 60it [00:50,  1.20it/s]Extractor Predicting: 61it [00:51,  1.19it/s]Extractor Predicting: 62it [00:52,  1.20it/s]Extractor Predicting: 63it [00:53,  1.20it/s]Extractor Predicting: 64it [00:54,  1.21it/s]Extractor Predicting: 65it [00:54,  1.20it/s]Extractor Predicting: 66it [00:55,  1.22it/s]Extractor Predicting: 67it [00:56,  1.24it/s]Extractor Predicting: 68it [00:57,  1.20it/s]Extractor Predicting: 69it [00:58,  1.20it/s]Extractor Predicting: 70it [00:59,  1.20it/s]Extractor Predicting: 71it [00:59,  1.22it/s]Extractor Predicting: 72it [01:00,  1.23it/s]Extractor Predicting: 73it [01:01,  1.22it/s]Extractor Predicting: 74it [01:02,  1.23it/s]Extractor Predicting: 75it [01:03,  1.20it/s]Extractor Predicting: 76it [01:04,  1.21it/s]Extractor Predicting: 77it [01:04,  1.20it/s]Extractor Predicting: 78it [01:05,  1.21it/s]Extractor Predicting: 79it [01:06,  1.22it/s]Extractor Predicting: 80it [01:07,  1.22it/s]Extractor Predicting: 81it [01:08,  1.21it/s]Extractor Predicting: 82it [01:08,  1.22it/s]Extractor Predicting: 83it [01:09,  1.18it/s]Extractor Predicting: 84it [01:10,  1.20it/s]Extractor Predicting: 85it [01:11,  1.19it/s]Extractor Predicting: 86it [01:12,  1.20it/s]Extractor Predicting: 87it [01:13,  1.18it/s]Extractor Predicting: 88it [01:13,  1.22it/s]Extractor Predicting: 89it [01:14,  1.22it/s]Extractor Predicting: 90it [01:15,  1.26it/s]Extractor Predicting: 91it [01:16,  1.29it/s]Extractor Predicting: 92it [01:16,  1.30it/s]Extractor Predicting: 93it [01:17,  1.27it/s]Extractor Predicting: 94it [01:18,  1.30it/s]Extractor Predicting: 95it [01:19,  1.29it/s]Extractor Predicting: 96it [01:20,  1.29it/s]Extractor Predicting: 97it [01:20,  1.28it/s]Extractor Predicting: 98it [01:21,  1.25it/s]Extractor Predicting: 99it [01:22,  1.23it/s]Extractor Predicting: 100it [01:23,  1.23it/s]Extractor Predicting: 101it [01:24,  1.27it/s]Extractor Predicting: 102it [01:25,  1.20it/s]Extractor Predicting: 103it [01:25,  1.22it/s]Extractor Predicting: 104it [01:26,  1.22it/s]Extractor Predicting: 105it [01:27,  1.25it/s]Extractor Predicting: 106it [01:28,  1.24it/s]Extractor Predicting: 107it [01:29,  1.25it/s]Extractor Predicting: 108it [01:29,  1.25it/s]Extractor Predicting: 109it [01:30,  1.27it/s]Extractor Predicting: 110it [01:31,  1.26it/s]Extractor Predicting: 111it [01:32,  1.27it/s]Extractor Predicting: 112it [01:32,  1.31it/s]Extractor Predicting: 113it [01:33,  1.32it/s]Extractor Predicting: 114it [01:34,  1.30it/s]Extractor Predicting: 115it [01:35,  1.30it/s]Extractor Predicting: 116it [01:35,  1.29it/s]Extractor Predicting: 117it [01:36,  1.30it/s]Extractor Predicting: 118it [01:37,  1.32it/s]Extractor Predicting: 119it [01:38,  1.30it/s]Extractor Predicting: 120it [01:39,  1.27it/s]Extractor Predicting: 121it [01:39,  1.33it/s]Extractor Predicting: 122it [01:40,  1.31it/s]Extractor Predicting: 123it [01:41,  1.27it/s]Extractor Predicting: 124it [01:42,  1.30it/s]Extractor Predicting: 125it [01:42,  1.30it/s]Extractor Predicting: 126it [01:43,  1.34it/s]Extractor Predicting: 127it [01:44,  1.33it/s]Extractor Predicting: 128it [01:45,  1.33it/s]Extractor Predicting: 129it [01:45,  1.29it/s]Extractor Predicting: 130it [01:46,  1.31it/s]Extractor Predicting: 131it [01:47,  1.34it/s]Extractor Predicting: 132it [01:48,  1.32it/s]Extractor Predicting: 133it [01:48,  1.29it/s]Extractor Predicting: 134it [01:49,  1.32it/s]Extractor Predicting: 135it [01:50,  1.33it/s]Extractor Predicting: 136it [01:51,  1.34it/s]Extractor Predicting: 137it [01:51,  1.32it/s]Extractor Predicting: 138it [01:52,  1.36it/s]Extractor Predicting: 139it [01:53,  1.37it/s]Extractor Predicting: 140it [01:54,  1.38it/s]Extractor Predicting: 141it [01:54,  1.40it/s]Extractor Predicting: 142it [01:55,  1.38it/s]Extractor Predicting: 143it [01:56,  1.35it/s]Extractor Predicting: 144it [01:56,  1.65it/s]Extractor Predicting: 144it [01:56,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:19,652 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:19,673 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:19,674 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:19,674 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:19,674 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:05:21,085 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:05:21,086 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:05:22,330 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:05:23,375 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:05:23,385 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:34,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:34,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:34,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:34,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:34,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:05:37,359 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:05:37,360 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:05:39,043 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:05:39,245 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:05:39,245 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.43490304709141275,
  "recall": 0.18014916810097534,
  "score": 0.2547667342799189,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13522
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13622, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.31it/s]Extractor Predicting: 3it [00:02,  1.29it/s]Extractor Predicting: 4it [00:03,  1.31it/s]Extractor Predicting: 5it [00:03,  1.29it/s]Extractor Predicting: 6it [00:04,  1.26it/s]Extractor Predicting: 7it [00:05,  1.26it/s]Extractor Predicting: 8it [00:06,  1.29it/s]Extractor Predicting: 9it [00:06,  1.29it/s]Extractor Predicting: 10it [00:07,  1.31it/s]Extractor Predicting: 11it [00:08,  1.28it/s]Extractor Predicting: 12it [00:09,  1.29it/s]Extractor Predicting: 13it [00:10,  1.26it/s]Extractor Predicting: 14it [00:10,  1.29it/s]Extractor Predicting: 15it [00:11,  1.29it/s]Extractor Predicting: 16it [00:12,  1.25it/s]Extractor Predicting: 17it [00:13,  1.25it/s]Extractor Predicting: 18it [00:14,  1.25it/s]Extractor Predicting: 19it [00:14,  1.27it/s]Extractor Predicting: 20it [00:15,  1.25it/s]Extractor Predicting: 21it [00:16,  1.27it/s]Extractor Predicting: 22it [00:17,  1.28it/s]Extractor Predicting: 23it [00:18,  1.25it/s]Extractor Predicting: 24it [00:18,  1.24it/s]Extractor Predicting: 25it [00:19,  1.24it/s]Extractor Predicting: 26it [00:20,  1.25it/s]Extractor Predicting: 27it [00:21,  1.24it/s]Extractor Predicting: 28it [00:22,  1.26it/s]Extractor Predicting: 29it [00:22,  1.26it/s]Extractor Predicting: 30it [00:23,  1.22it/s]Extractor Predicting: 31it [00:24,  1.18it/s]Extractor Predicting: 32it [00:25,  1.19it/s]Extractor Predicting: 33it [00:26,  1.20it/s]Extractor Predicting: 34it [00:27,  1.23it/s]Extractor Predicting: 35it [00:27,  1.23it/s]Extractor Predicting: 36it [00:28,  1.27it/s]Extractor Predicting: 37it [00:29,  1.25it/s]Extractor Predicting: 38it [00:30,  1.23it/s]Extractor Predicting: 39it [00:31,  1.23it/s]Extractor Predicting: 40it [00:31,  1.22it/s]Extractor Predicting: 41it [00:32,  1.22it/s]Extractor Predicting: 42it [00:33,  1.23it/s]Extractor Predicting: 43it [00:34,  1.22it/s]Extractor Predicting: 44it [00:35,  1.21it/s]Extractor Predicting: 45it [00:36,  1.22it/s]Extractor Predicting: 46it [00:36,  1.22it/s]Extractor Predicting: 47it [00:37,  1.24it/s]Extractor Predicting: 48it [00:38,  1.24it/s]Extractor Predicting: 49it [00:39,  1.26it/s]Extractor Predicting: 50it [00:39,  1.25it/s]Extractor Predicting: 51it [00:40,  1.29it/s]Extractor Predicting: 52it [00:41,  1.29it/s]Extractor Predicting: 53it [00:42,  1.29it/s]Extractor Predicting: 54it [00:43,  1.28it/s]Extractor Predicting: 55it [00:43,  1.27it/s]Extractor Predicting: 56it [00:44,  1.25it/s]Extractor Predicting: 57it [00:45,  1.21it/s]Extractor Predicting: 58it [00:46,  1.19it/s]Extractor Predicting: 59it [00:47,  1.20it/s]Extractor Predicting: 60it [00:48,  1.22it/s]Extractor Predicting: 61it [00:48,  1.20it/s]Extractor Predicting: 62it [00:49,  1.19it/s]Extractor Predicting: 63it [00:50,  1.24it/s]Extractor Predicting: 64it [00:51,  1.22it/s]Extractor Predicting: 65it [00:52,  1.25it/s]Extractor Predicting: 66it [00:52,  1.23it/s]Extractor Predicting: 67it [00:53,  1.23it/s]Extractor Predicting: 68it [00:54,  1.22it/s]Extractor Predicting: 69it [00:55,  1.21it/s]Extractor Predicting: 70it [00:56,  1.16it/s]Extractor Predicting: 71it [00:57,  1.18it/s]Extractor Predicting: 72it [00:58,  1.18it/s]Extractor Predicting: 73it [00:58,  1.18it/s]Extractor Predicting: 74it [00:59,  1.19it/s]Extractor Predicting: 75it [01:00,  1.17it/s]Extractor Predicting: 76it [01:01,  1.19it/s]Extractor Predicting: 77it [01:02,  1.17it/s]Extractor Predicting: 78it [01:03,  1.18it/s]Extractor Predicting: 79it [01:03,  1.19it/s]Extractor Predicting: 80it [01:04,  1.18it/s]Extractor Predicting: 81it [01:05,  1.19it/s]Extractor Predicting: 82it [01:06,  1.17it/s]Extractor Predicting: 83it [01:07,  1.17it/s]Extractor Predicting: 84it [01:08,  1.20it/s]Extractor Predicting: 85it [01:08,  1.20it/s]Extractor Predicting: 86it [01:09,  1.20it/s]Extractor Predicting: 87it [01:10,  1.19it/s]Extractor Predicting: 88it [01:11,  1.23it/s]Extractor Predicting: 89it [01:12,  1.23it/s]Extractor Predicting: 90it [01:13,  1.22it/s]Extractor Predicting: 91it [01:13,  1.27it/s]Extractor Predicting: 92it [01:14,  1.25it/s]Extractor Predicting: 93it [01:15,  1.28it/s]Extractor Predicting: 94it [01:16,  1.27it/s]Extractor Predicting: 95it [01:16,  1.26it/s]Extractor Predicting: 96it [01:17,  1.26it/s]Extractor Predicting: 97it [01:18,  1.24it/s]Extractor Predicting: 98it [01:19,  1.23it/s]Extractor Predicting: 99it [01:20,  1.19it/s]Extractor Predicting: 100it [01:21,  1.20it/s]Extractor Predicting: 101it [01:21,  1.20it/s]Extractor Predicting: 102it [01:22,  1.17it/s]Extractor Predicting: 103it [01:23,  1.19it/s]Extractor Predicting: 104it [01:24,  1.20it/s]Extractor Predicting: 105it [01:25,  1.22it/s]Extractor Predicting: 106it [01:26,  1.23it/s]Extractor Predicting: 107it [01:26,  1.23it/s]Extractor Predicting: 108it [01:27,  1.23it/s]Extractor Predicting: 109it [01:28,  1.24it/s]Extractor Predicting: 110it [01:29,  1.22it/s]Extractor Predicting: 111it [01:30,  1.13it/s]Extractor Predicting: 112it [01:31,  1.15it/s]Extractor Predicting: 113it [01:31,  1.19it/s]Extractor Predicting: 114it [01:32,  1.19it/s]Extractor Predicting: 115it [01:33,  1.22it/s]Extractor Predicting: 116it [01:34,  1.18it/s]Extractor Predicting: 117it [01:35,  1.16it/s]Extractor Predicting: 118it [01:36,  1.16it/s]Extractor Predicting: 119it [01:37,  1.16it/s]Extractor Predicting: 120it [01:37,  1.16it/s]Extractor Predicting: 121it [01:38,  1.17it/s]Extractor Predicting: 122it [01:39,  1.20it/s]Extractor Predicting: 123it [01:40,  1.19it/s]Extractor Predicting: 124it [01:41,  1.17it/s]Extractor Predicting: 125it [01:42,  1.16it/s]Extractor Predicting: 126it [01:43,  1.13it/s]Extractor Predicting: 127it [01:44,  1.14it/s]Extractor Predicting: 128it [01:44,  1.15it/s]Extractor Predicting: 129it [01:45,  1.14it/s]Extractor Predicting: 130it [01:46,  1.16it/s]Extractor Predicting: 131it [01:47,  1.16it/s]Extractor Predicting: 132it [01:48,  1.15it/s]Extractor Predicting: 133it [01:49,  1.15it/s]Extractor Predicting: 134it [01:50,  1.16it/s]Extractor Predicting: 135it [01:50,  1.18it/s]Extractor Predicting: 136it [01:51,  1.21it/s]Extractor Predicting: 137it [01:52,  1.23it/s]Extractor Predicting: 138it [01:53,  1.22it/s]Extractor Predicting: 139it [01:54,  1.23it/s]Extractor Predicting: 140it [01:54,  1.22it/s]Extractor Predicting: 141it [01:55,  1.18it/s]Extractor Predicting: 142it [01:56,  1.18it/s]Extractor Predicting: 143it [01:57,  1.26it/s]Extractor Predicting: 143it [01:57,  1.22it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:45,056 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:45,058 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:45,059 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:45,059 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:45,059 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:07:45,679 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:07:45,683 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:07:46,327 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:07:47,367 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:07:47,367 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:50,351 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:50,353 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:50,353 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:50,353 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:50,353 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:07:50,989 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:07:50,993 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:07:51,552 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:07:51,724 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:07:51,724 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3201196709050112,
  "recall": 0.12507305669199298,
  "score": 0.179869720529523,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 464
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 564, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.08it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 2it [00:01,  1.39it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5833333333333334,
  "recall": 0.08974358974358974,
  "score": 0.15555555555555556,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_5_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'located on terrain feature', 'spouse'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
